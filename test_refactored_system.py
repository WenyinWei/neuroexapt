#!/usr/bin/env python3
"""
æµ‹è¯•é‡æ„åçš„æ™ºèƒ½DNMç³»ç»Ÿ

éªŒè¯ä»£ç å®¡æŸ¥é—®é¢˜çš„è§£å†³å’Œæ”¶æ•›ç›‘æ§æ”¹è¿›
"""

import torch
import torch.nn as nn
import numpy as np
import logging
from typing import Dict, Any

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def create_test_model():
    """åˆ›å»ºæµ‹è¯•æ¨¡å‹"""
    class TestResNet(nn.Module):
        def __init__(self):
            super().__init__()
            self.conv1 = nn.Conv2d(3, 64, 7, 2, 3)
            self.bn1 = nn.BatchNorm2d(64)
            self.relu = nn.ReLU(inplace=True)
            
            # ç‰¹å¾å—
            self.feature_block1 = nn.Sequential(
                nn.Conv2d(64, 128, 3, 2, 1),
                nn.BatchNorm2d(128),
                nn.ReLU(inplace=True),
                nn.Conv2d(128, 128, 3, 1, 1),
                nn.BatchNorm2d(128)
            )
            
            self.feature_block2 = nn.Sequential(
                nn.Conv2d(128, 256, 3, 2, 1),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True),
                nn.Conv2d(256, 256, 3, 1, 1),
                nn.BatchNorm2d(256)
            )
            
            # åˆ†ç±»å™¨
            self.classifier = nn.Sequential(
                nn.AdaptiveAvgPool2d((1, 1)),
                nn.Flatten(),
                nn.Linear(256, 128),
                nn.ReLU(inplace=True),
                nn.Dropout(0.5),
                nn.Linear(128, 10)
            )
            
        def forward(self, x):
            x = self.relu(self.bn1(self.conv1(x)))
            x = self.feature_block1(x)
            x = self.feature_block2(x)
            x = self.classifier(x)
            return x
    
    return TestResNet()

def simulate_network_state(model, batch_size=32):
    """æ¨¡æ‹Ÿç½‘ç»œçŠ¶æ€æ•è·"""
    
    model.eval()
    activations = {}
    gradients = {}
    
    def activation_hook(name):
        def hook(module, input, output):
            if isinstance(output, torch.Tensor):
                activations[name] = output.detach().clone()
        return hook
    
    def gradient_hook(name):
        def hook(module, grad_input, grad_output):
            if grad_output[0] is not None:
                gradients[name] = grad_output[0].detach().clone()
        return hook
    
    # æ³¨å†Œé’©å­
    hooks = []
    for name, module in model.named_modules():
        if isinstance(module, (nn.Conv2d, nn.Linear, nn.BatchNorm2d)):
            hooks.append(module.register_forward_hook(activation_hook(name)))
            hooks.append(module.register_backward_hook(gradient_hook(name)))
    
    # æ¨¡æ‹Ÿå‰å‘å’Œåå‘ä¼ æ’­
    x = torch.randn(batch_size, 3, 224, 224)
    y = torch.randint(0, 10, (batch_size,))
    
    # å‰å‘ä¼ æ’­
    model.train()
    output = model(x)
    
    # åå‘ä¼ æ’­
    loss = nn.CrossEntropyLoss()(output, y)
    loss.backward()
    
    # æ¸…ç†é’©å­
    for hook in hooks:
        hook.remove()
    
    # æ¨¡æ‹Ÿæ€§èƒ½å†å²
    performance_history = [
        0.65, 0.68, 0.72, 0.74, 0.76, 0.77, 0.77, 0.78, 0.78, 0.78,  # æ€§èƒ½åœæ»
        0.78, 0.77, 0.78, 0.78, 0.79, 0.79, 0.79, 0.79, 0.79, 0.80   # è½»å¾®æ”¹è¿›
    ]
    
    return {
        'activations': activations,
        'gradients': gradients,
        'performance_history': performance_history,
        'current_epoch': 20,
        'current_loss': float(loss.item())
    }

def test_refactored_bayesian_engine():
    """æµ‹è¯•é‡æ„åçš„è´å¶æ–¯å¼•æ“"""
    
    logger.info("ğŸ§ª æµ‹è¯•é‡æ„åçš„è´å¶æ–¯å½¢æ€å‘ç”Ÿå¼•æ“")
    
    # åˆ›å»ºæ¨¡å‹å’Œä¸Šä¸‹æ–‡
    model = create_test_model()
    context = simulate_network_state(model)
    
    logger.info(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    logger.info(f"ğŸ“Š æ•è·æ¿€æ´»: {len(context['activations'])}ä¸ª")
    logger.info(f"ğŸ“Š æ•è·æ¢¯åº¦: {len(context['gradients'])}ä¸ª")
    
    # æµ‹è¯•é‡æ„åçš„è´å¶æ–¯å¼•æ“
    try:
        from neuroexapt.core.refactored_bayesian_morphogenesis import RefactoredBayesianMorphogenesisEngine
        
        # åˆ›å»ºå¼•æ“å¹¶è®¾ç½®ç§¯ææ¨¡å¼
        bayesian_engine = RefactoredBayesianMorphogenesisEngine()
        bayesian_engine.set_aggressive_mode()
        
        logger.info("âœ… é‡æ„åçš„è´å¶æ–¯å¼•æ“åˆ›å»ºæˆåŠŸ")
        
        # æ‰§è¡Œåˆ†æ
        result = bayesian_engine.bayesian_morphogenesis_analysis(model, context)
        
        # æ£€æŸ¥ç»“æœ
        optimal_decisions = result.get('optimal_decisions', [])
        execution_plan = result.get('execution_plan', {})
        bayesian_analysis = result.get('bayesian_analysis', {})
        
        logger.info(f"ğŸ¯ å‘ç°æœ€ä¼˜å†³ç­–: {len(optimal_decisions)}ä¸ª")
        logger.info(f"ğŸ“‹ æ‰§è¡Œè®¡åˆ’: {execution_plan.get('execute', False)}")
        logger.info(f"ğŸ² å†³ç­–ç½®ä¿¡åº¦: {bayesian_analysis.get('decision_confidence', 0):.3f}")
        
        # è¯¦ç»†è¾“å‡ºå†³ç­–ä¿¡æ¯
        for i, decision in enumerate(optimal_decisions):
            logger.info(f"  å†³ç­–{i+1}: {decision.get('layer_name', '')} -> {decision.get('mutation_type', '')}")
            logger.info(f"    æˆåŠŸæ¦‚ç‡: {decision.get('success_probability', 0):.3f}")
            logger.info(f"    æœŸæœ›æ”¹è¿›: {decision.get('expected_improvement', 0):.4f}")
            logger.info(f"    æœŸæœ›æ•ˆç”¨: {decision.get('expected_utility', 0):.4f}")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ é‡æ„åè´å¶æ–¯å¼•æ“æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def test_enhanced_convergence_monitor():
    """æµ‹è¯•å¢å¼ºæ”¶æ•›ç›‘æ§å™¨"""
    
    logger.info("ğŸ§ª æµ‹è¯•å¢å¼ºæ”¶æ•›ç›‘æ§å™¨")
    
    try:
        from neuroexapt.core.enhanced_convergence_monitor import EnhancedConvergenceMonitor
        
        # åˆ›å»ºç§¯ææ¨¡å¼çš„ç›‘æ§å™¨
        monitor = EnhancedConvergenceMonitor(mode='aggressive')
        
        logger.info(f"âœ… å¢å¼ºæ”¶æ•›ç›‘æ§å™¨åˆ›å»ºæˆåŠŸ (æ¨¡å¼: {monitor.mode})")
        
        # æ¨¡æ‹Ÿæ€§èƒ½å†å²æµ‹è¯•
        test_scenarios = [
            {
                'name': 'æ—©æœŸè®­ç»ƒ',
                'epochs': [1, 2, 3, 4, 5],
                'performances': [0.60, 0.65, 0.70, 0.72, 0.74],
                'losses': [1.5, 1.3, 1.1, 1.0, 0.9]
            },
            {
                'name': 'æ€§èƒ½åœæ»',
                'epochs': [6, 7, 8, 9, 10],
                'performances': [0.74, 0.74, 0.75, 0.74, 0.74],
                'losses': [0.9, 0.9, 0.88, 0.89, 0.9]
            },
            {
                'name': 'æ€§èƒ½ä¸‹é™',
                'epochs': [11, 12, 13, 14, 15],
                'performances': [0.74, 0.72, 0.70, 0.68, 0.66],
                'losses': [0.9, 1.0, 1.1, 1.2, 1.3]
            }
        ]
        
        for scenario in test_scenarios:
            logger.info(f"\nğŸ“Š æµ‹è¯•åœºæ™¯: {scenario['name']}")
            
            for epoch, perf, loss in zip(scenario['epochs'], scenario['performances'], scenario['losses']):
                result = monitor.should_allow_morphogenesis(
                    current_epoch=epoch,
                    current_performance=perf,
                    current_loss=loss,
                    gradient_norm=np.random.uniform(0.5, 2.0)
                )
                
                logger.info(f"  Epoch {epoch}: å‡†ç¡®ç‡={perf:.2f}, æŸå¤±={loss:.2f}")
                logger.info(f"    å…è®¸å˜å¼‚: {result['allow']}")
                logger.info(f"    åŸå› : {result['reason']}")
                logger.info(f"    ç½®ä¿¡åº¦: {result['confidence']:.2f}")
                
                if result['allow']:
                    logger.info(f"    âœ… å˜å¼‚è¢«å…è®¸: {result.get('suggestion', '')}")
                    break
                else:
                    logger.info(f"    âŒ å˜å¼‚è¢«é˜»æ­¢: {result.get('suggestion', '')}")
        
        return monitor
        
    except Exception as e:
        logger.error(f"âŒ å¢å¼ºæ”¶æ•›ç›‘æ§å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def test_integrated_system():
    """æµ‹è¯•å®Œæ•´çš„é›†æˆç³»ç»Ÿ"""
    
    logger.info("ğŸ§ª æµ‹è¯•å®Œæ•´çš„é‡æ„é›†æˆç³»ç»Ÿ")
    
    try:
        from neuroexapt.core.intelligent_dnm_integration import IntelligentDNMCore
        
        # åˆ›å»ºé›†æˆç³»ç»Ÿ
        dnm_core = IntelligentDNMCore()
        
        logger.info("âœ… æ™ºèƒ½DNMé›†æˆç³»ç»Ÿåˆ›å»ºæˆåŠŸ")
        logger.info(f"ğŸš€ å½“å‰æ¨¡å¼: ç§¯ææ¨¡å¼")
        
        # è·å–ç³»ç»ŸçŠ¶æ€
        status = dnm_core.get_system_status()
        logger.info(f"ğŸ“Š ç³»ç»ŸçŠ¶æ€: {status.get('config', {}).get('aggressive_mutation_mode', False)}")
        
        # åˆ›å»ºæ¨¡å‹å’Œä¸Šä¸‹æ–‡
        model = create_test_model()
        context = simulate_network_state(model)
        
        # æ‰§è¡Œå½¢æ€å‘ç”Ÿåˆ†æ
        result = dnm_core.enhanced_morphogenesis_execution(model, context)
        
        # åˆ†æç»“æœ
        model_modified = result.get('model_modified', False)
        morphogenesis_events = result.get('morphogenesis_events', [])
        intelligent_analysis = result.get('intelligent_analysis', {})
        
        logger.info(f"ğŸ”§ æ¨¡å‹æ˜¯å¦ä¿®æ”¹: {model_modified}")
        logger.info(f"ğŸ§¬ å˜å¼‚äº‹ä»¶æ•°é‡: {len(morphogenesis_events)}")
        logger.info(f"ğŸ§  æ™ºèƒ½åˆ†æç»“æœ:")
        logger.info(f"  å€™é€‰ç‚¹å‘ç°: {intelligent_analysis.get('candidates_discovered', 0)}ä¸ª")
        logger.info(f"  ç­–ç•¥è¯„ä¼°: {intelligent_analysis.get('strategies_evaluated', 0)}ä¸ª")
        logger.info(f"  æœ€ç»ˆå†³ç­–: {intelligent_analysis.get('final_decisions', 0)}ä¸ª")
        logger.info(f"  æ‰§è¡Œç½®ä¿¡åº¦: {intelligent_analysis.get('execution_confidence', 0):.3f}")
        logger.info(f"  æ€§èƒ½æ€åŠ¿: {intelligent_analysis.get('performance_trend', 'unknown')}")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ é›†æˆç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def test_configuration_system():
    """æµ‹è¯•é…ç½®ç³»ç»Ÿ"""
    
    logger.info("ğŸ§ª æµ‹è¯•é…ç½®ç³»ç»Ÿ")
    
    try:
        from neuroexapt.core.bayesian_prediction.bayesian_config import BayesianConfigManager
        
        # åˆ›å»ºé…ç½®ç®¡ç†å™¨
        config_manager = BayesianConfigManager()
        config = config_manager.get_config()
        
        logger.info("âœ… é…ç½®ç®¡ç†å™¨åˆ›å»ºæˆåŠŸ")
        logger.info(f"ğŸ“Š é»˜è®¤ç½®ä¿¡åº¦é˜ˆå€¼: {config.dynamic_thresholds['confidence_threshold']}")
        logger.info(f"ğŸ“Š æœ€å°æœŸæœ›æ”¹è¿›: {config.dynamic_thresholds['min_expected_improvement']}")
        logger.info(f"ğŸ“Š è’™ç‰¹å¡ç½—æ ·æœ¬æ•°: {config.mc_samples}")
        
        # æµ‹è¯•ç§¯ææ¨¡å¼
        config_manager.reset_to_aggressive_mode()
        aggressive_config = config_manager.get_config()
        
        logger.info("ğŸš€ åˆ‡æ¢åˆ°ç§¯ææ¨¡å¼:")
        logger.info(f"  ç½®ä¿¡åº¦é˜ˆå€¼: {aggressive_config.dynamic_thresholds['confidence_threshold']}")
        logger.info(f"  æœŸæœ›æ”¹è¿›é˜ˆå€¼: {aggressive_config.dynamic_thresholds['min_expected_improvement']}")
        logger.info(f"  æ¢ç´¢å¥–åŠ±: {aggressive_config.utility_params['exploration_bonus']}")
        
        # æµ‹è¯•ä¿å®ˆæ¨¡å¼
        config_manager.reset_to_conservative_mode()
        conservative_config = config_manager.get_config()
        
        logger.info("ğŸ›¡ï¸ åˆ‡æ¢åˆ°ä¿å®ˆæ¨¡å¼:")
        logger.info(f"  ç½®ä¿¡åº¦é˜ˆå€¼: {conservative_config.dynamic_thresholds['confidence_threshold']}")
        logger.info(f"  æœŸæœ›æ”¹è¿›é˜ˆå€¼: {conservative_config.dynamic_thresholds['min_expected_improvement']}")
        logger.info(f"  é£é™©åŒæ¶: {conservative_config.utility_params['risk_aversion']}")
        
        return config_manager
        
    except Exception as e:
        logger.error(f"âŒ é…ç½®ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    
    logger.info("ğŸš€ å¼€å§‹é‡æ„ç³»ç»Ÿç»¼åˆæµ‹è¯•")
    logger.info("="*60)
    
    # æµ‹è¯•1: é…ç½®ç³»ç»Ÿ
    logger.info("\n" + "="*60)
    logger.info("ğŸ“ æµ‹è¯•1: é…ç½®ç³»ç»Ÿ")
    config_manager = test_configuration_system()
    
    # æµ‹è¯•2: é‡æ„åçš„è´å¶æ–¯å¼•æ“
    logger.info("\n" + "="*60)
    logger.info("ğŸ§  æµ‹è¯•2: é‡æ„åçš„è´å¶æ–¯å¼•æ“")
    bayesian_result = test_refactored_bayesian_engine()
    
    # æµ‹è¯•3: å¢å¼ºæ”¶æ•›ç›‘æ§å™¨
    logger.info("\n" + "="*60)
    logger.info("â±ï¸ æµ‹è¯•3: å¢å¼ºæ”¶æ•›ç›‘æ§å™¨")
    enhanced_monitor = test_enhanced_convergence_monitor()
    
    # æµ‹è¯•4: å®Œæ•´é›†æˆç³»ç»Ÿ
    logger.info("\n" + "="*60)
    logger.info("ğŸ”§ æµ‹è¯•4: å®Œæ•´é›†æˆç³»ç»Ÿ")
    integrated_result = test_integrated_system()
    
    # æ€»ç»“
    logger.info("\n" + "="*60)
    logger.info("ğŸ“‹ æµ‹è¯•æ€»ç»“")
    logger.info("="*60)
    
    success_count = 0
    total_tests = 4
    
    if config_manager is not None:
        logger.info("âœ… é…ç½®ç³»ç»Ÿ: é€šè¿‡")
        success_count += 1
    else:
        logger.info("âŒ é…ç½®ç³»ç»Ÿ: å¤±è´¥")
    
    if bayesian_result is not None and bayesian_result.get('optimal_decisions'):
        logger.info("âœ… é‡æ„è´å¶æ–¯å¼•æ“: é€šè¿‡")
        success_count += 1
    else:
        logger.info("âŒ é‡æ„è´å¶æ–¯å¼•æ“: å¤±è´¥")
    
    if enhanced_monitor is not None:
        logger.info("âœ… å¢å¼ºæ”¶æ•›ç›‘æ§å™¨: é€šè¿‡")
        success_count += 1
    else:
        logger.info("âŒ å¢å¼ºæ”¶æ•›ç›‘æ§å™¨: å¤±è´¥")
    
    if integrated_result is not None:
        logger.info("âœ… å®Œæ•´é›†æˆç³»ç»Ÿ: é€šè¿‡")
        success_count += 1
    else:
        logger.info("âŒ å®Œæ•´é›†æˆç³»ç»Ÿ: å¤±è´¥")
    
    logger.info(f"\nğŸ¯ æ€»ä½“æµ‹è¯•ç»“æœ: {success_count}/{total_tests} é€šè¿‡")
    
    if success_count == total_tests:
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼é‡æ„æˆåŠŸè§£å†³äº†ä»£ç å®¡æŸ¥ä¸­çš„é—®é¢˜")
    else:
        logger.info("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
    
    return success_count == total_tests

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)