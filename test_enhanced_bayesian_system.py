#!/usr/bin/env python3
"""
æµ‹è¯•å¢å¼ºè´å¶æ–¯å½¢æ€å‘ç”Ÿç³»ç»Ÿ

éªŒè¯æ–°çš„æ™ºèƒ½æ¶æ„å˜å¼‚å¼•æ“æ˜¯å¦èƒ½å¤Ÿï¼š
1. æˆåŠŸæ£€æµ‹åˆ°å˜å¼‚å€™é€‰ç‚¹
2. è¿›è¡Œè´å¶æ–¯æ¨æ–­å¹¶ç”Ÿæˆå†³ç­–
3. æä¾›åˆç†çš„æ€§èƒ½æ”¹è¿›é¢„æµ‹
4. åœ¨çº¿å­¦ä¹ å’Œé€‚åº”
"""

import torch
import torch.nn as nn
import numpy as np
import logging
from typing import Dict, Any
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '.'))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def create_test_model():
    """åˆ›å»ºä¸€ä¸ªæµ‹è¯•ç”¨çš„ç®€å•ResNetæ¨¡å‹"""
    
    class BasicBlock(nn.Module):
        def __init__(self, in_channels, out_channels, stride=1):
            super().__init__()
            self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)
            self.bn1 = nn.BatchNorm2d(out_channels)
            self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)
            self.bn2 = nn.BatchNorm2d(out_channels)
            
            self.shortcut = nn.Sequential()
            if stride != 1 or in_channels != out_channels:
                self.shortcut = nn.Sequential(
                    nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),
                    nn.BatchNorm2d(out_channels)
                )
        
        def forward(self, x):
            out = torch.relu(self.bn1(self.conv1(x)))
            out = self.bn2(self.conv2(out))
            out += self.shortcut(x)
            out = torch.relu(out)
            return out
    
    class TestResNet(nn.Module):
        def __init__(self):
            super().__init__()
            self.conv1 = nn.Conv2d(3, 64, 7, 2, 3, bias=False)
            self.bn1 = nn.BatchNorm2d(64)
            self.maxpool = nn.MaxPool2d(3, 2, 1)
            
            # ç‰¹å¾å—
            self.feature_block1 = nn.Sequential(
                BasicBlock(64, 128, 2),
                BasicBlock(128, 128, 1)
            )
            self.feature_block2 = nn.Sequential(
                BasicBlock(128, 256, 2),
                BasicBlock(256, 256, 1)
            )
            self.feature_block3 = nn.Sequential(
                BasicBlock(256, 512, 2),
                BasicBlock(512, 512, 1)
            )
            
            # åˆ†ç±»å™¨
            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
            self.classifier = nn.Sequential(
                nn.Flatten(),
                nn.Linear(512, 256),
                nn.ReLU(),
                nn.Dropout(0.5),
                nn.Linear(256, 128),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(128, 10)
            )
        
        def forward(self, x):
            x = self.maxpool(torch.relu(self.bn1(self.conv1(x))))
            x = self.feature_block1(x)
            x = self.feature_block2(x)
            x = self.feature_block3(x)
            x = self.avgpool(x)
            x = self.classifier(x)
            return x
    
    return TestResNet()

def simulate_network_state_capture(model: nn.Module, batch_size: int = 32):
    """æ¨¡æ‹Ÿç½‘ç»œçŠ¶æ€æ•è·ï¼ˆæ¿€æ´»å€¼å’Œæ¢¯åº¦ï¼‰"""
    
    model.train()
    
    # åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥
    x = torch.randn(batch_size, 3, 32, 32)
    y = torch.randint(0, 10, (batch_size,))
    
    # æ¿€æ´»å€¼å­˜å‚¨
    activations = {}
    gradients = {}
    
    # æ³¨å†Œå‰å‘é’©å­
    def save_activation(name):
        def hook(module, input, output):
            activations[name] = output.detach()
        return hook
    
    # æ³¨å†Œåå‘é’©å­
    def save_gradient(name):
        def hook(module, grad_input, grad_output):
            if grad_output[0] is not None:
                gradients[name] = grad_output[0].detach()
        return hook
    
    handles = []
    for name, module in model.named_modules():
        if isinstance(module, (nn.Conv2d, nn.Linear)):
            handles.append(module.register_forward_hook(save_activation(name)))
            handles.append(module.register_backward_hook(save_gradient(name)))
    
    # å‰å‘ä¼ æ’­
    output = model(x)
    loss = nn.CrossEntropyLoss()(output, y)
    
    # åå‘ä¼ æ’­
    loss.backward()
    
    # æ¸…ç†é’©å­
    for handle in handles:
        handle.remove()
    
    return activations, gradients, loss.item()

def create_test_context(model: nn.Module, epoch: int = 15):
    """åˆ›å»ºæµ‹è¯•ä¸Šä¸‹æ–‡"""
    
    # æ¨¡æ‹Ÿæ€§èƒ½å†å²ï¼ˆæ˜¾ç¤ºåœæ»ï¼‰
    performance_history = [
        0.72, 0.74, 0.76, 0.78, 0.79, 0.80, 0.805, 0.807, 0.808, 0.808,
        0.8081, 0.8082, 0.8079, 0.8080, 0.8079  # æœ€è¿‘å‡ ä¸ªepochåœæ»
    ]
    
    # æ•è·ç½‘ç»œçŠ¶æ€
    activations, gradients, train_loss = simulate_network_state_capture(model)
    
    context = {
        'epoch': epoch,
        'performance_history': performance_history,
        'train_loss': train_loss,
        'learning_rate': 0.1,
        'activations': activations,
        'gradients': gradients,
        'targets': torch.randint(0, 10, (32,))  # æ¨¡æ‹Ÿç›®æ ‡æ ‡ç­¾
    }
    
    return context

def test_bayesian_morphogenesis_engine():
    """æµ‹è¯•å¢å¼ºè´å¶æ–¯å½¢æ€å‘ç”Ÿå¼•æ“"""
    
    logger.info("ğŸ§ª å¼€å§‹æµ‹è¯•å¢å¼ºè´å¶æ–¯å½¢æ€å‘ç”Ÿç³»ç»Ÿ")
    
    try:
        # å¯¼å…¥æ–°çš„è´å¶æ–¯å¼•æ“
        from neuroexapt.core.enhanced_bayesian_morphogenesis import BayesianMorphogenesisEngine
        
        # åˆ›å»ºæµ‹è¯•æ¨¡å‹å’Œå¼•æ“
        model = create_test_model()
        bayesian_engine = BayesianMorphogenesisEngine()
        
        logger.info(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ: {sum(p.numel() for p in model.parameters()):,} å‚æ•°")
        
        # å¯ç”¨ç§¯ææ¨¡å¼
        bayesian_engine.dynamic_thresholds['min_expected_improvement'] = 0.001
        bayesian_engine.dynamic_thresholds['confidence_threshold'] = 0.2
        
        # åˆ›å»ºæµ‹è¯•ä¸Šä¸‹æ–‡
        context = create_test_context(model)
        logger.info(f"âœ… æµ‹è¯•ä¸Šä¸‹æ–‡åˆ›å»ºæˆåŠŸ: {len(context['activations'])}ä¸ªæ¿€æ´», {len(context['gradients'])}ä¸ªæ¢¯åº¦")
        
        # æ‰§è¡Œè´å¶æ–¯åˆ†æ
        logger.info("ğŸš€ å¼€å§‹è´å¶æ–¯å½¢æ€å‘ç”Ÿåˆ†æ...")
        result = bayesian_engine.bayesian_morphogenesis_analysis(model, context)
        
        # åˆ†æç»“æœ
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š è´å¶æ–¯åˆ†æç»“æœ:")
        logger.info("="*60)
        
        bayesian_analysis = result.get('bayesian_analysis', {})
        optimal_decisions = result.get('optimal_decisions', [])
        execution_plan = result.get('execution_plan', {})
        bayesian_insights = result.get('bayesian_insights', {})
        
        logger.info(f"ğŸ¯ å€™é€‰ç‚¹å‘ç°: {bayesian_analysis.get('candidates_found', 0)}ä¸ª")
        logger.info(f"â­ æœ€ä¼˜å†³ç­–: {len(optimal_decisions)}ä¸ª")
        logger.info(f"ğŸ² å†³ç­–ç½®ä¿¡åº¦: {bayesian_analysis.get('decision_confidence', 0.0):.3f}")
        logger.info(f"ğŸš€ æ˜¯å¦æ‰§è¡Œ: {'æ˜¯' if execution_plan.get('execute', False) else 'å¦'}")
        
        if optimal_decisions:
            logger.info(f"\nğŸ“‹ æœ€ä¼˜å†³ç­–è¯¦æƒ…:")
            for i, decision in enumerate(optimal_decisions[:3]):  # æ˜¾ç¤ºå‰3ä¸ª
                logger.info(f"  {i+1}. ç›®æ ‡å±‚: {decision.get('layer_name', 'N/A')}")
                logger.info(f"     å˜å¼‚ç±»å‹: {decision.get('mutation_type', 'N/A')}")
                logger.info(f"     æˆåŠŸæ¦‚ç‡: {decision.get('success_probability', 0.0):.3f}")
                logger.info(f"     æœŸæœ›æ”¹è¿›: {decision.get('expected_improvement', 0.0):.4f}")
                logger.info(f"     æœŸæœ›æ•ˆç”¨: {decision.get('expected_utility', 0.0):.4f}")
                logger.info(f"     å†³ç­–ç†ç”±: {decision.get('rationale', 'N/A')}")
                logger.info("")
        
        # æµ‹è¯•è´å¶æ–¯æ´å¯Ÿ
        if bayesian_insights:
            logger.info(f"ğŸ’¡ è´å¶æ–¯æ´å¯Ÿ:")
            logger.info(f"   æœ€æœ‰å‰æ™¯çš„å˜å¼‚: {bayesian_insights.get('most_promising_mutation', {}).get('mutation_type', 'N/A')}")
            logger.info(f"   æœŸæœ›æ€§èƒ½æå‡: {bayesian_insights.get('expected_performance_gain', 0.0):.4f}")
            logger.info(f"   é£é™©è¯„ä¼°: {bayesian_insights.get('risk_assessment', {}).get('overall_risk', 0.0):.3f}")
        
        # æµ‹è¯•åœ¨çº¿å­¦ä¹ 
        logger.info(f"\nğŸ§  æµ‹è¯•åœ¨çº¿å­¦ä¹ åŠŸèƒ½...")
        if optimal_decisions:
            first_decision = optimal_decisions[0]
            
            # æ¨¡æ‹Ÿå˜å¼‚æˆåŠŸ
            bayesian_engine.update_mutation_outcome(
                mutation_type=first_decision['mutation_type'],
                layer_name=first_decision['layer_name'],
                success=True,
                performance_change=0.015,  # 1.5%çš„æ€§èƒ½æ”¹è¿›
                context=context
            )
            
            # æ¨¡æ‹Ÿå¦ä¸€ä¸ªå˜å¼‚å¤±è´¥
            bayesian_engine.update_mutation_outcome(
                mutation_type='width_expansion',
                layer_name='feature_block1.0.conv1',
                success=False,
                performance_change=-0.005,
                context=context
            )
            
            logger.info("âœ… åœ¨çº¿å­¦ä¹ æ›´æ–°å®Œæˆ")
        
        # å†æ¬¡åˆ†æä»¥éªŒè¯å­¦ä¹ æ•ˆæœ
        logger.info(f"\nğŸ”„ éªŒè¯å­¦ä¹ æ•ˆæœ...")
        context['epoch'] = 16  # æ–°çš„epoch
        context['performance_history'].append(0.823)  # æ¨¡æ‹Ÿæ€§èƒ½æ”¹è¿›
        
        result2 = bayesian_engine.bayesian_morphogenesis_analysis(model, context)
        optimal_decisions2 = result2.get('optimal_decisions', [])
        
        logger.info(f"å­¦ä¹ åçš„å†³ç­–æ•°é‡: {len(optimal_decisions2)}")
        if optimal_decisions2:
            logger.info(f"æ–°çš„æœ€ä¼˜å†³ç­–: {optimal_decisions2[0]['mutation_type']} @ {optimal_decisions2[0]['layer_name']}")
        
        # æµ‹è¯•ç»“æœéªŒè¯
        success_metrics = {
            'candidates_found': bayesian_analysis.get('candidates_found', 0) > 0,
            'decisions_generated': len(optimal_decisions) > 0,
            'execution_plan_valid': execution_plan.get('execute', False),
            'confidence_reasonable': bayesian_analysis.get('decision_confidence', 0.0) > 0.1,
            'learning_functional': len(bayesian_engine.mutation_history) > 0
        }
        
        logger.info(f"\nâœ… æµ‹è¯•ç»“æœéªŒè¯:")
        for metric, passed in success_metrics.items():
            status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
            logger.info(f"   {metric}: {status}")
        
        overall_success = all(success_metrics.values())
        if overall_success:
            logger.info(f"\nğŸ‰ å¢å¼ºè´å¶æ–¯å½¢æ€å‘ç”Ÿç³»ç»Ÿæµ‹è¯•æˆåŠŸï¼")
            logger.info(f"ç³»ç»Ÿç°åœ¨èƒ½å¤Ÿæ›´æ™ºèƒ½åœ°æ£€æµ‹å˜å¼‚å€™é€‰ç‚¹å¹¶åšå‡ºå†³ç­–ã€‚")
        else:
            logger.warning(f"\nâš ï¸ éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")
            
        return overall_success, result
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False, None

def test_intelligent_dnm_integration():
    """æµ‹è¯•æ™ºèƒ½DNMé›†æˆç³»ç»Ÿ"""
    
    logger.info("\n" + "="*60)
    logger.info("ğŸ§ª æµ‹è¯•æ™ºèƒ½DNMé›†æˆç³»ç»Ÿ")
    logger.info("="*60)
    
    try:
        from neuroexapt.core.intelligent_dnm_integration import IntelligentDNMCore
        
        # åˆ›å»ºæµ‹è¯•æ¨¡å‹å’ŒDNMæ ¸å¿ƒ
        model = create_test_model()
        dnm_core = IntelligentDNMCore()
        
        # å¯ç”¨ç§¯ææ¨¡å¼
        dnm_core.enable_aggressive_bayesian_mode()
        
        # åˆ›å»ºæµ‹è¯•ä¸Šä¸‹æ–‡
        context = create_test_context(model)
        
        # æ‰§è¡Œå¢å¼ºå½¢æ€å‘ç”Ÿåˆ†æ
        logger.info("ğŸš€ æ‰§è¡Œå¢å¼ºå½¢æ€å‘ç”Ÿåˆ†æ...")
        result = dnm_core.enhanced_morphogenesis_execution(model, context)
        
        # åˆ†æé›†æˆç»“æœ
        logger.info(f"\nğŸ“Š é›†æˆç³»ç»Ÿç»“æœ:")
        logger.info(f"æ¨¡å‹æ˜¯å¦ä¿®æ”¹: {result.get('model_modified', False)}")
        logger.info(f"å˜å¼‚äº‹ä»¶æ•°: {len(result.get('morphogenesis_events', []))}")
        logger.info(f"æ™ºèƒ½åˆ†æè¯¦æƒ…: {result.get('intelligent_analysis', {})}")
        
        # è·å–è´å¶æ–¯æ´å¯Ÿ
        insights = dnm_core.get_bayesian_insights()
        logger.info(f"\nğŸ’¡ è´å¶æ–¯å¼•æ“çŠ¶æ€:")
        logger.info(f"å˜å¼‚å†å²é•¿åº¦: {insights.get('mutation_history_length', 0)}")
        logger.info(f"æ€§èƒ½å†å²é•¿åº¦: {insights.get('performance_history_length', 0)}")
        logger.info(f"å½“å‰é˜ˆå€¼: {insights.get('dynamic_thresholds', {})}")
        
        integration_success = (
            len(result.get('morphogenesis_events', [])) > 0 or
            result.get('intelligent_analysis', {}).get('candidates_discovered', 0) > 0
        )
        
        if integration_success:
            logger.info(f"\nâœ… æ™ºèƒ½DNMé›†æˆæµ‹è¯•æˆåŠŸï¼")
        else:
            logger.info(f"\nâš ï¸ æ™ºèƒ½DNMé›†æˆæœªäº§ç”Ÿå˜å¼‚äº‹ä»¶")
            
        return integration_success, result
        
    except Exception as e:
        logger.error(f"âŒ é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False, None

if __name__ == "__main__":
    logger.info("ğŸš€ å¼€å§‹æµ‹è¯•å¢å¼ºè´å¶æ–¯å½¢æ€å‘ç”Ÿç³»ç»Ÿ")
    
    # æµ‹è¯•è´å¶æ–¯å¼•æ“
    bayesian_success, bayesian_result = test_bayesian_morphogenesis_engine()
    
    # æµ‹è¯•é›†æˆç³»ç»Ÿ
    integration_success, integration_result = test_intelligent_dnm_integration()
    
    # æœ€ç»ˆæŠ¥å‘Š
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š æœ€ç»ˆæµ‹è¯•æŠ¥å‘Š")
    logger.info("="*60)
    logger.info(f"è´å¶æ–¯å¼•æ“æµ‹è¯•: {'âœ… æˆåŠŸ' if bayesian_success else 'âŒ å¤±è´¥'}")
    logger.info(f"DNMé›†æˆæµ‹è¯•: {'âœ… æˆåŠŸ' if integration_success else 'âŒ å¤±è´¥'}")
    
    if bayesian_success and integration_success:
        logger.info(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¢å¼ºè´å¶æ–¯å½¢æ€å‘ç”Ÿç³»ç»Ÿå·²æˆåŠŸéƒ¨ç½²ã€‚")
        logger.info(f"ç³»ç»Ÿç°åœ¨å…·å¤‡æ›´å¼ºçš„æ™ºèƒ½å†³ç­–èƒ½åŠ›ï¼Œèƒ½å¤Ÿï¼š")
        logger.info(f"  1. æ›´ç§¯æåœ°æ£€æµ‹å˜å¼‚å€™é€‰ç‚¹")
        logger.info(f"  2. åŸºäºè´å¶æ–¯æ¨æ–­è¿›è¡Œæ™ºèƒ½å†³ç­–")
        logger.info(f"  3. é¢„æµ‹å˜å¼‚çš„æˆåŠŸæ¦‚ç‡å’Œæ€§èƒ½æ”¹è¿›")
        logger.info(f"  4. é€šè¿‡åœ¨çº¿å­¦ä¹ ä¸æ–­ä¼˜åŒ–å†³ç­–")
    else:
        logger.warning(f"\nâš ï¸ éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ç³»ç»Ÿã€‚")
    
    logger.info(f"\næµ‹è¯•å®Œæˆã€‚")