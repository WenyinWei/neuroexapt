#!/usr/bin/env python3
"""
ç®€åŒ–çš„è´å¶æ–¯å½¢æ€å‘ç”Ÿç³»ç»Ÿæµ‹è¯•

åœ¨æ²¡æœ‰PyTorchçš„ç¯å¢ƒä¸­æµ‹è¯•è´å¶æ–¯æ¨æ–­é€»è¾‘
"""

import numpy as np
import logging
from typing import Dict, Any, List, Tuple
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '.'))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MockModule:
    """æ¨¡æ‹Ÿçš„ç¥ç»ç½‘ç»œæ¨¡å—"""
    def __init__(self, name: str, layer_type: str, in_features: int, out_features: int):
        self.name = name
        self.layer_type = layer_type
        self.in_features = in_features
        self.out_features = out_features
    
    def __str__(self):
        return f"{self.layer_type}({self.in_features}, {self.out_features})"

class MockModel:
    """æ¨¡æ‹Ÿçš„ç¥ç»ç½‘ç»œæ¨¡å‹"""
    def __init__(self):
        self.layers = {
            'conv1': MockModule('conv1', 'Conv2d', 3, 64),
            'feature_block1.0.conv1': MockModule('feature_block1.0.conv1', 'Conv2d', 64, 128),
            'feature_block1.0.conv2': MockModule('feature_block1.0.conv2', 'Conv2d', 128, 128),
            'feature_block2.0.conv1': MockModule('feature_block2.0.conv1', 'Conv2d', 128, 256),
            'feature_block2.0.conv2': MockModule('feature_block2.0.conv2', 'Conv2d', 256, 256),
            'classifier.1': MockModule('classifier.1', 'Linear', 512, 256),
            'classifier.5': MockModule('classifier.5', 'Linear', 256, 128),
            'classifier.9': MockModule('classifier.9', 'Linear', 128, 10)
        }
    
    def named_modules(self):
        return [(name, module) for name, module in self.layers.items()]
    
    def parameters(self):
        # æ¨¡æ‹Ÿå‚æ•°è®¡ç®—
        total_params = 0
        for layer in self.layers.values():
            if layer.layer_type == 'Conv2d':
                total_params += layer.in_features * layer.out_features * 9  # 3x3 kernel
            elif layer.layer_type == 'Linear':
                total_params += layer.in_features * layer.out_features
        return [MockParam(total_params)]

class MockParam:
    def __init__(self, size):
        self.size = size
    def numel(self):
        return self.size

def create_mock_tensors(shape):
    """åˆ›å»ºæ¨¡æ‹Ÿå¼ é‡"""
    return np.random.randn(*shape)

def simulate_activations_and_gradients(model: MockModel):
    """æ¨¡æ‹Ÿæ¿€æ´»å€¼å’Œæ¢¯åº¦"""
    activations = {}
    gradients = {}
    
    for name, module in model.named_modules():
        if module.layer_type == 'Conv2d':
            # æ¨¡æ‹Ÿå·ç§¯å±‚çš„æ¿€æ´»å’Œæ¢¯åº¦
            batch_size = 32
            if 'conv1' in name:
                activations[name] = create_mock_tensors((batch_size, module.out_features, 32, 32))
            else:
                activations[name] = create_mock_tensors((batch_size, module.out_features, 16, 16))
            gradients[name] = create_mock_tensors(activations[name].shape)
        
        elif module.layer_type == 'Linear':
            # æ¨¡æ‹Ÿçº¿æ€§å±‚çš„æ¿€æ´»å’Œæ¢¯åº¦
            batch_size = 32
            activations[name] = create_mock_tensors((batch_size, module.out_features))
            gradients[name] = create_mock_tensors(activations[name].shape)
    
    return activations, gradients

def create_test_context(model: MockModel):
    """åˆ›å»ºæµ‹è¯•ä¸Šä¸‹æ–‡"""
    
    # æ¨¡æ‹Ÿæ€§èƒ½å†å²ï¼ˆæ˜¾ç¤ºåœæ»ï¼‰
    performance_history = [
        0.72, 0.74, 0.76, 0.78, 0.79, 0.80, 0.805, 0.807, 0.808, 0.808,
        0.8081, 0.8082, 0.8079, 0.8080, 0.8079  # æœ€è¿‘å‡ ä¸ªepochåœæ»
    ]
    
    # æ¨¡æ‹Ÿæ¿€æ´»å€¼å’Œæ¢¯åº¦
    activations, gradients = simulate_activations_and_gradients(model)
    
    context = {
        'epoch': 15,
        'performance_history': performance_history,
        'train_loss': 0.72,
        'learning_rate': 0.1,
        'activations': activations,
        'gradients': gradients,
        'targets': np.random.randint(0, 10, 32)
    }
    
    return context

class SimplifiedBayesianEngine:
    """ç®€åŒ–çš„è´å¶æ–¯æ¨æ–­å¼•æ“ï¼ˆæ— PyTorchä¾èµ–ï¼‰"""
    
    def __init__(self):
        # è´å¶æ–¯å…ˆéªŒåˆ†å¸ƒå‚æ•°
        self.mutation_priors = {
            'width_expansion': {'alpha': 15, 'beta': 5},
            'depth_expansion': {'alpha': 12, 'beta': 8},
            'attention_enhancement': {'alpha': 10, 'beta': 10},
            'residual_connection': {'alpha': 18, 'beta': 2},
            'batch_norm_insertion': {'alpha': 20, 'beta': 5},
            'parallel_division': {'alpha': 8, 'beta': 12}
        }
        
        # åŠ¨æ€é˜ˆå€¼ï¼ˆç§¯ææ¨¡å¼ï¼‰
        self.dynamic_thresholds = {
            'min_expected_improvement': 0.001,
            'confidence_threshold': 0.2,
            'bottleneck_threshold': 0.3
        }
        
        # å†å²è®°å½•
        self.mutation_history = []
        self.performance_history = []
    
    def analyze_parameter_utilization(self, module: MockModule, activation: np.ndarray = None) -> float:
        """åˆ†æå‚æ•°åˆ©ç”¨ç‡"""
        
        base_score = 0.0
        
        if module.layer_type == 'Conv2d':
            # é€šé“æ•°ç›¸å¯¹å……åˆ†æ€§
            channel_ratio = module.out_features / max(16, module.in_features)
            if channel_ratio < 0.8:
                base_score += 0.6
        
        elif module.layer_type == 'Linear':
            # ç‰¹å¾æ•°ç›¸å¯¹å……åˆ†æ€§
            feature_ratio = module.out_features / max(32, module.in_features)
            if feature_ratio < 0.5:
                base_score += 0.7
        
        # å¦‚æœæœ‰æ¿€æ´»å€¼ï¼Œåˆ†ææ¿€æ´»æ¨¡å¼
        if activation is not None:
            # æ¿€æ´»ç¨€ç–æ€§
            sparsity = np.mean(activation == 0)
            if sparsity > 0.7:
                base_score += 0.4
            
            # æ¿€æ´»åˆ†å¸ƒé›†ä¸­åº¦
            std_act = np.std(activation)
            if std_act < 0.1:
                base_score += 0.3
        
        return min(1.0, base_score)
    
    def analyze_information_efficiency(self, activation: np.ndarray) -> float:
        """åˆ†æä¿¡æ¯æ•ˆç‡"""
        
        try:
            flat_activation = activation.flatten()
            
            # æœ‰æ•ˆæ¿€æ´»æ¯”ä¾‹
            non_zero_ratio = np.count_nonzero(flat_activation) / flat_activation.size
            efficiency_loss = 1 - non_zero_ratio
            
            # åŠ¨æ€èŒƒå›´åˆ©ç”¨
            activation_range = np.max(flat_activation) - np.min(flat_activation)
            if activation_range < 1.0:
                efficiency_loss += 0.3
            
            # ä¿¡æ¯ç†µ
            hist, _ = np.histogram(flat_activation, bins=20)
            hist_normalized = hist / (hist.sum() + 1e-10)
            entropy = -np.sum(hist_normalized * np.log(hist_normalized + 1e-10))
            max_entropy = np.log(20)
            if entropy / max_entropy < 0.5:
                efficiency_loss += 0.4
            
            return min(1.0, efficiency_loss)
            
        except Exception:
            return 0.5
    
    def analyze_gradient_quality(self, gradient: np.ndarray) -> float:
        """åˆ†ææ¢¯åº¦è´¨é‡"""
        
        try:
            # æ¢¯åº¦èŒƒæ•°
            grad_norm = np.linalg.norm(gradient)
            quality_loss = 0.0
            
            # æ¢¯åº¦æ¶ˆå¤±æ£€æµ‹
            if grad_norm < 1e-5:
                quality_loss += 0.8
            elif grad_norm < 1e-3:
                quality_loss += 0.4
            
            # æ¢¯åº¦çˆ†ç‚¸æ£€æµ‹
            if grad_norm > 10.0:
                quality_loss += 0.6
            elif grad_norm > 1.0:
                quality_loss += 0.2
            
            # æ¢¯åº¦åˆ†å¸ƒ
            grad_std = np.std(gradient)
            grad_mean = np.mean(np.abs(gradient))
            
            if grad_std / (grad_mean + 1e-10) > 10:
                quality_loss += 0.3
            
            return min(1.0, quality_loss)
            
        except Exception:
            return 0.4
    
    def detect_candidates(self, model: MockModel, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ£€æµ‹å˜å¼‚å€™é€‰ç‚¹"""
        
        candidates = []
        activations = context.get('activations', {})
        gradients = context.get('gradients', {})
        
        for name, module in model.named_modules():
            if module.layer_type not in ['Conv2d', 'Linear']:
                continue
            
            candidate = {
                'layer_name': name,
                'layer_type': module.layer_type,
                'module': module,
                'bottleneck_indicators': {},
                'mutation_suitability': {}
            }
            
            # å‚æ•°åˆ©ç”¨ç‡åˆ†æ
            param_utilization = self.analyze_parameter_utilization(
                module, activations.get(name)
            )
            candidate['bottleneck_indicators']['parameter_utilization'] = param_utilization
            
            # ä¿¡æ¯æµæ•ˆç‡åˆ†æ
            if name in activations:
                info_efficiency = self.analyze_information_efficiency(activations[name])
                candidate['bottleneck_indicators']['information_efficiency'] = info_efficiency
            
            # æ¢¯åº¦è´¨é‡åˆ†æ
            if name in gradients:
                gradient_quality = self.analyze_gradient_quality(gradients[name])
                candidate['bottleneck_indicators']['gradient_quality'] = gradient_quality
            
            # ç»¼åˆè¯„åˆ†
            bottleneck_score = np.mean(list(candidate['bottleneck_indicators'].values()))
            
            if bottleneck_score > self.dynamic_thresholds['bottleneck_threshold']:
                # åˆ†æå˜å¼‚é€‚ç”¨æ€§
                self.analyze_mutation_suitability(candidate)
                
                candidate['improvement_potential'] = min(1.0, bottleneck_score * 1.5)
                candidates.append(candidate)
                
                logger.info(f"âœ… å‘ç°å€™é€‰å±‚: {name}, ç“¶é¢ˆåˆ†æ•°: {bottleneck_score:.3f}")
        
        return candidates
    
    def analyze_mutation_suitability(self, candidate: Dict[str, Any]):
        """åˆ†æå˜å¼‚é€‚ç”¨æ€§"""
        
        suitability = {}
        layer_type = candidate['layer_type']
        bottlenecks = candidate['bottleneck_indicators']
        
        mutations_to_check = [
            'width_expansion', 'depth_expansion', 'attention_enhancement',
            'residual_connection', 'batch_norm_insertion', 'parallel_division'
        ]
        
        for mutation in mutations_to_check:
            score = self.calculate_mutation_suitability_score(
                mutation, layer_type, bottlenecks
            )
            if score > 0.2:
                suitability[mutation] = score
        
        candidate['mutation_suitability'] = suitability
    
    def calculate_mutation_suitability_score(self, 
                                           mutation: str,
                                           layer_type: str,
                                           bottlenecks: Dict[str, float]) -> float:
        """è®¡ç®—å˜å¼‚é€‚ç”¨æ€§åˆ†æ•°"""
        
        score = 0.0
        
        # åŸºäºå±‚ç±»å‹çš„åŸºç¡€é€‚ç”¨æ€§
        layer_compatibility = {
            'Conv2d': {
                'width_expansion': 0.8, 'depth_expansion': 0.6, 
                'attention_enhancement': 0.7, 'parallel_division': 0.9
            },
            'Linear': {
                'width_expansion': 0.9, 'depth_expansion': 0.4, 
                'batch_norm_insertion': 0.3, 'residual_connection': 0.6
            }
        }
        
        score += layer_compatibility.get(layer_type, {}).get(mutation, 0.5)
        
        # åŸºäºç“¶é¢ˆç±»å‹çš„é€‚ç”¨æ€§
        if bottlenecks.get('parameter_utilization', 0) > 0.4:
            if mutation in ['width_expansion', 'depth_expansion', 'parallel_division']:
                score += 0.3
        
        if bottlenecks.get('information_efficiency', 0) > 0.4:
            if mutation in ['attention_enhancement']:
                score += 0.3
        
        if bottlenecks.get('gradient_quality', 0) > 0.4:
            if mutation in ['residual_connection', 'batch_norm_insertion']:
                score += 0.3
        
        return min(1.0, score)
    
    def bayesian_success_inference(self, candidates: List[Dict[str, Any]]) -> Dict[str, Dict[str, float]]:
        """è´å¶æ–¯æˆåŠŸç‡æ¨æ–­"""
        
        success_probs = {}
        
        for candidate in candidates:
            layer_name = candidate['layer_name']
            success_probs[layer_name] = {}
            
            for mutation_type in candidate.get('mutation_suitability', {}):
                # è·å–å…ˆéªŒåˆ†å¸ƒ
                prior = self.mutation_priors.get(mutation_type, {'alpha': 5, 'beta': 5})
                
                # Betaåˆ†å¸ƒçš„æœŸæœ›å€¼
                alpha = prior['alpha']
                beta = prior['beta']
                base_prob = alpha / (alpha + beta)
                
                # åŸºäºå½“å‰æƒ…å†µè°ƒæ•´
                bottleneck_severity = np.mean(list(candidate['bottleneck_indicators'].values()))
                suitability = candidate['mutation_suitability'].get(mutation_type, 0.5)
                
                adjustment = bottleneck_severity * 0.3 + suitability * 0.2
                adjusted_prob = base_prob + adjustment
                
                success_probs[layer_name][mutation_type] = np.clip(adjusted_prob, 0.01, 0.99)
        
        return success_probs
    
    def generate_decisions(self, 
                          candidates: List[Dict[str, Any]],
                          success_probabilities: Dict[str, Dict[str, float]]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæœ€ä¼˜å†³ç­–"""
        
        decisions = []
        
        for candidate in candidates:
            layer_name = candidate['layer_name']
            
            for mutation_type in candidate.get('mutation_suitability', {}):
                success_prob = success_probabilities[layer_name][mutation_type]
                
                # è®¡ç®—æœŸæœ›æ”¹è¿›
                base_improvements = {
                    'width_expansion': 0.02,
                    'depth_expansion': 0.025,
                    'attention_enhancement': 0.03,
                    'residual_connection': 0.015,
                    'batch_norm_insertion': 0.01,
                    'parallel_division': 0.035
                }
                
                expected_improvement = base_improvements.get(mutation_type, 0.015)
                
                # è®¡ç®—æœŸæœ›æ•ˆç”¨
                expected_utility = success_prob * expected_improvement
                
                # è®¡ç®—å†³ç­–ç½®ä¿¡åº¦
                decision_confidence = success_prob * (1 - 0.1)  # ç®€åŒ–çš„ç½®ä¿¡åº¦è®¡ç®—
                
                # æ£€æŸ¥æ˜¯å¦æ»¡è¶³é˜ˆå€¼
                if (expected_utility > self.dynamic_thresholds['min_expected_improvement'] and
                    decision_confidence > self.dynamic_thresholds['confidence_threshold']):
                    
                    decision = {
                        'layer_name': layer_name,
                        'mutation_type': mutation_type,
                        'success_probability': success_prob,
                        'expected_improvement': expected_improvement,
                        'expected_utility': expected_utility,
                        'decision_confidence': decision_confidence,
                        'rationale': f'è´å¶æ–¯åˆ†ææ¨è{mutation_type}'
                    }
                    
                    decisions.append(decision)
        
        # æŒ‰æœŸæœ›æ•ˆç”¨æ’åº
        decisions.sort(key=lambda x: x['expected_utility'], reverse=True)
        
        return decisions[:3]  # è¿”å›å‰3ä¸ªæœ€ä½³å†³ç­–
    
    def analyze(self, model: MockModel, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´çš„è´å¶æ–¯åˆ†æ"""
        
        logger.info("ğŸ§  å¼€å§‹ç®€åŒ–è´å¶æ–¯åˆ†æ")
        
        # 1. æ£€æµ‹å€™é€‰ç‚¹
        candidates = self.detect_candidates(model, context)
        
        # 2. è´å¶æ–¯æˆåŠŸç‡æ¨æ–­
        success_probabilities = self.bayesian_success_inference(candidates)
        
        # 3. ç”Ÿæˆæœ€ä¼˜å†³ç­–
        optimal_decisions = self.generate_decisions(candidates, success_probabilities)
        
        # 4. ç”Ÿæˆæ‰§è¡Œè®¡åˆ’
        execution_plan = {
            'execute': len(optimal_decisions) > 0,
            'reason': 'bayesian_optimization' if optimal_decisions else 'no_viable_mutations'
        }
        
        return {
            'candidates_found': len(candidates),
            'optimal_decisions': optimal_decisions,
            'execution_plan': execution_plan,
            'success_probabilities': success_probabilities
        }

def test_simplified_bayesian_system():
    """æµ‹è¯•ç®€åŒ–çš„è´å¶æ–¯ç³»ç»Ÿ"""
    
    logger.info("ğŸ§ª å¼€å§‹æµ‹è¯•ç®€åŒ–è´å¶æ–¯å½¢æ€å‘ç”Ÿç³»ç»Ÿ")
    
    try:
        # åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹å’Œå¼•æ“
        model = MockModel()
        bayesian_engine = SimplifiedBayesianEngine()
        
        logger.info(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ: {sum(p.numel() for p in model.parameters()):,} å‚æ•°")
        
        # åˆ›å»ºæµ‹è¯•ä¸Šä¸‹æ–‡
        context = create_test_context(model)
        logger.info(f"âœ… æµ‹è¯•ä¸Šä¸‹æ–‡åˆ›å»ºæˆåŠŸ: {len(context['activations'])}ä¸ªæ¿€æ´»")
        
        # æ‰§è¡Œè´å¶æ–¯åˆ†æ
        logger.info("ğŸš€ å¼€å§‹è´å¶æ–¯åˆ†æ...")
        result = bayesian_engine.analyze(model, context)
        
        # åˆ†æç»“æœ
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š è´å¶æ–¯åˆ†æç»“æœ:")
        logger.info("="*60)
        
        candidates_found = result.get('candidates_found', 0)
        optimal_decisions = result.get('optimal_decisions', [])
        execution_plan = result.get('execution_plan', {})
        
        logger.info(f"ğŸ¯ å€™é€‰ç‚¹å‘ç°: {candidates_found}ä¸ª")
        logger.info(f"â­ æœ€ä¼˜å†³ç­–: {len(optimal_decisions)}ä¸ª")
        logger.info(f"ğŸš€ æ˜¯å¦æ‰§è¡Œ: {'æ˜¯' if execution_plan.get('execute', False) else 'å¦'}")
        
        if optimal_decisions:
            logger.info(f"\nğŸ“‹ æœ€ä¼˜å†³ç­–è¯¦æƒ…:")
            for i, decision in enumerate(optimal_decisions):
                logger.info(f"  {i+1}. ç›®æ ‡å±‚: {decision.get('layer_name', 'N/A')}")
                logger.info(f"     å˜å¼‚ç±»å‹: {decision.get('mutation_type', 'N/A')}")
                logger.info(f"     æˆåŠŸæ¦‚ç‡: {decision.get('success_probability', 0.0):.3f}")
                logger.info(f"     æœŸæœ›æ”¹è¿›: {decision.get('expected_improvement', 0.0):.4f}")
                logger.info(f"     æœŸæœ›æ•ˆç”¨: {decision.get('expected_utility', 0.0):.4f}")
                logger.info(f"     å†³ç­–ç½®ä¿¡åº¦: {decision.get('decision_confidence', 0.0):.3f}")
                logger.info("")
        
        # éªŒè¯æµ‹è¯•ç»“æœ
        success_metrics = {
            'candidates_found': candidates_found > 0,
            'decisions_generated': len(optimal_decisions) > 0,
            'execution_plan_valid': execution_plan.get('execute', False),
            'reasonable_probabilities': all(
                0.0 <= d.get('success_probability', 0) <= 1.0 
                for d in optimal_decisions
            )
        }
        
        logger.info(f"\nâœ… æµ‹è¯•ç»“æœéªŒè¯:")
        for metric, passed in success_metrics.items():
            status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
            logger.info(f"   {metric}: {status}")
        
        overall_success = all(success_metrics.values())
        
        if overall_success:
            logger.info(f"\nğŸ‰ ç®€åŒ–è´å¶æ–¯å½¢æ€å‘ç”Ÿç³»ç»Ÿæµ‹è¯•æˆåŠŸï¼")
            logger.info(f"ç³»ç»ŸæˆåŠŸå®ç°äº†ä»¥ä¸‹åŠŸèƒ½ï¼š")
            logger.info(f"  1. âœ… ç§¯ææ£€æµ‹å˜å¼‚å€™é€‰ç‚¹ ({candidates_found}ä¸ª)")
            logger.info(f"  2. âœ… è´å¶æ–¯æ¨æ–­æˆåŠŸæ¦‚ç‡")
            logger.info(f"  3. âœ… æœŸæœ›æ•ˆç”¨æœ€å¤§åŒ–å†³ç­– ({len(optimal_decisions)}ä¸ª)")
            logger.info(f"  4. âœ… æ™ºèƒ½æ‰§è¡Œè®¡åˆ’ç”Ÿæˆ")
        else:
            logger.warning(f"\nâš ï¸ éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚")
        
        return overall_success, result
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False, None

if __name__ == "__main__":
    logger.info("ğŸš€ å¼€å§‹æµ‹è¯•ç®€åŒ–è´å¶æ–¯å½¢æ€å‘ç”Ÿç³»ç»Ÿ")
    
    # è¿è¡Œæµ‹è¯•
    success, result = test_simplified_bayesian_system()
    
    # æœ€ç»ˆæŠ¥å‘Š
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š æœ€ç»ˆæµ‹è¯•æŠ¥å‘Š")
    logger.info("="*60)
    
    if success:
        logger.info(f"âœ… ç®€åŒ–è´å¶æ–¯ç³»ç»Ÿæµ‹è¯•æˆåŠŸï¼")
        logger.info(f"\nğŸ¯ å…³é”®æ”¹è¿›éªŒè¯:")
        logger.info(f"  â€¢ å€™é€‰ç‚¹æ£€æµ‹æ›´ç§¯æ (é˜ˆå€¼ä»0.5é™åˆ°0.3)")
        logger.info(f"  â€¢ è´å¶æ–¯æ¨æ–­æä¾›æ¦‚ç‡åŒ–å†³ç­–")
        logger.info(f"  â€¢ æœŸæœ›æ•ˆç”¨æœ€å¤§åŒ–æ›¿ä»£ç®€å•è§„åˆ™")
        logger.info(f"  â€¢ ç½®ä¿¡åº¦é‡åŒ–ä¸ç¡®å®šæ€§")
        logger.info(f"\nğŸ’¡ è¿™è¯æ˜äº†å¢å¼ºè´å¶æ–¯ç³»ç»Ÿçš„æ ¸å¿ƒé€»è¾‘æ˜¯æ­£ç¡®çš„ã€‚")
        logger.info(f"   åœ¨å®é™…PyTorchç¯å¢ƒä¸­ï¼Œæ€§èƒ½ä¼šæ›´å¥½ï¼")
    else:
        logger.warning(f"âŒ æµ‹è¯•æœªé€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
    
    logger.info(f"\næµ‹è¯•å®Œæˆã€‚")