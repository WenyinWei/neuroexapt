#!/usr/bin/env python3
"""
é«˜æ•ˆvsä¼ ç»Ÿæ¶æ„å¯¹æ¯”éªŒè¯

ç›´æ¥å¯¹æ¯”å‚æ•°é‡ã€å†…å­˜ä½¿ç”¨å’Œå‰å‘ä¼ æ’­é€Ÿåº¦
"""

import torch
import torch.nn as nn
import time
import sys
import os

sys.path.append('.')
from neuroexapt.core.model import Network

def count_parameters(model):
    """è®¡ç®—æ¨¡å‹å‚æ•°é‡"""
    return sum(p.numel() for p in model.parameters())

def measure_memory_usage(model, input_tensor):
    """æµ‹é‡GPUå†…å­˜ä½¿ç”¨"""
    torch.cuda.reset_peak_memory_stats()
    model(input_tensor)
    return torch.cuda.max_memory_allocated() / 1024 / 1024  # MB

def measure_speed(model, input_tensor, runs=10):
    """æµ‹é‡å‰å‘ä¼ æ’­é€Ÿåº¦"""
    # é¢„çƒ­
    for _ in range(5):
        _ = model(input_tensor)
    
    # æµ‹è¯•
    torch.cuda.synchronize()
    start = time.perf_counter()
    for _ in range(runs):
        _ = model(input_tensor)
    torch.cuda.synchronize()
    
    return (time.perf_counter() - start) / runs * 1000  # ms

def main():
    print("ğŸ” ä¼ ç»Ÿvsé«˜æ•ˆæ¶æ„å¯¹æ¯”éªŒè¯")
    print("=" * 60)
    
    # æµ‹è¯•é…ç½®
    C = 16
    batch_size = 32
    input_tensor = torch.randn(batch_size, 3, 32, 32, device='cuda')
    
    # åˆ›å»ºä¼ ç»Ÿç½‘ç»œï¼ˆå‚æ•°é‡å·¨å¤§çš„ç‰ˆæœ¬ï¼‰
    print("ğŸ“Š åˆ›å»ºä¼ ç»Ÿç½‘ç»œ...")
    traditional_model = Network(
        C=C,
        num_classes=10,
        layers=6,
        potential_layers=4,
        quiet=True
    ).cuda()
    
    traditional_params = count_parameters(traditional_model)
    
    # åˆ›å»ºç®€åŒ–ç½‘ç»œï¼ˆå‡å°‘å‚æ•°çš„ç‰ˆæœ¬ï¼‰
    print("ğŸ“Š åˆ›å»ºç®€åŒ–ç½‘ç»œ...")
    simplified_model = Network(
        C=C,
        num_classes=10,
        layers=4,  # å‡å°‘å±‚æ•°
        potential_layers=2,  # å‡å°‘æ½œåœ¨å±‚æ•°
        quiet=True
    ).cuda()
    
    simplified_params = count_parameters(simplified_model)
    
    print(f"\nğŸ“ˆ å‚æ•°é‡å¯¹æ¯”:")
    print(f"   ä¼ ç»Ÿç½‘ç»œ: {traditional_params:,} å‚æ•°")
    print(f"   ç®€åŒ–ç½‘ç»œ: {simplified_params:,} å‚æ•°")
    print(f"   å‚æ•°å‡å°‘: {(1 - simplified_params/traditional_params)*100:.1f}%")
    
    # æµ‹è¯•å†…å­˜ä½¿ç”¨
    print(f"\nğŸ’¾ å†…å­˜ä½¿ç”¨å¯¹æ¯”:")
    traditional_memory = measure_memory_usage(traditional_model, input_tensor)
    simplified_memory = measure_memory_usage(simplified_model, input_tensor)
    
    print(f"   ä¼ ç»Ÿç½‘ç»œ: {traditional_memory:.1f} MB")
    print(f"   ç®€åŒ–ç½‘ç»œ: {simplified_memory:.1f} MB")
    print(f"   å†…å­˜èŠ‚çœ: {(1 - simplified_memory/traditional_memory)*100:.1f}%")
    
    # æµ‹è¯•é€Ÿåº¦
    print(f"\nâš¡ é€Ÿåº¦å¯¹æ¯”:")
    traditional_speed = measure_speed(traditional_model, input_tensor)
    simplified_speed = measure_speed(simplified_model, input_tensor)
    
    print(f"   ä¼ ç»Ÿç½‘ç»œ: {traditional_speed:.2f} ms")
    print(f"   ç®€åŒ–ç½‘ç»œ: {simplified_speed:.2f} ms")
    print(f"   é€Ÿåº¦æå‡: {traditional_speed/simplified_speed:.1f}x")
    
    # è¿›ä¸€æ­¥ä¼˜åŒ–å»ºè®®
    print(f"\nğŸ’¡ è¿›ä¸€æ­¥ä¼˜åŒ–ç­–ç•¥:")
    print("1. å‚æ•°å…±äº«: ç›¸åŒæ“ä½œè·¨å±‚å…±äº«å‚æ•°")
    print("2. åŠ¨æ€å‰ªæ: è®­ç»ƒä¸­å‰ªé™¤ä½æƒé‡æ“ä½œ")
    print("3. æ¸è¿›æœç´¢: åˆ†é˜¶æ®µå¢åŠ æœç´¢å¤æ‚åº¦")
    print("4. æ“ä½œèåˆ: ä½¿ç”¨CUDA/Tritonèåˆæ“ä½œ")
    
    # æ¸…ç†å†…å­˜
    del traditional_model, simplified_model
    torch.cuda.empty_cache()
    
    print(f"\nğŸ¯ æ¨èé…ç½® (å¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡):")
    print("   - layers=4 (instead of 10)")
    print("   - potential_layers=2 (instead of 4)")
    print("   - init_channels=16 (åˆç†èµ·å§‹é€šé“æ•°)")
    print("   - å¯ç”¨æ‰€æœ‰CPUä¼˜åŒ–å’ŒTritonåŠ é€Ÿ")

if __name__ == "__main__":
    main() 