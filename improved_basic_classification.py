"""
æ”¹è¿›çš„åŸºç¡€åˆ†ç±»ç¤ºä¾‹ - ä½¿ç”¨ä¿å®ˆçš„ä¿¡æ¯è®ºç­–ç•¥
å±•ç¤ºå¦‚ä½•é€šè¿‡è°ƒæ•´ä¿¡æ¯è®ºç­–ç•¥æ¥ä¿æŒæ›´é«˜çš„å‡†ç¡®ç‡
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import numpy as np
import sys
import os

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from neuroexapt.core.operators import PruneByEntropy, AddBlock, ExpandWithMI, create_conv_block
from neuroexapt.neuroexapt import NeuroExapt


class ImprovedCNN(nn.Module):
    """æ”¹è¿›çš„CNNæ¶æ„ï¼Œå…·æœ‰æ›´å¥½çš„é€‚åº”æ€§"""
    
    def __init__(self, num_classes=10):
        super(ImprovedCNN, self).__init__()
        
        self.features = nn.Sequential(
            # ç¬¬ä¸€ä¸ªå·ç§¯å—
            nn.Conv2d(3, 32, kernel_size=3, padding=1),   # features.0
            nn.BatchNorm2d(32),                           # features.1  
            nn.ReLU(inplace=True),                        # features.2
            nn.MaxPool2d(kernel_size=2, stride=2),        # features.3
            
            # ç¬¬äºŒä¸ªå·ç§¯å—
            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # features.4
            nn.BatchNorm2d(64),                           # features.5
            nn.ReLU(inplace=True),                        # features.6
            nn.MaxPool2d(kernel_size=2, stride=2),        # features.7
        )
        
        self.classifier = nn.Sequential(
            nn.Dropout(0.3),  # è¾ƒå°‘çš„dropout
            nn.Linear(64 * 8 * 8, 256),  # æ›´å¤§çš„éšè—å±‚
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
        
        self._initialize_weights()
    
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)


def create_smart_expansion_block(out_channels: int) -> nn.Module:
    """åˆ›å»ºæ™ºèƒ½æ‰©å±•å—ï¼ŒåŒ…å«æ®‹å·®è¿æ¥"""
    return nn.Sequential(
        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
        nn.BatchNorm2d(out_channels),
        nn.ReLU(inplace=True),
        nn.Conv2d(out_channels, out_channels, kernel_size=1),  # 1x1å‹ç¼©
        nn.BatchNorm2d(out_channels),
    )


def load_optimized_data(batch_size=128):
    """åŠ è½½ä¼˜åŒ–çš„æ•°æ®"""
    # æ”¹è¿›çš„æ•°æ®å¢å¼º
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.ColorJitter(brightness=0.1, contrast=0.1),  # è½»å¾®é¢œè‰²æŠ–åŠ¨
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])

    trainset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform_train
    )
    # ä½¿ç”¨æ›´å¤§çš„å­é›†è¿›è¡Œæµ‹è¯•
    subset_size = 5000
    indices = torch.randperm(len(trainset))[:subset_size].tolist()
    trainset = torch.utils.data.Subset(trainset, indices)
    
    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)

    testset = torchvision.datasets.CIFAR10(
        root='./data', train=False, download=False, transform=transform_test
    )
    # ä½¿ç”¨1000ä¸ªæµ‹è¯•æ ·æœ¬
    test_indices = torch.randperm(len(testset))[:1000].tolist()
    testset = torch.utils.data.Subset(testset, test_indices)
    
    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)
    
    return trainloader, testloader


def evaluate_model(model, dataloader, device):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    model.eval()
    correct = 0
    total = 0
    total_loss = 0
    criterion = nn.CrossEntropyLoss()
    
    with torch.no_grad():
        for data, targets in dataloader:
            data, targets = data.to(device), targets.to(device)
            outputs = model(data)
            loss = criterion(outputs, targets)
            
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += targets.size(0)
            correct += (predicted == targets).sum().item()
    
    accuracy = 100 * correct / total
    avg_loss = total_loss / len(dataloader)
    model.train()
    return accuracy, avg_loss


def main():
    print("ğŸš€ æ”¹è¿›çš„NeuroExaptåˆ†ç±»ç¤ºä¾‹ - ä¿å®ˆä¿¡æ¯è®ºç­–ç•¥")
    print("=" * 60)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è¶…å‚æ•°è®¾ç½®
    num_epochs = 20
    batch_size = 128
    learning_rate = 0.001
    evolution_frequency = 2  # æ¯2ä¸ªepochæ£€æŸ¥ä¸€æ¬¡
    
    # åŠ è½½æ•°æ®
    print("ğŸ“Š åŠ è½½ä¼˜åŒ–çš„CIFAR-10æ•°æ®...")
    trainloader, testloader = load_optimized_data(batch_size=batch_size)
    print(f"è®­ç»ƒæ‰¹æ¬¡: {len(trainloader)}, æµ‹è¯•æ‰¹æ¬¡: {len(testloader)}")
    
    # åˆ›å»ºæ”¹è¿›çš„æ¨¡å‹
    print("ğŸ—ï¸ åˆ›å»ºæ”¹è¿›çš„CNNæ¶æ„...")
    model = ImprovedCNN(num_classes=10).to(device)
    criterion = nn.CrossEntropyLoss()
    
    initial_params = sum(p.numel() for p in model.parameters())
    print(f"åˆå§‹å‚æ•°æ•°é‡: {initial_params:,}")
    
    # ä¿å®ˆçš„æ“ä½œç¬¦è®¾ç½®
    print("ğŸ”§ è®¾ç½®ä¿å®ˆçš„ç»“æ„æ“ä½œç¬¦...")
    operators = [
        # ğŸ”„ åªå¯¹å®‰å…¨çš„å±‚è¿›è¡Œå‰ªæ
        PruneByEntropy('features.2'),  # ReLUå±‚
        PruneByEntropy('features.6'),  # ReLUå±‚
        
        # ğŸ”„ ä¼˜å…ˆæ‰©å±•æ“ä½œ
        AddBlock('features.3', create_smart_expansion_block),  # ç¬¬ä¸€ä¸ªæ± åŒ–å
        AddBlock('features.7', create_smart_expansion_block),  # ç¬¬äºŒä¸ªæ± åŒ–å
        
        # ğŸ”„ ä¿å®ˆçš„å®½åº¦æ‰©å±•
        ExpandWithMI(expansion_factor=1.15),  # è½»å¾®æ‰©å±•
    ]
    
    # åˆ›å»ºNeuroExaptå®ä¾‹ï¼ˆä¿å®ˆé…ç½®ï¼‰
    print("ğŸ§  åˆå§‹åŒ–ä¿å®ˆçš„NeuroExaptæ¡†æ¶...")
    neuroexapt = NeuroExapt(
        model=model,
        criterion=criterion,
        dataloader=trainloader,
        operators=operators,
        device=device,
        lambda_entropy=0.003,      # ğŸ”„ éå¸¸ä½çš„ç†µæ­£åˆ™åŒ–
        lambda_bayesian=0.001,     # ğŸ”„ éå¸¸ä½çš„è´å¶æ–¯æ­£åˆ™åŒ–
        input_shape=(3, 32, 32),
        enable_validation=True
    )
    
    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    
    # è®°å½•å†å²
    history = {
        'train_acc': [],
        'test_acc': [],
        'train_loss': [],
        'test_loss': [],
        'evolutions': []
    }
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    print("=" * 60)
    
    # åˆå§‹è¯„ä¼°
    initial_test_acc, initial_test_loss = evaluate_model(model, testloader, device)
    print(f"åˆå§‹æµ‹è¯•å‡†ç¡®ç‡: {initial_test_acc:.2f}%, æŸå¤±: {initial_test_loss:.4f}")
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(num_epochs):
        print(f"\nEpoch [{epoch+1:2d}/{num_epochs}]")
        
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        epoch_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (data, targets) in enumerate(trainloader):
            data, targets = data.to(device), targets.to(device)
            
            optimizer.zero_grad()
            outputs = model(data)
            loss = criterion(outputs, targets)
            
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += targets.size(0)
            correct += (predicted == targets).sum().item()
        
        # è®¡ç®—æŒ‡æ ‡
        train_acc = 100 * correct / total
        train_loss = epoch_loss / len(trainloader)
        test_acc, test_loss = evaluate_model(model, testloader, device)
        
        # è®°å½•å†å²
        history['train_acc'].append(train_acc)
        history['test_acc'].append(test_acc)
        history['train_loss'].append(train_loss)
        history['test_loss'].append(test_loss)
        
        print(f"è®­ç»ƒ - å‡†ç¡®ç‡: {train_acc:.2f}%, æŸå¤±: {train_loss:.4f}")
        print(f"æµ‹è¯• - å‡†ç¡®ç‡: {test_acc:.2f}%, æŸå¤±: {test_loss:.4f}")
        
        # æ¶æ„è¿›åŒ–æ£€æŸ¥
        if (epoch + 1) % evolution_frequency == 0:
            print(f"\nğŸ”„ è¿›åŒ–æ£€æŸ¥ (Epoch {epoch + 1})...")
            
            performance_metrics = {
                'val_accuracy': test_acc,
                'train_accuracy': train_acc,
                'loss': train_loss,
                'test_loss': test_loss
            }
            
            # å‡†ç¡®ç‡ä¿æŠ¤æç¤º
            if test_acc < 60.0:
                print(f"ğŸ›¡ï¸ å‡†ç¡®ç‡ä¿æŠ¤æ¿€æ´»: {test_acc:.1f}% < 60%")
            
            prev_params = sum(p.numel() for p in model.parameters())
            
            try:
                evolved_model, action_taken = neuroexapt.evolution_engine.step(
                    epoch=epoch + 1,
                    performance_metrics=performance_metrics,
                    dataloader=trainloader,
                    criterion=criterion
                )
                
                if action_taken:
                    model = evolved_model
                    neuroexapt.model = model
                    new_params = sum(p.numel() for p in model.parameters())
                    
                    print(f"âœ… è¿›åŒ–æˆåŠŸ! åŠ¨ä½œ: {action_taken}")
                    print(f"ğŸ“Š å‚æ•°: {prev_params:,} -> {new_params:,} (Î”{new_params - prev_params:+,})")
                    
                    # è¿›åŒ–åè¯„ä¼°
                    post_evo_acc, post_evo_loss = evaluate_model(model, testloader, device)
                    print(f"ğŸ¯ è¿›åŒ–åå‡†ç¡®ç‡: {post_evo_acc:.2f}% (å˜åŒ–: {post_evo_acc - test_acc:+.2f}%)")
                    
                    if post_evo_acc < test_acc - 3.0:
                        print("âš ï¸ å‡†ç¡®ç‡ä¸‹é™è¾ƒå¤§ï¼Œç­–ç•¥éœ€è¦è°ƒæ•´")
                    
                    # è®°å½•è¿›åŒ–
                    history['evolutions'].append({
                        'epoch': epoch + 1,
                        'action': str(action_taken),
                        'param_change': new_params - prev_params,
                        'acc_before': test_acc,
                        'acc_after': post_evo_acc
                    })
                    
                    # é‡ç½®ä¼˜åŒ–å™¨
                    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
                    
                else:
                    print("â„¹ï¸ æ— éœ€è¿›åŒ–")
                    
            except Exception as e:
                print(f"âŒ è¿›åŒ–å¤±è´¥: {e}")
        
        print("-" * 40)
    
    # æœ€ç»ˆç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ¯ è®­ç»ƒå®Œæˆï¼æœ€ç»ˆç»“æœ:")
    
    final_test_acc, final_test_loss = evaluate_model(model, testloader, device)
    final_params = sum(p.numel() for p in model.parameters())
    
    print(f"ğŸ“Š å‡†ç¡®ç‡å˜åŒ–: {initial_test_acc:.2f}% -> {final_test_acc:.2f}% (Î”{final_test_acc - initial_test_acc:+.2f}%)")
    print(f"ğŸ“Š å‚æ•°å˜åŒ–: {initial_params:,} -> {final_params:,} (Î”{final_params - initial_params:+,})")
    print(f"ğŸ“Š è¿›åŒ–æ¬¡æ•°: {len(history['evolutions'])}")
    
    # è¿›åŒ–å†å²
    if history['evolutions']:
        print(f"\nğŸ”„ è¿›åŒ–å†å²:")
        for evo in history['evolutions']:
            print(f"  Epoch {evo['epoch']:2d}: {evo['action']}")
            print(f"    å‚æ•°å˜åŒ–: {evo['param_change']:+,}")
            print(f"    å‡†ç¡®ç‡: {evo['acc_before']:.1f}% -> {evo['acc_after']:.1f}%")
    
    # æ€§èƒ½è¯„ä¼°
    accuracy_maintained = final_test_acc >= initial_test_acc - 2.0
    print(f"\n{'âœ…' if accuracy_maintained else 'âŒ'} ä¿å®ˆç­–ç•¥è¯„ä¼°: {'æˆåŠŸ' if accuracy_maintained else 'éœ€æ”¹è¿›'}")
    
    # æ¸…ç†
    neuroexapt.cleanup()
    
    return final_test_acc >= initial_test_acc - 2.0


if __name__ == "__main__":
    torch.manual_seed(42)
    np.random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(42)
    
    try:
        success = main()
        if success:
            print("\nğŸ‰ æ”¹è¿›ç­–ç•¥æµ‹è¯•æˆåŠŸï¼ä¿å®ˆçš„ä¿¡æ¯è®ºç­–ç•¥æœ‰æ•ˆä¿æŒäº†å‡†ç¡®ç‡ã€‚")
        else:
            print("\nâš ï¸ ç­–ç•¥ä»éœ€è¿›ä¸€æ­¥è°ƒæ•´ã€‚")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc() 