"""
æµ‹è¯•å¹³æ»‘é€šé“æ‰©å±• - éªŒè¯å‚æ•°è¿ç§»çš„å¹³æ»‘æ€§
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import numpy as np
import sys
import os

# Add the current directory to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from neuroexapt.core.smart_channel_expander import SmartChannelExpander

class TestCNN(nn.Module):
    """ç®€å•çš„æµ‹è¯•CNN"""
    def __init__(self):
        super(TestCNN, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(32, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(64, 10)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

def load_cifar10_sample(batch_size=64):
    """åŠ è½½CIFAR-10æ•°æ®é›†çš„å°æ ·æœ¬ç”¨äºæµ‹è¯•"""
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])
    
    # ä½¿ç”¨å°æ ·æœ¬è¿›è¡Œå¿«é€Ÿæµ‹è¯•
    dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    
    # åªä½¿ç”¨å‰1000ä¸ªæ ·æœ¬
    indices = list(range(1000))
    subset = torch.utils.data.Subset(dataset, indices)
    
    dataloader = DataLoader(subset, batch_size=batch_size, shuffle=True)
    return dataloader

def evaluate_model(model, dataloader, device):
    """è¯„ä¼°æ¨¡å‹å‡†ç¡®ç‡"""
    model.eval()
    correct = 0
    total = 0
    total_loss = 0
    criterion = nn.CrossEntropyLoss()
    
    with torch.no_grad():
        for data, targets in dataloader:
            data, targets = data.to(device), targets.to(device)
            try:
                outputs = model(data)
                loss = criterion(outputs, targets)
                total_loss += loss.item()
                
                _, predicted = torch.max(outputs.data, 1)
                total += targets.size(0)
                correct += (predicted == targets).sum().item()
            except Exception as e:
                print(f"Warning: Evaluation failed: {e}")
                continue
    
    accuracy = 100.0 * correct / total if total > 0 else 0.0
    avg_loss = total_loss / len(dataloader) if len(dataloader) > 0 else 0.0
    
    model.train()
    return accuracy, avg_loss

def train_brief(model, dataloader, device, num_epochs=3):
    """ç®€çŸ­è®­ç»ƒç”¨äºæµ‹è¯•"""
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    for epoch in range(num_epochs):
        epoch_loss = 0
        for batch_idx, (data, targets) in enumerate(dataloader):
            data, targets = data.to(device), targets.to(device)
            
            optimizer.zero_grad()
            outputs = model(data)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            
            if batch_idx >= 10:  # åªè®­ç»ƒå‡ ä¸ªbatch
                break
        
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/(batch_idx+1):.4f}")

def test_smooth_expansion():
    """æµ‹è¯•å¹³æ»‘é€šé“æ‰©å±•"""
    print("=" * 60)
    print("ğŸ§ª å¹³æ»‘é€šé“æ‰©å±•æµ‹è¯•")
    print("=" * 60)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device: {device}")
    
    # åˆ›å»ºæ¨¡å‹
    model = TestCNN().to(device)
    
    # åŠ è½½æ•°æ®
    dataloader = load_cifar10_sample(batch_size=64)
    
    # åˆå§‹è®­ç»ƒ
    print("\nğŸ“š Initial training...")
    train_brief(model, dataloader, device, num_epochs=5)
    
    # è¯„ä¼°åˆå§‹æ€§èƒ½
    initial_accuracy, initial_loss = evaluate_model(model, dataloader, device)
    print(f"\nğŸ“Š Initial Performance:")
    print(f"   Accuracy: {initial_accuracy:.2f}%")
    print(f"   Loss: {initial_loss:.4f}")
    
    # æ£€æŸ¥åˆå§‹å‚æ•°
    initial_params = sum(p.numel() for p in model.parameters())
    print(f"   Parameters: {initial_params:,}")
    
    # åˆ›å»ºå¹³æ»‘æ‰©å±•å™¨
    expander = SmartChannelExpander(accuracy_threshold=0.7)
    
    # æ¨¡æ‹Ÿä½å‡†ç¡®ç‡çŠ¶æ€æ¥è§¦å‘æ‰©å±•
    test_state = {
        'current_performance': initial_accuracy / 100.0,
        'val_accuracy': initial_accuracy,
        'epoch': 5
    }
    
    print(f"\nğŸ”„ Applying smooth channel expansion...")
    print(f"   Simulated accuracy: {initial_accuracy:.2f}%")
    
    # åº”ç”¨å¹³æ»‘æ‰©å±•
    evolved_model = expander(model, test_state)
    
    if evolved_model is not None:
        print("âœ… Channel expansion successful!")
        
        # æ£€æŸ¥æ‰©å±•åçš„å‚æ•°
        expanded_params = sum(p.numel() for p in evolved_model.parameters())
        print(f"   Parameters: {initial_params:,} â†’ {expanded_params:,}")
        print(f"   Increase: {expanded_params - initial_params:,} (+{((expanded_params - initial_params) / initial_params * 100):.1f}%)")
        
        # ç«‹å³è¯„ä¼°æ‰©å±•åçš„æ€§èƒ½ï¼ˆæ— éœ€é‡æ–°è®­ç»ƒï¼‰
        immediate_accuracy, immediate_loss = evaluate_model(evolved_model, dataloader, device)
        print(f"\nğŸ“Š Immediate Post-Expansion Performance:")
        print(f"   Accuracy: {immediate_accuracy:.2f}%")
        print(f"   Loss: {immediate_loss:.4f}")
        print(f"   Accuracy Change: {immediate_accuracy - initial_accuracy:+.2f}%")
        
        # éªŒè¯å¹³æ»‘æ€§
        accuracy_drop = initial_accuracy - immediate_accuracy
        if accuracy_drop > 5.0:  # å¦‚æœå‡†ç¡®ç‡ä¸‹é™è¶…è¿‡5%
            print(f"âš ï¸  WARNING: Significant accuracy drop detected: {accuracy_drop:.2f}%")
            print("   This indicates the parameter migration is not smooth enough!")
        elif accuracy_drop > 0:
            print(f"âœ… Acceptable accuracy drop: {accuracy_drop:.2f}%")
            print("   Parameter migration is working smoothly!")
        else:
            print(f"ğŸ‰ Accuracy improved immediately: {-accuracy_drop:+.2f}%")
            print("   Excellent parameter migration!")
        
        # çŸ­æš‚é‡æ–°è®­ç»ƒä»¥éªŒè¯æ¢å¤èƒ½åŠ›
        print(f"\nğŸ“š Brief retraining after expansion...")
        train_brief(evolved_model, dataloader, device, num_epochs=3)
        
        # æœ€ç»ˆè¯„ä¼°
        final_accuracy, final_loss = evaluate_model(evolved_model, dataloader, device)
        print(f"\nğŸ“Š Final Performance After Retraining:")
        print(f"   Accuracy: {final_accuracy:.2f}%")
        print(f"   Loss: {final_loss:.4f}")
        print(f"   Total Change: {final_accuracy - initial_accuracy:+.2f}%")
        
        # åˆ†æç»“æœ
        print(f"\nğŸ” Smooth Migration Analysis:")
        print(f"   Initial â†’ Immediate: {initial_accuracy:.2f}% â†’ {immediate_accuracy:.2f}% ({immediate_accuracy - initial_accuracy:+.2f}%)")
        print(f"   Immediate â†’ Final: {immediate_accuracy:.2f}% â†’ {final_accuracy:.2f}% ({final_accuracy - immediate_accuracy:+.2f}%)")
        print(f"   Overall Improvement: {final_accuracy - initial_accuracy:+.2f}%")
        
        # æˆåŠŸæ ‡å‡†
        if accuracy_drop < 5.0 and final_accuracy >= initial_accuracy:
            print(f"\nğŸ‰ SMOOTH MIGRATION SUCCESS!")
            print(f"   âœ… Immediate drop: {accuracy_drop:.2f}% < 5.0%")
            print(f"   âœ… Final improvement: {final_accuracy - initial_accuracy:+.2f}%")
        else:
            print(f"\nâŒ SMOOTH MIGRATION NEEDS IMPROVEMENT!")
            print(f"   âš ï¸ Immediate drop: {accuracy_drop:.2f}%")
            print(f"   âš ï¸ Final change: {final_accuracy - initial_accuracy:+.2f}%")
        
    else:
        print("âŒ Channel expansion failed!")
        print("   No expansion was applied.")
    
    print("\n" + "=" * 60)
    print("ğŸ§ª å¹³æ»‘é€šé“æ‰©å±•æµ‹è¯•å®Œæˆ")
    print("=" * 60)

if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    try:
        test_smooth_expansion()
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc() 