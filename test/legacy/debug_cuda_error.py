#!/usr/bin/env python3
"""
CUDAé”™è¯¯æ·±åº¦è°ƒè¯•è„šæœ¬
æ‰¾å‡ºçœŸæ­£çš„é—®é¢˜æ‰€åœ¨ï¼Œè€Œä¸æ˜¯ç®€å•ç¦ç”¨åŠŸèƒ½
"""

import torch
import sys
import os
import traceback
import warnings

# Add the project root to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def check_cuda_environment():
    """æ£€æŸ¥CUDAç¯å¢ƒçŠ¶æ€"""
    print("ğŸ” CUDAç¯å¢ƒæ£€æŸ¥:")
    print(f"   PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"   CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
        
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            mem_total = props.total_memory / 1024**3
            mem_alloc = torch.cuda.memory_allocated(i) / 1024**3
            mem_reserved = torch.cuda.memory_reserved(i) / 1024**3
            
            print(f"   GPU {i}: {props.name}")
            print(f"     æ€»å†…å­˜: {mem_total:.2f}GB")
            print(f"     å·²åˆ†é…: {mem_alloc:.2f}GB")
            print(f"     å·²ä¿ç•™: {mem_reserved:.2f}GB")
            print(f"     è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")

def test_basic_operations():
    """æµ‹è¯•åŸºæœ¬CUDAæ“ä½œ"""
    print("\nğŸ§ª åŸºæœ¬CUDAæ“ä½œæµ‹è¯•:")
    
    try:
        # 1. åŸºæœ¬å¼ é‡æ“ä½œ
        x = torch.randn(4, 16, 32, 32, device='cuda')
        y = torch.randn(4, 16, 32, 32, device='cuda')
        z = x + y
        print("âœ… åŸºæœ¬å¼ é‡è¿ç®—æ­£å¸¸")
        
        # 2. åŸºæœ¬å·ç§¯æ“ä½œ
        conv = torch.nn.Conv2d(16, 32, 3, padding=1).cuda()
        out = conv(x)
        print("âœ… åŸºæœ¬å·ç§¯æ“ä½œæ­£å¸¸")
        
        # 3. åˆ†ç»„å·ç§¯ï¼ˆdepthwiseï¼‰
        dw_conv = torch.nn.Conv2d(16, 16, 3, padding=1, groups=16).cuda()
        dw_out = dw_conv(x)
        print("âœ… åˆ†ç»„å·ç§¯æ“ä½œæ­£å¸¸")
        
        # 4. å†…å­˜æ¸…ç†
        del x, y, z, out, dw_out
        torch.cuda.empty_cache()
        print("âœ… å†…å­˜æ¸…ç†æ­£å¸¸")
        
        return True
        
    except Exception as e:
        print(f"âŒ åŸºæœ¬æ“ä½œå¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_sepconv_step_by_step():
    """é€æ­¥æµ‹è¯•åˆ†ç¦»å·ç§¯ï¼Œæ‰¾å‡ºç¡®åˆ‡çš„é—®é¢˜ç‚¹"""
    print("\nğŸ”¬ åˆ†ç¦»å·ç§¯é€æ­¥è°ƒè¯•:")
    
    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ® - ä½¿ç”¨è¾ƒå°çš„å°ºå¯¸
        B, C, H, W = 2, 8, 16, 16
        print(f"   æµ‹è¯•å°ºå¯¸: B={B}, C={C}, H={H}, W={W}")
        
        # Step 1: åˆ›å»ºè¾“å…¥å¼ é‡
        x = torch.randn(B, C, H, W, device='cuda', dtype=torch.float32)
        print(f"âœ… è¾“å…¥å¼ é‡åˆ›å»ºæˆåŠŸ: {x.shape}, device={x.device}, dtype={x.dtype}")
        print(f"   å†…å­˜æ£€æŸ¥: {torch.cuda.memory_allocated()/1024**2:.1f}MB")
        
        # Step 2: åˆ›å»ºæ·±åº¦å·ç§¯æƒé‡
        dw_weight = torch.randn(C, 1, 3, 3, device='cuda', dtype=torch.float32)
        print(f"âœ… æ·±åº¦å·ç§¯æƒé‡åˆ›å»ºæˆåŠŸ: {dw_weight.shape}")
        
        # Step 3: åˆ›å»ºç‚¹å·ç§¯æƒé‡
        C_out = 16
        pw_weight = torch.randn(C_out, C, 1, 1, device='cuda', dtype=torch.float32)
        print(f"âœ… ç‚¹å·ç§¯æƒé‡åˆ›å»ºæˆåŠŸ: {pw_weight.shape}")
        
        # Step 4: æ‰§è¡Œæ·±åº¦å·ç§¯
        print("\nğŸ“ æ‰§è¡Œæ·±åº¦å·ç§¯...")
        dw_out = torch.nn.functional.conv2d(
            x, dw_weight, bias=None, stride=1, 
            padding=1, groups=C
        )
        print(f"âœ… æ·±åº¦å·ç§¯æˆåŠŸ: {dw_out.shape}")
        print(f"   å†…å­˜æ£€æŸ¥: {torch.cuda.memory_allocated()/1024**2:.1f}MB")
        
        # Step 5: æ‰§è¡Œç‚¹å·ç§¯
        print("\nğŸ“ æ‰§è¡Œç‚¹å·ç§¯...")
        pw_out = torch.nn.functional.conv2d(dw_out, pw_weight, bias=None)
        print(f"âœ… ç‚¹å·ç§¯æˆåŠŸ: {pw_out.shape}")
        print(f"   å†…å­˜æ£€æŸ¥: {torch.cuda.memory_allocated()/1024**2:.1f}MB")
        
        # Step 6: æµ‹è¯•å¸¦biasçš„ç‚¹å·ç§¯
        print("\nğŸ“ æµ‹è¯•å¸¦biasçš„ç‚¹å·ç§¯...")
        bias = torch.randn(C_out, device='cuda', dtype=torch.float32)
        pw_out_bias = torch.nn.functional.conv2d(dw_out, pw_weight, bias=bias)
        print(f"âœ… å¸¦biasçš„ç‚¹å·ç§¯æˆåŠŸ: {pw_out_bias.shape}")
        
        # Step 7: æ¸…ç†å†…å­˜
        del x, dw_weight, pw_weight, dw_out, pw_out, bias, pw_out_bias
        torch.cuda.empty_cache()
        print("âœ… å†…å­˜æ¸…ç†å®Œæˆ")
        
        return True
        
    except Exception as e:
        print(f"âŒ åˆ†ç¦»å·ç§¯å¤±è´¥: {e}")
        print(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
        traceback.print_exc()
        
        # æ‰“å°è¯¦ç»†çš„tensorä¿¡æ¯
        import gc
        for obj in gc.get_objects():
            if torch.is_tensor(obj) and obj.is_cuda:
                print(f"   æœªæ¸…ç†çš„tensor: {obj.shape}, {obj.dtype}, {obj.device}")
        
        return False

def test_triton_sepconv():
    """æµ‹è¯•Tritonåˆ†ç¦»å·ç§¯çš„å…·ä½“å®ç°"""
    print("\nâš¡ Tritonåˆ†ç¦»å·ç§¯è°ƒè¯•:")
    
    try:
        from neuroexapt.kernels.sepconv_triton import sepconv_forward_generic, TRITON_AVAILABLE, _TRITON_DISABLED
        
        print(f"   Tritonå¯ç”¨: {TRITON_AVAILABLE}")
        print(f"   Tritonç¦ç”¨: {_TRITON_DISABLED}")
        
        if not TRITON_AVAILABLE:
            print("âš ï¸ Tritonä¸å¯ç”¨ï¼Œå°†æµ‹è¯•PyTorch fallbackè·¯å¾„")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        B, C, H, W = 2, 8, 16, 16
        x = torch.randn(B, C, H, W, device='cuda', dtype=torch.float32)
        dw_weight = torch.randn(C, 1, 3, 3, device='cuda', dtype=torch.float32)
        pw_weight = torch.randn(16, C, 1, 1, device='cuda', dtype=torch.float32)
        
        print(f"   è¾“å…¥å½¢çŠ¶: x={x.shape}, dw={dw_weight.shape}, pw={pw_weight.shape}")
        
        # ç¡®ä¿å¼ é‡æ˜¯è¿ç»­çš„
        if not x.is_contiguous():
            x = x.contiguous()
            print("   è¾“å…¥å¼ é‡å·²è½¬ä¸ºè¿ç»­")
        
        if not dw_weight.is_contiguous():
            dw_weight = dw_weight.contiguous()
            print("   æ·±åº¦æƒé‡å·²è½¬ä¸ºè¿ç»­")
            
        if not pw_weight.is_contiguous():
            pw_weight = pw_weight.contiguous()
            print("   ç‚¹æƒé‡å·²è½¬ä¸ºè¿ç»­")
        
        # æ‰§è¡Œsepconv
        print("\nğŸ“ æ‰§è¡ŒTriton/PyTorch sepconv...")
        result = sepconv_forward_generic(x, dw_weight, pw_weight, bias=None)
        print(f"âœ… SepconvæˆåŠŸ: {result.shape}")
        
        # æµ‹è¯•å¸¦bias
        bias = torch.randn(16, device='cuda', dtype=torch.float32)
        result_bias = sepconv_forward_generic(x, dw_weight, pw_weight, bias=bias)
        print(f"âœ… å¸¦biasçš„SepconvæˆåŠŸ: {result_bias.shape}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Triton sepconvå¤±è´¥: {e}")
        print(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
        traceback.print_exc()
        return False

def test_operations_integration():
    """æµ‹è¯•operations.pyä¸­çš„SepConvé›†æˆ"""
    print("\nğŸ§© SepConvæ¨¡å—é›†æˆæµ‹è¯•:")
    
    try:
        from neuroexapt.core.operations import SepConv
        
        # åˆ›å»ºSepConvæ¨¡å—
        sepconv_module = SepConv(8, 16, 3, 1, 1, affine=True).cuda()
        print("âœ… SepConvæ¨¡å—åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºè¾“å…¥
        x = torch.randn(2, 8, 16, 16, device='cuda')
        print(f"   è¾“å…¥å½¢çŠ¶: {x.shape}")
        
        # æ‰§è¡Œå‰å‘ä¼ æ’­
        print("\nğŸ“ æ‰§è¡ŒSepConvå‰å‘ä¼ æ’­...")
        output = sepconv_module(x)
        print(f"âœ… SepConvå‰å‘ä¼ æ’­æˆåŠŸ: {output.shape}")
        
        # æµ‹è¯•æ¢¯åº¦
        print("\nğŸ“ æµ‹è¯•æ¢¯åº¦è®¡ç®—...")
        loss = output.sum()
        loss.backward()
        print("âœ… æ¢¯åº¦è®¡ç®—æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ SepConvæ¨¡å—æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_with_exact_error_conditions():
    """ä½¿ç”¨å¯¼è‡´é”™è¯¯çš„ç¡®åˆ‡æ¡ä»¶è¿›è¡Œæµ‹è¯•"""
    print("\nğŸ¯ åŸå§‹é”™è¯¯æ¡ä»¶é‡ç°æµ‹è¯•:")
    
    try:
        # ä½¿ç”¨åŸå§‹çš„å‚æ•°
        from neuroexapt.core.model import Network
        
        print("   åˆ›å»ºåŸå§‹å¤§å°çš„ç½‘ç»œ...")
        model = Network(C=32, num_classes=10, layers=16, potential_layers=4).cuda()
        print(f"âœ… ç½‘ç»œåˆ›å»ºæˆåŠŸ: {sum(p.numel() for p in model.parameters())} å‚æ•°")
        
        # åˆ›å»ºåŸå§‹å¤§å°çš„è¾“å…¥
        batch_size = 64
        x = torch.randn(batch_size, 3, 32, 32, device='cuda')
        print(f"   è¾“å…¥å½¢çŠ¶: {x.shape}")
        
        # æ‰§è¡Œå‰å‘ä¼ æ’­
        print("\nğŸ“ æ‰§è¡Œç½‘ç»œå‰å‘ä¼ æ’­...")
        
        # ç›‘æ§å†…å­˜ä½¿ç”¨
        torch.cuda.reset_peak_memory_stats()
        initial_memory = torch.cuda.memory_allocated()
        
        output = model(x)
        
        peak_memory = torch.cuda.max_memory_allocated()
        final_memory = torch.cuda.memory_allocated()
        
        print(f"âœ… ç½‘ç»œå‰å‘ä¼ æ’­æˆåŠŸ: {output.shape}")
        print(f"   å†…å­˜ä½¿ç”¨: åˆå§‹{initial_memory/1024**2:.1f}MB -> å³°å€¼{peak_memory/1024**2:.1f}MB -> æœ€ç»ˆ{final_memory/1024**2:.1f}MB")
        
        return True
        
    except Exception as e:
        print(f"âŒ åŸå§‹æ¡ä»¶æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        
        # æ£€æŸ¥å¯èƒ½çš„åŸå› 
        print("\nğŸ” å¯èƒ½çš„å¤±è´¥åŸå› åˆ†æ:")
        print(f"   å½“å‰GPUå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated()/1024**2:.1f}MB")
        print(f"   å½“å‰GPUå†…å­˜ç¼“å­˜: {torch.cuda.memory_reserved()/1024**2:.1f}MB")
        
        props = torch.cuda.get_device_properties(0)
        total_memory = props.total_memory / 1024**2
        print(f"   GPUæ€»å†…å­˜: {total_memory:.1f}MB")
        
        if torch.cuda.memory_allocated() / props.total_memory > 0.8:
            print("âš ï¸ å¯èƒ½æ˜¯å†…å­˜ä¸è¶³å¯¼è‡´çš„é—®é¢˜")
        
        return False

def main():
    """ä¸»è°ƒè¯•å‡½æ•°"""
    print("ğŸ” CUDAé”™è¯¯æ·±åº¦è°ƒè¯•")
    print("=" * 60)
    print("ç›®æ ‡: æ‰¾å‡ºçœŸæ­£çš„é—®é¢˜æ‰€åœ¨ï¼Œä¿æŒTritonä¼˜åŒ–")
    print("=" * 60)
    
    # è®¾ç½®è­¦å‘Šè¿‡æ»¤
    warnings.filterwarnings('error', category=UserWarning)
    
    test_results = {}
    
    # 1. ç¯å¢ƒæ£€æŸ¥
    check_cuda_environment()
    
    # 2. åŸºæœ¬æ“ä½œæµ‹è¯•
    test_results['basic_ops'] = test_basic_operations()
    
    # 3. åˆ†ç¦»å·ç§¯é€æ­¥æµ‹è¯•
    test_results['sepconv_steps'] = test_sepconv_step_by_step()
    
    # 4. Tritonåˆ†ç¦»å·ç§¯æµ‹è¯•
    test_results['triton_sepconv'] = test_triton_sepconv()
    
    # 5. SepConvæ¨¡å—é›†æˆæµ‹è¯•
    test_results['sepconv_module'] = test_operations_integration()
    
    # 6. åŸå§‹é”™è¯¯æ¡ä»¶æµ‹è¯•
    test_results['original_conditions'] = test_with_exact_error_conditions()
    
    # ç»“æœåˆ†æ
    print("\n" + "=" * 60)
    print("ğŸ“Š è°ƒè¯•ç»“æœæ€»ç»“:")
    print("=" * 60)
    
    passed_tests = sum(test_results.values())
    total_tests = len(test_results)
    
    for test_name, result in test_results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"   {test_name}: {status}")
    
    print(f"\né€šè¿‡ç‡: {passed_tests}/{total_tests} ({passed_tests/total_tests*100:.1f}%)")
    
    # æ ¹æ®ç»“æœç»™å‡ºå»ºè®®
    if all(test_results.values()):
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼åŸå§‹CUDAé”™è¯¯å¯èƒ½å·²ç»è§£å†³ã€‚")
        print("ğŸ’¡ å»ºè®®: å°è¯•è¿è¡ŒåŸå§‹çš„åˆ†ç¦»è®­ç»ƒå‘½ä»¤")
        print("   python examples/basic_classification.py --mode separated --epochs 10 --batch_size 32")
    
    elif test_results['basic_ops'] and test_results['sepconv_steps']:
        print("\nâš ï¸ åŸºæœ¬æ“ä½œæ­£å¸¸ï¼Œä½†é«˜çº§åŠŸèƒ½æœ‰é—®é¢˜")
        print("ğŸ’¡ å»ºè®®: æ£€æŸ¥Tritonå†…æ ¸å®ç°æˆ–æ¨¡å—é›†æˆé—®é¢˜")
    
    elif not test_results['basic_ops']:
        print("\nâŒ åŸºæœ¬CUDAæ“ä½œå¤±è´¥")
        print("ğŸ’¡ å»ºè®®: æ£€æŸ¥CUDAé©±åŠ¨ã€PyTorchå®‰è£…æˆ–GPUç¡¬ä»¶é—®é¢˜")
    
    else:
        print("\nğŸ” éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥")
        
        # è¯†åˆ«å¤±è´¥æ¨¡å¼
        if not test_results['original_conditions']:
            print("ğŸ’¡ å¯èƒ½æ˜¯å¤§å‹ç½‘ç»œçš„å†…å­˜æˆ–è®¡ç®—é—®é¢˜")
        if not test_results['triton_sepconv']:
            print("ğŸ’¡ å¯èƒ½æ˜¯Tritonå†…æ ¸çš„å…·ä½“å®ç°é—®é¢˜")
        if not test_results['sepconv_module']:
            print("ğŸ’¡ å¯èƒ½æ˜¯PyTorchæ¨¡å—é›†æˆé—®é¢˜")

if __name__ == "__main__":
    main() 