#!/usr/bin/env python3
"""
Enhanced DNM Framework Debug Test
æµ‹è¯•å¢å¼ºDNMæ¡†æ¶çš„è°ƒè¯•è¾“å‡ºåŠŸèƒ½
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from collections import defaultdict
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '.'))

from neuroexapt.core.enhanced_dnm_framework import EnhancedDNMFramework

def create_simple_model():
    """åˆ›å»ºç®€å•çš„æµ‹è¯•æ¨¡å‹"""
    return nn.Sequential(
        nn.Linear(784, 128),
        nn.ReLU(),
        nn.Linear(128, 64),
        nn.ReLU(), 
        nn.Linear(64, 10)
    )

def generate_mock_data(batch_size=32):
    """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®"""
    x = torch.randn(batch_size, 784)
    y = torch.randint(0, 10, (batch_size,))
    return x, y

def collect_activations_and_gradients(model, x, y):
    """æ”¶é›†æ¿€æ´»å€¼å’Œæ¢¯åº¦ç”¨äºåˆ†æ"""
    activations = {}
    gradients = {}
    
    # æ·»åŠ hookæ¥æ”¶é›†æ¿€æ´»å€¼
    hooks = []
    layer_names = []
    
    def make_activation_hook(name):
        def hook(module, input, output):
            activations[name] = output.detach()
        return hook
    
    # æ³¨å†Œå‰å‘hook
    for name, module in model.named_modules():
        if isinstance(module, (nn.Linear, nn.Conv2d)):
            layer_names.append(name)
            hooks.append(module.register_forward_hook(make_activation_hook(name)))
    
    # å‰å‘ä¼ æ’­
    model.train()
    output = model(x)
    loss = nn.CrossEntropyLoss()(output, y)
    
    # åå‘ä¼ æ’­
    loss.backward()
    
    # æ”¶é›†æ¢¯åº¦
    for name, module in model.named_modules():
        if isinstance(module, (nn.Linear, nn.Conv2d)) and name in layer_names:
            if hasattr(module, 'weight') and module.weight.grad is not None:
                gradients[name] = module.weight.grad.detach().clone()
    
    # æ¸…ç†hooks
    for hook in hooks:
        hook.remove()
    
    return activations, gradients, loss.item()

def test_enhanced_dnm_with_debug():
    """æµ‹è¯•å¢å¼ºDNMæ¡†æ¶çš„è°ƒè¯•è¾“å‡º"""
    print("=" * 80)
    print("ğŸ§¬ å¢å¼ºDNMæ¡†æ¶è°ƒè¯•æµ‹è¯•")
    print("=" * 80)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ“± åˆ›å»ºæµ‹è¯•æ¨¡å‹...")
    model = create_simple_model().to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # åˆ›å»ºDNMæ¡†æ¶
    print("\nğŸ§¬ åˆå§‹åŒ–å¢å¼ºDNMæ¡†æ¶...")
    dnm_framework = EnhancedDNMFramework()
    
    # æ¨¡æ‹Ÿè®­ç»ƒå¾ªç¯
    print("\nğŸš€ å¼€å§‹æ¨¡æ‹Ÿè®­ç»ƒ...")
    for epoch in range(5):
        print(f"\n" + "="*60)
        print(f"ğŸ“Š Epoch {epoch + 1}/5")
        print("="*60)
        
        # ç”Ÿæˆæ•°æ®
        x, y = generate_mock_data()
        x, y = x.to(device), y.to(device)
        
        # æ”¶é›†æ¿€æ´»å€¼å’Œæ¢¯åº¦
        print(f"\nğŸ” æ”¶é›†æ¿€æ´»å€¼å’Œæ¢¯åº¦...")
        activations, gradients, loss_value = collect_activations_and_gradients(model, x, y)
        
        print(f"ğŸ“Š è®­ç»ƒæŸå¤±: {loss_value:.4f}")
        print(f"ğŸ“Š æ”¶é›†åˆ° {len(activations)} å±‚æ¿€æ´»å€¼, {len(gradients)} å±‚æ¢¯åº¦")
        
        # æ›´æ–°æ€§èƒ½å†å²
        accuracy = np.random.uniform(0.75, 0.95)  # æ¨¡æ‹Ÿå‡†ç¡®ç‡
        dnm_framework.update_performance_history(accuracy)
        print(f"ğŸ“Š æ¨¡æ‹Ÿå‡†ç¡®ç‡: {accuracy:.4f}")
        
        # å‡†å¤‡ä¸Šä¸‹æ–‡
        context = {
            'epoch': epoch,
            'activations': activations,
            'gradients': gradients,
            'performance_history': dnm_framework.performance_history,
            'loss': loss_value,
            'accuracy': accuracy
        }
        
        # æ‰§è¡Œå½¢æ€å‘ç”Ÿ
        print(f"\nğŸ§¬ æ‰§è¡Œå½¢æ€å‘ç”Ÿæ£€æŸ¥...")
        results = dnm_framework.execute_morphogenesis(model, context)
        
        # è¾“å‡ºç»“æœ
        print(f"\nğŸ“‹ å½¢æ€å‘ç”Ÿç»“æœ:")
        print(f"  - æ¨¡å‹æ˜¯å¦ä¿®æ”¹: {results['model_modified']}")
        print(f"  - æ–°å¢å‚æ•°: {results['parameters_added']:,}")
        print(f"  - å½¢æ€å‘ç”Ÿç±»å‹: {results['morphogenesis_type']}")
        print(f"  - è§¦å‘åŸå› æ•°é‡: {len(results.get('trigger_reasons', []))}")
        
        if results['model_modified']:
            print(f"  - å†³ç­–ç½®ä¿¡åº¦: {results.get('decision_confidence', 0):.3f}")
            print(f"  - é¢„æœŸæ”¹è¿›: {results.get('expected_improvement', 0):.3f}")
            model = results['new_model']
            print(f"âœ… æ¨¡å‹å·²æ›´æ–°ï¼")
        else:
            print(f"âŒ æœªè§¦å‘å½¢æ€å‘ç”Ÿ")
        
        # ä¼˜åŒ–å™¨æ­¥éª¤
        optimizer.zero_grad()
        
        print(f"\n" + "-"*50)
    
    # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
    print(f"\n" + "="*80)
    print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡")
    print("="*80)
    
    summary = dnm_framework.get_morphogenesis_summary()
    print(f"æ€»å½¢æ€å‘ç”Ÿäº‹ä»¶: {summary['total_events']}")
    print(f"æ€»æ–°å¢å‚æ•°: {summary['total_parameters_added']:,}")
    print(f"å½¢æ€å‘ç”Ÿç±»å‹åˆ†å¸ƒ: {summary['morphogenesis_types']}")
    
    if summary['events']:
        print(f"\nè¯¦ç»†äº‹ä»¶åˆ—è¡¨:")
        for i, event in enumerate(summary['events'], 1):
            print(f"  {i}. Epoch {event['epoch']}: {event['type']} "
                  f"(å‚æ•°+{event['parameters_added']:,}, ç½®ä¿¡åº¦{event['confidence']:.3f})")
    
    print(f"\nâœ… è°ƒè¯•æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    test_enhanced_dnm_with_debug()