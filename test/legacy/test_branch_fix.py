#!/usr/bin/env python3
"""
æµ‹è¯•å¤šåˆ†æ”¯æ¶æ„ä¿®å¤
éªŒè¯grow_widthæ“ä½œåçš„forward/backwardæ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import sys
import os

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥ä¿®å¤åçš„ASO-SEç»„ä»¶
from examples.aso_se_classification import GrowableConvBlock, GrowingNetwork

def test_branch_forward_backward():
    """æµ‹è¯•åˆ†æ”¯å‰å‘å’Œåå‘ä¼ æ’­"""
    print("ğŸ§ª æµ‹è¯•å¤šåˆ†æ”¯æ¶æ„çš„forward/backward...")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæµ‹è¯•å—
    block = GrowableConvBlock(block_id=0, in_channels=64, out_channels=128, stride=1).to(device)
    
    # æ·»åŠ ä¸€äº›åˆ†æ”¯
    print("æ·»åŠ åˆ†æ”¯...")
    block.add_branch()
    block.add_branch()
    
    print(f"åˆå§‹åˆ†æ”¯æ•°: {len(block.branches)}")
    print(f"åˆå§‹è¾“å‡ºé€šé“: {block.out_channels}")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    x = torch.randn(4, 64, 32, 32, device=device, requires_grad=True)
    
    # æµ‹è¯•åˆå§‹å‰å‘ä¼ æ’­
    print("\n1ï¸âƒ£ æµ‹è¯•åˆå§‹å‰å‘ä¼ æ’­...")
    try:
        out1 = block(x)
        print(f"âœ… åˆå§‹å‰å‘ä¼ æ’­æˆåŠŸ: {x.shape} -> {out1.shape}")
        
        # æµ‹è¯•åå‘ä¼ æ’­
        loss1 = out1.sum()
        loss1.backward()
        print("âœ… åˆå§‹åå‘ä¼ æ’­æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ åˆå§‹æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # æ¸…é™¤æ¢¯åº¦
    x.grad = None
    
    # æ‰§è¡Œé€šé“æ‰©å±•
    print("\n2ï¸âƒ£ æ‰§è¡Œé€šé“æ‰©å±•...")
    try:
        new_channels = int(block.out_channels * 1.5)  # æ‰©å±•1.5å€
        success = block.expand_channels(new_channels)
        
        if success:
            print(f"âœ… é€šé“æ‰©å±•æˆåŠŸ: {128} -> {new_channels}")
            print(f"æ–°åˆ†æ”¯æ•°: {len(block.branches)}")
        else:
            print("âŒ é€šé“æ‰©å±•å¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ é€šé“æ‰©å±•å¼‚å¸¸: {e}")
        return False
    
    # æµ‹è¯•æ‰©å±•åçš„å‰å‘ä¼ æ’­
    print("\n3ï¸âƒ£ æµ‹è¯•æ‰©å±•åå‰å‘ä¼ æ’­...")
    try:
        x_new = torch.randn(4, 64, 32, 32, device=device, requires_grad=True)
        out2 = block(x_new)
        print(f"âœ… æ‰©å±•åå‰å‘ä¼ æ’­æˆåŠŸ: {x_new.shape} -> {out2.shape}")
        
        # æµ‹è¯•åå‘ä¼ æ’­ - è¿™æ˜¯å…³é”®æµ‹è¯•ï¼
        loss2 = out2.sum()
        loss2.backward()
        print("âœ… æ‰©å±•ååå‘ä¼ æ’­æˆåŠŸ - CUDAé”™è¯¯å·²ä¿®å¤ï¼")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ‰©å±•åæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_network_growth():
    """æµ‹è¯•å®Œæ•´ç½‘ç»œçš„ç”Ÿé•¿"""
    print("\nğŸŒ æµ‹è¯•å®Œæ•´ç½‘ç»œç”Ÿé•¿...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆ›å»ºå°å‹æµ‹è¯•ç½‘ç»œ
    network = GrowingNetwork(
        initial_channels=32,
        num_classes=10,
        initial_depth=3
    ).to(device)
    
    print(f"åˆå§‹å‚æ•°æ•°é‡: {sum(p.numel() for p in network.parameters()):,}")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    x = torch.randn(2, 3, 32, 32, device=device)
    y = torch.randint(0, 10, (2,), device=device)
    
    # æµ‹è¯•åˆå§‹çŠ¶æ€
    print("\næµ‹è¯•åˆå§‹ç½‘ç»œ...")
    try:
        logits = network(x)
        loss = F.cross_entropy(logits, y)
        loss.backward()
        print("âœ… åˆå§‹ç½‘ç»œè®­ç»ƒæˆåŠŸ")
    except Exception as e:
        print(f"âŒ åˆå§‹ç½‘ç»œå¤±è´¥: {e}")
        return False
    
    # æ‰§è¡Œå®½åº¦ç”Ÿé•¿
    print("\næ‰§è¡Œå®½åº¦ç”Ÿé•¿...")
    try:
        success = network.grow_width(expansion_factor=1.5)
        if success:
            print(f"âœ… å®½åº¦ç”Ÿé•¿æˆåŠŸ")
            print(f"æ–°å‚æ•°æ•°é‡: {sum(p.numel() for p in network.parameters()):,}")
        else:
            print("âŒ å®½åº¦ç”Ÿé•¿å¤±è´¥")
            return False
    except Exception as e:
        print(f"âŒ å®½åº¦ç”Ÿé•¿å¼‚å¸¸: {e}")
        return False
    
    # æµ‹è¯•ç”Ÿé•¿åçš„è®­ç»ƒ
    print("\næµ‹è¯•ç”Ÿé•¿åç½‘ç»œ...")
    try:
        network.zero_grad()  # æ¸…é™¤æ¢¯åº¦
        x_new = torch.randn(2, 3, 32, 32, device=device)
        y_new = torch.randint(0, 10, (2,), device=device)
        
        logits = network(x_new)
        loss = F.cross_entropy(logits, y_new)
        loss.backward()
        print("âœ… ç”Ÿé•¿åç½‘ç»œè®­ç»ƒæˆåŠŸ - å¤šåˆ†æ”¯CUDAé”™è¯¯å·²ä¿®å¤ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ ç”Ÿé•¿åç½‘ç»œå¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ”§ å¤šåˆ†æ”¯æ¶æ„CUDAé”™è¯¯ä¿®å¤éªŒè¯")
    print("=" * 60)
    
    # å¯ç”¨è°ƒè¯•æ¨¡å¼
    torch.autograd.set_detect_anomaly(True)
    
    # æµ‹è¯•1: åˆ†æ”¯çº§åˆ«æµ‹è¯•
    test1_success = test_branch_forward_backward()
    
    # æµ‹è¯•2: ç½‘ç»œçº§åˆ«æµ‹è¯•
    test2_success = test_network_growth()
    
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"åˆ†æ”¯çº§åˆ«æµ‹è¯•: {'âœ… é€šè¿‡' if test1_success else 'âŒ å¤±è´¥'}")
    print(f"ç½‘ç»œçº§åˆ«æµ‹è¯•: {'âœ… é€šè¿‡' if test2_success else 'âŒ å¤±è´¥'}")
    
    if test1_success and test2_success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¤šåˆ†æ”¯CUDAé”™è¯¯å·²æˆåŠŸä¿®å¤ï¼")
        print("ğŸ’¡ ä¿®å¤è¦ç‚¹:")
        print("   1. ä½¿ç”¨learnable projectionæ›¿ä»£F.padé›¶å¡«å……")
        print("   2. å®‰å…¨çš„å‚æ•°è¿ç§»å’Œåˆ†æ”¯é‡å»º")
        print("   3. å¤±è´¥æ—¶çš„ä¼˜é›…é™çº§å¤„ç†")
        print("   4. é¿å…åœ¨éå†æ—¶ç›´æ¥ä¿®æ”¹åˆ—è¡¨")
        return True
    else:
        print("\nâš ï¸ ä»æœ‰é—®é¢˜éœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥")
        return False

if __name__ == "__main__":
    main() 