"""
é€šé“æ‰©å±• vs æ·±åº¦å¢åŠ å¯¹æ¯”æµ‹è¯•
æ¯”è¾ƒä¸¤ç§æ‰©å±•æ–¹å¼çš„æ•ˆæœï¼Œç¡®å®šå“ªç§æ›´é€‚åˆæå‡å‡†ç¡®ç‡
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import numpy as np
import sys
import os
import gc
import copy

# Add the current directory to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from neuroexapt.core.smart_channel_expander import SmartChannelExpander
from neuroexapt.core.depth_expansion_operators import DepthExpansionOperator

class ComparisonCNN(nn.Module):
    """ç”¨äºå¯¹æ¯”æµ‹è¯•çš„CNN"""
    def __init__(self):
        super(ComparisonCNN, self).__init__()
        self.features = nn.Sequential(
            # Block 1
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            
            # Block 2
            nn.Conv2d(32, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            
            # Block 3
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(64, 32),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(32, 10)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

def load_test_data(batch_size=64):
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])
    
    dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    
    # ä½¿ç”¨å‰2000ä¸ªæ ·æœ¬è¿›è¡Œæµ‹è¯•
    indices = list(range(2000))
    subset = torch.utils.data.Subset(dataset, indices)
    
    dataloader = DataLoader(subset, batch_size=batch_size, shuffle=True)
    return dataloader

def evaluate_model(model, dataloader, device):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    correct = 0
    total = 0
    total_loss = 0
    criterion = nn.CrossEntropyLoss()
    
    with torch.no_grad():
        for data, targets in dataloader:
            data, targets = data.to(device), targets.to(device)
            try:
                outputs = model(data)
                loss = criterion(outputs, targets)
                total_loss += loss.item()
                
                _, predicted = torch.max(outputs.data, 1)
                total += targets.size(0)
                correct += (predicted == targets).sum().item()
            except Exception as e:
                print(f"âŒ Evaluation failed: {e}")
                return None, None
    
    accuracy = 100.0 * correct / total if total > 0 else 0.0
    avg_loss = total_loss / len(dataloader) if len(dataloader) > 0 else 0.0
    
    model.train()
    return accuracy, avg_loss

def train_brief(model, dataloader, device, num_epochs=5):
    """ç®€çŸ­è®­ç»ƒ"""
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    for epoch in range(num_epochs):
        epoch_loss = 0
        batches_processed = 0
        
        for batch_idx, (data, targets) in enumerate(dataloader):
            data, targets = data.to(device), targets.to(device)
            
            optimizer.zero_grad()
            outputs = model(data)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            batches_processed += 1
            
            if batch_idx >= 15:  # é™åˆ¶æ¯ä¸ªepochçš„batchæ•°
                break
        
        avg_loss = epoch_loss / batches_processed if batches_processed > 0 else 0
        print(f"   Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}")
    
    return model

def test_expansion_method(model, dataloader, device, method_name, expander):
    """æµ‹è¯•å•ä¸€æ‰©å±•æ–¹æ³•"""
    print(f"\nğŸ§ª Testing {method_name}...")
    
    # å…‹éš†æ¨¡å‹ä»¥é¿å…äº’ç›¸å½±å“
    test_model = copy.deepcopy(model).to(device)
    
    # åˆå§‹è®­ç»ƒ
    print(f"   ğŸ“š Initial training...")
    test_model = train_brief(test_model, dataloader, device, num_epochs=3)
    
    # è¯„ä¼°åˆå§‹æ€§èƒ½
    initial_accuracy, initial_loss = evaluate_model(test_model, dataloader, device)
    initial_params = sum(p.numel() for p in test_model.parameters())
    
    print(f"   ğŸ“Š Initial Performance:")
    print(f"      Accuracy: {initial_accuracy:.2f}%")
    print(f"      Loss: {initial_loss:.4f}")
    print(f"      Parameters: {initial_params:,}")
    
    # åº”ç”¨æ‰©å±•
    test_state = {
        'current_performance': initial_accuracy / 100.0,
        'val_accuracy': initial_accuracy,
        'epoch': 5
    }
    
    print(f"   ğŸ”„ Applying {method_name}...")
    evolved_model = expander(test_model, test_state)
    
    if evolved_model is not None:
        # æ£€æŸ¥æ‰©å±•åçš„æ¶æ„
        expanded_params = sum(p.numel() for p in evolved_model.parameters())
        param_increase = expanded_params - initial_params
        param_increase_pct = (param_increase / initial_params) * 100
        
        print(f"   âœ… {method_name} successful!")
        print(f"      Parameters: {initial_params:,} â†’ {expanded_params:,}")
        print(f"      Increase: +{param_increase:,} (+{param_increase_pct:.1f}%)")
        
        # ç«‹å³è¯„ä¼°
        immediate_accuracy, immediate_loss = evaluate_model(evolved_model, dataloader, device)
        immediate_change = immediate_accuracy - initial_accuracy
        
        print(f"   ğŸ“Š Immediate Post-Expansion:")
        print(f"      Accuracy: {immediate_accuracy:.2f}% ({immediate_change:+.2f}%)")
        print(f"      Loss: {immediate_loss:.4f}")
        
        # çŸ­æš‚é‡æ–°è®­ç»ƒ
        print(f"   ğŸ“š Brief retraining...")
        evolved_model = train_brief(evolved_model, dataloader, device, num_epochs=3)
        
        # æœ€ç»ˆè¯„ä¼°
        final_accuracy, final_loss = evaluate_model(evolved_model, dataloader, device)
        final_change = final_accuracy - initial_accuracy
        recovery = final_accuracy - immediate_accuracy
        
        print(f"   ğŸ“Š Final Performance:")
        print(f"      Accuracy: {final_accuracy:.2f}% ({final_change:+.2f}%)")
        print(f"      Loss: {final_loss:.4f}")
        print(f"      Recovery: {recovery:+.2f}%")
        
        return {
            'method': method_name,
            'success': True,
            'initial_accuracy': initial_accuracy,
            'immediate_accuracy': immediate_accuracy,
            'final_accuracy': final_accuracy,
            'immediate_change': immediate_change,
            'final_change': final_change,
            'recovery': recovery,
            'initial_params': initial_params,
            'expanded_params': expanded_params,
            'param_increase': param_increase,
            'param_increase_pct': param_increase_pct
        }
    else:
        print(f"   âŒ {method_name} failed!")
        return {
            'method': method_name,
            'success': False,
            'initial_accuracy': initial_accuracy,
            'initial_params': initial_params
        }

def test_combined_expansion():
    """å¯¹æ¯”æµ‹è¯•é€šé“æ‰©å±•å’Œæ·±åº¦å¢åŠ """
    print("=" * 80)
    print("ğŸ§ª é€šé“æ‰©å±• vs æ·±åº¦å¢åŠ å¯¹æ¯”æµ‹è¯•")
    print("=" * 80)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device: {device}")
    
    # åŠ è½½æ•°æ®
    dataloader = load_test_data(batch_size=64)
    
    # åˆ›å»ºåŸºç¡€æ¨¡å‹
    base_model = ComparisonCNN().to(device)
    
    # åˆ›å»ºæ‰©å±•å™¨
    channel_expander = SmartChannelExpander(accuracy_threshold=0.9)  # ç¡®ä¿è§¦å‘
    depth_expander = DepthExpansionOperator(min_accuracy_for_pruning=0.9)  # ç¡®ä¿è§¦å‘
    
    # æµ‹è¯•ä¸¤ç§æ–¹æ³•
    results = []
    
    # æµ‹è¯•é€šé“æ‰©å±•
    channel_result = test_expansion_method(
        base_model, dataloader, device, 
        "Channel Expansion", channel_expander
    )
    results.append(channel_result)
    
    # æ¸…ç†å†…å­˜
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # æµ‹è¯•æ·±åº¦å¢åŠ 
    depth_result = test_expansion_method(
        base_model, dataloader, device, 
        "Depth Increase", depth_expander
    )
    results.append(depth_result)
    
    # æ¯”è¾ƒç»“æœ
    print(f"\n" + "=" * 80)
    print("ğŸ“Š COMPARISON RESULTS")
    print("=" * 80)
    
    for result in results:
        if result['success']:
            print(f"\nğŸ”¸ {result['method']}:")
            print(f"   Immediate Impact: {result['immediate_change']:+.2f}%")
            print(f"   Final Improvement: {result['final_change']:+.2f}%")
            print(f"   Recovery Ability: {result['recovery']:+.2f}%")
            print(f"   Parameter Increase: +{result['param_increase_pct']:.1f}%")
            print(f"   Efficiency: {result['final_change']/result['param_increase_pct']:.3f} accuracy/param%")
        else:
            print(f"\nğŸ”¸ {result['method']}: FAILED")
    
    # ç¡®å®šæœ€ä½³æ–¹æ³•
    successful_results = [r for r in results if r['success']]
    
    if len(successful_results) >= 2:
        # æ¯”è¾ƒæŒ‡æ ‡
        channel_res = next(r for r in successful_results if 'Channel' in r['method'])
        depth_res = next(r for r in successful_results if 'Depth' in r['method'])
        
        print(f"\nğŸ† HEAD-TO-HEAD COMPARISON:")
        print(f"   Final Accuracy:")
        print(f"      Channel Expansion: {channel_res['final_change']:+.2f}%")
        print(f"      Depth Increase:    {depth_res['final_change']:+.2f}%")
        
        print(f"   Immediate Stability:")
        print(f"      Channel Expansion: {channel_res['immediate_change']:+.2f}%")
        print(f"      Depth Increase:    {depth_res['immediate_change']:+.2f}%")
        
        print(f"   Parameter Efficiency:")
        channel_efficiency = channel_res['final_change'] / channel_res['param_increase_pct']
        depth_efficiency = depth_res['final_change'] / depth_res['param_increase_pct']
        print(f"      Channel Expansion: {channel_efficiency:.4f}")
        print(f"      Depth Increase:    {depth_efficiency:.4f}")
        
        # ç»¼åˆè¯„åˆ†
        channel_score = (
            channel_res['final_change'] * 0.4 +  # æœ€ç»ˆæ”¹è¿›æƒé‡40%
            max(0, channel_res['immediate_change']) * 0.3 +  # ç«‹å³ç¨³å®šæ€§æƒé‡30%
            channel_efficiency * 10 * 0.3  # å‚æ•°æ•ˆç‡æƒé‡30%
        )
        
        depth_score = (
            depth_res['final_change'] * 0.4 +
            max(0, depth_res['immediate_change']) * 0.3 +
            depth_efficiency * 10 * 0.3
        )
        
        print(f"\nğŸ¯ RECOMMENDATION:")
        if channel_score > depth_score:
            print(f"   ğŸ¥‡ CHANNEL EXPANSION is recommended!")
            print(f"      Score: {channel_score:.2f} vs {depth_score:.2f}")
            print(f"      Reasons: Better accuracy improvement and/or stability")
        elif depth_score > channel_score:
            print(f"   ğŸ¥‡ DEPTH INCREASE is recommended!")
            print(f"      Score: {depth_score:.2f} vs {channel_score:.2f}")
            print(f"      Reasons: Better accuracy improvement and/or stability")
        else:
            print(f"   ğŸ¤ Both methods are equally effective!")
            print(f"      Scores: Channel {channel_score:.2f}, Depth {depth_score:.2f}")
        
        print(f"\nğŸ’¡ PRACTICAL INSIGHTS:")
        if abs(channel_res['immediate_change']) < abs(depth_res['immediate_change']):
            print(f"   â€¢ Channel expansion is more stable (less immediate accuracy drop)")
        else:
            print(f"   â€¢ Depth increase is more stable (less immediate accuracy drop)")
        
        if channel_res['final_change'] > depth_res['final_change']:
            print(f"   â€¢ Channel expansion provides better final accuracy improvement")
        else:
            print(f"   â€¢ Depth increase provides better final accuracy improvement")
        
        if channel_efficiency > depth_efficiency:
            print(f"   â€¢ Channel expansion is more parameter-efficient")
        else:
            print(f"   â€¢ Depth increase is more parameter-efficient")
    
    elif len(successful_results) == 1:
        winner = successful_results[0]
        print(f"\nğŸ† WINNER: {winner['method']}")
        print(f"   Only method that worked successfully!")
    
    else:
        print(f"\nâŒ Both methods failed!")
        print(f"   éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•å’Œä¼˜åŒ–")
    
    print(f"\n" + "=" * 80)
    print("ğŸ§ª å¯¹æ¯”æµ‹è¯•å®Œæˆ")
    print("=" * 80)

if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    try:
        test_combined_expansion()
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc() 