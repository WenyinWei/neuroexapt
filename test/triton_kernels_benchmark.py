#!/usr/bin/env python3
"""
NeuroExapt ç°æœ‰Tritonå†…æ ¸åŸºå‡†æµ‹è¯•

ä¸“é—¨æµ‹è¯•é¡¹ç›®ä¸­å·²å®ç°çš„Tritonä¼˜åŒ–å†…æ ¸çš„æ€§èƒ½ï¼š
1. CUDA SoftmaxSum æ‰©å±•
2. Triton åˆ†ç¦»å·ç§¯å†…æ ¸
3. Triton æ± åŒ–å†…æ ¸
4. MixedOp ä¼˜åŒ–
"""

import torch
import torch.nn.functional as F
import time
import json
import os
import sys
from typing import Dict, List, Tuple
from datetime import datetime
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def benchmark_cuda_softmax_sum():
    """åŸºå‡†æµ‹è¯•CUDA SoftmaxSumæ‰©å±•"""
    print("ğŸ”¥ æµ‹è¯• CUDA SoftmaxSum æ‰©å±•")
    print("-" * 40)
    
    try:
        from neuroexapt.cuda_ops import SoftmaxSumFunction, CUDA_AVAILABLE
        print(f"CUDAå¯ç”¨: {CUDA_AVAILABLE}")
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥CUDAæ¨¡å—: {e}")
        return {}
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"æµ‹è¯•è®¾å¤‡: {device}")
    
    # æµ‹è¯•é…ç½® - æ¨¡æ‹Ÿå…¸å‹çš„NASåœºæ™¯
    test_configs = [
        {"name": "Small", "N": 4, "B": 2, "C": 16, "H": 16, "W": 16},
        {"name": "Medium", "N": 8, "B": 4, "C": 32, "H": 32, "W": 32},
        {"name": "Large", "N": 12, "B": 8, "C": 64, "H": 64, "W": 64},
        {"name": "XLarge", "N": 16, "B": 16, "C": 128, "H": 128, "W": 128},
    ]
    
    results = {}
    warmup_runs = 5
    benchmark_runs = 20
    
    def pytorch_softmax_sum(x, logits):
        """PyTorchå‚è€ƒå®ç°"""
        weights = torch.softmax(logits, 0)
        return (x * weights.view(-1, 1, 1, 1, 1)).sum(dim=0)
    
    for config in test_configs:
        print(f"\nğŸ§ª æµ‹è¯•é…ç½®: {config['name']}")
        N, B, C, H, W = config["N"], config["B"], config["C"], config["H"], config["W"]
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        x = torch.randn(N, B, C, H, W, device=device, requires_grad=True)
        logits = torch.randn(N, device=device, requires_grad=True)
        
        print(f"è¾“å…¥å½¢çŠ¶: x={list(x.shape)}, logits={list(logits.shape)}")
        print(f"æ€»å…ƒç´ æ•°: {x.numel():,}")
        print(f"å†…å­˜ä½¿ç”¨: {x.numel() * 4 / 1024 / 1024:.1f} MB")
        
        try:
            # é¢„çƒ­
            for _ in range(warmup_runs):
                _ = pytorch_softmax_sum(x, logits)
                _ = SoftmaxSumFunction.apply(x, logits)
            
            # åŸºå‡†æµ‹è¯• PyTorch
            if device == "cuda":
                torch.cuda.synchronize()
            start_time = time.perf_counter()
            for _ in range(benchmark_runs):
                pytorch_result = pytorch_softmax_sum(x, logits)
            if device == "cuda":
                torch.cuda.synchronize()
            pytorch_time = (time.perf_counter() - start_time) / benchmark_runs * 1000
            
            # åŸºå‡†æµ‹è¯• CUDAä¼˜åŒ–ç‰ˆæœ¬
            if device == "cuda":
                torch.cuda.synchronize()
            start_time = time.perf_counter()
            for _ in range(benchmark_runs):
                cuda_result = SoftmaxSumFunction.apply(x, logits)
            if device == "cuda":
                torch.cuda.synchronize()
            cuda_time = (time.perf_counter() - start_time) / benchmark_runs * 1000
            
            # éªŒè¯æ­£ç¡®æ€§
            max_diff = torch.max(torch.abs(pytorch_result - cuda_result)).item()
            is_correct = max_diff < 1e-3
            
            # è®¡ç®—åŠ é€Ÿæ¯”
            speedup = pytorch_time / cuda_time if cuda_time > 0 else 0
            
            # å†…å­˜ä½¿ç”¨åˆ†æ
            pytorch_memory = x.numel() * x.element_size() * 2  # x + intermediate results
            cuda_memory = x.numel() * x.element_size()  # èåˆæ“ä½œå‡å°‘ä¸­é—´ç»“æœ
            memory_savings = (pytorch_memory - cuda_memory) / pytorch_memory * 100
            
            result = {
                'config': config,
                'pytorch_time_ms': pytorch_time,
                'cuda_time_ms': cuda_time,
                'speedup': speedup,
                'max_difference': max_diff,
                'is_correct': is_correct,
                'memory_savings_percent': memory_savings,
                'device': device
            }
            
            results[config['name']] = result
            
            print(f"âœ… PyTorch: {pytorch_time:.2f}ms")
            print(f"âš¡ CUDA: {cuda_time:.2f}ms")
            print(f"ğŸš€ åŠ é€Ÿæ¯”: {speedup:.2f}x")
            print(f"ğŸ¯ æ•°å€¼è¯¯å·®: {max_diff:.2e} {'âœ…' if is_correct else 'âŒ'}")
            print(f"ğŸ’¾ å†…å­˜èŠ‚çœ: {memory_savings:.1f}%")
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            results[config['name']] = {'error': str(e)}
    
    return results

def benchmark_triton_sepconv():
    """åŸºå‡†æµ‹è¯•Tritonåˆ†ç¦»å·ç§¯å†…æ ¸"""
    print("\nâš¡ æµ‹è¯• Triton åˆ†ç¦»å·ç§¯å†…æ ¸")
    print("-" * 40)
    
    try:
        from neuroexapt.kernels import sepconv_forward_generic, TRITON_AVAILABLE
        print(f"Tritonå¯ç”¨: {TRITON_AVAILABLE}")
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥Tritonæ¨¡å—: {e}")
        return {}
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # æµ‹è¯•é…ç½®
    test_configs = [
        {"name": "Small", "B": 2, "C": 16, "H": 32, "W": 32, "K": 3},
        {"name": "Medium", "B": 4, "C": 32, "H": 64, "W": 64, "K": 3},
        {"name": "Large", "B": 8, "C": 64, "H": 128, "W": 128, "K": 3},
    ]
    
    results = {}
    warmup_runs = 5
    benchmark_runs = 10
    
    def pytorch_sepconv(x, dw_weight, pw_weight, bias=None):
        """PyTorchå‚è€ƒå®ç°"""
        # æ·±åº¦å·ç§¯
        y = F.conv2d(x, dw_weight, bias=None, stride=1, padding=1, groups=x.size(1))
        # ç‚¹å·ç§¯
        y = F.conv2d(y, pw_weight, bias=bias)
        return y
    
    for config in test_configs:
        print(f"\nğŸ§ª æµ‹è¯•é…ç½®: {config['name']}")
        B, C, H, W, K = config["B"], config["C"], config["H"], config["W"], config["K"]
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        x = torch.randn(B, C, H, W, device=device, requires_grad=True)
        dw_weight = torch.randn(C, 1, K, K, device=device, requires_grad=True)
        pw_weight = torch.randn(C*2, C, 1, 1, device=device, requires_grad=True)
        bias = torch.randn(C*2, device=device, requires_grad=True)
        
        print(f"è¾“å…¥å½¢çŠ¶: x={list(x.shape)}")
        print(f"DWæƒé‡: {list(dw_weight.shape)}")
        print(f"PWæƒé‡: {list(pw_weight.shape)}")
        
        try:
            # é¢„çƒ­
            for _ in range(warmup_runs):
                _ = pytorch_sepconv(x, dw_weight, pw_weight, bias)
                _ = sepconv_forward_generic(x, dw_weight, pw_weight, bias)
            
            # åŸºå‡†æµ‹è¯• PyTorch
            if device == "cuda":
                torch.cuda.synchronize()
            start_time = time.perf_counter()
            for _ in range(benchmark_runs):
                pytorch_result = pytorch_sepconv(x, dw_weight, pw_weight, bias)
            if device == "cuda":
                torch.cuda.synchronize()
            pytorch_time = (time.perf_counter() - start_time) / benchmark_runs * 1000
            
            # åŸºå‡†æµ‹è¯• Triton
            if device == "cuda":
                torch.cuda.synchronize()
            start_time = time.perf_counter()
            for _ in range(benchmark_runs):
                triton_result = sepconv_forward_generic(x, dw_weight, pw_weight, bias)
            if device == "cuda":
                torch.cuda.synchronize()
            triton_time = (time.perf_counter() - start_time) / benchmark_runs * 1000
            
            # éªŒè¯æ­£ç¡®æ€§
            max_diff = torch.max(torch.abs(pytorch_result - triton_result)).item()
            is_correct = max_diff < 1e-3
            
            speedup = pytorch_time / triton_time if triton_time > 0 else 0
            
            result = {
                'config': config,
                'pytorch_time_ms': pytorch_time,
                'triton_time_ms': triton_time,
                'speedup': speedup,
                'max_difference': max_diff,
                'is_correct': is_correct,
                'device': device
            }
            
            results[config['name']] = result
            
            print(f"âœ… PyTorch: {pytorch_time:.2f}ms")
            print(f"âš¡ Triton: {triton_time:.2f}ms")
            print(f"ğŸš€ åŠ é€Ÿæ¯”: {speedup:.2f}x")
            print(f"ğŸ¯ æ•°å€¼è¯¯å·®: {max_diff:.2e} {'âœ…' if is_correct else 'âŒ'}")
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            results[config['name']] = {'error': str(e)}
    
    return results

def benchmark_triton_pooling():
    """åŸºå‡†æµ‹è¯•Tritonæ± åŒ–å†…æ ¸"""
    print("\nğŸŠ æµ‹è¯• Triton æ± åŒ–å†…æ ¸")
    print("-" * 40)
    
    try:
        from neuroexapt.kernels.pool_triton import avg_pool3x3_forward, max_pool3x3_forward, TRITON_AVAILABLE
        print(f"Tritonå¯ç”¨: {TRITON_AVAILABLE}")
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥Tritonæ± åŒ–æ¨¡å—: {e}")
        return {}
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # æµ‹è¯•é…ç½®
    test_configs = [
        {"name": "Small", "B": 4, "C": 32, "H": 32, "W": 32},
        {"name": "Medium", "B": 8, "C": 64, "H": 64, "W": 64},
        {"name": "Large", "B": 16, "C": 128, "H": 128, "W": 128},
    ]
    
    results = {'avgpool': {}, 'maxpool': {}}
    warmup_runs = 5
    benchmark_runs = 15
    
    for config in test_configs:
        print(f"\nğŸ§ª æµ‹è¯•é…ç½®: {config['name']}")
        B, C, H, W = config["B"], config["C"], config["H"], config["W"]
        
        x = torch.randn(B, C, H, W, device=device)
        print(f"è¾“å…¥å½¢çŠ¶: {list(x.shape)}")
        
        # æµ‹è¯•å¹³å‡æ± åŒ–
        try:
            # é¢„çƒ­
            for _ in range(warmup_runs):
                _ = F.avg_pool2d(x, 3, stride=1, padding=1)
                _ = avg_pool3x3_forward(x, stride=1)
            
            # PyTorchå¹³å‡æ± åŒ–
            if device == "cuda":
                torch.cuda.synchronize()
            start_time = time.perf_counter()
            for _ in range(benchmark_runs):
                pytorch_avg = F.avg_pool2d(x, 3, stride=1, padding=1)
            if device == "cuda":
                torch.cuda.synchronize()
            pytorch_avg_time = (time.perf_counter() - start_time) / benchmark_runs * 1000
            
            # Tritonå¹³å‡æ± åŒ–
            if device == "cuda":
                torch.cuda.synchronize()
            start_time = time.perf_counter()
            for _ in range(benchmark_runs):
                triton_avg = avg_pool3x3_forward(x, stride=1)
            if device == "cuda":
                torch.cuda.synchronize()
            triton_avg_time = (time.perf_counter() - start_time) / benchmark_runs * 1000
            
            # éªŒè¯å¹³å‡æ± åŒ–æ­£ç¡®æ€§
            avg_diff = torch.max(torch.abs(pytorch_avg - triton_avg)).item()
            avg_correct = avg_diff < 1e-3
            avg_speedup = pytorch_avg_time / triton_avg_time if triton_avg_time > 0 else 0
            
            results['avgpool'][config['name']] = {
                'config': config,
                'pytorch_time_ms': pytorch_avg_time,
                'triton_time_ms': triton_avg_time,
                'speedup': avg_speedup,
                'max_difference': avg_diff,
                'is_correct': avg_correct,
                'device': device
            }
            
            print(f"å¹³å‡æ± åŒ–:")
            print(f"  PyTorch: {pytorch_avg_time:.2f}ms")
            print(f"  Triton: {triton_avg_time:.2f}ms")
            print(f"  åŠ é€Ÿæ¯”: {avg_speedup:.2f}x")
            print(f"  è¯¯å·®: {avg_diff:.2e} {'âœ…' if avg_correct else 'âŒ'}")
            
        except Exception as e:
            print(f"âŒ å¹³å‡æ± åŒ–æµ‹è¯•å¤±è´¥: {e}")
            results['avgpool'][config['name']] = {'error': str(e)}
        
        # æµ‹è¯•æœ€å¤§æ± åŒ–
        try:
            # é¢„çƒ­
            for _ in range(warmup_runs):
                _ = F.max_pool2d(x, 3, stride=1, padding=1)
                _ = max_pool3x3_forward(x, stride=1)
            
            # PyTorchæœ€å¤§æ± åŒ–
            if device == "cuda":
                torch.cuda.synchronize()
            start_time = time.perf_counter()
            for _ in range(benchmark_runs):
                pytorch_max = F.max_pool2d(x, 3, stride=1, padding=1)
            if device == "cuda":
                torch.cuda.synchronize()
            pytorch_max_time = (time.perf_counter() - start_time) / benchmark_runs * 1000
            
            # Tritonæœ€å¤§æ± åŒ–
            if device == "cuda":
                torch.cuda.synchronize()
            start_time = time.perf_counter()
            for _ in range(benchmark_runs):
                triton_max = max_pool3x3_forward(x, stride=1)
            if device == "cuda":
                torch.cuda.synchronize()
            triton_max_time = (time.perf_counter() - start_time) / benchmark_runs * 1000
            
            # éªŒè¯æœ€å¤§æ± åŒ–æ­£ç¡®æ€§
            max_diff = torch.max(torch.abs(pytorch_max - triton_max)).item()
            max_correct = max_diff < 1e-3
            max_speedup = pytorch_max_time / triton_max_time if triton_max_time > 0 else 0
            
            results['maxpool'][config['name']] = {
                'config': config,
                'pytorch_time_ms': pytorch_max_time,
                'triton_time_ms': triton_max_time,
                'speedup': max_speedup,
                'max_difference': max_diff,
                'is_correct': max_correct,
                'device': device
            }
            
            print(f"æœ€å¤§æ± åŒ–:")
            print(f"  PyTorch: {pytorch_max_time:.2f}ms")
            print(f"  Triton: {triton_max_time:.2f}ms")
            print(f"  åŠ é€Ÿæ¯”: {max_speedup:.2f}x")
            print(f"  è¯¯å·®: {max_diff:.2e} {'âœ…' if max_correct else 'âŒ'}")
            
        except Exception as e:
            print(f"âŒ æœ€å¤§æ± åŒ–æµ‹è¯•å¤±è´¥: {e}")
            results['maxpool'][config['name']] = {'error': str(e)}
    
    return results

def generate_comprehensive_report(all_results):
    """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ğŸ“Š NeuroExapt Triton å†…æ ¸æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
    print("="*60)
    
    # ç»Ÿè®¡æ‰€æœ‰æˆåŠŸçš„æµ‹è¯•
    all_speedups = []
    operation_stats = {}
    
    # å¤„ç†SoftmaxSumç»“æœ
    if 'softmax_sum' in all_results:
        for name, result in all_results['softmax_sum'].items():
            if 'speedup' in result and result['speedup'] > 0:
                all_speedups.append(result['speedup'])
                if 'SoftmaxSum' not in operation_stats:
                    operation_stats['SoftmaxSum'] = []
                operation_stats['SoftmaxSum'].append(result['speedup'])
    
    # å¤„ç†åˆ†ç¦»å·ç§¯ç»“æœ
    if 'sepconv' in all_results:
        for name, result in all_results['sepconv'].items():
            if 'speedup' in result and result['speedup'] > 0:
                all_speedups.append(result['speedup'])
                if 'SepConv' not in operation_stats:
                    operation_stats['SepConv'] = []
                operation_stats['SepConv'].append(result['speedup'])
    
    # å¤„ç†æ± åŒ–ç»“æœ
    if 'pooling' in all_results:
        for pool_type in ['avgpool', 'maxpool']:
            if pool_type in all_results['pooling']:
                for name, result in all_results['pooling'][pool_type].items():
                    if 'speedup' in result and result['speedup'] > 0:
                        all_speedups.append(result['speedup'])
                        op_name = f"{'AvgPool' if pool_type == 'avgpool' else 'MaxPool'}"
                        if op_name not in operation_stats:
                            operation_stats[op_name] = []
                        operation_stats[op_name].append(result['speedup'])
    
    # æ€»ä½“ç»Ÿè®¡
    if all_speedups:
        print(f"ğŸš€ æ€»ä½“æ€§èƒ½ç»Ÿè®¡:")
        print(f"   æµ‹è¯•æ€»æ•°: {len(all_speedups)}")
        print(f"   å¹³å‡åŠ é€Ÿæ¯”: {np.mean(all_speedups):.2f}x")
        print(f"   æœ€å¤§åŠ é€Ÿæ¯”: {np.max(all_speedups):.2f}x")
        print(f"   æœ€å°åŠ é€Ÿæ¯”: {np.min(all_speedups):.2f}x")
        print(f"   ä¸­ä½æ•°åŠ é€Ÿæ¯”: {np.median(all_speedups):.2f}x")
        
        print(f"\nğŸ“ˆ æŒ‰æ“ä½œç±»å‹ç»Ÿè®¡:")
        for op, speedups in operation_stats.items():
            avg_speedup = np.mean(speedups)
            max_speedup = np.max(speedups)
            print(f"   {op}: {avg_speedup:.2f}x å¹³å‡, {max_speedup:.2f}x æœ€å¤§ ({len(speedups)} ä¸ªæµ‹è¯•)")
        
        # æ€§èƒ½åˆ†çº§
        excellent = len([s for s in all_speedups if s > 2.0])
        good = len([s for s in all_speedups if 1.5 <= s <= 2.0])
        moderate = len([s for s in all_speedups if 1.0 <= s < 1.5])
        poor = len([s for s in all_speedups if s < 1.0])
        
        print(f"\nğŸ† æ€§èƒ½åˆ†çº§:")
        print(f"   ä¼˜ç§€ (>2.0x): {excellent} ä¸ªæµ‹è¯•")
        print(f"   è‰¯å¥½ (1.5-2.0x): {good} ä¸ªæµ‹è¯•")
        print(f"   ä¸€èˆ¬ (1.0-1.5x): {moderate} ä¸ªæµ‹è¯•")
        print(f"   è¾ƒå·® (<1.0x): {poor} ä¸ªæµ‹è¯•")
        
        print(f"\nğŸ’¡ å»ºè®®:")
        if excellent > 0:
            print(f"   âœ… {excellent} ä¸ªæµ‹è¯•æ˜¾ç¤ºæ˜¾è‘—æ€§èƒ½æå‡ï¼Œæ¨èåœ¨ç”Ÿäº§ä¸­ä½¿ç”¨")
        if good > 0:
            print(f"   âœ… {good} ä¸ªæµ‹è¯•æ˜¾ç¤ºè‰¯å¥½æ€§èƒ½æå‡")
        if moderate > 0:
            print(f"   âš ï¸  {moderate} ä¸ªæµ‹è¯•æ˜¾ç¤ºé€‚ä¸­æ€§èƒ½æå‡ï¼Œå¯æ ¹æ®å…·ä½“åœºæ™¯é€‰æ‹©")
        if poor > 0:
            print(f"   âŒ {poor} ä¸ªæµ‹è¯•æ€§èƒ½ä¸ä½³ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸçš„æ€§èƒ½æµ‹è¯•ç»“æœ")
    
    return {
        'total_tests': len(all_speedups),
        'avg_speedup': np.mean(all_speedups) if all_speedups else 0,
        'max_speedup': np.max(all_speedups) if all_speedups else 0,
        'operation_stats': operation_stats,
        'performance_grades': {
            'excellent': excellent if all_speedups else 0,
            'good': good if all_speedups else 0,
            'moderate': moderate if all_speedups else 0,
            'poor': poor if all_speedups else 0
        } if all_speedups else {}
    }

def save_results_to_file(all_results, summary):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    os.makedirs("data/triton_benchmarks", exist_ok=True)
    detailed_file = f"data/triton_benchmarks/triton_kernels_benchmark_{timestamp}.json"
    
    with open(detailed_file, 'w', encoding='utf-8') as f:
        json.dump({
            'metadata': {
                'timestamp': datetime.now().isoformat(),
                'device': 'cuda' if torch.cuda.is_available() else 'cpu',
                'pytorch_version': torch.__version__,
                'cuda_available': torch.cuda.is_available(),
                'cuda_device': torch.cuda.get_device_name() if torch.cuda.is_available() else None,
            },
            'detailed_results': all_results,
            'summary': summary
        }, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {detailed_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª NeuroExapt Triton å†…æ ¸åŸºå‡†æµ‹è¯•")
    print("=" * 50)
    print("æµ‹è¯•å·²å®ç°çš„ä¼˜åŒ–å†…æ ¸æ€§èƒ½è¡¨ç°")
    print()
    
    all_results = {}
    
    # è¿è¡Œå„é¡¹æµ‹è¯•
    print("ğŸ”¥ ç¬¬ä¸€é¡¹ï¼šCUDA SoftmaxSum æ‰©å±•æµ‹è¯•")
    all_results['softmax_sum'] = benchmark_cuda_softmax_sum()
    
    print("\nâš¡ ç¬¬äºŒé¡¹ï¼šTriton åˆ†ç¦»å·ç§¯å†…æ ¸æµ‹è¯•")
    all_results['sepconv'] = benchmark_triton_sepconv()
    
    print("\nğŸŠ ç¬¬ä¸‰é¡¹ï¼šTriton æ± åŒ–å†…æ ¸æµ‹è¯•")
    all_results['pooling'] = benchmark_triton_pooling()
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    summary = generate_comprehensive_report(all_results)
    
    # ä¿å­˜ç»“æœ
    save_results_to_file(all_results, summary)
    
    print(f"\nğŸ‰ åŸºå‡†æµ‹è¯•å®Œæˆï¼")
    print(f"è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹ data/triton_benchmarks/ ç›®å½•")

if __name__ == "__main__":
    main() 