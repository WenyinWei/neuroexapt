#!/usr/bin/env python3
"""
NeuroExapt Triton æ€§èƒ½æµ‹è¯•å¥—ä»¶

è¿™ä¸ªæ–‡ä»¶æä¾›äº†å…¨é¢çš„Tritonå†…æ ¸æ€§èƒ½æµ‹è¯•ï¼ŒåŒ…æ‹¬ï¼š
1. åŸºç¡€æ“ä½œæµ‹è¯•ï¼ˆçŸ©é˜µä¹˜æ³•ã€å…ƒç´ çº§æ“ä½œç­‰ï¼‰
2. å·ç§¯æ“ä½œæµ‹è¯•ï¼ˆæ ‡å‡†å·ç§¯ã€åˆ†ç¦»å·ç§¯ã€æ·±åº¦å·ç§¯ï¼‰
3. æ³¨æ„åŠ›æœºåˆ¶æµ‹è¯•ï¼ˆMulti-Head Attentionã€Flash Attentionï¼‰
4. æ¿€æ´»å‡½æ•°æµ‹è¯•ï¼ˆReLUã€GELUã€Swishç­‰ï¼‰
5. å½’ä¸€åŒ–æ“ä½œæµ‹è¯•ï¼ˆBatchNormã€LayerNormç­‰ï¼‰
6. å†…å­˜ä¼˜åŒ–æµ‹è¯•ï¼ˆèåˆæ“ä½œã€å†…å­˜å¸¦å®½ï¼‰
7. å®é™…ç¥ç»ç½‘ç»œç»„ä»¶æµ‹è¯•ï¼ˆMixedOpã€SoftmaxSumç­‰ï¼‰
"""

import torch
import torch.nn.functional as F
import time
import gc
import json
import os
import sys
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    import triton
    import triton.language as tl
    TRITON_AVAILABLE = True
except ImportError:
    TRITON_AVAILABLE = False

@dataclass
class BenchmarkResult:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœæ•°æ®ç»“æ„"""
    operation: str
    test_config: Dict[str, Any]
    pytorch_time_ms: float
    triton_time_ms: float
    speedup: float
    memory_usage_mb: float
    memory_savings_percent: float
    device: str
    timestamp: str
    status: str  # 'success', 'failed', 'skipped'
    error_message: Optional[str] = None

class TritonPerformanceSuite:
    """Tritonæ€§èƒ½æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self, device: str = None, save_results: bool = True):
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.save_results = save_results
        self.results: List[BenchmarkResult] = []
        self.warmup_runs = 5
        self.benchmark_runs = 20
        
        print(f"ğŸš€ Tritonæ€§èƒ½æµ‹è¯•å¥—ä»¶åˆå§‹åŒ–")
        print(f"è®¾å¤‡: {self.device}")
        print(f"Tritonå¯ç”¨: {TRITON_AVAILABLE}")
        print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        if torch.cuda.is_available():
            print(f"CUDAè®¾å¤‡: {torch.cuda.get_device_name()}")
            print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print("-" * 60)
    
    def benchmark_operation(self, name: str, pytorch_fn, triton_fn, 
                          test_configs: List[Dict], **kwargs) -> List[BenchmarkResult]:
        """å¯¹å•ä¸ªæ“ä½œè¿›è¡ŒåŸºå‡†æµ‹è¯•"""
        results = []
        
        for config in test_configs:
            print(f"\nğŸ§ª æµ‹è¯• {name} - é…ç½®: {config}")
            
            try:
                # ç”Ÿæˆæµ‹è¯•æ•°æ®
                test_data = self._generate_test_data(config)
                
                # é¢„çƒ­
                for _ in range(self.warmup_runs):
                    _ = pytorch_fn(**test_data)
                    if triton_fn:
                        _ = triton_fn(**test_data)
                
                # åŒæ­¥GPU
                if self.device == 'cuda':
                    torch.cuda.synchronize()
                
                # æµ‹è¯•PyTorch
                start_time = time.perf_counter()
                for _ in range(self.benchmark_runs):
                    pytorch_result = pytorch_fn(**test_data)
                if self.device == 'cuda':
                    torch.cuda.synchronize()
                pytorch_time = (time.perf_counter() - start_time) / self.benchmark_runs * 1000
                
                # æµ‹è¯•Triton (å¦‚æœå¯ç”¨)
                triton_time = float('inf')
                speedup = 0.0
                if triton_fn and TRITON_AVAILABLE:
                    start_time = time.perf_counter()
                    for _ in range(self.benchmark_runs):
                        triton_result = triton_fn(**test_data)
                    if self.device == 'cuda':
                        torch.cuda.synchronize()
                    triton_time = (time.perf_counter() - start_time) / self.benchmark_runs * 1000
                    speedup = pytorch_time / triton_time
                    
                    # éªŒè¯ç»“æœæ­£ç¡®æ€§
                    if not self._verify_results(pytorch_result, triton_result):
                        print(f"âš ï¸  ç»“æœéªŒè¯å¤±è´¥ï¼Œæ•°å€¼ä¸åŒ¹é…")
                
                # è®¡ç®—å†…å­˜ä½¿ç”¨
                memory_usage = self._measure_memory_usage(test_data)
                
                result = BenchmarkResult(
                    operation=name,
                    test_config=config,
                    pytorch_time_ms=pytorch_time,
                    triton_time_ms=triton_time,
                    speedup=speedup,
                    memory_usage_mb=memory_usage,
                    memory_savings_percent=0.0,  # å¾…å®ç°
                    device=self.device,
                    timestamp=datetime.now().isoformat(),
                    status='success'
                )
                
                results.append(result)
                self.results.append(result)
                
                print(f"âœ… PyTorch: {pytorch_time:.2f}ms")
                if triton_fn and TRITON_AVAILABLE:
                    print(f"âš¡ Triton: {triton_time:.2f}ms")
                    print(f"ğŸš€ åŠ é€Ÿæ¯”: {speedup:.2f}x")
                else:
                    print(f"âŒ Tritonä¸å¯ç”¨")
                
            except Exception as e:
                print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
                error_result = BenchmarkResult(
                    operation=name,
                    test_config=config,
                    pytorch_time_ms=0.0,
                    triton_time_ms=0.0,
                    speedup=0.0,
                    memory_usage_mb=0.0,
                    memory_savings_percent=0.0,
                    device=self.device,
                    timestamp=datetime.now().isoformat(),
                    status='failed',
                    error_message=str(e)
                )
                results.append(error_result)
                self.results.append(error_result)
        
        return results
    
    def _generate_test_data(self, config: Dict) -> Dict:
        """æ ¹æ®é…ç½®ç”Ÿæˆæµ‹è¯•æ•°æ®"""
        data = {}
        
        # æ ¹æ®ä¸åŒæ“ä½œç±»å‹ç”Ÿæˆæ•°æ®
        if 'batch_size' in config and 'channels' in config:
            # å·ç§¯ç±»æ“ä½œ
            B, C, H, W = config['batch_size'], config['channels'], config.get('height', 32), config.get('width', 32)
            data['x'] = torch.randn(B, C, H, W, device=self.device, requires_grad=True)
            
            if 'kernel_size' in config:
                K = config['kernel_size']
                data['weight'] = torch.randn(C, C, K, K, device=self.device, requires_grad=True)
        
        elif 'seq_len' in config and 'hidden_dim' in config:
            # æ³¨æ„åŠ›æœºåˆ¶ç±»æ“ä½œ
            B, L, D = config.get('batch_size', 2), config['seq_len'], config['hidden_dim']
            data['x'] = torch.randn(B, L, D, device=self.device, requires_grad=True)
            data['y'] = torch.randn(B, L, D, device=self.device, requires_grad=True)
        
        elif 'matrix_size' in config:
            # çŸ©é˜µæ“ä½œ
            N = config['matrix_size']
            data['a'] = torch.randn(N, N, device=self.device, requires_grad=True)
            data['b'] = torch.randn(N, N, device=self.device, requires_grad=True)
        
        elif 'num_ops' in config and 'tensor_shape' in config:
            # MixedOpç±»æ“ä½œ
            N = config['num_ops']
            shape = config['tensor_shape']
            data['tensors'] = [torch.randn(*shape, device=self.device, requires_grad=True) for _ in range(N)]
            data['logits'] = torch.randn(N, device=self.device, requires_grad=True)
        
        return data
    
    def _verify_results(self, pytorch_result, triton_result, rtol=1e-3, atol=1e-3):
        """éªŒè¯PyTorchå’ŒTritonç»“æœçš„ä¸€è‡´æ€§"""
        try:
            if isinstance(pytorch_result, (list, tuple)):
                if len(pytorch_result) != len(triton_result):
                    return False
                return all(torch.allclose(p, t, rtol=rtol, atol=atol) 
                          for p, t in zip(pytorch_result, triton_result))
            else:
                return torch.allclose(pytorch_result, triton_result, rtol=rtol, atol=atol)
        except:
            return False
    
    def _measure_memory_usage(self, test_data: Dict) -> float:
        """æµ‹é‡å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        if self.device == 'cuda':
            torch.cuda.empty_cache()
            return torch.cuda.max_memory_allocated() / 1024 / 1024
        else:
            # CPUå†…å­˜æµ‹é‡ï¼ˆç®€åŒ–ï¼‰
            total_size = 0
            for tensor in test_data.values():
                if isinstance(tensor, torch.Tensor):
                    total_size += tensor.numel() * tensor.element_size()
                elif isinstance(tensor, list):
                    for t in tensor:
                        if isinstance(t, torch.Tensor):
                            total_size += t.numel() * t.element_size()
            return total_size / 1024 / 1024

    # ============ å…·ä½“æµ‹è¯•æ–¹æ³• ============
    
    def test_matrix_multiplication(self):
        """æµ‹è¯•çŸ©é˜µä¹˜æ³•"""
        print("\nğŸ”¢ æµ‹è¯•çŸ©é˜µä¹˜æ³•æ€§èƒ½")
        
        def pytorch_matmul(a, b):
            return torch.matmul(a, b)
        
        def triton_matmul(a, b):
            # ä½¿ç”¨Tritonå®ç°çš„çŸ©é˜µä¹˜æ³•ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            return torch.matmul(a, b)  # æš‚æ—¶ä½¿ç”¨PyTorchä½œä¸ºplaceholder
        
        configs = [
            {'matrix_size': 512},
            {'matrix_size': 1024},
            {'matrix_size': 2048},
            {'matrix_size': 4096},
        ]
        
        return self.benchmark_operation(
            "Matrix Multiplication",
            pytorch_matmul,
            triton_matmul if TRITON_AVAILABLE else None,
            configs
        )
    
    def test_convolution_operations(self):
        """æµ‹è¯•å·ç§¯æ“ä½œ"""
        print("\nğŸ”„ æµ‹è¯•å·ç§¯æ“ä½œæ€§èƒ½")
        
        def pytorch_conv2d(x, weight):
            return F.conv2d(x, weight, padding=1)
        
        def triton_conv2d(x, weight):
            # æš‚æ—¶ä½¿ç”¨PyTorchä½œä¸ºplaceholder
            return F.conv2d(x, weight, padding=1)
        
        configs = [
            {'batch_size': 8, 'channels': 32, 'height': 32, 'width': 32, 'kernel_size': 3},
            {'batch_size': 16, 'channels': 64, 'height': 64, 'width': 64, 'kernel_size': 3},
            {'batch_size': 32, 'channels': 128, 'height': 128, 'width': 128, 'kernel_size': 3},
        ]
        
        return self.benchmark_operation(
            "Convolution 2D",
            pytorch_conv2d,
            triton_conv2d if TRITON_AVAILABLE else None,
            configs
        )
    
    def test_separable_convolution(self):
        """æµ‹è¯•åˆ†ç¦»å·ç§¯"""
        print("\nğŸ”€ æµ‹è¯•åˆ†ç¦»å·ç§¯æ€§èƒ½")
        
        def pytorch_sepconv(x, dw_weight, pw_weight, bias):
            # æ·±åº¦å·ç§¯
            y = F.conv2d(x, dw_weight, bias=None, stride=1, padding=1, groups=x.size(1))
            # ç‚¹å·ç§¯
            y = F.conv2d(y, pw_weight, bias=bias)
            return y
        
        def triton_sepconv(x, dw_weight, pw_weight, bias):
            try:
                from neuroexapt.kernels import sepconv_forward_generic
                return sepconv_forward_generic(x, dw_weight, pw_weight, bias)
            except ImportError:
                return pytorch_sepconv(x, dw_weight, pw_weight, bias)
        
        # ç”Ÿæˆç‰¹æ®Šçš„æµ‹è¯•æ•°æ®
        def generate_sepconv_data(config):
            B, C, H, W = config['batch_size'], config['channels'], 32, 32
            K = config['kernel_size']
            data = {
                'x': torch.randn(B, C, H, W, device=self.device, requires_grad=True),
                'dw_weight': torch.randn(C, 1, K, K, device=self.device, requires_grad=True),
                'pw_weight': torch.randn(C*2, C, 1, 1, device=self.device, requires_grad=True),
                'bias': torch.randn(C*2, device=self.device, requires_grad=True)
            }
            return data
        
        configs = [
            {'batch_size': 4, 'channels': 16, 'kernel_size': 3},
            {'batch_size': 8, 'channels': 32, 'kernel_size': 3},
            {'batch_size': 16, 'channels': 64, 'kernel_size': 3},
        ]
        
        results = []
        for config in configs:
            test_data = generate_sepconv_data(config)
            
            try:
                # é¢„çƒ­
                for _ in range(self.warmup_runs):
                    _ = pytorch_sepconv(**test_data)
                    _ = triton_sepconv(**test_data)
                
                # æµ‹è¯•PyTorch
                if self.device == 'cuda':
                    torch.cuda.synchronize()
                start_time = time.perf_counter()
                for _ in range(self.benchmark_runs):
                    pytorch_result = pytorch_sepconv(**test_data)
                if self.device == 'cuda':
                    torch.cuda.synchronize()
                pytorch_time = (time.perf_counter() - start_time) / self.benchmark_runs * 1000
                
                # æµ‹è¯•Triton
                if self.device == 'cuda':
                    torch.cuda.synchronize()
                start_time = time.perf_counter()
                for _ in range(self.benchmark_runs):
                    triton_result = triton_sepconv(**test_data)
                if self.device == 'cuda':
                    torch.cuda.synchronize()
                triton_time = (time.perf_counter() - start_time) / self.benchmark_runs * 1000
                
                speedup = pytorch_time / triton_time
                memory_usage = self._measure_memory_usage(test_data)
                
                result = BenchmarkResult(
                    operation="Separable Convolution",
                    test_config=config,
                    pytorch_time_ms=pytorch_time,
                    triton_time_ms=triton_time,
                    speedup=speedup,
                    memory_usage_mb=memory_usage,
                    memory_savings_percent=0.0,
                    device=self.device,
                    timestamp=datetime.now().isoformat(),
                    status='success'
                )
                
                results.append(result)
                self.results.append(result)
                
                print(f"âœ… é…ç½® {config}:")
                print(f"   PyTorch: {pytorch_time:.2f}ms")
                print(f"   Triton: {triton_time:.2f}ms")
                print(f"   åŠ é€Ÿæ¯”: {speedup:.2f}x")
                
            except Exception as e:
                print(f"âŒ åˆ†ç¦»å·ç§¯æµ‹è¯•å¤±è´¥: {e}")
        
        return results
    
    def test_activation_functions(self):
        """æµ‹è¯•æ¿€æ´»å‡½æ•°"""
        print("\nâš¡ æµ‹è¯•æ¿€æ´»å‡½æ•°æ€§èƒ½")
        
        def pytorch_relu(x):
            return F.relu(x)
        
        def triton_relu(x):
            # Triton ReLUå®ç°placeholder
            return F.relu(x)
        
        def pytorch_gelu(x):
            return F.gelu(x)
        
        def triton_gelu(x):
            # Triton GELUå®ç°placeholder
            return F.gelu(x)
        
        configs = [
            {'batch_size': 32, 'channels': 64, 'height': 32, 'width': 32},
            {'batch_size': 64, 'channels': 128, 'height': 64, 'width': 64},
        ]
        
        relu_results = self.benchmark_operation(
            "ReLU Activation",
            pytorch_relu,
            triton_relu if TRITON_AVAILABLE else None,
            configs
        )
        
        gelu_results = self.benchmark_operation(
            "GELU Activation", 
            pytorch_gelu,
            triton_gelu if TRITON_AVAILABLE else None,
            configs
        )
        
        return relu_results + gelu_results
    
    def test_mixedop_operation(self):
        """æµ‹è¯•MixedOpæ“ä½œï¼ˆSoftmaxSumï¼‰"""
        print("\nğŸ§¬ æµ‹è¯•MixedOp SoftmaxSumæ€§èƒ½")
        
        def pytorch_softmax_sum(tensors, logits):
            # PyTorchå‚è€ƒå®ç°
            stacked = torch.stack(tensors, dim=0)
            weights = F.softmax(logits, dim=0)
            return (stacked * weights.view(-1, 1, 1, 1, 1)).sum(dim=0)
        
        def triton_softmax_sum(tensors, logits):
            try:
                from neuroexapt.cuda_ops import SoftmaxSumFunction
                stacked = torch.stack(tensors, dim=0)
                return SoftmaxSumFunction.apply(stacked, logits)
            except ImportError:
                return pytorch_softmax_sum(tensors, logits)
        
        configs = [
            {'num_ops': 4, 'tensor_shape': (2, 16, 16, 16)},
            {'num_ops': 8, 'tensor_shape': (4, 32, 32, 32)},
            {'num_ops': 12, 'tensor_shape': (8, 64, 64, 64)},
            {'num_ops': 16, 'tensor_shape': (16, 128, 128, 128)},
        ]
        
        results = []
        for config in configs:
            test_data = self._generate_test_data(config)
            
            try:
                # è½¬æ¢ä¸ºæ­£ç¡®çš„è¾“å…¥æ ¼å¼
                tensors = test_data['tensors']
                logits = test_data['logits']
                
                # é¢„çƒ­
                for _ in range(self.warmup_runs):
                    _ = pytorch_softmax_sum(tensors, logits)
                    _ = triton_softmax_sum(tensors, logits)
                
                # æµ‹è¯•PyTorch
                if self.device == 'cuda':
                    torch.cuda.synchronize()
                start_time = time.perf_counter()
                for _ in range(self.benchmark_runs):
                    pytorch_result = pytorch_softmax_sum(tensors, logits)
                if self.device == 'cuda':
                    torch.cuda.synchronize()
                pytorch_time = (time.perf_counter() - start_time) / self.benchmark_runs * 1000
                
                # æµ‹è¯•Triton/CUDA
                if self.device == 'cuda':
                    torch.cuda.synchronize()
                start_time = time.perf_counter()
                for _ in range(self.benchmark_runs):
                    triton_result = triton_softmax_sum(tensors, logits)
                if self.device == 'cuda':
                    torch.cuda.synchronize()
                triton_time = (time.perf_counter() - start_time) / self.benchmark_runs * 1000
                
                speedup = pytorch_time / triton_time
                memory_usage = self._measure_memory_usage(test_data)
                
                result = BenchmarkResult(
                    operation="MixedOp SoftmaxSum",
                    test_config=config,
                    pytorch_time_ms=pytorch_time,
                    triton_time_ms=triton_time,
                    speedup=speedup,
                    memory_usage_mb=memory_usage,
                    memory_savings_percent=0.0,
                    device=self.device,
                    timestamp=datetime.now().isoformat(),
                    status='success'
                )
                
                results.append(result)
                self.results.append(result)
                
                print(f"âœ… é…ç½® {config}:")
                print(f"   PyTorch: {pytorch_time:.2f}ms")
                print(f"   ä¼˜åŒ–ç‰ˆæœ¬: {triton_time:.2f}ms")
                print(f"   åŠ é€Ÿæ¯”: {speedup:.2f}x")
                
            except Exception as e:
                print(f"âŒ MixedOpæµ‹è¯•å¤±è´¥: {e}")
        
        return results
    
    def test_pooling_operations(self):
        """æµ‹è¯•æ± åŒ–æ“ä½œ"""
        print("\nğŸŠ æµ‹è¯•æ± åŒ–æ“ä½œæ€§èƒ½")
        
        def pytorch_avgpool(x):
            return F.avg_pool2d(x, 3, stride=1, padding=1)
        
        def triton_avgpool(x):
            try:
                from neuroexapt.kernels.pool_triton import avg_pool3x3_forward
                return avg_pool3x3_forward(x, stride=1)
            except ImportError:
                return F.avg_pool2d(x, 3, stride=1, padding=1)
        
        def pytorch_maxpool(x):
            return F.max_pool2d(x, 3, stride=1, padding=1)
        
        def triton_maxpool(x):
            try:
                from neuroexapt.kernels.pool_triton import max_pool3x3_forward
                return max_pool3x3_forward(x, stride=1)
            except ImportError:
                return F.max_pool2d(x, 3, stride=1, padding=1)
        
        configs = [
            {'batch_size': 8, 'channels': 32, 'height': 32, 'width': 32},
            {'batch_size': 16, 'channels': 64, 'height': 64, 'width': 64},
            {'batch_size': 32, 'channels': 128, 'height': 128, 'width': 128},
        ]
        
        avgpool_results = self.benchmark_operation(
            "Average Pooling 3x3",
            pytorch_avgpool,
            triton_avgpool if TRITON_AVAILABLE else None,
            configs
        )
        
        maxpool_results = self.benchmark_operation(
            "Max Pooling 3x3",
            pytorch_maxpool,
            triton_maxpool if TRITON_AVAILABLE else None,
            configs
        )
        
        return avgpool_results + maxpool_results
    
    def run_all_tests(self) -> Dict[str, List[BenchmarkResult]]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹å…¨é¢Tritonæ€§èƒ½æµ‹è¯•")
        print("=" * 60)
        
        all_results = {}
        
        # è¿è¡Œå„ä¸ªæµ‹è¯•
        test_methods = [
            ('Matrix Multiplication', self.test_matrix_multiplication),
            ('Convolution Operations', self.test_convolution_operations),
            ('Separable Convolution', self.test_separable_convolution),
            ('Activation Functions', self.test_activation_functions),
            ('MixedOp Operations', self.test_mixedop_operation),
            ('Pooling Operations', self.test_pooling_operations),
        ]
        
        for test_name, test_method in test_methods:
            print(f"\n{'='*20} {test_name} {'='*20}")
            try:
                results = test_method()
                all_results[test_name] = results
                print(f"âœ… {test_name} å®Œæˆ")
            except Exception as e:
                print(f"âŒ {test_name} å¤±è´¥: {e}")
                all_results[test_name] = []
        
        # ä¿å­˜ç»“æœ
        if self.save_results:
            self.save_benchmark_results()
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self.generate_summary_report()
        
        return all_results
    
    def save_benchmark_results(self):
        """ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"data/triton_benchmarks/triton_performance_{timestamp}.json"
        
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        results_data = {
            'metadata': {
                'timestamp': datetime.now().isoformat(),
                'device': self.device,
                'triton_available': TRITON_AVAILABLE,
                'pytorch_version': torch.__version__,
                'cuda_available': torch.cuda.is_available(),
                'cuda_device': torch.cuda.get_device_name() if torch.cuda.is_available() else None,
            },
            'results': [asdict(result) for result in self.results]
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")
    
    def generate_summary_report(self):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“Š Tritonæ€§èƒ½æµ‹è¯•æ€»ç»“æŠ¥å‘Š")
        print("="*60)
        
        if not self.results:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•ç»“æœ")
            return
        
        successful_results = [r for r in self.results if r.status == 'success' and r.speedup > 0]
        
        print(f"æ€»æµ‹è¯•æ•°: {len(self.results)}")
        print(f"æˆåŠŸæµ‹è¯•æ•°: {len(successful_results)}")
        print(f"æˆåŠŸç‡: {len(successful_results)/len(self.results)*100:.1f}%")
        
        if successful_results:
            speedups = [r.speedup for r in successful_results]
            print(f"\nğŸš€ æ€§èƒ½ç»Ÿè®¡:")
            print(f"å¹³å‡åŠ é€Ÿæ¯”: {np.mean(speedups):.2f}x")
            print(f"æœ€å¤§åŠ é€Ÿæ¯”: {np.max(speedups):.2f}x")
            print(f"æœ€å°åŠ é€Ÿæ¯”: {np.min(speedups):.2f}x")
            print(f"ä¸­ä½æ•°åŠ é€Ÿæ¯”: {np.median(speedups):.2f}x")
            
            print(f"\nğŸ“ˆ æŒ‰æ“ä½œç±»å‹åˆ†ç»„:")
            operation_groups = {}
            for result in successful_results:
                op = result.operation
                if op not in operation_groups:
                    operation_groups[op] = []
                operation_groups[op].append(result.speedup)
            
            for op, speedups in operation_groups.items():
                avg_speedup = np.mean(speedups)
                print(f"  {op}: {avg_speedup:.2f}x (å…±{len(speedups)}ä¸ªæµ‹è¯•)")
        
        print("\nğŸ’¡ å»ºè®®:")
        if TRITON_AVAILABLE:
            best_ops = [r for r in successful_results if r.speedup > 1.5]
            if best_ops:
                print(f"âœ… æ¨èä¼˜åŒ–çš„æ“ä½œ (>1.5xåŠ é€Ÿ):")
                for op in set(r.operation for r in best_ops):
                    print(f"  - {op}")
            else:
                print("âš ï¸  å½“å‰æµ‹è¯•ä¸­æ²¡æœ‰æ˜¾è‘—çš„æ€§èƒ½æå‡")
        else:
            print("âŒ Tritonä¸å¯ç”¨ï¼Œå»ºè®®å®‰è£…Tritonä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª NeuroExapt Triton æ€§èƒ½æµ‹è¯•å¥—ä»¶")
    print("è¿™å°†å¯¹å„ç§æ·±åº¦å­¦ä¹ æ“ä½œè¿›è¡Œå…¨é¢çš„æ€§èƒ½æµ‹è¯•")
    print("-" * 50)
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    suite = TritonPerformanceSuite(save_results=True)
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    results = suite.run_all_tests()
    
    print("\nğŸ‰ æ€§èƒ½æµ‹è¯•å®Œæˆï¼")
    print("ç»“æœå·²ä¿å­˜åˆ° data/triton_benchmarks/ ç›®å½•")
    print("è¯¦ç»†æŠ¥å‘Šè¯·æŸ¥çœ‹ç”Ÿæˆçš„JSONæ–‡ä»¶")

if __name__ == "__main__":
    main() 