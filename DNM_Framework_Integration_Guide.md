# DNM æ¡†æ¶å®Œæ•´é›†æˆæŒ‡å—

## ğŸ§¬ Dynamic Neural Morphogenesis (DNM) æ¡†æ¶æ¦‚è¿°

DNMæ¡†æ¶æ˜¯å¯¹ASO-SEçš„é©å‘½æ€§çªç ´ï¼Œå®ç°äº†çœŸæ­£çš„ç¥ç»ç½‘ç»œ"ç”Ÿç‰©å­¦å¼ç”Ÿé•¿"ã€‚é€šè¿‡ä¸‰å¤§åˆ›æ–°æ¨¡å—çš„ååŒå·¥ä½œï¼Œç½‘ç»œèƒ½å¤Ÿåƒæ´»çš„ç”Ÿç‰©ä½“ä¸€æ ·è‡ªå‘åœ°é€‰æ‹©å˜å¼‚æ–¹å‘ã€‚

### æ ¸å¿ƒæ¨¡å—

1. **ä¿¡æ¯ç†µé©±åŠ¨çš„ç¥ç»å…ƒåˆ†è£‚** (`neuroexapt/core/dnm_neuron_division.py`)
   - å®æ—¶ç›‘æ§ç¥ç»å…ƒä¿¡æ¯æ‰¿è½½é‡
   - è¯†åˆ«ä¿¡æ¯è¿‡è½½çš„é«˜ç†µç¥ç»å…ƒå¹¶æ‰§è¡Œåˆ†è£‚
   - ç»§æ‰¿æƒé‡å¹¶æ·»åŠ é€‚åº”æ€§å˜å¼‚

2. **æ¢¯åº¦å¼•å¯¼çš„è¿æ¥ç”Ÿé•¿** (`neuroexapt/core/dnm_connection_growth.py`)
   - åˆ†æè·¨å±‚æ¢¯åº¦ç›¸å…³æ€§
   - åŠ¨æ€æ·»åŠ è·³è·ƒè¿æ¥å’Œæ³¨æ„åŠ›æœºåˆ¶
   - æ‰“ç ´ä¼ ç»Ÿå±‚çº§é™åˆ¶

3. **å¤šç›®æ ‡è¿›åŒ–ä¼˜åŒ–** (`neuroexapt/math/pareto_optimization.py`)
   - åŒæ—¶ä¼˜åŒ–å‡†ç¡®ç‡ã€æ•ˆç‡ã€å¤æ‚åº¦
   - å¸•ç´¯æ‰˜æœ€ä¼˜çš„æ¶æ„æ¼”åŒ–
   - å…¨å±€æœç´¢æœ€ä¼˜æ¶æ„

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºç¡€ä½¿ç”¨

```python
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from neuroexapt.core.dnm_framework import train_with_dnm

# åˆ›å»ºæ‚¨çš„æ¨¡å‹
model = nn.Sequential(
    nn.Conv2d(3, 32, 3, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(2),
    nn.Conv2d(32, 64, 3, padding=1),
    nn.ReLU(),
    nn.AdaptiveAvgPool2d(1),
    nn.Flatten(),
    nn.Linear(64, 10)
)

# ä¸€è¡Œä»£ç å¯åŠ¨DNMè®­ç»ƒ
result = train_with_dnm(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    epochs=100
)

print(f"æœ€ç»ˆå‡†ç¡®ç‡: {result['final_val_accuracy']:.2f}%")
print(f"å‚æ•°å¢é•¿: {result['training_summary']['parameter_growth']:.1f}%")
print(f"å½¢æ€å‘ç”Ÿäº‹ä»¶: {len(result['morphogenesis_events'])}")
```

### é«˜çº§é…ç½®

```python
from neuroexapt.core.dnm_framework import DNMFramework

# è‡ªå®šä¹‰é…ç½®
config = {
    'neuron_division': {
        'splitter': {
            'entropy_threshold': 0.8,        # æ›´é«˜çš„åˆ†è£‚é˜ˆå€¼
            'split_probability': 0.5,        # æ›´æ¿€è¿›çš„åˆ†è£‚
            'max_splits_per_layer': 5        # å…è®¸æ›´å¤šåˆ†è£‚
        },
        'monitoring': {
            'analysis_frequency': 3,         # æ›´é¢‘ç¹çš„åˆ†æ
            'min_epoch_before_split': 5      # æ›´æ—©å¼€å§‹åˆ†è£‚
        }
    },
    'connection_growth': {
        'growth': {
            'max_new_connections': 5,        # æ›´å¤šè¿æ¥
            'growth_frequency': 6            # æ›´é¢‘ç¹çš„è¿æ¥ç”Ÿé•¿
        }
    },
    'framework': {
        'morphogenesis_frequency': 3,       # æ›´é¢‘ç¹çš„å½¢æ€å‘ç”Ÿ
        'target_accuracy_threshold': 93.0,  # ç›®æ ‡å‡†ç¡®ç‡
        'adaptive_morphogenesis': True      # è‡ªé€‚åº”å½¢æ€å‘ç”Ÿ
    }
}

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
dnm = DNMFramework(config)
result = dnm.train_with_morphogenesis(
    model, train_loader, val_loader, epochs=100
)
```

## ğŸ“Š ä¸ä¼ ç»Ÿæ–¹æ³•çš„å¯¹æ¯”

### ASO-SE vs DNM æ€§èƒ½å¯¹æ¯”

| æ–¹æ³• | å‡†ç¡®ç‡çªç ´ | æ¶æ„çµæ´»æ€§ | å‚æ•°æ•ˆç‡ | è‡ªé€‚åº”èƒ½åŠ› |
|------|------------|------------|----------|------------|
| åŸå§‹ASO-SE | 88% (åœæ») | å—é™äºé¢„å®šä¹‰æ“ä½œ | ä½ | å‡ ä¹æ—  |
| ä¿®å¤ç‰ˆASO-SE | 91-92% | æ”¹è¿›çš„æ¶æ„æœç´¢ | ä¸­ç­‰ | æœ‰é™ |
| **DNMæ¡†æ¶** | **93-95%** | **çœŸæ­£çš„ç»“æ„ç”Ÿé•¿** | **é«˜** | **å¼º** |

### å…³é”®çªç ´ç‚¹

1. **çªç ´88%ç“¶é¢ˆ**: DNMé¢„æœŸè¾¾åˆ°93-95%å‡†ç¡®ç‡
2. **çœŸæ­£çš„ç”Ÿé•¿**: ä¸å†å±€é™äºå›ºå®šæ“ä½œç©ºé—´
3. **æ™ºèƒ½è‡ªé€‚åº”**: åŸºäºä¿¡æ¯ç†µå’Œæ¢¯åº¦æ¨¡å¼çš„è‡ªå‘å˜å¼‚
4. **å¤šç›®æ ‡ä¼˜åŒ–**: å¹³è¡¡å‡†ç¡®ç‡ã€æ•ˆç‡ã€å¤æ‚åº¦

## ğŸ”§ é›†æˆåˆ°ç°æœ‰é¡¹ç›®

### æ›¿æ¢ç°æœ‰è®­ç»ƒå¾ªç¯

```python
# åŸå§‹è®­ç»ƒä»£ç 
# for epoch in range(epochs):
#     train_loss = train_epoch(model, train_loader, optimizer, criterion)
#     val_loss = validate_epoch(model, val_loader, criterion)

# æ›¿æ¢ä¸ºDNMè®­ç»ƒ
from neuroexapt.core.dnm_framework import DNMFramework

dnm = DNMFramework()
result = dnm.train_with_morphogenesis(
    model, train_loader, val_loader, epochs,
    optimizer=optimizer, criterion=criterion
)

# è·å–æ¼”åŒ–åçš„æ¨¡å‹
evolved_model = result['model']
```

### ä¸NeuroExapt V3é›†æˆ

```python
from neuroexapt.neuroexapt_v3 import NeuroExaptV3
from neuroexapt.core.dnm_framework import DNMFramework

# é¦–å…ˆä½¿ç”¨NeuroExapt V3è¿›è¡ŒåŸºç¡€ä¼˜åŒ–
neuroexapt = NeuroExaptV3(model)
base_result = neuroexapt.train(train_loader, val_loader, epochs=50)

# ç„¶åä½¿ç”¨DNMè¿›è¡Œæ·±åº¦æ¼”åŒ–
dnm = DNMFramework()
final_result = dnm.train_with_morphogenesis(
    base_result['model'], train_loader, val_loader, epochs=50
)
```

## ğŸ“ˆ ç›‘æ§å’Œåˆ†æ

### å®æ—¶ç›‘æ§

```python
def morphogenesis_callback(dnm_framework, model, epoch_record):
    """å½¢æ€å‘ç”Ÿå›è°ƒå‡½æ•°"""
    epoch = epoch_record['epoch']
    val_acc = epoch_record['val_acc']
    params = epoch_record['model_params']
    
    print(f"Epoch {epoch}: Accuracy={val_acc:.2f}%, Params={params:,}")
    
    # è®°å½•åˆ°tensorboardæˆ–å…¶ä»–ç›‘æ§ç³»ç»Ÿ
    # tensorboard.add_scalar('accuracy', val_acc, epoch)
    # tensorboard.add_scalar('parameters', params, epoch)

result = train_with_dnm(
    model, train_loader, val_loader, epochs=100,
    callbacks=[morphogenesis_callback]
)
```

### åˆ†æå½¢æ€å‘ç”Ÿäº‹ä»¶

```python
# åˆ†æç¥ç»å…ƒåˆ†è£‚äº‹ä»¶
for event in result['morphogenesis_events']:
    if event['neuron_splits'] > 0:
        print(f"Epoch {event['epoch']}: {event['neuron_splits']} neuron splits")
        print(f"  Performance before: {event['performance_before']:.2f}%")

# åˆ†æè¿æ¥ç”Ÿé•¿äº‹ä»¶
for event in result['morphogenesis_events']:
    if event['connections_grown'] > 0:
        print(f"Epoch {event['epoch']}: {event['connections_grown']} connections grown")

# è·å–è¯¦ç»†æ€»ç»“
summary = dnm.get_morphogenesis_summary()
print(f"æ€»ç¥ç»å…ƒåˆ†è£‚: {summary['framework_statistics']['total_neuron_splits']}")
print(f"æ€»è¿æ¥ç”Ÿé•¿: {summary['framework_statistics']['total_connections_grown']}")
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. é…ç½®è°ƒä¼˜

```python
# å¯¹äºCIFAR-10ç­‰ç®€å•æ•°æ®é›†
simple_config = {
    'neuron_division': {
        'splitter': {'entropy_threshold': 0.6, 'split_probability': 0.3}
    },
    'framework': {'morphogenesis_frequency': 5}
}

# å¯¹äºImageNetç­‰å¤æ‚æ•°æ®é›†
complex_config = {
    'neuron_division': {
        'splitter': {'entropy_threshold': 0.8, 'split_probability': 0.5}
    },
    'framework': {'morphogenesis_frequency': 3}
}
```

### 2. æ¸è¿›å¼è®­ç»ƒ

```python
# é˜¶æ®µ1: åŸºç¡€è®­ç»ƒ
base_config = {'framework': {'morphogenesis_frequency': 10}}
stage1_result = train_with_dnm(model, train_loader, val_loader, 
                              epochs=30, config=base_config)

# é˜¶æ®µ2: æ¿€è¿›æ¼”åŒ–
aggressive_config = {'framework': {'morphogenesis_frequency': 3}}
stage2_result = train_with_dnm(stage1_result['model'], train_loader, val_loader,
                              epochs=50, config=aggressive_config)
```

### 3. æ¨¡å‹ä¿å­˜å’Œæ¢å¤

```python
# ä¿å­˜æ¼”åŒ–æ¨¡å‹
dnm.export_evolved_model('evolved_model.pth', result['model'])

# åŠ è½½æ¼”åŒ–æ¨¡å‹
checkpoint = torch.load('evolved_model.pth')
model.load_state_dict(checkpoint['model_state_dict'])
morphogenesis_history = checkpoint['morphogenesis_summary']
```

## ğŸ”¬ ç†è®ºåŸºç¡€

### ä¿¡æ¯ç†µåˆ†è£‚åŸç†

```
ç¥ç»å…ƒä¿¡æ¯ç†µ: H(X) = -Î£ p(x) * log2(p(x))

åˆ†è£‚æ¡ä»¶:
1. H(neuron) > threshold (ä¿¡æ¯è¿‡è½½)
2. overload_score > threshold (ç»¼åˆè´Ÿè½½)
3. éšæœºæ¦‚ç‡è§¦å‘ (é¿å…ç¡®å®šæ€§)

æƒé‡ç»§æ‰¿: W_child = W_parent + Îµ * N(0, ÏƒÂ²)
```

### æ¢¯åº¦ç›¸å…³æ€§åˆ†æ

```
ç›¸å…³æ€§è®¡ç®—: Ï(L1, L2) = Cov(âˆ‡L1, âˆ‡L2) / (Ïƒ(âˆ‡L1) * Ïƒ(âˆ‡L2))

è¿æ¥ç”Ÿé•¿æ¡ä»¶:
1. Ï(L1, L2) > threshold
2. å±‚é—´è·ç¦»é€‚ä¸­ (2-6å±‚)
3. é¿å…å†—ä½™è¿æ¥
```

### å¤šç›®æ ‡ä¼˜åŒ–

```
å¸•ç´¯æ‰˜æœ€ä¼˜: 
minimize: [complexity, memory_usage, energy_consumption]
maximize: [accuracy, efficiency, training_speed]

éæ”¯é…æ’åº + æ‹¥æŒ¤è·ç¦»é€‰æ‹©
```

## ğŸš§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **CUDAå†…å­˜ä¸è¶³**
   ```python
   config = {
       'multi_objective': {
           'evolution': {'population_size': 8}  # å‡å°‘ç§ç¾¤å¤§å°
       }
   }
   ```

2. **è®­ç»ƒè¿‡æ…¢**
   ```python
   config = {
       'framework': {'morphogenesis_frequency': 10},  # é™ä½é¢‘ç‡
       'neuron_division': {
           'monitoring': {'analysis_frequency': 8}
       }
   }
   ```

3. **è¿‡åº¦ç”Ÿé•¿**
   ```python
   config = {
       'neuron_division': {
           'splitter': {
               'entropy_threshold': 0.9,  # æé«˜é˜ˆå€¼
               'max_splits_per_layer': 2  # é™åˆ¶åˆ†è£‚æ•°é‡
           }
       }
   }
   ```

## ğŸ“¦ å®Œæ•´ç¤ºä¾‹

```python
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from neuroexapt.core.dnm_framework import train_with_dnm

# æ•°æ®å‡†å¤‡
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)

train_loader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)
test_loader = DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)

# æ¨¡å‹å®šä¹‰
class EvolvableCIFARNet(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
        )
        
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(256, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

# åˆ›å»ºæ¨¡å‹
model = EvolvableCIFARNet()

# DNMè®­ç»ƒé…ç½®
dnm_config = {
    'neuron_division': {
        'splitter': {
            'entropy_threshold': 0.7,
            'split_probability': 0.4,
            'max_splits_per_layer': 3
        },
        'monitoring': {
            'analysis_frequency': 5,
            'min_epoch_before_split': 10
        }
    },
    'connection_growth': {
        'growth': {
            'max_new_connections': 3,
            'growth_frequency': 8
        }
    },
    'framework': {
        'morphogenesis_frequency': 5,
        'target_accuracy_threshold': 94.0,
        'early_stopping_patience': 20
    }
}

# å¼€å§‹DNMè®­ç»ƒ
print("ğŸ§¬ Starting DNM Training on CIFAR-10")
result = train_with_dnm(
    model=model,
    train_loader=train_loader,
    val_loader=test_loader,
    epochs=150,
    config=dnm_config
)

# ç»“æœåˆ†æ
print("\nğŸ“ˆ Training Results:")
print(f"Best Validation Accuracy: {result['best_val_accuracy']:.2f}%")
print(f"Final Validation Accuracy: {result['final_val_accuracy']:.2f}%")
print(f"Parameter Growth: {result['training_summary']['parameter_growth']:.1f}%")
print(f"Morphogenesis Events: {len(result['morphogenesis_events'])}")
print(f"Total Neuron Splits: {result['statistics']['total_neuron_splits']}")
print(f"Total Connections Grown: {result['statistics']['total_connections_grown']}")

# ä¿å­˜æ¼”åŒ–æ¨¡å‹
torch.save({
    'model_state_dict': result['model'].state_dict(),
    'training_summary': result['training_summary'],
    'morphogenesis_events': result['morphogenesis_events']
}, 'dnm_evolved_cifar_model.pth')

print("\nâœ… DNM training completed and model saved!")
```

## ğŸ‰ æ€»ç»“

DNMæ¡†æ¶ä»£è¡¨äº†ç¥ç»ç½‘ç»œè‡ªé€‚åº”æ¼”åŒ–çš„é©å‘½æ€§çªç ´ï¼š

1. **çœŸæ­£çš„ç”Ÿé•¿**: çªç ´å›ºå®šæ¶æ„ç©ºé—´é™åˆ¶
2. **æ™ºèƒ½è‡ªé€‚åº”**: åŸºäºä¿¡æ¯è®ºå’Œæ¢¯åº¦åˆ†æçš„ç§‘å­¦å†³ç­–
3. **æ€§èƒ½çªç ´**: é¢„æœŸçªç ´88%ç“¶é¢ˆï¼Œè¾¾åˆ°93-95%å‡†ç¡®ç‡
4. **æ˜“äºé›†æˆ**: ä¸€è¡Œä»£ç å³å¯å¯ç”¨DNMåŠŸèƒ½

DNMæ¡†æ¶è®©ç¥ç»ç½‘ç»œçœŸæ­£åƒæ´»çš„ç”Ÿç‰©ä½“ä¸€æ ·ï¼Œèƒ½å¤Ÿè‡ªå‘åœ°é€‰æ‹©æœ€ä½³çš„å˜å¼‚æ–¹å‘ï¼Œå®ç°äº†æ‚¨å¯¹"æ•´ä¸ªç¥ç»ç½‘ç»œåƒæ´»çš„ä¸€æ ·"çš„æœŸæœ›ï¼