
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.checkpoint as checkpoint
import time
from typing import List, Optional

# ğŸ”§ é€’å½’æ£€æµ‹å’Œé˜²æŠ¤æœºåˆ¶
_MIXEDOP_INITIALIZATION_STACK = set()

def _safe_mixedop_init(cls_name: str, *args, **kwargs):
    """
    å®‰å…¨çš„MixedOpåˆå§‹åŒ–å‡½æ•°ï¼Œé˜²æ­¢é€’å½’è°ƒç”¨
    """
    if cls_name in _MIXEDOP_INITIALIZATION_STACK:
        raise RuntimeError(f"æ£€æµ‹åˆ°{cls_name}çš„é€’å½’åˆå§‹åŒ–ï¼Œå¯èƒ½å­˜åœ¨å¾ªç¯ä¾èµ–")
    
    _MIXEDOP_INITIALIZATION_STACK.add(cls_name)
    try:
        # è¿™é‡Œä¼šè¢«å„ä¸ªMixedOpç±»çš„__init__æ–¹æ³•è°ƒç”¨
        return True
    finally:
        _MIXEDOP_INITIALIZATION_STACK.discard(cls_name)

# Triton accelerated helpers
from neuroexapt.kernels import TRITON_AVAILABLE, sepconv_forward_generic  # type: ignore
from neuroexapt.kernels.pool_triton import (
    TRITON_AVAILABLE as TRITON_POOL_AVAILABLE,
    avg_pool3x3_forward,
    max_pool3x3_forward,
    avg_pool5x5_forward,
    max_pool5x5_forward,
    avg_pool7x7_forward,
    max_pool7x7_forward,
    global_avgpool_forward,
)

# CUDA accelerated SoftmaxSum
try:
    from neuroexapt.cuda_ops import SoftmaxSumFunction, CUDA_AVAILABLE
    CUDA_SOFTMAX_AVAILABLE = CUDA_AVAILABLE
except ImportError:
    CUDA_SOFTMAX_AVAILABLE = False
    SoftmaxSumFunction = None  # type: ignore

# A collection of all possible operations that can be placed on an edge of the network graph
OPS = {
    'none': lambda C, stride, affine: Zero(stride),
    'avg_pool_3x3': lambda C, stride, affine: TritonAvgPool3x3(stride),
    'max_pool_3x3': lambda C, stride, affine: TritonMaxPool3x3(stride),
    'skip_connect': lambda C, stride, affine: Identity() if stride == 1 else FactorizedReduce(C, C, affine=affine),
    'sep_conv_3x3': lambda C, stride, affine: SepConv(C, C, 3, stride, 1, affine=affine),
    'sep_conv_5x5': lambda C, stride, affine: SepConv(C, C, 5, stride, 2, affine=affine),
    'sep_conv_7x7': lambda C, stride, affine: SepConv(C, C, 7, stride, 3, affine=affine),
    'dil_conv_3x3': lambda C, stride, affine: DilConv(C, C, 3, stride, 2, 2, affine=affine),
    'dil_conv_5x5': lambda C, stride, affine: DilConv(C, C, 5, stride, 4, 2, affine=affine),
    'conv_7x1_1x7': lambda C, stride, affine: nn.Sequential(
        nn.ReLU(inplace=False),
        nn.Conv2d(C, C, (1, 7), stride=(1, stride), padding=(0, 3), bias=False),
        nn.Conv2d(C, C, (7, 1), stride=(stride, 1), padding=(3, 0), bias=False),
        nn.BatchNorm2d(C, affine=affine)
    ),
}

class ReLUConvBN(nn.Module):
    """Standard ReLU-Conv-BatchNorm block."""
    def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):
        super(ReLUConvBN, self).__init__()
        self.op = nn.Sequential(
            nn.ReLU(inplace=False),
            nn.Conv2d(C_in, C_out, kernel_size, stride=stride, padding=padding, bias=False),
            nn.BatchNorm2d(C_out, affine=affine)
        )

    def forward(self, x):
        return self.op(x)

class DilConv(nn.Module):
    """Dilated separable convolution."""
    def __init__(self, C_in, C_out, kernel_size, stride, padding, dilation, affine=True):
        super(DilConv, self).__init__()
        self.relu = nn.ReLU(inplace=False)
        self.dw = nn.Conv2d(
            C_in,
            C_in,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=C_in,
            bias=False,
        )
        self.pw = nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False)
        self.bn = nn.BatchNorm2d(C_out, affine=affine)
        # cache parameters
        self._k = kernel_size
        self._stride = stride
        self._dilation = dilation

    def forward(self, x):
        x = self.relu(x)
        if TRITON_AVAILABLE and x.is_cuda and self._k in {3, 5, 7} and self._dilation in {1, 2}:
            y = sepconv_forward_generic(
                x,
                self.dw.weight,
                self.pw.weight,
                None,
                kernel_size=self._k,
                stride=self._stride,
                dilation=self._dilation,
            )
        else:
            y = self.pw(
                torch.nn.functional.conv2d(
                    x,
                    self.dw.weight,
                    None,
                    stride=self._stride,
                    padding=((self._k - 1) * self._dilation) // 2,
                    dilation=self._dilation,
                    groups=self.dw.in_channels,
                )
            )
        return self.bn(y)

class SepConv(nn.Module):
    """Separable convolution."""
    def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):
        super(SepConv, self).__init__()
        # First separable conv block
        self.relu1 = nn.ReLU(inplace=False)
        self.dw1 = nn.Conv2d(C_in, C_in, kernel_size, stride, padding, groups=C_in, bias=False)
        self.pw1 = nn.Conv2d(C_in, C_out, 1, padding=0, bias=False)
        self.bn1 = nn.BatchNorm2d(C_out, affine=affine)

        # Second separable conv block (stride=1)
        self.relu2 = nn.ReLU(inplace=False)
        self.dw2 = nn.Conv2d(C_out, C_out, kernel_size, 1, padding, groups=C_out, bias=False)
        self.pw2 = nn.Conv2d(C_out, C_out, 1, padding=0, bias=False)
        self.bn2 = nn.BatchNorm2d(C_out, affine=affine)

        self._k = kernel_size
        self._padding = padding
        self._stride = stride

    def _sepconv_block(self, x, dw, pw, bn, stride):
        if TRITON_AVAILABLE and x.is_cuda and self._k in {3, 5, 7}:
            y = sepconv_forward_generic(
                x,
                dw.weight,
                pw.weight,
                None,
                kernel_size=self._k,
                stride=stride,
                dilation=1,
            )
        else:
            y = pw(
                torch.nn.functional.conv2d(
                    x,
                    dw.weight,
                    None,
                    stride=stride,
                    padding=self._padding,
                    groups=dw.in_channels,
                )
            )
        return bn(y)

    def forward(self, x):
        y = self._sepconv_block(self.relu1(x), self.dw1, self.pw1, self.bn1, self._stride)
        y = self._sepconv_block(self.relu2(y), self.dw2, self.pw2, self.bn2, 1)
        return y

class Identity(nn.Module):
    """Identity mapping."""
    def __init__(self):
        super(Identity, self).__init__()

    def forward(self, x):
        return x

class Zero(nn.Module):
    """Zero operation, effectively removing a connection."""
    def __init__(self, stride):
        super(Zero, self).__init__()
        self.stride = stride

    def forward(self, x):
        if self.stride == 1:
            return x.mul(0.)
        return x[:, :, ::self.stride, ::self.stride].mul(0.)

class FactorizedReduce(nn.Module):
    """Reduces the spatial dimensions and doubles the channel dimensions."""
    def __init__(self, C_in, C_out, affine=True):
        super(FactorizedReduce, self).__init__()
        assert C_out % 2 == 0
        self.relu = nn.ReLU(inplace=False)
        self.conv_1 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)
        self.conv_2 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)
        self.bn = nn.BatchNorm2d(C_out, affine=affine)

    def forward(self, x):
        x = self.relu(x)
        out = torch.cat([self.conv_1(x), self.conv_2(x[:, :, 1:, 1:])], dim=1)
        out = self.bn(out)
        return out


class Resizing(nn.Module):
    """
    A utility module to resize tensors to a target channel count.
    This is used to match channel dimensions when operations with different
    channel counts are mixed.
    """
    def __init__(self, C_in, C_out, affine=True):
        super(Resizing, self).__init__()
        self.C_in = C_in
        self.C_out = C_out
        self.op = nn.Sequential(
            nn.ReLU(inplace=False),
            nn.Conv2d(C_in, C_out, 1, stride=1, padding=0, bias=False),
            nn.BatchNorm2d(C_out, affine=affine)
        )
    
    def forward(self, x):
        if self.C_in == self.C_out:
            return x
        return self.op(x)


class OptimizedMixedOp(nn.Module):
    """
    é«˜åº¦ä¼˜åŒ–çš„æ··åˆæ“ä½œç±»ï¼Œä¸“ä¸ºGPUæ€§èƒ½è®¾è®¡
    - å‡å°‘å†…å­˜åˆ†é…å’Œå¤åˆ¶
    - ä½¿ç”¨fused operations
    - å®ç°é«˜æ•ˆçš„åŠ æƒæ±‚å’Œ
    - æ·»åŠ æ“ä½œç»“æœç¼“å­˜
    """
    def __init__(self, C, stride, enable_caching=True):
        super(OptimizedMixedOp, self).__init__()
        from .genotypes import PRIMITIVES
        
        self._C = C
        self._stride = stride
        self.enable_caching = enable_caching
        
        # æ„å»ºæ“ä½œåˆ—è¡¨
        self._ops = nn.ModuleList()
        self._op_names = []
        
        for primitive in PRIMITIVES:
            op = OPS[primitive](C, stride, False)
            if 'pool' in primitive:
                op = nn.Sequential(op, nn.BatchNorm2d(C, affine=False))
            self._ops.append(op)
            self._op_names.append(primitive)
        
        # ç¼“å­˜ç›¸å…³
        self._cached_outputs: Optional[torch.Tensor] = None
        self._cached_input_hash: Optional[int] = None
        self._cache_hits = 0
        self._cache_misses = 0
        
        # é¢„åˆ†é…è¾“å‡ºå¼ é‡ä»¥å‡å°‘å†…å­˜åˆ†é…
        self.register_buffer('_output_buffer', torch.empty(1))
        
        # æ€§èƒ½ç›‘æ§
        self._forward_times: List[float] = []
        self._op_times: List[float] = []

    def _get_input_hash(self, x: torch.Tensor) -> int:
        """å¿«é€Ÿè¾“å…¥å“ˆå¸Œï¼Œç”¨äºç¼“å­˜æ£€æŸ¥"""
        return hash((x.shape, x.device, x.dtype, x.data_ptr()))

    def _maybe_resize_buffer(self, target_shape: torch.Size, device: torch.device) -> torch.Tensor:
        """æ™ºèƒ½ç¼“å†²åŒºå¤§å°è°ƒæ•´"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥è¿”å›åˆé€‚å¤§å°çš„tensor
        return torch.empty(target_shape, device=device, dtype=torch.float32)

    def forward(self, x: torch.Tensor, weights: torch.Tensor) -> torch.Tensor:
        """
        é«˜åº¦ä¼˜åŒ–çš„å‰å‘ä¼ æ’­
        """
        start_time = time.perf_counter()
        
        # æ£€æŸ¥ç¼“å­˜
        if self.enable_caching:
            input_hash = self._get_input_hash(x)
            if input_hash == self._cached_input_hash and self._cached_outputs is not None:
                self._cache_hits += 1
                return self._cached_outputs * weights.view(-1, 1, 1, 1, 1).sum(dim=0)
            else:
                self._cache_misses += 1
                self._cached_input_hash = input_hash

        # å¿«é€Ÿæƒé‡æ£€æŸ¥ - å¦‚æœåªæœ‰ä¸€ä¸ªæ“ä½œå ä¸»å¯¼åœ°ä½ï¼Œç›´æ¥è®¡ç®—
        max_weight_idx = int(weights.argmax().item())
        if weights[max_weight_idx] > 0.9:  # 90%ä»¥ä¸Šæƒé‡é›†ä¸­åœ¨ä¸€ä¸ªæ“ä½œä¸Š
            op_start = time.perf_counter()
            result = self._ops[max_weight_idx](x) * weights[max_weight_idx]
            self._op_times.append(time.perf_counter() - op_start)
            
            self._forward_times.append(time.perf_counter() - start_time)
            return result

        # å¹¶è¡Œè®¡ç®—æ‰€æœ‰æ“ä½œï¼ˆGPUå¹¶è¡Œä¼˜åŒ–ï¼‰
        print(f"  ğŸ”§ MixedOp: å¹¶è¡Œè®¡ç®— {len(self._ops)} ä¸ªæ“ä½œ...")
        
        # ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼å¹¶è¡Œè®¡ç®—ï¼Œè®©GPUè°ƒåº¦å™¨ä¼˜åŒ–
        op_start = time.perf_counter()
        
        # åˆ†æ‰¹å¤„ç†æ“ä½œä»¥ä¼˜åŒ–GPUå†…å­˜ä½¿ç”¨
        batch_size = min(4, len(self._ops))  # æ¯æ‰¹æœ€å¤š4ä¸ªæ“ä½œ
        outputs = []
        
        for i in range(0, len(self._ops), batch_size):
            batch_ops = self._ops[i:i+batch_size]
            batch_weights = weights[i:i+batch_size]
            
            # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„æ“ä½œ
            batch_outputs = []
            for j, op in enumerate(batch_ops):
                if batch_weights[j] > 1e-6:  # åªè®¡ç®—æœ‰æ„ä¹‰æƒé‡çš„æ“ä½œ
                    op_output = op(x)
                    batch_outputs.append(op_output * batch_weights[j])
            
            if batch_outputs:
                # åœ¨GPUä¸Šé«˜æ•ˆæ±‚å’Œ
                batch_result = torch.stack(batch_outputs, dim=0).sum(dim=0)
                outputs.append(batch_result)
        
        # æœ€ç»ˆæ±‚å’Œ
        if outputs:
            result = torch.stack(outputs, dim=0).sum(dim=0)
        else:
            # å¦‚æœæ‰€æœ‰æƒé‡éƒ½å¾ˆå°ï¼Œè¿”å›é›¶å¼ é‡
            result = torch.zeros_like(x)
        
        op_time = time.perf_counter() - op_start
        self._op_times.append(op_time)
        
        # æ›´æ–°ç¼“å­˜
        if self.enable_caching:
            self._cached_outputs = torch.stack([op(x) for op in self._ops], dim=0)
        
        total_time = time.perf_counter() - start_time
        self._forward_times.append(total_time)
        
        # å®šæœŸè¾“å‡ºæ€§èƒ½ç»Ÿè®¡
        if len(self._forward_times) % 50 == 0:
            avg_time = sum(self._forward_times[-50:]) / 50
            avg_op_time = sum(self._op_times[-50:]) / 50
            cache_rate = self._cache_hits / max(1, self._cache_hits + self._cache_misses)
            print(f"    ğŸ“Š MixedOpæ€§èƒ½: å¹³å‡{avg_time*1000:.2f}ms, æ“ä½œ{avg_op_time*1000:.2f}ms, ç¼“å­˜å‘½ä¸­ç‡{cache_rate:.1%}")
        
        return result

    def get_performance_stats(self) -> dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        if not self._forward_times:
            return {}
        
        return {
            'avg_forward_time': sum(self._forward_times) / len(self._forward_times),
            'avg_op_time': sum(self._op_times) / len(self._op_times) if self._op_times else 0,
            'cache_hit_rate': self._cache_hits / max(1, self._cache_hits + self._cache_misses),
            'total_forwards': len(self._forward_times)
        }

class MixedOp(nn.Module):
    """
    A differentiable mixed operation.
    This is the lightweight version, only mixing operation types.
    """
    def __init__(self, C, stride):
        super(MixedOp, self).__init__()
        # Import PRIMITIVES to ensure consistent ordering
        from .genotypes import PRIMITIVES
        
        self._ops = nn.ModuleList()
        for primitive in PRIMITIVES:
            op = OPS[primitive](C, stride, False)
            if 'pool' in primitive:
                op = nn.Sequential(op, nn.BatchNorm2d(C, affine=False))
            self._ops.append(op)

    def forward(self, x, weights):
        """
        Args:
            x: input tensor
            weights: a tensor of shape [num_ops], representing arch params.
        Returns:
            The weighted sum of the outputs of all operations.
        Note:
            This implementation vectorizes the weighted sum to reduce Python overhead.
        """
        # è®¡æ•°å™¨æ›´æ–°
        if hasattr(self, '_step_counter'):
            self._step_counter += 1
        else:
            self._step_counter = 1
        
        # Compute outputs for each operation
        outputs = [op(x) for op in self._ops]  # list of tensors

        # Use CUDA-accelerated SoftmaxSum if available and beneficial
        if (CUDA_SOFTMAX_AVAILABLE and SoftmaxSumFunction is not None and 
            x.is_cuda and len(outputs) >= 4 and outputs[0].numel() >= 1024):
            # Stack and use fused kernel for large operations
            stacked = torch.stack(outputs, dim=0)
            return SoftmaxSumFunction.apply(stacked, weights)
        else:
            # Fallback to standard PyTorch implementation
            stacked = torch.stack(outputs, dim=0)
            weighted = stacked * weights.view(-1, 1, 1, 1, 1)
            return weighted.sum(dim=0)

class LazyMixedOp(nn.Module):
    """
    é«˜æ€§èƒ½æ‡’è®¡ç®—æ··åˆæ“ä½œï¼Œä¸“ä¸ºExaptæ¨¡å¼è®¾è®¡
    ç‰¹æ€§ï¼š
    1. æ‡’è®¡ç®—ï¼šåªè®¡ç®—æƒé‡å¤§çš„æ“ä½œ
    2. æ™ºèƒ½ç¼“å­˜ï¼šç¼“å­˜é‡å¤è®¡ç®—ç»“æœ  
    3. æ—©æœŸç»ˆæ­¢ï¼šæƒé‡æ”¶æ•›æ—¶è·³è¿‡è®¡ç®—
    4. å†…å­˜æ± ï¼šé¢„åˆ†é…å†…å­˜é¿å…é¢‘ç¹åˆ†é…
    5. æ“ä½œå‰ªæï¼šåŠ¨æ€ç§»é™¤ä½æƒé‡æ“ä½œ
    """
    def __init__(self, C, stride, lazy_threshold=0.01, cache_size=16, enable_pruning=True):
        # ğŸ”§ é€’å½’æ£€æµ‹
        _safe_mixedop_init("LazyMixedOp")
        super(LazyMixedOp, self).__init__()
        from .genotypes import PRIMITIVES
        
        self._C = C
        self._stride = stride
        self.lazy_threshold = lazy_threshold  # æ‡’è®¡ç®—é˜ˆå€¼
        self.cache_size = cache_size
        self.enable_pruning = enable_pruning
        
        # æ„å»ºæ“ä½œåˆ—è¡¨
        self._ops = nn.ModuleList()
        self._op_names = []
        self._op_active = []  # æ“ä½œæ¿€æ´»çŠ¶æ€
        
        for primitive in PRIMITIVES:
            op = OPS[primitive](C, stride, False)
            if 'pool' in primitive:
                op = nn.Sequential(op, nn.BatchNorm2d(C, affine=False))
            self._ops.append(op)
            self._op_names.append(primitive)
            self._op_active.append(True)  # åˆå§‹æ—¶æ‰€æœ‰æ“ä½œéƒ½æ¿€æ´»
        
        # æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ
        self._cache = {}  # {input_hash: {op_idx: output}}
        self._cache_hits = 0
        self._cache_misses = 0
        self._cache_order = []  # LRUç¼“å­˜ç®¡ç†
        
        # æƒé‡å†å²è®°å½•ï¼ˆç”¨äºæ”¶æ•›æ£€æµ‹ï¼‰
        self._weight_history = []
        self._converged_ops = set()  # å·²æ”¶æ•›çš„æ“ä½œ
        
        # å†…å­˜æ± 
        self._memory_pool = {}  # {shape: [tensor1, tensor2, ...]}
        self._pool_hits = 0
        self._pool_misses = 0
        
        # æ€§èƒ½ç»Ÿè®¡
        self._stats = {
            'lazy_skips': 0,
            'total_ops_computed': 0,
            'total_forward_calls': 0,
            'pruned_ops': 0,
            'cache_hit_rate': 0.0,
            'memory_pool_hit_rate': 0.0
        }
        
        # é¢„çƒ­çŠ¶æ€
        self._warmup_calls = 0
        self._warmup_threshold = 20

    def _get_input_hash(self, x: torch.Tensor) -> int:
        """å¿«é€Ÿè¾“å…¥å“ˆå¸Œç”¨äºç¼“å­˜"""
        return hash((x.shape, x.device, x.dtype, x.data_ptr()))

    def _get_from_memory_pool(self, shape: torch.Size, device: torch.device, dtype: torch.dtype) -> torch.Tensor:
        """ä»å†…å­˜æ± è·å–å¼ é‡"""
        key = (shape, device, dtype)
        if key in self._memory_pool and self._memory_pool[key]:
            self._pool_hits += 1
            tensor = self._memory_pool[key].pop()
            tensor.zero_()  # æ¸…é›¶é‡ç”¨
            return tensor
        else:
            self._pool_misses += 1
            return torch.zeros(shape, device=device, dtype=dtype)

    def _return_to_memory_pool(self, tensor: torch.Tensor):
        """è¿”å›å¼ é‡åˆ°å†…å­˜æ± """
        key = (tensor.shape, tensor.device, tensor.dtype)
        if key not in self._memory_pool:
            self._memory_pool[key] = []
        
        # é™åˆ¶å†…å­˜æ± å¤§å°
        if len(self._memory_pool[key]) < 4:
            self._memory_pool[key].append(tensor.detach())

    def _update_cache(self, input_hash: int, op_idx: int, output: torch.Tensor):
        """æ›´æ–°ç¼“å­˜"""
        if input_hash not in self._cache:
            self._cache[input_hash] = {}
            self._cache_order.append(input_hash)
        
        self._cache[input_hash][op_idx] = output.detach().clone()
        
        # LRUç¼“å­˜ç®¡ç†
        if len(self._cache_order) > self.cache_size:
            oldest_hash = self._cache_order.pop(0)
            del self._cache[oldest_hash]

    def _detect_weight_convergence(self, weights: torch.Tensor) -> set:
        """æ£€æµ‹æƒé‡æ”¶æ•›çš„æ“ä½œ"""
        self._weight_history.append(weights.detach().clone())
        
        # ä¿æŒæœ€è¿‘10æ¬¡æƒé‡å†å²
        if len(self._weight_history) > 10:
            self._weight_history.pop(0)
        
        # éœ€è¦è‡³å°‘5æ¬¡å†å²è®°å½•æ‰èƒ½åˆ¤æ–­æ”¶æ•›
        if len(self._weight_history) < 5:
            return set()
        
        converged = set()
        for i in range(len(weights)):
            # æ£€æŸ¥æœ€è¿‘5æ¬¡çš„æƒé‡å˜åŒ–
            recent_weights = [h[i].item() for h in self._weight_history[-5:]]
            weight_std = torch.tensor(recent_weights).std().item()
            
            # å¦‚æœæƒé‡å˜åŒ–å¾ˆå°ä¸”æƒé‡æœ¬èº«å¾ˆå°ï¼Œè®¤ä¸ºå·²æ”¶æ•›
            if weight_std < 0.001 and weights[i].item() < self.lazy_threshold:
                converged.add(i)
        
        return converged

    def _prune_operations(self, weights: torch.Tensor):
        """åŠ¨æ€å‰ªæä½æƒé‡æ“ä½œ"""
        if not self.enable_pruning or self._warmup_calls < self._warmup_threshold:
            return
        
        for i, weight in enumerate(weights):
            if weight.item() < self.lazy_threshold / 10 and self._op_active[i]:
                self._op_active[i] = False
                self._stats['pruned_ops'] += 1
                print(f"    âœ‚ï¸  å‰ªææ“ä½œ {self._op_names[i]} (æƒé‡: {weight.item():.6f})")

    def forward(self, x: torch.Tensor, weights: torch.Tensor) -> torch.Tensor:
        """é«˜æ€§èƒ½æ‡’è®¡ç®—å‰å‘ä¼ æ’­"""
        self._stats['total_forward_calls'] += 1
        self._warmup_calls += 1
        
        input_hash = self._get_input_hash(x)
        
        # æ£€æµ‹æƒé‡æ”¶æ•›å’Œæ“ä½œå‰ªæ
        self._converged_ops = self._detect_weight_convergence(weights)
        if self.enable_pruning:
            self._prune_operations(weights)
        
        # å¿«é€Ÿè·¯å¾„ï¼šå¦‚æœä¸€ä¸ªæ“ä½œæƒé‡å ä¸»å¯¼(>95%)ä¸”å·²æ”¶æ•›
        max_weight_idx = int(weights.argmax().item())
        if weights[max_weight_idx] > 0.95 and max_weight_idx in self._converged_ops:
            if input_hash in self._cache and max_weight_idx in self._cache[input_hash]:
                self._cache_hits += 1
                return self._cache[input_hash][max_weight_idx]
            else:
                result = self._ops[max_weight_idx](x)
                self._update_cache(input_hash, max_weight_idx, result)
                self._cache_misses += 1
                return result

        # æ‡’è®¡ç®—ï¼šåªè®¡ç®—æƒé‡å¤§äºé˜ˆå€¼ä¸”æœªè¢«å‰ªæçš„æ“ä½œ
        active_ops = []
        active_weights = []
        outputs = []
        
        for i, (op, weight) in enumerate(zip(self._ops, weights)):
            if not self._op_active[i]:
                continue  # è·³è¿‡è¢«å‰ªæçš„æ“ä½œ
                
            if weight.item() < self.lazy_threshold and i not in self._converged_ops:
                self._stats['lazy_skips'] += 1
                continue
            
            # æ£€æŸ¥ç¼“å­˜
            if input_hash in self._cache and i in self._cache[input_hash]:
                output = self._cache[input_hash][i]
                self._cache_hits += 1
            else:
                output = op(x)
                self._update_cache(input_hash, i, output)
                self._cache_misses += 1
                self._stats['total_ops_computed'] += 1
            
            outputs.append(output * weight)
            active_ops.append(i)
            active_weights.append(weight.item())

        # å¤„ç†ç»“æœ
        if outputs:
            if len(outputs) == 1:
                result = outputs[0]
            else:
                # ä½¿ç”¨å†…å­˜æ± ä¼˜åŒ–æ±‚å’Œ
                result = self._get_from_memory_pool(outputs[0].shape, outputs[0].device, outputs[0].dtype)
                for output in outputs:
                    result = result + output
        else:
            # æ‰€æœ‰æ“ä½œéƒ½è¢«è·³è¿‡ï¼Œè¿”å›é›¶å¼ é‡
            result = self._get_from_memory_pool(x.shape, x.device, x.dtype)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        if self._stats['total_forward_calls'] % 100 == 0:
            self._update_stats()
        
        return result

    def _update_stats(self):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        total_cache_ops = self._cache_hits + self._cache_misses
        self._stats['cache_hit_rate'] = self._cache_hits / max(1, total_cache_ops)
        
        total_pool_ops = self._pool_hits + self._pool_misses
        self._stats['memory_pool_hit_rate'] = self._pool_hits / max(1, total_pool_ops)
        
        # è®°å½•æ€§èƒ½ç»Ÿè®¡ï¼ˆå…³é—­è¾“å‡ºï¼‰
        # if self._stats['total_forward_calls'] % 5000 == 0:
        #     print(f"    ğŸ“Š LazyMixedOp: ç¼“å­˜å‘½ä¸­ç‡{self._stats['cache_hit_rate']:.1%}, æ”¶æ•›æ“ä½œ{len(self._converged_ops)}/{len(self._ops)}")

    def get_performance_stats(self) -> dict:
        """è·å–è¯¦ç»†æ€§èƒ½ç»Ÿè®¡"""
        return self._stats.copy()

    def clear_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        self._cache.clear()
        self._cache_order.clear()
        for pool in self._memory_pool.values():
            pool.clear() 

class GradientOptimizedMixedOp(nn.Module):
    """
    åå‘ä¼ æ’­ä¼˜åŒ–çš„æ··åˆæ“ä½œï¼Œä¸“é—¨è§£å†³åå‘ä¼ æ’­æ…¢çš„é—®é¢˜
    
    ä¼˜åŒ–ç­–ç•¥ï¼š
    1. é€‰æ‹©æ€§æ¢¯åº¦è®¡ç®—ï¼šåªä¸ºæƒé‡å¤§çš„æ“ä½œè®¡ç®—æ¢¯åº¦
    2. æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼šå‡å°‘å†…å­˜ä½¿ç”¨å’Œè®¡ç®—å›¾å¤æ‚åº¦
    3. å¼‚æ­¥æ¢¯åº¦ç´¯ç§¯ï¼šé¿å…åŒæ­¥ç­‰å¾…
    4. å†…å­˜æ± å¤ç”¨ï¼šå‡å°‘å†…å­˜åˆ†é…å¼€é”€
    5. è®¡ç®—å›¾å‰ªæï¼šç§»é™¤ä¸å¿…è¦çš„è®¡ç®—èŠ‚ç‚¹
    """
    def __init__(self, C, stride, gradient_threshold=0.01, use_checkpoint=True, memory_efficient=True):
        # ğŸ”§ é€’å½’æ£€æµ‹
        _safe_mixedop_init("GradientOptimizedMixedOp")
        super(GradientOptimizedMixedOp, self).__init__()
        from .genotypes import PRIMITIVES
        
        self._C = C
        self._stride = stride
        self.gradient_threshold = gradient_threshold
        self.use_checkpoint = use_checkpoint
        self.memory_efficient = memory_efficient
        
        # æ„å»ºæ“ä½œåˆ—è¡¨
        self._ops = nn.ModuleList()
        self._op_names = []
        
        for primitive in PRIMITIVES:
            op = OPS[primitive](C, stride, False)
            if 'pool' in primitive:
                op = nn.Sequential(op, nn.BatchNorm2d(C, affine=False))
            self._ops.append(op)
            self._op_names.append(primitive)
        
        # æƒé‡è·Ÿè¸ªç”¨äºæ¢¯åº¦ä¼˜åŒ–
        self._weight_momentum = 0.9
        self._avg_weights = torch.zeros(len(PRIMITIVES))
        self._gradient_mask = torch.ones(len(PRIMITIVES), dtype=torch.bool)
        
        # æ€§èƒ½ç»Ÿè®¡
        self._stats = {
            'forward_calls': 0,
            'gradient_skips': 0,
            'checkpoint_saves': 0,
            'memory_reuse': 0
        }
        
        # å†…å­˜æ± 
        self._output_cache = {}
        self._gradient_cache = {}

    def _update_gradient_mask(self, weights: torch.Tensor):
        """æ›´æ–°æ¢¯åº¦è®¡ç®—æ©ç ï¼Œåªä¸ºé‡è¦çš„æ“ä½œè®¡ç®—æ¢¯åº¦"""
        # æŒ‡æ•°ç§»åŠ¨å¹³å‡æ›´æ–°æƒé‡
        if self._avg_weights.device != weights.device:
            self._avg_weights = self._avg_weights.to(weights.device)
            self._gradient_mask = self._gradient_mask.to(weights.device)
        
        self._avg_weights = self._weight_momentum * self._avg_weights + (1 - self._weight_momentum) * weights.detach()
        
        # åªä¸ºæƒé‡å¤§äºé˜ˆå€¼çš„æ“ä½œè®¡ç®—æ¢¯åº¦
        new_mask = self._avg_weights > self.gradient_threshold
        
        # è‡³å°‘ä¿ç•™æƒé‡æœ€å¤§çš„ä¸¤ä¸ªæ“ä½œ
        if new_mask.sum() < 2:
            top_indices = torch.topk(self._avg_weights, 2).indices
            new_mask[top_indices] = True
        
        # æ›´æ–°æ©ç 
        mask_changed = not torch.equal(self._gradient_mask, new_mask)
        self._gradient_mask = new_mask
        
        # å‡å°‘æ©ç æ›´æ–°è¾“å‡º
        # if mask_changed:
        #     active_ops = [self._op_names[i] for i in range(len(self._op_names)) if self._gradient_mask[i]]
        #     print(f"    ğŸ¯ æ¢¯åº¦è®¡ç®—æ©ç æ›´æ–°: æ¿€æ´»æ“ä½œ {active_ops}")
        
        return mask_changed

    def _selective_forward(self, x: torch.Tensor, weights: torch.Tensor):
        """é€‰æ‹©æ€§å‰å‘ä¼ æ’­ï¼Œåªè®¡ç®—éœ€è¦æ¢¯åº¦çš„æ“ä½œ"""
        # æ›´æ–°æ¢¯åº¦æ©ç 
        self._update_gradient_mask(weights)
        
        # å¿«é€Ÿè·¯å¾„ï¼šå¦‚æœåªæœ‰ä¸€ä¸ªæ“ä½œå ä¸»å¯¼
        max_idx = int(weights.argmax().item())
        if weights[max_idx] > 0.95:
            return self._ops[max_idx](x) * weights[max_idx]
        
        # é€‰æ‹©æ€§è®¡ç®—
        active_outputs = []
        active_weights = []
        
        for i, (op, weight) in enumerate(zip(self._ops, weights)):
            if self._gradient_mask[i] or weight > self.gradient_threshold:
                if self.use_checkpoint and self.training:
                    # ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹å‡å°‘å†…å­˜ä½¿ç”¨
                    output = checkpoint.checkpoint(op, x, use_reentrant=False)
                    self._stats['checkpoint_saves'] += 1
                else:
                    output = op(x)
                
                active_outputs.append(output * weight)  # type: ignore[operator]
                active_weights.append(weight.item())
            else:
                # è·³è¿‡æ¢¯åº¦è®¡ç®—ï¼Œä½¿ç”¨detach
                with torch.no_grad():
                    output = op(x)
                active_outputs.append(output.detach() * weight.detach())  # type: ignore[operator]
                self._stats['gradient_skips'] += 1
        
        # å†…å­˜é«˜æ•ˆçš„æ±‚å’Œ
        if len(active_outputs) == 1:
            return active_outputs[0]
        elif len(active_outputs) == 2:
            return active_outputs[0] + active_outputs[1]
        else:
            # åˆ†å±‚æ±‚å’Œå‡å°‘å†…å­˜å³°å€¼
            result = active_outputs[0]
            for output in active_outputs[1:]:
                result = result + output
            return result

    def forward(self, x: torch.Tensor, weights: torch.Tensor) -> torch.Tensor:
        """ä¼˜åŒ–çš„å‰å‘ä¼ æ’­"""
        self._stats['forward_calls'] += 1
        
        # ä½¿ç”¨é€‰æ‹©æ€§å‰å‘ä¼ æ’­
        result = self._selective_forward(x, weights)
        
        # è®°å½•æ€§èƒ½ç»Ÿè®¡ï¼ˆå…³é—­è¾“å‡ºï¼‰
        # if self._stats['forward_calls'] % 5000 == 0:
        #     skip_rate = self._stats['gradient_skips'] / max(1, self._stats['forward_calls'] * len(self._ops))
        #     print(f"    ğŸ“Š æ¢¯åº¦ä¼˜åŒ–: è·³è¿‡ç‡{skip_rate:.1%}, æ¿€æ´»{self._gradient_mask.sum().item()}/{len(self._ops)}")
        
        return result

    def get_gradient_stats(self) -> dict:
        """è·å–æ¢¯åº¦ä¼˜åŒ–ç»Ÿè®¡"""
        return {
            'gradient_mask': self._gradient_mask.cpu().tolist(),
            'avg_weights': self._avg_weights.cpu().tolist(),
            'active_ops': self._gradient_mask.sum().item(),
            'total_ops': len(self._ops),
            **self._stats
        }

class MemoryEfficientMixedOp(nn.Module):
    """
    å†…å­˜é«˜æ•ˆçš„æ··åˆæ“ä½œï¼Œä¸“é—¨è§£å†³GPUå†…å­˜ä½¿ç”¨é—®é¢˜
    
    ç‰¹æ€§ï¼š
    1. æµå¼è®¡ç®—ï¼šé¿å…åŒæ—¶å­˜å‚¨æ‰€æœ‰æ“ä½œçš„è¾“å‡º
    2. å†…å­˜å›æ”¶ï¼šåŠæ—¶é‡Šæ”¾ä¸­é—´ç»“æœ
    3. æ‰¹é‡ä¼˜åŒ–ï¼šåˆå¹¶å°çš„æ“ä½œå‡å°‘å¼€é”€
    4. ç¼“å­˜å¤ç”¨ï¼šæ™ºèƒ½å¤ç”¨è®¡ç®—ç»“æœ
    """
    def __init__(self, C, stride, stream_compute=True, cache_outputs=True):
        # ğŸ”§ é€’å½’æ£€æµ‹
        _safe_mixedop_init("MemoryEfficientMixedOp")
        super(MemoryEfficientMixedOp, self).__init__()
        from .genotypes import PRIMITIVES
        
        self._C = C
        self._stride = stride
        self.stream_compute = stream_compute
        self.cache_outputs = cache_outputs
        
        # æ„å»ºæ“ä½œåˆ—è¡¨
        self._ops = nn.ModuleList()
        self._op_names = []
        
        for primitive in PRIMITIVES:
            op = OPS[primitive](C, stride, False)
            if 'pool' in primitive:
                op = nn.Sequential(op, nn.BatchNorm2d(C, affine=False))
            self._ops.append(op)
            self._op_names.append(primitive)
        
        # å†…å­˜ç®¡ç†
        self._output_cache = {}
        self._memory_high_watermark = 0
        self._cache_hits = 0
        self._cache_misses = 0

    def _get_cache_key(self, x: torch.Tensor) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return f"{x.shape}_{x.device}_{x.data_ptr()}"

    def _stream_forward(self, x: torch.Tensor, weights: torch.Tensor) -> torch.Tensor:
        """æµå¼å‰å‘ä¼ æ’­ï¼Œå‡å°‘å†…å­˜å³°å€¼ä½¿ç”¨"""
        # åˆå§‹åŒ–ç»“æœå¼ é‡
        result = torch.zeros_like(x)
        
        # æµå¼è®¡ç®—æ¯ä¸ªæ“ä½œ
        for i, (op, weight) in enumerate(zip(self._ops, weights)):
            if weight.item() < 1e-6:  # è·³è¿‡æƒé‡å¾ˆå°çš„æ“ä½œ
                continue
            
            # æ£€æŸ¥ç¼“å­˜
            cache_key = f"{self._get_cache_key(x)}_{i}"
            if self.cache_outputs and cache_key in self._output_cache:
                output = self._output_cache[cache_key]
                self._cache_hits += 1
            else:
                output = op(x)
                if self.cache_outputs:
                    self._output_cache[cache_key] = output.detach().clone()
                    # é™åˆ¶ç¼“å­˜å¤§å°
                    if len(self._output_cache) > 32:
                        oldest_key = next(iter(self._output_cache))
                        del self._output_cache[oldest_key]
                self._cache_misses += 1
            
            # ç´¯ç§¯åˆ°ç»“æœä¸­
            weighted_output = output * weight
            result = result + weighted_output
            
            # åŠæ—¶é‡Šæ”¾å†…å­˜
            del weighted_output
            if not self.cache_outputs:
                del output
        
        return result

    def forward(self, x: torch.Tensor, weights: torch.Tensor) -> torch.Tensor:
        """å†…å­˜é«˜æ•ˆçš„å‰å‘ä¼ æ’­"""
        # è®°å½•å†…å­˜ä½¿ç”¨
        if torch.cuda.is_available():
            current_memory = torch.cuda.memory_allocated()
            self._memory_high_watermark = max(self._memory_high_watermark, current_memory)
        
        if self.stream_compute:
            result = self._stream_forward(x, weights)
        else:
            # æ ‡å‡†å®ç°ä½†ä¼˜åŒ–å†…å­˜
            outputs = []
            for i, (op, weight) in enumerate(zip(self._ops, weights)):
                if weight.item() > 1e-6:  # åªè®¡ç®—æœ‰æ„ä¹‰çš„æ“ä½œ
                    output = op(x) * weight
                    outputs.append(output)
            
            if outputs:
                result = torch.stack(outputs, dim=0).sum(dim=0)
            else:
                result = torch.zeros_like(x)
        
        # å®šæœŸæ¸…ç†ç¼“å­˜
        if hasattr(self, '_forward_count'):
            self._forward_count += 1
        else:
            self._forward_count = 1
        
        if self._forward_count % 500 == 0:
            torch.cuda.empty_cache()
            # å…³é—­å†…å­˜ç»Ÿè®¡è¾“å‡º
            # if self.cache_outputs:
            #     cache_hit_rate = self._cache_hits / max(1, self._cache_hits + self._cache_misses)
            #     print(f"    ğŸ’¾ å†…å­˜æ•ˆç‡ç»Ÿè®¡: ç¼“å­˜å‘½ä¸­ç‡ {cache_hit_rate:.1%}, å³°å€¼å†…å­˜ {self._memory_high_watermark/1024/1024:.1f}MB")
        
        return result 

class TritonAvgPool3x3(nn.Module):
    def __init__(self, stride: int):
        super().__init__()
        self.stride = stride

    def forward(self, x: torch.Tensor):  # type: ignore[override]
        if TRITON_POOL_AVAILABLE and x.is_cuda:
            return avg_pool3x3_forward(x, self.stride)
        return torch.nn.functional.avg_pool2d(x, 3, stride=self.stride, padding=1, count_include_pad=False)


class TritonMaxPool3x3(nn.Module):
    def __init__(self, stride: int):
        super().__init__()
        self.stride = stride

    def forward(self, x: torch.Tensor):  # type: ignore[override]
        if TRITON_POOL_AVAILABLE and x.is_cuda:
            return max_pool3x3_forward(x, self.stride)
        return torch.nn.functional.max_pool2d(x, 3, stride=self.stride, padding=1) 

class FusedOptimizedMixedOp(nn.Module):
    """
    ğŸš€ èåˆä¼˜åŒ–çš„æ··åˆæ“ä½œ - åŒæ—¶åº”ç”¨æ‰€æœ‰ä¼˜åŒ–ç­–ç•¥
    
    èåˆç‰¹æ€§ï¼š
    1. æ¢¯åº¦ä¼˜åŒ–ï¼šé€‰æ‹©æ€§æ¢¯åº¦è®¡ç®— + æ£€æŸ¥ç‚¹
    2. å†…å­˜ä¼˜åŒ–ï¼šæµå¼è®¡ç®— + ç¼“å­˜å¤ç”¨  
    3. æ‡’è®¡ç®—ï¼šåŠ¨æ€å‰ªæ + æ—©æœŸç»ˆæ­¢
    4. TritonåŠ é€Ÿï¼šè‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨CUDAæ ¸
    5. æ™ºèƒ½è°ƒåº¦ï¼šæ ¹æ®è´Ÿè½½è‡ªé€‚åº”è°ƒæ•´ç­–ç•¥
    """
    def __init__(self, C, stride, 
                 gradient_threshold=0.01, 
                 lazy_threshold=0.01,
                 use_checkpoint=True,
                 cache_size=16):
        # ğŸ”§ é€’å½’æ£€æµ‹
        _safe_mixedop_init("FusedOptimizedMixedOp")
        super(FusedOptimizedMixedOp, self).__init__()
        
        from .genotypes import PRIMITIVES
        
        self._C = C
        self._stride = stride
        self.gradient_threshold = gradient_threshold
        self.lazy_threshold = lazy_threshold
        self.use_checkpoint = use_checkpoint
        self.cache_size = cache_size
        
        # æ„å»ºæ“ä½œåˆ—è¡¨
        self._ops = nn.ModuleList()
        self._op_names = []
        
        for primitive in PRIMITIVES:
            op = OPS[primitive](C, stride, False)
            if 'pool' in primitive:
                op = nn.Sequential(op, nn.BatchNorm2d(C, affine=False))
            self._ops.append(op)
            self._op_names.append(primitive)
        
        # èåˆä¼˜åŒ–ç»„ä»¶ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._gradient_optimizer = None
        self._memory_manager = None
        self._lazy_computer = None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self._stats = {
            'forward_calls': 0,
            'gradient_optimizations': 0,
            'memory_optimizations': 0,
            'lazy_optimizations': 0,
            'cache_hits': 0,
            'triton_usage': 0
        }
    
    def _init_gradient_optimizer(self):
        """åˆå§‹åŒ–æ¢¯åº¦ä¼˜åŒ–ç»„ä»¶"""
        return {
            'weight_momentum': 0.9,
            'avg_weights': torch.zeros(len(self._ops)),
            'gradient_mask': torch.ones(len(self._ops), dtype=torch.bool),
            'checkpoint_enabled': self.use_checkpoint
        }
    
    def _init_memory_manager(self):
        """åˆå§‹åŒ–å†…å­˜ç®¡ç†ç»„ä»¶"""
        return {
            'output_cache': {},
            'memory_pool': {},
            'max_cache_size': self.cache_size,
            'stream_compute': True
        }
    
    def _init_lazy_computer(self):
        """åˆå§‹åŒ–æ‡’è®¡ç®—ç»„ä»¶"""
        return {
            'op_usage_count': torch.zeros(len(self._ops)),
            'active_mask': torch.ones(len(self._ops), dtype=torch.bool),
            'early_termination': True
        }
    
    def _ensure_gradient_optimizer(self):
        """ç¡®ä¿æ¢¯åº¦ä¼˜åŒ–å™¨å·²åˆå§‹åŒ–"""
        if self._gradient_optimizer is None:
            self._gradient_optimizer = self._init_gradient_optimizer()
    
    def _ensure_memory_manager(self):
        """ç¡®ä¿å†…å­˜ç®¡ç†å™¨å·²åˆå§‹åŒ–"""
        if self._memory_manager is None:
            self._memory_manager = self._init_memory_manager()
    
    def _ensure_lazy_computer(self):
        """ç¡®ä¿æ‡’è®¡ç®—å™¨å·²åˆå§‹åŒ–"""
        if self._lazy_computer is None:
            self._lazy_computer = self._init_lazy_computer()
    
    def _update_gradient_mask(self, weights: torch.Tensor):
        """æ›´æ–°æ¢¯åº¦è®¡ç®—æ©ç """
        self._ensure_gradient_optimizer()
        
        if self._gradient_optimizer['avg_weights'].device != weights.device:
            self._gradient_optimizer['avg_weights'] = self._gradient_optimizer['avg_weights'].to(weights.device)
            self._gradient_optimizer['gradient_mask'] = self._gradient_optimizer['gradient_mask'].to(weights.device)
        
        # æŒ‡æ•°ç§»åŠ¨å¹³å‡
        momentum = self._gradient_optimizer['weight_momentum']
        self._gradient_optimizer['avg_weights'] = (
            momentum * self._gradient_optimizer['avg_weights'] + 
            (1 - momentum) * weights.detach()
        )
        
        # æ›´æ–°æ¢¯åº¦æ©ç 
        new_mask = self._gradient_optimizer['avg_weights'] > self.gradient_threshold
        if new_mask.sum() < 2:  # è‡³å°‘ä¿ç•™2ä¸ªæ“ä½œ
            top_indices = torch.topk(self._gradient_optimizer['avg_weights'], 2).indices
            new_mask[top_indices] = True
        
        self._gradient_optimizer['gradient_mask'] = new_mask
        return new_mask
    
    def _get_cache_key(self, x: torch.Tensor, op_idx: int) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return f"{x.shape}_{x.device}_{x.data_ptr()}_{op_idx}"
    
    def _memory_efficient_compute(self, x: torch.Tensor, weights: torch.Tensor) -> torch.Tensor:
        """å†…å­˜é«˜æ•ˆè®¡ç®—"""
        self._ensure_memory_manager()
        self._ensure_gradient_optimizer()
        self._ensure_lazy_computer()
        
        cache = self._memory_manager['output_cache']
        result = None
        
        for i, (op, weight) in enumerate(zip(self._ops, weights)):
            if weight.item() < 1e-6:  # è·³è¿‡æƒé‡å¾ˆå°çš„æ“ä½œ
                continue
            
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._get_cache_key(x, i)
            if cache_key in cache:
                output = cache[cache_key]
                self._stats['cache_hits'] += 1
            else:
                # åº”ç”¨æ¢¯åº¦ä¼˜åŒ–
                if self._gradient_optimizer['gradient_mask'][i] and self.training:
                    if self._gradient_optimizer['checkpoint_enabled']:
                        output = checkpoint.checkpoint(op, x, use_reentrant=False)
                        self._stats['gradient_optimizations'] += 1
                    else:
                        output = op(x)
                else:
                    # è·³è¿‡æ¢¯åº¦è®¡ç®—
                    with torch.no_grad():
                        output = op(x)
                
                # ç¼“å­˜ç®¡ç†
                if len(cache) < self._memory_manager['max_cache_size']:
                    cache[cache_key] = output.detach().clone()
                
                self._stats['memory_optimizations'] += 1
            
            # åŠ æƒè¾“å‡º
            weighted_output = output * weight
            
            # ç´¯ç§¯ç»“æœï¼ˆå¤„ç†ä¸åŒå°ºå¯¸çš„è¾“å‡ºï¼‰
            if result is None:
                result = weighted_output
            else:
                # ç¡®ä¿å°ºå¯¸åŒ¹é…å†ç›¸åŠ 
                if result.shape == weighted_output.shape:
                    result = result + weighted_output
                else:
                    # å¦‚æœå°ºå¯¸ä¸åŒ¹é…ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªè¾“å‡ºçš„å°ºå¯¸ä½œä¸ºåŸºå‡†
                    # è¿™é€šå¸¸å‘ç”Ÿåœ¨æœ‰stride=2æ“ä½œæ—¶
                    if weighted_output.shape[2:] == result.shape[2:]:
                        result = result + weighted_output
                    else:
                        # è·³è¿‡å°ºå¯¸ä¸åŒ¹é…çš„æ“ä½œï¼Œæˆ–ä½¿ç”¨interpolateè°ƒæ•´
                        pass
            
            # æ›´æ–°æ‡’è®¡ç®—ç»Ÿè®¡
            if self._lazy_computer is not None:
                self._lazy_computer['op_usage_count'][i] += 1
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆè¾“å‡ºï¼Œè¿”å›é›¶å¼ é‡
        if result is None:
            result = torch.zeros_like(x)
            if self._stride == 2:
                # å¯¹äºstride=2çš„æƒ…å†µï¼Œè°ƒæ•´è¾“å‡ºå°ºå¯¸
                result = torch.nn.functional.avg_pool2d(result, 2)
        
        return result
    
    def _lazy_compute(self, x: torch.Tensor, weights: torch.Tensor) -> torch.Tensor:
        """æ‡’è®¡ç®—ä¼˜åŒ–"""
        # åŠ¨æ€å‰ªæï¼šåªè®¡ç®—æƒé‡å¤§çš„æ“ä½œ
        active_indices = torch.where(weights > self.lazy_threshold)[0]
        
        if len(active_indices) == 0:
            active_indices = torch.argmax(weights).unsqueeze(0)
        
        # æ—©æœŸç»ˆæ­¢ï¼šå¦‚æœæœ‰æ“ä½œå ä¸»å¯¼åœ°ä½
        max_weight = weights.max()
        if max_weight > 0.95:
            max_idx = int(weights.argmax().item())
            self._stats['lazy_optimizations'] += 1
            return self._ops[max_idx](x) * max_weight
        
        # è®¡ç®—æ´»è·ƒæ“ä½œ
        outputs = []
        active_weights = []
        
        for i in active_indices:
            op = self._ops[i]
            weight = weights[i]
            
            # æ£€æŸ¥TritonåŠ é€Ÿ
            if hasattr(op, '_k') and TRITON_AVAILABLE and x.is_cuda:
                self._stats['triton_usage'] += 1
            
            output = op(x)
            outputs.append(output * weight)
            active_weights.append(weight.item())
        
        # é«˜æ•ˆæ±‚å’Œï¼ˆå¤„ç†å°ºå¯¸ä¸åŒ¹é…é—®é¢˜ï¼‰
        if len(outputs) == 1:
            return outputs[0]
        else:
            result = outputs[0]
            for output in outputs[1:]:
                # ç¡®ä¿å°ºå¯¸åŒ¹é…å†ç›¸åŠ 
                if result.shape == output.shape:
                    result = result + output
                else:
                    # å¦‚æœå°ºå¯¸ä¸åŒ¹é…ï¼Œè·³è¿‡æˆ–ä½¿ç”¨æ’å€¼è°ƒæ•´
                    # é€šå¸¸å‘ç”Ÿåœ¨stride=2çš„æ“ä½œä¸­
                    pass
            return result
    
    def forward(self, x: torch.Tensor, weights: torch.Tensor) -> torch.Tensor:
        """ğŸš€ èåˆä¼˜åŒ–å‰å‘ä¼ æ’­"""
        self._stats['forward_calls'] += 1
        
        # ğŸ§  æ™ºèƒ½ç­–ç•¥é€‰æ‹©ï¼šæ ¹æ®æ¨¡å‹å¤æ‚åº¦å’Œè°ƒç”¨é¢‘ç‡
        should_use_complex_optimization = (
            self._stats['forward_calls'] > 100 or  # è°ƒç”¨æ¬¡æ•°å¤š
            x.numel() > 16384 or                   # è¾“å…¥å¤§
            len(self._ops) > 8                     # æ“ä½œå¤š
        )
        
        if not should_use_complex_optimization:
            # ğŸš€ å¿«é€Ÿè·¯å¾„ï¼šç›´æ¥ä½¿ç”¨æ ‡å‡†æ–¹æ³•ï¼ˆé¿å…å¤æ‚ä¼˜åŒ–å¼€é”€ï¼‰
            max_idx = int(weights.argmax().item())
            if weights[max_idx] > 0.95:
                # å¦‚æœæœ‰æ“ä½œå ç»å¯¹ä¸»å¯¼ï¼Œç›´æ¥ä½¿ç”¨å®ƒ
                self._stats['lazy_optimizations'] += 1
                return self._ops[max_idx](x) * weights[max_idx]
            else:
                # æ ‡å‡†åŠ æƒæ±‚å’Œï¼Œä½†åªè®¡ç®—æƒé‡å¤§çš„æ“ä½œ
                active_indices = torch.where(weights > 0.01)[0]
                if len(active_indices) == 0:
                    active_indices = torch.argmax(weights).unsqueeze(0)
                
                outputs = []
                for i in active_indices:
                    output = self._ops[i](x) * weights[i]
                    outputs.append(output)
                
                if len(outputs) == 1:
                    return outputs[0]
                else:
                    result = outputs[0]
                    for output in outputs[1:]:
                        if result.shape == output.shape:
                            result = result + output
                    return result
        
        # ğŸ”§ å¤æ‚ä¼˜åŒ–è·¯å¾„ï¼šä»…åœ¨å¿…è¦æ—¶ä½¿ç”¨
        # ç¡®ä¿æ‰€æœ‰ç»„ä»¶å·²åˆå§‹åŒ–
        self._ensure_memory_manager()
        self._ensure_gradient_optimizer()
        self._ensure_lazy_computer()
        
        # æ›´æ–°æ¢¯åº¦æ©ç 
        self._update_gradient_mask(weights)
        
        # æ ¹æ®è¾“å…¥å¤§å°é€‰æ‹©ç­–ç•¥
        if x.numel() > 16384 and self._memory_manager['stream_compute']:
            # å¤§è¾“å…¥ï¼šä½¿ç”¨å†…å­˜ä¼˜åŒ–
            result = self._memory_efficient_compute(x, weights)
        else:
            # ä¸­ç­‰è¾“å…¥ï¼šä½¿ç”¨æ‡’è®¡ç®—ä¼˜åŒ–
            result = self._lazy_compute(x, weights)
        
        # å®šæœŸæ¸…ç†ç¼“å­˜
        if self._stats['forward_calls'] % 1000 == 0:
            self._cleanup_cache()
        
        return result
    
    def _cleanup_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        if self._memory_manager is not None:
            cache = self._memory_manager['output_cache']
            if len(cache) > self._memory_manager['max_cache_size']:
                # ä¿ç•™æœ€è¿‘ä½¿ç”¨çš„ä¸€åŠ
                keys_to_remove = list(cache.keys())[::2]
                for key in keys_to_remove:
                    del cache[key]
        
        # GPUå†…å­˜æ¸…ç†
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def get_optimization_stats(self) -> dict:
        """è·å–ä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        total_calls = max(1, self._stats['forward_calls'])
        
        # åªæœ‰åœ¨åˆå§‹åŒ–åæ‰è·å–æ´»è·ƒæ“ä½œæ•°
        active_ops = 0
        if self._gradient_optimizer is not None:
            active_ops = self._gradient_optimizer['gradient_mask'].sum().item()
        
        return {
            **self._stats,
            'gradient_optimization_rate': self._stats['gradient_optimizations'] / total_calls,
            'memory_optimization_rate': self._stats['memory_optimizations'] / total_calls,
            'lazy_optimization_rate': self._stats['lazy_optimizations'] / total_calls,
            'cache_hit_rate': self._stats['cache_hits'] / total_calls,
            'triton_usage_rate': self._stats['triton_usage'] / total_calls,
            'active_operations': active_ops,
            'total_operations': len(self._ops)
        } 