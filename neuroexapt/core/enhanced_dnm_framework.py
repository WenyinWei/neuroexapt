#!/usr/bin/env python3
"""
Enhanced Dynamic Neural Morphogenesis (DNM) Framework - å¢å¼ºç‰ˆ

ğŸ§¬ æ ¸å¿ƒæ”¹è¿›ï¼š
1. å¤šç»´åº¦ç“¶é¢ˆåˆ†æ - æ·±åº¦ã€å®½åº¦ã€ä¿¡æ¯æµã€æ¢¯åº¦æµã€å®¹é‡ç“¶é¢ˆ
2. é«˜çº§å½¢æ€å‘ç”Ÿç­–ç•¥ - ä¸²è¡Œåˆ†è£‚ã€å¹¶è¡Œåˆ†è£‚ã€æ··åˆåˆ†è£‚
3. æ™ºèƒ½å†³ç­–åˆ¶å®š - åŸºäºç“¶é¢ˆç±»å‹çš„æœ€ä¼˜ç­–ç•¥é€‰æ‹©
4. æ€§èƒ½å¯¼å‘ - è¿½æ±‚æ›´é«˜çš„å‡†ç¡®ç‡çªç ´

ğŸ¯ ç›®æ ‡ï¼šå®ç°90%+çš„å‡†ç¡®ç‡ï¼Œæ¢ç´¢ç½‘ç»œç»“æ„çš„æ— é™å¯èƒ½
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from abc import ABC, abstractmethod
import math
from collections import defaultdict, deque
import copy

# å¯¼å…¥é«˜çº§å½¢æ€å‘ç”Ÿæ¨¡å—
from .advanced_morphogenesis import (
    AdvancedBottleneckAnalyzer,
    AdvancedMorphogenesisExecutor,
    IntelligentMorphogenesisDecisionMaker,
    MorphogenesisType,
    MorphogenesisDecision
)

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

@dataclass
class EnhancedMorphogenesisEvent:
    """å¢å¼ºçš„å½¢æ€å‘ç”Ÿäº‹ä»¶è®°å½•"""
    epoch: int
    event_type: str  # 'width_expansion', 'serial_division', 'parallel_division', 'hybrid_division'
    location: str
    trigger_reason: str
    performance_before: float
    performance_after: Optional[float] = None
    parameters_added: int = 0
    complexity_change: float = 0.0
    morphogenesis_type: MorphogenesisType = MorphogenesisType.WIDTH_EXPANSION
    confidence: float = 0.0
    expected_improvement: float = 0.0

class EnhancedInformationTheoryTrigger:
    """å¢å¼ºçš„ä¿¡æ¯è®ºè§¦å‘å™¨"""
    
    def __init__(self, entropy_threshold: float = 0.1, complexity_threshold: float = 0.7):
        self.entropy_threshold = entropy_threshold
        self.complexity_threshold = complexity_threshold
        self.history = deque(maxlen=15)
        
    def should_trigger(self, context: Dict[str, Any]) -> Tuple[bool, str]:
        activations = context.get('activations', {})
        gradients = context.get('gradients', {})
        
        if not activations or not gradients:
            return False, "ç¼ºå°‘æ¿€æ´»å€¼æˆ–æ¢¯åº¦ä¿¡æ¯"
            
        # è®¡ç®—ç»¼åˆå¤æ‚åº¦åˆ†æ•°
        complexity_score = self._compute_complexity_score(activations, gradients)
        
        self.history.append({
            'complexity_score': complexity_score,
            'epoch': context.get('epoch', 0)
        })
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´å¤æ‚çš„ç»“æ„å˜å¼‚
        if complexity_score > self.complexity_threshold:
            return True, f"å¤æ‚åº¦ç“¶é¢ˆæ£€æµ‹ï¼šåˆ†æ•°={complexity_score:.4f}"
            
        return False, "å¤æ‚åº¦æŒ‡æ ‡æœªè¾¾åˆ°è§¦å‘æ¡ä»¶"
    
    def _compute_complexity_score(self, activations: Dict[str, torch.Tensor], 
                                gradients: Dict[str, torch.Tensor]) -> float:
        """è®¡ç®—ç½‘ç»œå¤æ‚åº¦åˆ†æ•°"""
        scores = []
        
        for name, activation in activations.items():
            if name not in gradients or gradients[name] is None:
                continue
                
            gradient = gradients[name]
            
            # 1. ä¿¡æ¯ç†µåˆ†æ
            entropy = self._compute_entropy(activation)
            
            # 2. æ¢¯åº¦å¤æ‚åº¦
            grad_complexity = self._compute_gradient_complexity(gradient)
            
            # 3. æ¿€æ´»æ¨¡å¼å¤æ‚åº¦
            activation_complexity = self._compute_activation_complexity(activation)
            
            # ç»¼åˆåˆ†æ•°
            layer_score = 0.4 * entropy + 0.3 * grad_complexity + 0.3 * activation_complexity
            scores.append(layer_score)
        
        return np.mean(scores) if scores else 0.0
    
    def _compute_entropy(self, activation: torch.Tensor) -> float:
        """è®¡ç®—æ¿€æ´»å€¼ç†µ"""
        if activation.numel() == 0:
            return 0.0
            
        activation_flat = activation.flatten()
        activation_abs = torch.abs(activation_flat) + 1e-8
        probs = activation_abs / torch.sum(activation_abs)
        
        entropy = -torch.sum(probs * torch.log(probs + 1e-8))
        # å½’ä¸€åŒ–åˆ° [0, 1]
        max_entropy = math.log(len(probs))
        return min(entropy.item() / max_entropy, 1.0) if max_entropy > 0 else 0.0
    
    def _compute_gradient_complexity(self, gradient: torch.Tensor) -> float:
        """è®¡ç®—æ¢¯åº¦å¤æ‚åº¦"""
        if gradient is None or gradient.numel() == 0:
            return 0.0
            
        grad_flat = gradient.flatten()
        
        # æ¢¯åº¦çš„æ ‡å‡†å·®/å‡å€¼æ¯”ç‡
        grad_std = torch.std(grad_flat)
        grad_mean = torch.mean(torch.abs(grad_flat))
        
        if grad_mean > 1e-8:
            complexity = grad_std / grad_mean
            return min(complexity.item(), 1.0)
        else:
            return 0.0
    
    def _compute_activation_complexity(self, activation: torch.Tensor) -> float:
        """è®¡ç®—æ¿€æ´»æ¨¡å¼å¤æ‚åº¦"""
        if activation.numel() == 0 or len(activation.shape) < 2:
            return 0.0
            
        # è®¡ç®—æ¿€æ´»æ¨¡å¼çš„å˜å¼‚ç³»æ•°
        activation_flat = activation.view(activation.shape[0], -1)
        
        # æ‰¹æ¬¡é—´çš„å˜å¼‚æ€§
        batch_means = torch.mean(activation_flat, dim=1)
        batch_std = torch.std(batch_means)
        batch_mean_avg = torch.mean(batch_means)
        
        if batch_mean_avg > 1e-8:
            complexity = batch_std / batch_mean_avg
            return min(complexity.item(), 1.0)
        else:
            return 0.0

class EnhancedBiologicalPrinciplesTrigger:
    """å¢å¼ºçš„ç”Ÿç‰©å­¦åŸç†è§¦å‘å™¨"""
    
    def __init__(self, maturation_threshold: float = 0.6):
        self.maturation_threshold = maturation_threshold
        self.development_history = deque(maxlen=20)
        
    def should_trigger(self, context: Dict[str, Any]) -> Tuple[bool, str]:
        """æ£€æµ‹æ˜¯å¦å¤„äºå…³é”®å‘è‚²æœŸ"""
        performance_history = context.get('performance_history', [])
        epoch = context.get('epoch', 0)
        
        if len(performance_history) < 10:
            return False, "æ€§èƒ½å†å²æ•°æ®ä¸è¶³"
        
        # æ£€æµ‹å‘è‚²é˜¶æ®µ
        maturation_score = self._compute_maturation_score(performance_history)
        
        self.development_history.append({
            'epoch': epoch,
            'maturation_score': maturation_score,
            'performance': performance_history[-1] if performance_history else 0.0
        })
        
        # æ£€æµ‹æ˜¯å¦éœ€è¦ç»“æ„åˆ†åŒ–
        if self._detect_structural_differentiation_need(maturation_score):
            return True, f"å…³é”®å‘è‚²æœŸæ£€æµ‹ï¼šæˆç†Ÿåº¦={maturation_score:.3f}ï¼Œé€‚åˆç»“æ„é‡ç»„"
            
        return False, "æœªå¤„äºå…³é”®å‘è‚²æœŸ"
    
    def _compute_maturation_score(self, performance_history: List[float]) -> float:
        """è®¡ç®—å‘è‚²æˆç†Ÿåº¦åˆ†æ•°"""
        if len(performance_history) < 5:
            return 0.0
        
        recent_performances = performance_history[-10:]
        
        # 1. æ€§èƒ½ç¨³å®šæ€§
        stability = 1.0 - np.std(recent_performances[-5:]) / (np.mean(recent_performances[-5:]) + 1e-8)
        
        # 2. æ”¹è¿›é€Ÿåº¦
        if len(recent_performances) >= 5:
            early_avg = np.mean(recent_performances[:5])
            late_avg = np.mean(recent_performances[-5:])
            improvement_rate = (late_avg - early_avg) / (early_avg + 1e-8)
        else:
            improvement_rate = 0.0
        
        # 3. æ€§èƒ½é«˜åº¦
        performance_level = recent_performances[-1] if recent_performances else 0.0
        
        # ç»¼åˆæˆç†Ÿåº¦åˆ†æ•°
        maturation = (
            0.4 * min(stability, 1.0) +
            0.3 * min(improvement_rate, 1.0) +
            0.3 * performance_level
        )
        
        return max(0.0, min(maturation, 1.0))
    
    def _detect_structural_differentiation_need(self, maturation_score: float) -> bool:
        """æ£€æµ‹æ˜¯å¦éœ€è¦ç»“æ„åˆ†åŒ–"""
        return maturation_score > self.maturation_threshold

class EnhancedCognitiveScienceTrigger:
    """å¢å¼ºçš„è®¤çŸ¥ç§‘å­¦è§¦å‘å™¨"""
    
    def __init__(self, forgetting_threshold: float = 0.05):
        self.forgetting_threshold = forgetting_threshold
        self.learning_patterns = deque(maxlen=25)
        
    def should_trigger(self, context: Dict[str, Any]) -> Tuple[bool, str]:
        """æ£€æµ‹è®¤çŸ¥ç“¶é¢ˆå’Œç¾éš¾æ€§é—å¿˜"""
        performance_history = context.get('performance_history', [])
        activations = context.get('activations', {})
        
        if len(performance_history) < 8:
            return False, "å­¦ä¹ å†å²æ•°æ®ä¸è¶³"
        
        # æ£€æµ‹ç¾éš¾æ€§é—å¿˜
        forgetting_detected = self._detect_catastrophic_forgetting(performance_history)
        
        # æ£€æµ‹å­¦ä¹ é¥±å’Œ
        saturation_detected = self._detect_learning_saturation(performance_history)
        
        # æ£€æµ‹ç‰¹å¾è¡¨ç¤ºå†²çª
        conflict_detected = self._detect_representation_conflict(activations)
        
        self.learning_patterns.append({
            'epoch': context.get('epoch', 0),
            'performance': performance_history[-1] if performance_history else 0.0,
            'forgetting_risk': forgetting_detected,
            'saturation_risk': saturation_detected,
            'conflict_risk': conflict_detected
        })
        
        if forgetting_detected or conflict_detected:
            reason = []
            if forgetting_detected:
                reason.append("ç¾éš¾æ€§é—å¿˜é£é™©")
            if conflict_detected:
                reason.append("ç‰¹å¾è¡¨ç¤ºå†²çª")
            return True, f"è®¤çŸ¥ç“¶é¢ˆæ£€æµ‹ï¼š{', '.join(reason)}ï¼Œéœ€è¦åˆ†åŒ–ä¸“é—¨åŒ–ç¥ç»å…ƒ"
            
        return False, "è®¤çŸ¥æŒ‡æ ‡æ­£å¸¸"
    
    def _detect_catastrophic_forgetting(self, performance_history: List[float]) -> bool:
        """æ£€æµ‹ç¾éš¾æ€§é—å¿˜"""
        if len(performance_history) < 8:
            return False
        
        # æ£€æŸ¥æœ€è¿‘æ€§èƒ½æ˜¯å¦æ˜¾è‘—ä¸‹é™
        recent_window = 5
        past_window = 5
        
        recent_avg = np.mean(performance_history[-recent_window:])
        past_avg = np.mean(performance_history[-(recent_window + past_window):-recent_window])
        
        if past_avg > 0:
            decline_ratio = (past_avg - recent_avg) / past_avg
            return decline_ratio > self.forgetting_threshold
        
        return False
    
    def _detect_learning_saturation(self, performance_history: List[float]) -> bool:
        """æ£€æµ‹å­¦ä¹ é¥±å’Œ"""
        if len(performance_history) < 10:
            return False
        
        # æ£€æŸ¥æœ€è¿‘10ä¸ªepochçš„æ”¹è¿›
        recent_performances = performance_history[-10:]
        improvements = [recent_performances[i] - recent_performances[i-1] 
                       for i in range(1, len(recent_performances))]
        
        avg_improvement = np.mean(improvements)
        return avg_improvement < 0.001  # æ”¹è¿›æå°
    
    def _detect_representation_conflict(self, activations: Dict[str, torch.Tensor]) -> bool:
        """æ£€æµ‹ç‰¹å¾è¡¨ç¤ºå†²çª"""
        if not activations:
            return False
        
        # ç®€åŒ–çš„å†²çªæ£€æµ‹ï¼šæ£€æŸ¥æ¿€æ´»æ¨¡å¼çš„ä¸€è‡´æ€§
        conflict_scores = []
        
        for name, activation in activations.items():
            if len(activation.shape) >= 2 and activation.shape[0] > 1:
                # è®¡ç®—æ‰¹æ¬¡å†…æ¿€æ´»æ¨¡å¼çš„ä¸€è‡´æ€§
                activation_flat = activation.view(activation.shape[0], -1)
                
                # è®¡ç®—æ ·æœ¬é—´çš„ç›¸å…³æ€§
                if activation_flat.shape[1] > 1:
                    try:
                        correlation_matrix = torch.corrcoef(activation_flat)
                        # å¯¹è§’çº¿å¤–çš„ç›¸å…³ç³»æ•°
                        mask = ~torch.eye(correlation_matrix.shape[0], dtype=torch.bool)
                        off_diagonal = correlation_matrix[mask]
                        
                        if len(off_diagonal) > 0:
                            consistency = torch.mean(torch.abs(off_diagonal))
                            conflict_scores.append(1.0 - consistency.item())
                    except:
                        continue
        
        if conflict_scores:
            avg_conflict = np.mean(conflict_scores)
            return avg_conflict > 0.7  # å†²çªé˜ˆå€¼
        
        return False

class EnhancedDNMFramework:
    """å¢å¼ºçš„DNMæ¡†æ¶"""
    
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or self._get_default_config()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.bottleneck_analyzer = AdvancedBottleneckAnalyzer()
        self.morphogenesis_executor = AdvancedMorphogenesisExecutor()
        self.decision_maker = IntelligentMorphogenesisDecisionMaker()
        
        # åˆå§‹åŒ–è§¦å‘å™¨
        self.triggers = {
            'information_theory': EnhancedInformationTheoryTrigger(),
            'biological_principles': EnhancedBiologicalPrinciplesTrigger(),
            'cognitive_science': EnhancedCognitiveScienceTrigger()
        }
        
        # è·Ÿè¸ªæ•°æ®
        self.morphogenesis_events = []
        self.performance_history = []
        self.activation_cache = {}
        self.gradient_cache = {}
        
    def _get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'trigger_interval': 3,  # æ¯3ä¸ªepochæ£€æŸ¥ä¸€æ¬¡
            'max_morphogenesis_per_epoch': 1,
            'performance_patience': 8,
            'min_improvement_threshold': 0.001,
            'max_parameter_growth_ratio': 0.5,  # æœ€å¤§å‚æ•°å¢é•¿50%
            'enable_serial_division': True,
            'enable_parallel_division': True,
            'enable_hybrid_division': True,
            'complexity_threshold': 0.6
        }
    
    def should_trigger_morphogenesis(self, context: Dict[str, Any]) -> Tuple[bool, List[str]]:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥è§¦å‘å½¢æ€å‘ç”Ÿ"""
        epoch = context.get('epoch', 0)
        
        # æ£€æŸ¥è§¦å‘é—´éš”
        if epoch % self.config['trigger_interval'] != 0:
            return False, []
        
        # æ£€æŸ¥å„ä¸ªè§¦å‘å™¨
        trigger_results = []
        trigger_reasons = []
        
        for name, trigger in self.triggers.items():
            try:
                should_trigger, reason = trigger.should_trigger(context)
                if should_trigger:
                    trigger_results.append(True)
                    trigger_reasons.append(f"{name}: {reason}")
                else:
                    trigger_results.append(False)
            except Exception as e:
                logger.warning(f"è§¦å‘å™¨ {name} æ‰§è¡Œå¤±è´¥: {e}")
                trigger_results.append(False)
        
        # è‡³å°‘æœ‰ä¸€ä¸ªè§¦å‘å™¨æ¿€æ´»
        should_trigger = any(trigger_results)
        
        return should_trigger, trigger_reasons
    
    def execute_morphogenesis(self, model: nn.Module, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå½¢æ€å‘ç”Ÿ"""
        results = {
            'model_modified': False,
            'new_model': model,
            'parameters_added': 0,
            'morphogenesis_events': 0,
            'morphogenesis_type': 'none',
            'trigger_reasons': []
        }
        
        try:
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥è§¦å‘
            should_trigger, trigger_reasons = self.should_trigger_morphogenesis(context)
            
            if not should_trigger:
                return results
            
            logger.info("ğŸ”„ Triggering advanced morphogenesis analysis...")
            
            # è¾“å‡ºè§¦å‘åŸå› 
            for reason in trigger_reasons:
                print(f"    - {reason}")
            
            results['trigger_reasons'] = trigger_reasons
            
            # æ‰§è¡Œç“¶é¢ˆåˆ†æ
            activations = context.get('activations', {})
            gradients = context.get('gradients', {})
            
            if not activations or not gradients:
                logger.warning("ç¼ºå°‘æ¿€æ´»å€¼æˆ–æ¢¯åº¦ä¿¡æ¯ï¼Œè·³è¿‡å½¢æ€å‘ç”Ÿ")
                return results
            
            bottleneck_analysis = self.bottleneck_analyzer.analyze_network_bottlenecks(
                model, activations, gradients
            )
            
            # åˆ¶å®šå†³ç­–
            performance_history = context.get('performance_history', [])
            decision = self.decision_maker.make_decision(bottleneck_analysis, performance_history)
            
            if decision is None:
                logger.info("æœªå‘ç°éœ€è¦å½¢æ€å‘ç”Ÿçš„ç“¶é¢ˆ")
                return results
            
            # æ‰§è¡Œå½¢æ€å‘ç”Ÿ
            new_model, parameters_added = self.morphogenesis_executor.execute_morphogenesis(
                model, decision
            )
            
            if parameters_added > 0:
                # è®°å½•äº‹ä»¶
                event = EnhancedMorphogenesisEvent(
                    epoch=context.get('epoch', 0),
                    event_type=decision.morphogenesis_type.value,
                    location=decision.target_location,
                    trigger_reason=decision.reasoning,
                    performance_before=performance_history[-1] if performance_history else 0.0,
                    parameters_added=parameters_added,
                    morphogenesis_type=decision.morphogenesis_type,
                    confidence=decision.confidence,
                    expected_improvement=decision.expected_improvement
                )
                
                self.morphogenesis_events.append(event)
                
                # æ›´æ–°ç»“æœ
                results.update({
                    'model_modified': True,
                    'new_model': new_model,
                    'parameters_added': parameters_added,
                    'morphogenesis_events': 1,
                    'morphogenesis_type': decision.morphogenesis_type.value,
                    'decision_confidence': decision.confidence,
                    'expected_improvement': decision.expected_improvement
                })
                
                logger.info(f"é«˜çº§å½¢æ€å‘ç”Ÿå®Œæˆ: {decision.morphogenesis_type.value}, æ–°å¢å‚æ•°: {parameters_added}")
                
        except Exception as e:
            logger.error(f"å½¢æ€å‘ç”Ÿæ‰§è¡Œå¤±è´¥: {e}")
            
        return results
    
    def update_performance_history(self, performance: float):
        """æ›´æ–°æ€§èƒ½å†å²"""
        self.performance_history.append(performance)
        
        # ä¿æŒå†å²é•¿åº¦
        if len(self.performance_history) > 100:
            self.performance_history = self.performance_history[-100:]
    
    def cache_activations_and_gradients(self, activations: Dict[str, torch.Tensor], 
                                       gradients: Dict[str, torch.Tensor]):
        """ç¼“å­˜æ¿€æ´»å€¼å’Œæ¢¯åº¦"""
        self.activation_cache = activations
        self.gradient_cache = gradients
    
    def get_morphogenesis_summary(self) -> Dict[str, Any]:
        """è·å–å½¢æ€å‘ç”Ÿæ€»ç»“"""
        if not self.morphogenesis_events:
            return {
                'total_events': 0,
                'total_parameters_added': 0,
                'morphogenesis_types': {},
                'events': []
            }
        
        # ç»Ÿè®¡å„ç§ç±»å‹çš„å½¢æ€å‘ç”Ÿ
        type_counts = defaultdict(int)
        for event in self.morphogenesis_events:
            type_counts[event.morphogenesis_type.value] += 1
        
        total_params = sum(event.parameters_added for event in self.morphogenesis_events)
        
        return {
            'total_events': len(self.morphogenesis_events),
            'total_parameters_added': total_params,
            'morphogenesis_types': dict(type_counts),
            'events': [
                {
                    'epoch': event.epoch,
                    'type': event.morphogenesis_type.value,
                    'location': event.location,
                    'parameters_added': event.parameters_added,
                    'confidence': event.confidence,
                    'expected_improvement': event.expected_improvement,
                    'reasoning': event.trigger_reason
                }
                for event in self.morphogenesis_events
            ]
        }