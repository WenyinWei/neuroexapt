#!/usr/bin/env python3
"""
@defgroup group_enhanced_dnm_framework Enhanced Dnm Framework
@ingroup core
Enhanced Dnm Framework module for NeuroExapt framework.

Enhanced Dynamic Neural Morphogenesis (DNM) Framework - å¢å¼ºç‰ˆ

ğŸ§¬ æ ¸å¿ƒæ”¹è¿›ï¼š
1. å¤šç»´åº¦ç“¶é¢ˆåˆ†æ - æ·±åº¦ã€å®½åº¦ã€ä¿¡æ¯æµã€æ¢¯åº¦æµã€å®¹é‡ç“¶é¢ˆ
2. é«˜çº§å½¢æ€å‘ç”Ÿç­–ç•¥ - ä¸²è¡Œåˆ†è£‚ã€å¹¶è¡Œåˆ†è£‚ã€æ··åˆåˆ†è£‚
3. æ™ºèƒ½å†³ç­–åˆ¶å®š - åŸºäºç“¶é¢ˆç±»å‹çš„æœ€ä¼˜ç­–ç•¥é€‰æ‹©
4. æ€§èƒ½å¯¼å‘ - è¿½æ±‚æ›´é«˜çš„å‡†ç¡®ç‡çªç ´

ğŸ¯ ç›®æ ‡ï¼šå®ç°90%+çš„å‡†ç¡®ç‡ï¼Œæ¢ç´¢ç½‘ç»œç»“æ„çš„æ— é™å¯èƒ½
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from abc import ABC, abstractmethod
import math
from collections import defaultdict, deque
import copy
import traceback
import time
import os

# å¯¼å…¥é«˜çº§å½¢æ€å‘ç”Ÿæ¨¡å—
from .advanced_morphogenesis import (
    AdvancedBottleneckAnalyzer,
    AdvancedMorphogenesisExecutor,
    IntelligentMorphogenesisDecisionMaker,
    MorphogenesisType,
    MorphogenesisDecision
)

# å¯¼å…¥ç»Ÿä¸€çš„æ—¥å¿—ç³»ç»Ÿ
from .logging_utils import ConfigurableLogger, logger


# ä¿æŒå‘åå…¼å®¹æ€§çš„DebugPrinterç±»
class DebugPrinter:
    """å‘åå…¼å®¹çš„è°ƒè¯•æ‰“å°å™¨ï¼ˆå·²åºŸå¼ƒï¼Œå»ºè®®ä½¿ç”¨loggerï¼‰"""
    
    def __init__(self, enabled: bool = True):
        self.enabled = enabled
        self._logger = logger
        import warnings
        warnings.warn("DebugPrinter is deprecated, use logger instead", DeprecationWarning)
    
    def print_debug(self, message: str, level: str = "INFO"):
        """æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼ˆå…¼å®¹æ¥å£ï¼‰"""
        if not self.enabled:
            return
        getattr(self._logger, level.lower(), self._logger.info)(message)
    
    def enter_section(self, section_name: str):
        """è¿›å…¥è°ƒè¯•åŒºåŸŸ"""
        self._logger.enter_section(section_name)
    
        """é€€å‡ºè°ƒè¯•åŒºåŸŸ"""
        self._logger.exit_section(section_name)



# ä¿æŒå‘åå…¼å®¹æ€§çš„DebugPrinterç±»
class DebugPrinter:
    """å‘åå…¼å®¹çš„è°ƒè¯•æ‰“å°å™¨ï¼ˆå·²åºŸå¼ƒï¼Œå»ºè®®ä½¿ç”¨loggerï¼‰"""
    
    def __init__(self, enabled: bool = True):
        self.enabled = enabled
        self._logger = logger
        import warnings
        warnings.warn(
            "DebugPrinter is deprecated. Use the global 'logger' instance instead.",
            DeprecationWarning,
            stacklevel=2
        )
    
    def print_debug(self, message: str, level: str = "INFO"):
        if not self.enabled:
            return
        
        level_map = {
            "INFO": self._logger.info,
            "SUCCESS": self._logger.success,
            "WARNING": self._logger.warning,
            "ERROR": self._logger.error,
            "DEBUG": self._logger.debug
        }
        level_map.get(level, self._logger.info)(message)
    
    def enter_section(self, section_name: str):
        if self.enabled:
            self._logger.enter_section(section_name)
    
    def exit_section(self, section_name: str):
        if self.enabled:
            self._logger.exit_section(section_name)
    
    def print_tensor_info(self, tensor: torch.Tensor, name: str):
        if self.enabled:
            self._logger.log_tensor_info(tensor, name)
    
    def print_model_info(self, model: nn.Module, name: str = "Model"):
        if self.enabled:
            self._logger.log_model_info(model, name)



@dataclass
class EnhancedMorphogenesisEvent:
    """å¢å¼ºçš„å½¢æ€å‘ç”Ÿäº‹ä»¶è®°å½•"""
    epoch: int
    event_type: str  # 'width_expansion', 'serial_division', 'parallel_division', 'hybrid_division'
    location: str
    trigger_reason: str
    performance_before: float
    performance_after: Optional[float] = None
    parameters_added: int = 0
    complexity_change: float = 0.0
    morphogenesis_type: MorphogenesisType = MorphogenesisType.WIDTH_EXPANSION
    confidence: float = 0.0
    expected_improvement: float = 0.0

class EnhancedInformationTheoryTrigger:
    """å¢å¼ºçš„ä¿¡æ¯è®ºè§¦å‘å™¨"""
    
    def __init__(self, entropy_threshold: float = 0.1, complexity_threshold: float = 0.7):
        self.entropy_threshold = entropy_threshold
        self.complexity_threshold = complexity_threshold
        self.history = deque(maxlen=15)
        
    def should_trigger(self, context: Dict[str, Any]) -> Tuple[bool, str]:
        logger.enter_section("ä¿¡æ¯è®ºè§¦å‘å™¨æ£€æŸ¥")
        
        activations = context.get('activations', {})
        gradients = context.get('gradients', {})
        
        logger.debug(f"è¾“å…¥æ•°æ®: æ¿€æ´»å€¼å±‚æ•°={len(activations)}, æ¢¯åº¦å±‚æ•°={len(gradients)}")
        
        if not activations or not gradients:
            logger.warning("âŒ ç¼ºå°‘æ¿€æ´»å€¼æˆ–æ¢¯åº¦ä¿¡æ¯")
            logger.exit_section("ä¿¡æ¯è®ºè§¦å‘å™¨æ£€æŸ¥")
            return False, "ç¼ºå°‘æ¿€æ´»å€¼æˆ–æ¢¯åº¦ä¿¡æ¯"
            
        # è®¡ç®—ç»¼åˆå¤æ‚åº¦åˆ†æ•°
        complexity_score = self._compute_complexity_score(activations, gradients)
        
        logger.info(f"å¤æ‚åº¦åˆ†æ•°: {complexity_score:.4f} (é˜ˆå€¼: {self.complexity_threshold})")
        
        self.history.append({
            'complexity_score': complexity_score,
            'epoch': context.get('epoch', 0)
        })
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´å¤æ‚çš„ç»“æ„å˜å¼‚
        if complexity_score > self.complexity_threshold:
            logger.success(f"è§¦å‘æ¡ä»¶æ»¡è¶³: {complexity_score:.4f} > {self.complexity_threshold}")
            logger.exit_section("ä¿¡æ¯è®ºè§¦å‘å™¨æ£€æŸ¥")
            return True, f"å¤æ‚åº¦ç“¶é¢ˆæ£€æµ‹ï¼šåˆ†æ•°={complexity_score:.4f}"
            
        logger.info(f"âŒ æœªè¾¾åˆ°è§¦å‘æ¡ä»¶: {complexity_score:.4f} <= {self.complexity_threshold}")
        logger.exit_section("ä¿¡æ¯è®ºè§¦å‘å™¨æ£€æŸ¥")
        return False, "å¤æ‚åº¦æŒ‡æ ‡æœªè¾¾åˆ°è§¦å‘æ¡ä»¶"
    
    def _compute_complexity_score(self, activations: Dict[str, torch.Tensor], 
                                gradients: Dict[str, torch.Tensor]) -> float:
        """è®¡ç®—ç½‘ç»œå¤æ‚åº¦åˆ†æ•°"""
        logger.enter_section("å¤æ‚åº¦åˆ†æ•°è®¡ç®—")
        scores = []
        
        for name, activation in activations.items():
            if name not in gradients or gradients[name] is None:
                logger.warning(f"âš ï¸ è·³è¿‡å±‚ {name}: ç¼ºå°‘æ¢¯åº¦ä¿¡æ¯")
                continue
                
            gradient = gradients[name]
            logger.log_tensor_info(activation, f"æ¿€æ´»å€¼[{name}]")
            logger.log_tensor_info(gradient, f"æ¢¯åº¦[{name}]")
            
            # 1. ä¿¡æ¯ç†µåˆ†æ
            entropy = self._compute_entropy(activation)
            logger.debug(f"ä¿¡æ¯ç†µ[{name}]: {entropy:.4f}")
            
            # 2. æ¢¯åº¦å¤æ‚åº¦
            grad_complexity = self._compute_gradient_complexity(gradient)
            logger.debug(f"æ¢¯åº¦å¤æ‚åº¦[{name}]: {grad_complexity:.4f}")
            
            # 3. æ¿€æ´»æ¨¡å¼å¤æ‚åº¦
            activation_complexity = self._compute_activation_complexity(activation)
            logger.debug(f"æ¿€æ´»å¤æ‚åº¦[{name}]: {activation_complexity:.4f}")
            
            # ç»¼åˆåˆ†æ•°
            layer_score = 0.4 * entropy + 0.3 * grad_complexity + 0.3 * activation_complexity
            scores.append(layer_score)
            logger.debug(f"å±‚åˆ†æ•°[{name}]: {layer_score:.4f}")
        
        final_score = np.mean(scores) if scores else 0.0
        logger.info(f"æœ€ç»ˆå¤æ‚åº¦åˆ†æ•°: {final_score:.4f} (å…±{len(scores)}å±‚)")
        logger.exit_section("å¤æ‚åº¦åˆ†æ•°è®¡ç®—")
        return final_score
    
    def _compute_entropy(self, activation: torch.Tensor) -> float:
        """è®¡ç®—æ¿€æ´»å€¼ç†µ"""
        if activation.numel() == 0:
            return 0.0
            
        activation_flat = activation.flatten()
        activation_abs = torch.abs(activation_flat) + 1e-8
        probs = activation_abs / torch.sum(activation_abs)
        
        entropy = -torch.sum(probs * torch.log(probs + 1e-8))
        # å½’ä¸€åŒ–åˆ° [0, 1]
        max_entropy = math.log(len(probs))
        return min(entropy.item() / max_entropy, 1.0) if max_entropy > 0 else 0.0
    
    def _compute_gradient_complexity(self, gradient: torch.Tensor) -> float:
        """è®¡ç®—æ¢¯åº¦å¤æ‚åº¦"""
        if gradient is None or gradient.numel() == 0:
            return 0.0
            
        grad_flat = gradient.flatten()
        
        # æ¢¯åº¦çš„æ ‡å‡†å·®/å‡å€¼æ¯”ç‡
        grad_std = torch.std(grad_flat)
        grad_mean = torch.mean(torch.abs(grad_flat))
        
        if grad_mean > 1e-8:
            complexity = grad_std / grad_mean
            return min(complexity.item(), 1.0)
        else:
            return 0.0
    
    def _compute_activation_complexity(self, activation: torch.Tensor) -> float:
        """è®¡ç®—æ¿€æ´»æ¨¡å¼å¤æ‚åº¦"""
        if activation.numel() == 0 or len(activation.shape) < 2:
            return 0.0
            
        # è®¡ç®—æ¿€æ´»æ¨¡å¼çš„å˜å¼‚ç³»æ•°
        activation_flat = activation.view(activation.shape[0], -1)
        
        # æ‰¹æ¬¡é—´çš„å˜å¼‚æ€§
        batch_means = torch.mean(activation_flat, dim=1)
        batch_std = torch.std(batch_means)
        batch_mean_avg = torch.mean(batch_means)
        
        if batch_mean_avg > 1e-8:
            complexity = batch_std / batch_mean_avg
            return min(complexity.item(), 1.0)
        else:
            return 0.0

class EnhancedBiologicalPrinciplesTrigger:
    """å¢å¼ºçš„ç”Ÿç‰©å­¦åŸç†è§¦å‘å™¨"""
    
    def __init__(self, maturation_threshold: float = 0.6):
        self.maturation_threshold = maturation_threshold
        self.development_history = deque(maxlen=20)
        
    def should_trigger(self, context: Dict[str, Any]) -> Tuple[bool, str]:
        """æ£€æµ‹æ˜¯å¦å¤„äºå…³é”®å‘è‚²æœŸ"""
        logger.enter_section("ç”Ÿç‰©å­¦åŸç†è§¦å‘å™¨æ£€æŸ¥")
        
        performance_history = context.get('performance_history', [])
        epoch = context.get('epoch', 0)
        
        logger.debug(f"å½“å‰epoch: {epoch}, æ€§èƒ½å†å²é•¿åº¦: {len(performance_history)}")
        
        if len(performance_history) < 10:
            logger.warning("âŒ æ€§èƒ½å†å²æ•°æ®ä¸è¶³ (éœ€è¦è‡³å°‘10ä¸ªæ•°æ®ç‚¹)")
            logger.exit_section("ç”Ÿç‰©å­¦åŸç†è§¦å‘å™¨æ£€æŸ¥")
            return False, "æ€§èƒ½å†å²æ•°æ®ä¸è¶³"
        
        # æ£€æµ‹å‘è‚²é˜¶æ®µ
        logger.debug("è®¡ç®—å‘è‚²æˆç†Ÿåº¦åˆ†æ•°...")
        maturation_score = self._compute_maturation_score(performance_history)
        logger.info(f"æˆç†Ÿåº¦åˆ†æ•°: {maturation_score:.4f} (é˜ˆå€¼: {self.maturation_threshold})")
        
        self.development_history.append({
            'epoch': epoch,
            'maturation_score': maturation_score,
            'performance': performance_history[-1] if performance_history else 0.0
        })
        
        # æ£€æµ‹æ˜¯å¦éœ€è¦ç»“æ„åˆ†åŒ–
        differentiation_needed = self._detect_structural_differentiation_need(maturation_score)
        if differentiation_needed:
            logger.success(f"ç»“æ„åˆ†åŒ–éœ€æ±‚: âœ…éœ€è¦")
        else:
            logger.debug(f"ç»“æ„åˆ†åŒ–éœ€æ±‚: âŒä¸éœ€è¦")
        
        if differentiation_needed:
            logger.success(f"âœ… è§¦å‘æ¡ä»¶æ»¡è¶³: æˆç†Ÿåº¦={maturation_score:.3f}")
            logger.exit_section("ç”Ÿç‰©å­¦åŸç†è§¦å‘å™¨æ£€æŸ¥")
            return True, f"å…³é”®å‘è‚²æœŸæ£€æµ‹ï¼šæˆç†Ÿåº¦={maturation_score:.3f}ï¼Œé€‚åˆç»“æ„é‡ç»„"
            
        logger.info("âŒ æœªè¾¾åˆ°è§¦å‘æ¡ä»¶: æœªå¤„äºå…³é”®å‘è‚²æœŸ")
        logger.exit_section("ç”Ÿç‰©å­¦åŸç†è§¦å‘å™¨æ£€æŸ¥")
        return False, "æœªå¤„äºå…³é”®å‘è‚²æœŸ"
    
    def _compute_maturation_score(self, performance_history: List[float]) -> float:
        """è®¡ç®—å‘è‚²æˆç†Ÿåº¦åˆ†æ•°"""
        if len(performance_history) < 5:
            return 0.0
        
        recent_performances = performance_history[-10:]
        
        # 1. æ€§èƒ½ç¨³å®šæ€§
        stability = 1.0 - np.std(recent_performances[-5:]) / (np.mean(recent_performances[-5:]) + 1e-8)
        
        # 2. æ”¹è¿›é€Ÿåº¦
        if len(recent_performances) >= 5:
            early_avg = np.mean(recent_performances[:5])
            late_avg = np.mean(recent_performances[-5:])
            improvement_rate = (late_avg - early_avg) / (early_avg + 1e-8)
        else:
            improvement_rate = 0.0
        
        # 3. æ€§èƒ½é«˜åº¦
        performance_level = recent_performances[-1] if recent_performances else 0.0
        
        # ç»¼åˆæˆç†Ÿåº¦åˆ†æ•°
        maturation = (
            0.4 * min(stability, 1.0) +
            0.3 * min(improvement_rate, 1.0) +
            0.3 * performance_level
        )
        
        return max(0.0, min(maturation, 1.0))
    
    def _detect_structural_differentiation_need(self, maturation_score: float) -> bool:
        """æ£€æµ‹æ˜¯å¦éœ€è¦ç»“æ„åˆ†åŒ–"""
        return maturation_score > self.maturation_threshold

class EnhancedCognitiveScienceTrigger:
    """å¢å¼ºçš„è®¤çŸ¥ç§‘å­¦è§¦å‘å™¨"""
    
    def __init__(self, forgetting_threshold: float = 0.05):
        self.forgetting_threshold = forgetting_threshold
        self.learning_patterns = deque(maxlen=25)
        
    def should_trigger(self, context: Dict[str, Any]) -> Tuple[bool, str]:
        """æ£€æµ‹è®¤çŸ¥ç“¶é¢ˆå’Œç¾éš¾æ€§é—å¿˜"""
        logger.enter_section("è®¤çŸ¥ç§‘å­¦è§¦å‘å™¨æ£€æŸ¥")
        
        performance_history = context.get('performance_history', [])
        activations = context.get('activations', {})
        
        logger.debug(f"æ€§èƒ½å†å²é•¿åº¦: {len(performance_history)}, æ¿€æ´»å€¼å±‚æ•°: {len(activations)}")
        
        if len(performance_history) < 8:
            logger.warning("âŒ å­¦ä¹ å†å²æ•°æ®ä¸è¶³ (éœ€è¦è‡³å°‘8ä¸ªæ•°æ®ç‚¹)")
            logger.exit_section("è®¤çŸ¥ç§‘å­¦è§¦å‘å™¨æ£€æŸ¥")
            return False, "å­¦ä¹ å†å²æ•°æ®ä¸è¶³"
        
        # æ£€æµ‹ç¾éš¾æ€§é—å¿˜
        logger.debug("æ£€æµ‹ç¾éš¾æ€§é—å¿˜...")
        forgetting_detected = self._detect_catastrophic_forgetting(performance_history)
        if forgetting_detected:
            logger.warning(f"ç¾éš¾æ€§é—å¿˜æ£€æµ‹: âœ…å‘ç°")
        else:
            logger.debug(f"ç¾éš¾æ€§é—å¿˜æ£€æµ‹: âŒæœªå‘ç°")
        
        # æ£€æµ‹å­¦ä¹ é¥±å’Œ
        logger.debug("æ£€æµ‹å­¦ä¹ é¥±å’Œ...")
        saturation_detected = self._detect_learning_saturation(performance_history)
        if saturation_detected:
            logger.warning(f"å­¦ä¹ é¥±å’Œæ£€æµ‹: âœ…å‘ç°")
        else:
            logger.debug(f"å­¦ä¹ é¥±å’Œæ£€æµ‹: âŒæœªå‘ç°")
        
        # æ£€æµ‹ç‰¹å¾è¡¨ç¤ºå†²çª
        logger.debug("æ£€æµ‹ç‰¹å¾è¡¨ç¤ºå†²çª...")
        conflict_detected = self._detect_representation_conflict(activations)
        if conflict_detected:
            logger.warning(f"ç‰¹å¾è¡¨ç¤ºå†²çªæ£€æµ‹: âœ…å‘ç°")
        else:
            logger.debug(f"ç‰¹å¾è¡¨ç¤ºå†²çªæ£€æµ‹: âŒæœªå‘ç°")
        
        self.learning_patterns.append({
            'epoch': context.get('epoch', 0),
            'performance': performance_history[-1] if performance_history else 0.0,
            'forgetting_risk': forgetting_detected,
            'saturation_risk': saturation_detected,
            'conflict_risk': conflict_detected
        })
        
        if forgetting_detected or conflict_detected:
            reason = []
            if forgetting_detected:
                reason.append("ç¾éš¾æ€§é—å¿˜é£é™©")
            if conflict_detected:
                reason.append("ç‰¹å¾è¡¨ç¤ºå†²çª")
            logger.success(f"âœ… è§¦å‘æ¡ä»¶æ»¡è¶³: {', '.join(reason)}")
            logger.exit_section("è®¤çŸ¥ç§‘å­¦è§¦å‘å™¨æ£€æŸ¥")
            return True, f"è®¤çŸ¥ç“¶é¢ˆæ£€æµ‹ï¼š{', '.join(reason)}ï¼Œéœ€è¦åˆ†åŒ–ä¸“é—¨åŒ–ç¥ç»å…ƒ"
            
        logger.info("âŒ æœªè¾¾åˆ°è§¦å‘æ¡ä»¶: è®¤çŸ¥æŒ‡æ ‡æ­£å¸¸")
        logger.exit_section("è®¤çŸ¥ç§‘å­¦è§¦å‘å™¨æ£€æŸ¥")
        return False, "è®¤çŸ¥æŒ‡æ ‡æ­£å¸¸"
    
    def _detect_catastrophic_forgetting(self, performance_history: List[float]) -> bool:
        """æ£€æµ‹ç¾éš¾æ€§é—å¿˜"""
        if len(performance_history) < 8:
            return False
        
        # æ£€æŸ¥æœ€è¿‘æ€§èƒ½æ˜¯å¦æ˜¾è‘—ä¸‹é™
        recent_window = 5
        past_window = 5
        
        recent_avg = np.mean(performance_history[-recent_window:])
        past_avg = np.mean(performance_history[-(recent_window + past_window):-recent_window])
        
        if past_avg > 0:
            decline_ratio = (past_avg - recent_avg) / past_avg
            return decline_ratio > self.forgetting_threshold
        
        return False
    
    def _detect_learning_saturation(self, performance_history: List[float]) -> bool:
        """æ£€æµ‹å­¦ä¹ é¥±å’Œ"""
        if len(performance_history) < 10:
            return False
        
        # æ£€æŸ¥æœ€è¿‘10ä¸ªepochçš„æ”¹è¿›
        recent_performances = performance_history[-10:]
        improvements = [recent_performances[i] - recent_performances[i-1] 
                       for i in range(1, len(recent_performances))]
        
        avg_improvement = np.mean(improvements)
        return avg_improvement < 0.001  # æ”¹è¿›æå°
    
    def _detect_representation_conflict(self, activations: Dict[str, torch.Tensor]) -> bool:
        """æ£€æµ‹ç‰¹å¾è¡¨ç¤ºå†²çª"""
        if not activations:
            return False
        
        # ç®€åŒ–çš„å†²çªæ£€æµ‹ï¼šæ£€æŸ¥æ¿€æ´»æ¨¡å¼çš„ä¸€è‡´æ€§
        conflict_scores = []
        
        for name, activation in activations.items():
            if len(activation.shape) >= 2 and activation.shape[0] > 1:
                # è®¡ç®—æ‰¹æ¬¡å†…æ¿€æ´»æ¨¡å¼çš„ä¸€è‡´æ€§
                activation_flat = activation.view(activation.shape[0], -1)
                
                # è®¡ç®—æ ·æœ¬é—´çš„ç›¸å…³æ€§
                if activation_flat.shape[1] > 1:
                    try:
                        correlation_matrix = torch.corrcoef(activation_flat)
                        # å¯¹è§’çº¿å¤–çš„ç›¸å…³ç³»æ•°
                        mask = ~torch.eye(correlation_matrix.shape[0], dtype=torch.bool)
                        off_diagonal = correlation_matrix[mask]
                        
                        if len(off_diagonal) > 0:
                            consistency = torch.mean(torch.abs(off_diagonal))
                            conflict_scores.append(1.0 - consistency.item())
                    except:
                        continue
        
        if conflict_scores:
            avg_conflict = np.mean(conflict_scores)
            return avg_conflict > 0.7  # å†²çªé˜ˆå€¼
        
        return False

class EnhancedDNMFramework:
    """å¢å¼ºçš„åŠ¨æ€ç¥ç»å½¢æ€å‘ç”Ÿæ¡†æ¶
    
    ğŸ§¬ æ”¯æŒä¼ ç»Ÿå½¢æ€å‘ç”Ÿå’Œæ¿€è¿›å¤šç‚¹å˜å¼‚
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        # ç°æœ‰åˆå§‹åŒ–ä»£ç 
        default_config = {
            'trigger_interval': 8,
            'performance_monitoring_window': 10,
            'morphogenesis_budget': 5000,
            'enable_aggressive_mode': True,  # æ–°å¢ï¼šæ¿€è¿›æ¨¡å¼å¼€å…³
            'accuracy_plateau_threshold': 0.1,  # æ–°å¢ï¼šå‡†ç¡®ç‡åœæ»é˜ˆå€¼
            'plateau_detection_window': 5,  # æ–°å¢ï¼šåœæ»æ£€æµ‹çª—å£
            'aggressive_trigger_accuracy': 0.92,  # æ–°å¢ï¼šæ¿€è¿›æ¨¡å¼è§¦å‘å‡†ç¡®ç‡
            'max_concurrent_mutations': 3,  # æ–°å¢ï¼šæœ€å¤§å¹¶å‘å˜å¼‚æ•°
        }
        
        self.config = {**default_config, **(config or {})}
        
        # åŸæœ‰ç»„ä»¶
        self.triggers = {
            'information_theory': EnhancedInformationTheoryTrigger(),
            'biological_principle': EnhancedBiologicalPrinciplesTrigger(),
            'cognitive_science': EnhancedCognitiveScienceTrigger()
        }
        
        self.bottleneck_analyzer = AdvancedBottleneckAnalyzer()
        self.decision_maker = IntelligentMorphogenesisDecisionMaker()
        self.executor = AdvancedMorphogenesisExecutor()
        
        # æ–°å¢æ¿€è¿›å½¢æ€å‘ç”Ÿç»„ä»¶
        if self.config['enable_aggressive_mode']:
            from .aggressive_morphogenesis import (
                AggressiveMorphogenesisAnalyzer,
                MultiPointMutationPlanner,
                AggressiveMorphogenesisExecutor
            )
            from .net2net_subnetwork_analyzer import Net2NetSubnetworkAnalyzer
            
            self.aggressive_analyzer = AggressiveMorphogenesisAnalyzer(
                accuracy_plateau_threshold=self.config['accuracy_plateau_threshold'],
                plateau_window=self.config['plateau_detection_window']
            )
            self.mutation_planner = MultiPointMutationPlanner(
                max_concurrent_mutations=self.config['max_concurrent_mutations'],
                parameter_budget=self.config['morphogenesis_budget']
            )
            self.aggressive_executor = AggressiveMorphogenesisExecutor()
            self.net2net_analyzer = Net2NetSubnetworkAnalyzer()
        
        # è®°å½•å’Œç›‘æ§
        self.morphogenesis_events = []
        self.performance_history = []
        self.aggressive_mode_active = False

    def check_morphogenesis_trigger(self, model: nn.Module, activations: Dict[str, torch.Tensor], 
                                  gradients: Dict[str, torch.Tensor], 
                                  performance_history: List[float], epoch: int) -> Tuple[bool, List[str]]:
        """æ£€æŸ¥æ˜¯å¦è§¦å‘å½¢æ€å‘ç”Ÿ - æ™ºèƒ½ç“¶é¢ˆæ£€æµ‹ç‰ˆæœ¬"""
        logger.enter_section("æ™ºèƒ½å½¢æ€å‘ç”Ÿè§¦å‘æ£€æŸ¥")
        
        # ğŸ“Š åŸºç¡€æ£€æŸ¥
        if len(performance_history) < 3:
            logger.info("âŒ æ€§èƒ½å†å²ä¸è¶³3ä¸ªç‚¹ï¼Œè·³è¿‡æ£€æŸ¥")
            logger.exit_section("æ™ºèƒ½å½¢æ€å‘ç”Ÿè§¦å‘æ£€æŸ¥")
            return False, []
        
        # ğŸ§  æ™ºèƒ½ç“¶é¢ˆæ£€æµ‹ç³»ç»Ÿ
        logger.info("ğŸ” å¼€å§‹æ™ºèƒ½ç“¶é¢ˆæ£€æµ‹...")
        
        # 1. æ€§èƒ½åœæ»æ£€æµ‹
        recent_performance = performance_history[-5:]  # æœ€è¿‘5ä¸ªepoch
        current_acc = recent_performance[-1]
        
        # è®¡ç®—åœæ»ä¸¥é‡ç¨‹åº¦
        if len(recent_performance) >= 3:
            improvement_trend = []
            for i in range(1, len(recent_performance)):
                improvement_trend.append(recent_performance[i] - recent_performance[i-1])
            
            avg_improvement = sum(improvement_trend) / len(improvement_trend)
            max_improvement = max(improvement_trend) if improvement_trend else 0
            stagnation_severity = max(0, -avg_improvement * 100)  # è½¬æ¢ä¸ºæ­£å€¼è¡¨ç¤ºåœæ»
            
            logger.info(f"ğŸ“ˆ æ€§èƒ½è¶‹åŠ¿åˆ†æ:")
            logger.info(f"  å½“å‰å‡†ç¡®ç‡: {current_acc:.4f}")
            logger.info(f"  å¹³å‡æ”¹è¿›: {avg_improvement:.6f}")
            logger.info(f"  åœæ»ä¸¥é‡ç¨‹åº¦: {stagnation_severity:.3f}")
        else:
            stagnation_severity = 0
            avg_improvement = 0
        
        # 2. ç½‘ç»œç“¶é¢ˆæ·±åº¦åˆ†æ
        logger.info("ğŸ”¬ æ‰§è¡Œç½‘ç»œç“¶é¢ˆæ·±åº¦åˆ†æ...")
        try:
            bottleneck_analysis = self.bottleneck_analyzer.analyze_network_bottlenecks(
                model, activations, gradients
            )
            
            # å¯»æ‰¾æœ€ä¸¥é‡çš„ç“¶é¢ˆ
            all_bottlenecks = {}
            for bottleneck_type, results in bottleneck_analysis.items():
                if isinstance(results, dict):
                    for layer_name, score in results.items():
                        if layer_name not in all_bottlenecks:
                            all_bottlenecks[layer_name] = {}
                        all_bottlenecks[layer_name][bottleneck_type] = score
            
            # è®¡ç®—ç»¼åˆç“¶é¢ˆåˆ†æ•°
            severe_bottlenecks = []
            for layer_name, bottleneck_scores in all_bottlenecks.items():
                # è®¡ç®—åŠ æƒå¹³å‡ç“¶é¢ˆåˆ†æ•°
                weights = {
                    'depth_bottlenecks': 0.3,
                    'width_bottlenecks': 0.25,
                    'information_flow_bottlenecks': 0.25,
                    'gradient_flow_bottlenecks': 0.2
                }
                
                combined_score = 0
                total_weight = 0
                for bottleneck_type, score in bottleneck_scores.items():
                    if bottleneck_type in weights:
                        combined_score += weights[bottleneck_type] * score
                        total_weight += weights[bottleneck_type]
                
                if total_weight > 0:
                    combined_score /= total_weight
                    
                    # ä¸¥é‡ç“¶é¢ˆé˜ˆå€¼
                    if combined_score > 0.6:  # é™ä½é˜ˆå€¼ï¼Œæ›´æ•æ„Ÿ
                        severe_bottlenecks.append((layer_name, combined_score, bottleneck_scores))
            
            # æ’åºæ‰¾å‡ºæœ€ä¸¥é‡çš„ç“¶é¢ˆ
            severe_bottlenecks.sort(key=lambda x: x[1], reverse=True)
            
            logger.info(f"ğŸ¯ å‘ç°{len(severe_bottlenecks)}ä¸ªä¸¥é‡ç“¶é¢ˆå±‚:")
            for layer_name, score, details in severe_bottlenecks[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                logger.info(f"  {layer_name}: ç»¼åˆåˆ†æ•°={score:.3f}")
                for bt, bs in details.items():
                    logger.info(f"    {bt}: {bs:.3f}")
            
        except Exception as e:
            logger.error(f"âŒ ç“¶é¢ˆåˆ†æå¤±è´¥: {e}")
            severe_bottlenecks = []
            bottleneck_analysis = {}
        
        # 3. Net2Netè¾“å‡ºåå‘æŠ•å½±åˆ†æ
        logger.info("ğŸ§ª æ‰§è¡ŒNet2Netè¾“å‡ºåå‘æŠ•å½±åˆ†æ...")
        try:
            from .net2net_subnetwork_analyzer import Net2NetSubnetworkAnalyzer
            net2net_analyzer = Net2NetSubnetworkAnalyzer()
            
            # æ„å»ºåˆ†æä¸Šä¸‹æ–‡
            current_accuracy = performance_history[-1] if performance_history else 0.0
            
            # åˆ›å»ºæ¨¡æ‹Ÿtargetsï¼ˆåœ¨æ²¡æœ‰çœŸå®targetsçš„æƒ…å†µä¸‹ï¼‰
            # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªåˆç†çš„é»˜è®¤å€¼ï¼Œå®é™…ä½¿ç”¨æ—¶åº”è¯¥ä¼ å…¥çœŸå®çš„targets
            dummy_targets = torch.randint(0, 10, (32,))  # CIFAR-10çš„10ä¸ªç±»åˆ«
            
            analysis_context = {
                'activations': activations,
                'gradients': gradients,
                'targets': dummy_targets,
                'current_accuracy': current_accuracy,
                'performance_history': performance_history,
                'epoch': epoch
            }
            
            net2net_results = net2net_analyzer.analyze_all_layers(
                model=model,
                context=analysis_context
            )
            
            # è¯†åˆ«Net2Netè®¤ä¸ºéœ€è¦æ”¹è¿›çš„å±‚
            improvement_candidates = []
            
            # å¤„ç†æ–°çš„Net2Netåˆ†æç»“æœç»“æ„
            layer_analyses = net2net_results.get('layer_analyses', {})
            leak_points = net2net_results.get('detected_leak_points', [])
            global_strategy = net2net_results.get('global_mutation_strategy', {})
            
            # ä»å±‚åˆ†æä¸­æå–æ”¹è¿›å€™é€‰
            for layer_name, analysis in layer_analyses.items():
                improvement_potential = analysis.get('mutation_prediction', {}).get('improvement_potential', 0)
                leak_assessment = analysis.get('leak_assessment', {})
                
                # ç»“åˆå˜å¼‚æ½œåŠ›å’Œæ¼ç‚¹è¯„ä¼°
                combined_potential = improvement_potential
                if leak_assessment.get('is_leak_point', False):
                    combined_potential += leak_assessment.get('leak_severity', 0) * 0.5
                
                if combined_potential > 0.3:  # æ”¹è¿›æ½œåŠ›é˜ˆå€¼
                    improvement_candidates.append((layer_name, combined_potential, analysis))
            
            # æ·»åŠ ä¸¥é‡æ¼ç‚¹ä½œä¸ºé«˜ä¼˜å…ˆçº§å€™é€‰
            for leak_point in leak_points:
                if leak_point['severity'] > 0.7:
                    layer_name = leak_point['layer_name']
                    if not any(cand[0] == layer_name for cand in improvement_candidates):
                        improvement_candidates.append((layer_name, leak_point['severity'], {
                            'leak_point': leak_point,
                            'recommendation': {'action': 'mutate', 'priority': 'critical'}
                        }))
            
            improvement_candidates.sort(key=lambda x: x[1], reverse=True)
            
            # æ˜¾ç¤ºè´å¶æ–¯é¢„æµ‹ç»“æœ
            bayesian_predictions = net2net_results.get('bayesian_benefit_predictions', {})
            comprehensive_strategies = net2net_results.get('comprehensive_mutation_strategies', {})
            metadata = net2net_results.get('analysis_metadata', {})
            strategy_summary = net2net_results.get('global_mutation_strategy', {}).get('comprehensive_strategies_summary', {})
            
            logger.info(f"ğŸš€ Net2Netå‘ç°{len(improvement_candidates)}ä¸ªæ”¹è¿›å€™é€‰:")
            logger.info(f"ğŸ•³ï¸ æ£€æµ‹åˆ°{len(leak_points)}ä¸ªä¿¡æ¯æ¼ç‚¹")
            logger.info(f"ğŸ§  è´å¶æ–¯é¢„æµ‹: {metadata.get('high_confidence_predictions', 0)}ä¸ªé«˜ç½®ä¿¡åº¦é¢„æµ‹")
            logger.info(f"â­ å¼ºçƒˆæ¨è: {metadata.get('strong_recommendations', 0)}ä¸ªå±‚")
            logger.info(f"ğŸ­ ç»¼åˆç­–ç•¥: {metadata.get('comprehensive_strategies_count', 0)}ä¸ªè¯¦ç»†å˜å¼‚ç­–ç•¥")
            
            # æ˜¾ç¤ºç»¼åˆç­–ç•¥åå¥½æ€»ç»“
            if strategy_summary:
                logger.info(f"ğŸ“Š ç­–ç•¥åå¥½: {strategy_summary.get('preferred_mutation_mode', 'unknown')} + {strategy_summary.get('preferred_combination_type', 'unknown')}")
                logger.info(f"ğŸ¯ ç»¼åˆæ”¶ç›Šé¢„æœŸ: {strategy_summary.get('total_expected_improvement', 0.0):.4f}")
            
            for layer_name, potential, details in improvement_candidates[:3]:
                recommendation = details.get('recommendation', {})
                leak_info = details.get('leak_point', {})
                
                # è·å–è´å¶æ–¯é¢„æµ‹ä¿¡æ¯
                bayesian_info = bayesian_predictions.get(layer_name, {})
                bayesian_pred = bayesian_info.get('bayesian_prediction', {})
                expected_gain = bayesian_pred.get('expected_accuracy_gain', 0)
                confidence = bayesian_pred.get('uncertainty_metrics', {}).get('prediction_confidence', 0)
                rec_strength = bayesian_pred.get('recommendation_strength', 'neutral')
                
                # è·å–ç»¼åˆç­–ç•¥ä¿¡æ¯
                comp_strategy_info = comprehensive_strategies.get(layer_name, {})
                comp_strategy = comp_strategy_info.get('comprehensive_strategy', {})
                mutation_mode = comp_strategy.get('mutation_mode', 'unknown')
                layer_combination = comp_strategy.get('layer_combination', {}).get('combination', 'unknown')
                total_gain = comp_strategy.get('expected_total_gain', 0)
                comp_confidence = comp_strategy.get('confidence', 0)
                
                if leak_info:
                    logger.info(f"  {layer_name}: æ¼ç‚¹ä¸¥é‡åº¦={potential:.3f}, ç±»å‹={leak_info.get('leak_type', 'unknown')}")
                    logger.info(f"    ğŸ§  è´å¶æ–¯é¢„æµ‹: æœŸæœ›æ”¶ç›Š={expected_gain:.4f}, ç½®ä¿¡åº¦={confidence:.3f}, æ¨è={rec_strength}")
                else:
                    logger.info(f"  {layer_name}: æ½œåŠ›={potential:.3f}, å»ºè®®={recommendation.get('action', 'unknown')}")
                    logger.info(f"    ğŸ§  è´å¶æ–¯é¢„æµ‹: æœŸæœ›æ”¶ç›Š={expected_gain:.4f}, ç½®ä¿¡åº¦={confidence:.3f}, æ¨è={rec_strength}")
                
                # æ˜¾ç¤ºç»¼åˆç­–ç•¥ä¿¡æ¯
                if comp_strategy_info:
                    logger.info(f"    ğŸ­ ç»¼åˆç­–ç•¥: {mutation_mode} + {layer_combination}")
                    logger.info(f"    ğŸ“ˆ æ€»æœŸæœ›æ”¶ç›Š: {total_gain:.4f}, ç»¼åˆç½®ä¿¡åº¦: {comp_confidence:.3f}")
            
        except Exception as e:
            logger.error(f"âŒ Net2Netåˆ†æå¤±è´¥: {e}")
            improvement_candidates = []
            net2net_results = {}
        
        # 4. æ™ºèƒ½è§¦å‘å†³ç­–
        logger.info("ğŸ¯ æ‰§è¡Œæ™ºèƒ½è§¦å‘å†³ç­–...")
        
        trigger_reasons = []
        should_trigger = False
        
        # å†³ç­–é€»è¾‘1: ä¸¥é‡ç“¶é¢ˆ + æ€§èƒ½åœæ»
        if severe_bottlenecks and stagnation_severity > 0.01:  # 0.01% åœæ»
            should_trigger = True
            top_bottleneck = severe_bottlenecks[0]
            trigger_reasons.append(f"ä¸¥é‡ç“¶é¢ˆæ£€æµ‹: {top_bottleneck[0]} (åˆ†æ•°={top_bottleneck[1]:.3f})")
            trigger_reasons.append(f"æ€§èƒ½åœæ»: {stagnation_severity:.3f}%")
        
        # å†³ç­–é€»è¾‘2: Net2Netå¼ºçƒˆå»ºè®®æ”¹è¿›
        if improvement_candidates and improvement_candidates[0][1] > 0.5:
            should_trigger = True
            top_candidate = improvement_candidates[0]
            trigger_reasons.append(f"Net2Netå¼ºçƒˆå»ºè®®: {top_candidate[0]} (æ½œåŠ›={top_candidate[1]:.3f})")
        
        # å†³ç­–é€»è¾‘3: å¤šä¸ªä¸­ç­‰ç“¶é¢ˆ + è½»å¾®åœæ»
        medium_bottlenecks = [b for b in severe_bottlenecks if 0.4 <= b[1] <= 0.6]
        if len(medium_bottlenecks) >= 2 and stagnation_severity > 0.005:  # 0.005% åœæ»
            should_trigger = True
            trigger_reasons.append(f"å¤šç‚¹ç“¶é¢ˆ: {len(medium_bottlenecks)}ä¸ªä¸­ç­‰ç“¶é¢ˆ")
            trigger_reasons.append(f"è½»å¾®åœæ»: {stagnation_severity:.3f}%")
        
        # å†³ç­–é€»è¾‘4: é•¿æœŸæ— æ”¹è¿›å¼ºåˆ¶è§¦å‘
        if avg_improvement <= 0 and len(performance_history) >= 8:
            recent_8 = performance_history[-8:]
            if max(recent_8) - min(recent_8) < 0.005:  # 8è½®å†…å˜åŒ–å°äº0.5%
                should_trigger = True
                trigger_reasons.append(f"é•¿æœŸæ— æ”¹è¿›å¼ºåˆ¶è§¦å‘: 8è½®å†…æœ€å¤§å˜åŒ–={max(recent_8) - min(recent_8):.4f}")
        
        # 5. æ¿€è¿›æ¨¡å¼æ£€æŸ¥ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰
        if (self.config.get('enable_aggressive_mode', False) and 
            current_acc > self.config.get('aggressive_trigger_accuracy', 0.85)):
            
            plateau_threshold = self.config.get('accuracy_plateau_threshold', 0.001)
            window_size = self.config.get('plateau_detection_window', 5)
            
            if len(recent_performance) >= window_size:
                performance_range = max(recent_performance) - min(recent_performance)
                if performance_range < plateau_threshold:
                    should_trigger = True
                    trigger_reasons.append(f"æ¿€è¿›æ¨¡å¼: é«˜å‡†ç¡®ç‡åœæ» (èŒƒå›´={performance_range:.4f})")
        
        # è¾“å‡ºå†³ç­–ç»“æœ
        if should_trigger:
            logger.info("âœ… è§¦å‘å½¢æ€å‘ç”Ÿ!")
            logger.info("ğŸ“‹ è§¦å‘åŸå› :")
            for reason in trigger_reasons:
                logger.info(f"  â€¢ {reason}")
            
            # ä¿å­˜åˆ†æç»“æœä¾›åç»­ä½¿ç”¨
            self._last_trigger_analysis = {
                'severe_bottlenecks': severe_bottlenecks,
                'improvement_candidates': improvement_candidates,
                'bottleneck_analysis': bottleneck_analysis,
                'net2net_results': net2net_results,
                'stagnation_severity': stagnation_severity,
                'performance_trend': avg_improvement
            }
        else:
            logger.info("âŒ æœªè¾¾åˆ°è§¦å‘æ¡ä»¶")
            logger.info(f"  ç“¶é¢ˆå±‚æ•°: {len(severe_bottlenecks)}")
            logger.info(f"  åœæ»ç¨‹åº¦: {stagnation_severity:.3f}%")
            logger.info(f"  æ”¹è¿›å€™é€‰: {len(improvement_candidates)}")
        
        logger.exit_section("æ™ºèƒ½å½¢æ€å‘ç”Ÿè§¦å‘æ£€æŸ¥")
        return should_trigger, trigger_reasons

    def execute_morphogenesis(self,
                            model: nn.Module,
                            activations_or_context,  # å…¼å®¹è€æ¥å£ï¼šå¯ä»¥æ˜¯context dictæˆ–activations dict
                            gradients: Optional[Dict[str, torch.Tensor]] = None,
                            performance_history: Optional[List[float]] = None,
                            epoch: Optional[int] = None,
                            targets: Optional[torch.Tensor] = None) -> Dict[str, Any]:
        """æ‰§è¡Œå½¢æ€å‘ç”Ÿ - æ”¯æŒä¼ ç»Ÿå’Œæ¿€è¿›æ¨¡å¼"""
        logger.enter_section("å¢å¼ºå½¢æ€å‘ç”Ÿæ‰§è¡Œ")
        logger.log_model_info(model, "è¾“å…¥æ¨¡å‹")
        
        # å…¼å®¹æ€§å¤„ç†ï¼šæ”¯æŒè€çš„contextæ¥å£å’Œæ–°çš„å‚æ•°æ¥å£
        if isinstance(activations_or_context, dict) and gradients is None:
            # è€æ¥å£ï¼šä¼ å…¥çš„æ˜¯contextå­—å…¸
            context = activations_or_context
            activations = context.get('activations', {})
            gradients = context.get('gradients', {})
            performance_history = context.get('performance_history', [])
            epoch = context.get('epoch', 0)
            targets = context.get('targets')
        else:
            # æ–°æ¥å£ï¼šç›´æ¥ä¼ å…¥å‚æ•°
            activations = activations_or_context
            if gradients is None or performance_history is None or epoch is None:
                logger.error("æ–°æ¥å£éœ€è¦æä¾›æ‰€æœ‰å¿…éœ€å‚æ•°ï¼šgradients, performance_history, epoch")
                return {
                    'model_modified': False,
                    'new_model': model,
                    'parameters_added': 0,
                    'morphogenesis_events': [],
                    'morphogenesis_type': 'error',
                    'trigger_reasons': [],
                    'error': 'missing_parameters'
                }
        
        try:
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³è§¦å‘æ¡ä»¶
            should_trigger, trigger_reasons = self.check_morphogenesis_trigger(
                model, activations, gradients, performance_history, epoch
            )
            
            if not should_trigger:
                logger.info("âŒ æœªæ»¡è¶³è§¦å‘æ¡ä»¶ï¼Œè·³è¿‡å½¢æ€å‘ç”Ÿ")
                logger.exit_section("å¢å¼ºå½¢æ€å‘ç”Ÿæ‰§è¡Œ")
                return {
                    'model_modified': False,
                    'new_model': model,
                    'parameters_added': 0,
                    'morphogenesis_events': [],
                    'morphogenesis_type': 'none',
                    'trigger_reasons': []
                }
            
            logger.success(f"æ»¡è¶³è§¦å‘æ¡ä»¶ï¼ŒåŸå› : {trigger_reasons}")
            
            # æ¿€è¿›æ¨¡å¼è·¯å¾„
            if self.aggressive_mode_active and self.config['enable_aggressive_mode']:
                return self._execute_aggressive_morphogenesis(
                    model, activations, gradients, targets, performance_history, epoch, trigger_reasons
                )
            
            # ä¼ ç»Ÿå½¢æ€å‘ç”Ÿè·¯å¾„
            return self._execute_traditional_morphogenesis(
                model, activations, gradients, performance_history, epoch, trigger_reasons
            )
            
        except Exception as e:
            logger.error(f"âŒ å½¢æ€å‘ç”Ÿæ‰§è¡Œå¤±è´¥: {e}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return {
                'model_modified': False,
                'new_model': model,
                'parameters_added': 0,
                'morphogenesis_events': [],
                'morphogenesis_type': 'error',
                'trigger_reasons': trigger_reasons,
                'error': str(e)
            }
        finally:
            logger.exit_section("å¢å¼ºå½¢æ€å‘ç”Ÿæ‰§è¡Œ")

    def _execute_aggressive_morphogenesis(self,
                                        model: nn.Module,
                                        activations: Dict[str, torch.Tensor],
                                        gradients: Dict[str, torch.Tensor],
                                        targets: Optional[torch.Tensor],
                                        performance_history: List[float],
                                        epoch: int,
                                        trigger_reasons: List[str]) -> Dict[str, Any]:
        """æ‰§è¡Œæ¿€è¿›å¤šç‚¹å½¢æ€å‘ç”Ÿ"""
        logger.enter_section("æ¿€è¿›å¤šç‚¹å½¢æ€å‘ç”Ÿ")
        
        try:
            # åå‘æ¢¯åº¦æŠ•å½±åˆ†æ
            if targets is None:
                # å¦‚æœæ²¡æœ‰æä¾›çœŸå®targetsï¼Œä½¿ç”¨æ¨¡æ‹Ÿtargets
                logger.warning("æœªæä¾›çœŸå®targetsï¼Œä½¿ç”¨æ¨¡æ‹Ÿtargetsè¿›è¡Œåˆ†æ")
                output_targets = torch.randint(0, 10, (128,))
            else:
                output_targets = targets
            
            bottleneck_signatures = self.aggressive_analyzer.analyze_reverse_gradient_projection(
                activations, gradients, output_targets
            )
            
            if not bottleneck_signatures:
                logger.warning("âŒ æœªæ£€æµ‹åˆ°ç“¶é¢ˆç­¾åï¼Œå›é€€åˆ°ä¼ ç»Ÿå½¢æ€å‘ç”Ÿ")
                return self._execute_traditional_morphogenesis(
                    model, activations, gradients, performance_history, epoch, trigger_reasons
                )
            
            # ä½¿ç”¨Net2Netåˆ†æå™¨è¿›ä¸€æ­¥åˆ†ææ¯ä¸ªç“¶é¢ˆå±‚çš„å˜å¼‚æ½œåŠ›
            logger.enter_section("Net2Netå­ç½‘ç»œæ½œåŠ›åˆ†æ")
            net2net_analyses = {}
            current_accuracy = performance_history[-1] if performance_history else 0.0
            
            for layer_name, signature in bottleneck_signatures.items():
                if signature.severity > 0.3:  # åªåˆ†æä¸¥é‡ç“¶é¢ˆ
                    try:
                        net2net_analysis = self.net2net_analyzer.analyze_layer_mutation_potential(
                            model, layer_name, activations, gradients, output_targets, current_accuracy
                        )
                        net2net_analyses[layer_name] = net2net_analysis
                        
                        # è®°å½•Net2Netåˆ†æç»“æœ
                        recommendation = net2net_analysis.get('recommendation', {})
                        logger.info(f"å±‚{layer_name}: {recommendation.get('action', 'unknown')} "
                                  f"(æ½œåŠ›={net2net_analysis.get('mutation_prediction', {}).get('improvement_potential', 0):.3f})")
                        
                    except Exception as e:
                        logger.warning(f"å±‚{layer_name}çš„Net2Netåˆ†æå¤±è´¥: {e}")
            
            logger.info(f"å®Œæˆ{len(net2net_analyses)}ä¸ªå±‚çš„Net2Netåˆ†æ")
            logger.exit_section("Net2Netå­ç½‘ç»œæ½œåŠ›åˆ†æ")
            
            # æ£€æµ‹åœæ»ä¸¥é‡ç¨‹åº¦
            _, stagnation_severity = self.aggressive_analyzer.detect_accuracy_plateau(performance_history)
            
            # åŸºäºNet2Netåˆ†æç»“æœæ”¹è¿›å˜å¼‚è§„åˆ’
            enhanced_bottleneck_signatures = self._enhance_bottleneck_signatures_with_net2net(
                bottleneck_signatures, net2net_analyses
            )
            
            # è§„åˆ’å¤šç‚¹å˜å¼‚
            mutations = self.mutation_planner.plan_aggressive_mutations(
                enhanced_bottleneck_signatures, performance_history, stagnation_severity
            )
            
            if not mutations:
                logger.warning("âŒ æœªç”Ÿæˆæœ‰æ•ˆçš„å˜å¼‚è®¡åˆ’ï¼Œå›é€€åˆ°ä¼ ç»Ÿå½¢æ€å‘ç”Ÿ")
                return self._execute_traditional_morphogenesis(
                    model, activations, gradients, performance_history, epoch, trigger_reasons
                )
            
            # æ‰§è¡Œæœ€ä½³å˜å¼‚ç­–ç•¥
            best_mutation = max(mutations, key=lambda m: m.expected_improvement - m.risk_assessment * 0.5)
            logger.info(f"é€‰æ‹©æœ€ä½³å˜å¼‚ç­–ç•¥: {best_mutation.coordination_strategy}, "
                       f"ç›®æ ‡ä½ç½®æ•°: {len(best_mutation.target_locations)}, "
                       f"æœŸæœ›æ”¹è¿›: {best_mutation.expected_improvement:.3f}")
            
            new_model, params_added, execution_result = self.aggressive_executor.execute_multi_point_mutation(
                model, best_mutation
            )
            
            # è®°å½•æ¿€è¿›å½¢æ€å‘ç”Ÿäº‹ä»¶
            morphogenesis_event = EnhancedMorphogenesisEvent(
                epoch=epoch,
                event_type='aggressive_multi_point',
                location=f"å¤šç‚¹({len(best_mutation.target_locations)}ä½ç½®)",
                trigger_reason='; '.join(trigger_reasons),
                performance_before=performance_history[-1] if performance_history else 0.0,
                parameters_added=params_added,
                morphogenesis_type=MorphogenesisType.HYBRID_DIVISION,  # ä»£è¡¨å¤šç‚¹å˜å¼‚
                confidence=1.0 - best_mutation.risk_assessment,
                expected_improvement=best_mutation.expected_improvement
            )
            
            self.morphogenesis_events.append(morphogenesis_event)
            
            logger.success(f"æ¿€è¿›å¤šç‚¹å½¢æ€å‘ç”Ÿå®Œæˆ: ç­–ç•¥={best_mutation.coordination_strategy}, "
                         f"æˆåŠŸå˜å¼‚={execution_result.get('successful_mutations', 0)}/"
                         f"{execution_result.get('total_mutations', 0)}, "
                         f"æ–°å¢å‚æ•°: {params_added:,}")
            
            # é‡ç½®æ¿€è¿›æ¨¡å¼çŠ¶æ€ï¼ˆç»™æ¨¡å‹å‡ ä¸ªepoché€‚åº”ï¼‰
            self.aggressive_mode_active = False
            
            return {
                'model_modified': params_added > 0,
                'new_model': new_model,
                'parameters_added': params_added,
                'morphogenesis_events': [morphogenesis_event],
                'morphogenesis_type': 'aggressive_multi_point',
                'trigger_reasons': trigger_reasons,
                'aggressive_details': {
                    'mutation_strategy': best_mutation.coordination_strategy,
                    'target_locations': best_mutation.target_locations,
                    'bottleneck_count': len(bottleneck_signatures),
                    'stagnation_severity': stagnation_severity,
                    'execution_result': execution_result,
                    'net2net_analyses': net2net_analyses  # åŒ…å«Net2Netåˆ†æç»“æœ
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ æ¿€è¿›å½¢æ€å‘ç”Ÿå¤±è´¥: {e}")
            logger.warning("å›é€€åˆ°ä¼ ç»Ÿå½¢æ€å‘ç”Ÿ")
            return self._execute_traditional_morphogenesis(
                model, activations, gradients, performance_history, epoch, trigger_reasons
            )
        finally:
            logger.exit_section("æ¿€è¿›å¤šç‚¹å½¢æ€å‘ç”Ÿ")

    def _execute_traditional_morphogenesis(self,
                                         model: nn.Module,
                                         activations: Dict[str, torch.Tensor],
                                         gradients: Dict[str, torch.Tensor],
                                         performance_history: List[float],
                                         epoch: int,
                                         trigger_reasons: List[str]) -> Dict[str, Any]:
        """æ‰§è¡Œæ™ºèƒ½ç“¶é¢ˆå¯¼å‘çš„å½¢æ€å‘ç”Ÿ"""
        logger.enter_section("æ™ºèƒ½ç“¶é¢ˆå¯¼å‘å½¢æ€å‘ç”Ÿ")
        
        try:
            # è·å–ä¹‹å‰ä¿å­˜çš„è§¦å‘åˆ†æç»“æœ
            trigger_analysis = getattr(self, '_last_trigger_analysis', None)
            
            if trigger_analysis is None:
                logger.warning("æœªæ‰¾åˆ°è§¦å‘åˆ†æç»“æœï¼Œæ‰§è¡Œæ–°çš„ç“¶é¢ˆåˆ†æ")
                # é‡æ–°åˆ†æç½‘ç»œç“¶é¢ˆ
                bottleneck_analysis = self.bottleneck_analyzer.analyze_network_bottlenecks(
                    model, activations, gradients
                )
                severe_bottlenecks = []
                improvement_candidates = []
                net2net_results = {}
            else:
                logger.info("ä½¿ç”¨ä¿å­˜çš„è§¦å‘åˆ†æç»“æœ")
                bottleneck_analysis = trigger_analysis.get('bottleneck_analysis', {})
                severe_bottlenecks = trigger_analysis.get('severe_bottlenecks', [])
                improvement_candidates = trigger_analysis.get('improvement_candidates', [])
                net2net_results = trigger_analysis.get('net2net_results', {})
            
            # æ™ºèƒ½å†³ç­–åˆ¶å®šï¼šåŸºäºç“¶é¢ˆåˆ†æå’ŒNet2Netå»ºè®®
            logger.info("ğŸ§  æ‰§è¡Œæ™ºèƒ½å†³ç­–åˆ¶å®š...")
            
            decision = None
            
            # ä¼˜å…ˆçº§1: Net2Netå¼ºçƒˆå»ºè®®çš„å±‚
            if improvement_candidates and improvement_candidates[0][1] > 0.5:
                target_info = improvement_candidates[0]
                layer_name = target_info[0]
                potential = target_info[1]
                analysis = target_info[2]
                recommendation = analysis.get('recommendation', {})
                
                # æ ¹æ®Net2Netçš„å»ºè®®é€‰æ‹©å½¢æ€å‘ç”Ÿç±»å‹
                suggested_action = recommendation.get('action', 'widen')
                if suggested_action == 'deepen':
                    morphogenesis_type = MorphogenesisType.SERIAL_DIVISION
                elif suggested_action == 'branch':
                    morphogenesis_type = MorphogenesisType.PARALLEL_DIVISION
                else:  # widen or other
                    morphogenesis_type = MorphogenesisType.HYBRID_DIVISION
                    
                decision = MorphogenesisDecision(
                    morphogenesis_type=morphogenesis_type,
                    target_location=layer_name,
                    confidence=min(0.9, potential),
                    expected_improvement=potential * 0.1,  # ä¿å®ˆä¼°è®¡
                    complexity_cost=0.3,
                    parameters_added=recommendation.get('estimated_params', 5000),
                    reasoning=f"Net2Netå¼ºçƒˆå»ºè®®: {suggested_action} (æ½œåŠ›={potential:.3f})"
                )
                
                logger.info(f"ğŸ¯ é‡‡ç”¨Net2Netå»ºè®®: {layer_name} -> {morphogenesis_type.value}")
                
            # ä¼˜å…ˆçº§2: ä¸¥é‡ç“¶é¢ˆå±‚
            elif severe_bottlenecks:
                target_info = severe_bottlenecks[0]
                layer_name = target_info[0]
                bottleneck_score = target_info[1]
                bottleneck_details = target_info[2]
                
                # æ ¹æ®ç“¶é¢ˆç±»å‹é€‰æ‹©å½¢æ€å‘ç”Ÿç­–ç•¥
                max_bottleneck_type = max(bottleneck_details.items(), key=lambda x: x[1])
                bottleneck_type_name = max_bottleneck_type[0]
                
                if 'depth' in bottleneck_type_name:
                    morphogenesis_type = MorphogenesisType.SERIAL_DIVISION
                    reasoning = f"æ·±åº¦ç“¶é¢ˆ: å¢åŠ ç½‘ç»œæ·±åº¦"
                elif 'width' in bottleneck_type_name:
                    morphogenesis_type = MorphogenesisType.HYBRID_DIVISION  
                    reasoning = f"å®½åº¦ç“¶é¢ˆ: å¢åŠ ç¥ç»å…ƒæ•°é‡"
                elif 'information_flow' in bottleneck_type_name:
                    morphogenesis_type = MorphogenesisType.PARALLEL_DIVISION
                    reasoning = f"ä¿¡æ¯æµç“¶é¢ˆ: åˆ›å»ºå¹¶è¡Œåˆ†æ”¯"
                else:
                    morphogenesis_type = MorphogenesisType.HYBRID_DIVISION
                    reasoning = f"æ··åˆç“¶é¢ˆ: ç»¼åˆæ”¹è¿›"
                    
                decision = MorphogenesisDecision(
                    morphogenesis_type=morphogenesis_type,
                    target_location=layer_name,
                    confidence=min(0.8, bottleneck_score),
                    expected_improvement=bottleneck_score * 0.05,
                    complexity_cost=0.4,
                    parameters_added=int(5000 * bottleneck_score),
                    reasoning=f"{reasoning} (ç“¶é¢ˆåˆ†æ•°={bottleneck_score:.3f})"
                )
                
                logger.info(f"ğŸ¯ é’ˆå¯¹ä¸¥é‡ç“¶é¢ˆ: {layer_name} -> {morphogenesis_type.value}")
                
            # ä¼˜å…ˆçº§3: å›é€€åˆ°ä¼ ç»Ÿå†³ç­–åˆ¶å®š
            if decision is None:
                logger.info("å›é€€åˆ°ä¼ ç»Ÿå†³ç­–åˆ¶å®šå™¨")
                if hasattr(self.decision_maker, 'make_morphogenesis_decision'):
                    decision = self.decision_maker.make_morphogenesis_decision(
                        model, activations, gradients, bottleneck_analysis, performance_history
                    )
                elif hasattr(self.decision_maker, 'make_decision'):
                    decision = self.decision_maker.make_decision(bottleneck_analysis, performance_history)
            
            if decision is None:
                logger.warning("âŒ å†³ç­–åˆ¶å®šå™¨æœªç”Ÿæˆæœ‰æ•ˆå†³ç­–")
                logger.exit_section("æ™ºèƒ½ç“¶é¢ˆå¯¼å‘å½¢æ€å‘ç”Ÿ")
                return {
                    'model_modified': False,
                    'new_model': model,
                    'parameters_added': 0,
                    'morphogenesis_events': [],
                    'morphogenesis_type': 'none',
                    'trigger_reasons': trigger_reasons
                }
            
            logger.info(f"ğŸ¯ æœ€ç»ˆå†³ç­–: {decision.morphogenesis_type.value}")
            logger.info(f"  ç›®æ ‡ä½ç½®: {decision.target_location}")
            logger.info(f"  ç½®ä¿¡åº¦: {decision.confidence:.3f}")
            logger.info(f"  é¢„æœŸæ”¹è¿›: {decision.expected_improvement:.3f}")
            logger.info(f"  å†³ç­–ä¾æ®: {decision.reasoning}")
            
            # æ‰§è¡Œå½¢æ€å‘ç”Ÿ
            try:
                if hasattr(self, 'morphogenesis_executor'):
                    new_model, parameters_added = self.morphogenesis_executor.execute_morphogenesis(model, decision)
                elif hasattr(self, 'executor'):
                    new_model, parameters_added = self.executor.execute_morphogenesis(model, decision)
                else:
                    raise AttributeError("æ‰¾ä¸åˆ°å½¢æ€å‘ç”Ÿæ‰§è¡Œå™¨")
                
                logger.success(f"âœ… æ™ºèƒ½å½¢æ€å‘ç”Ÿæ‰§è¡ŒæˆåŠŸ")
                logger.info(f"  æ–°å¢å‚æ•°: {parameters_added:,}")
                logger.info(f"  æ¨¡å‹æ€»å‚æ•°: {sum(p.numel() for p in new_model.parameters()):,}")
                
                # è®°å½•äº‹ä»¶
                morphogenesis_event = EnhancedMorphogenesisEvent(
                    epoch=epoch,
                    event_type=decision.morphogenesis_type.value,
                    location=decision.target_location,
                    trigger_reason='; '.join(trigger_reasons),
                    performance_before=performance_history[-1] if performance_history else 0.0,
                    parameters_added=parameters_added,
                    morphogenesis_type=decision.morphogenesis_type,
                    confidence=decision.confidence,
                    expected_improvement=decision.expected_improvement
                )
                
                self.morphogenesis_events.append(morphogenesis_event)
                
                logger.exit_section("æ™ºèƒ½ç“¶é¢ˆå¯¼å‘å½¢æ€å‘ç”Ÿ")
                return {
                    'model_modified': True,
                    'new_model': new_model,
                    'parameters_added': parameters_added,
                    'morphogenesis_events': [morphogenesis_event],
                    'morphogenesis_type': decision.morphogenesis_type.value,
                    'trigger_reasons': trigger_reasons,
                    'decision_confidence': decision.confidence,
                    'bottleneck_analysis': bottleneck_analysis,
                    'intelligent_decision': True
                }
                
            except Exception as e:
                logger.error(f"âŒ æ™ºèƒ½å½¢æ€å‘ç”Ÿæ‰§è¡Œå¤±è´¥: {e}")
                import traceback
                logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                logger.exit_section("æ™ºèƒ½ç“¶é¢ˆå¯¼å‘å½¢æ€å‘ç”Ÿ")
                return {
                    'model_modified': False,
                    'new_model': model,
                    'parameters_added': 0,
                    'morphogenesis_events': [],
                    'morphogenesis_type': 'error',
                    'trigger_reasons': trigger_reasons,
                    'error': str(e)
                }
                
        except Exception as e:
            logger.error(f"âŒ æ™ºèƒ½ç“¶é¢ˆå¯¼å‘å½¢æ€å‘ç”Ÿå¤±è´¥: {e}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return {
                'model_modified': False,
                'new_model': model,
                'parameters_added': 0,
                'morphogenesis_events': [],
                'morphogenesis_type': 'error',
                'trigger_reasons': trigger_reasons,
                'error': str(e)
            }
        finally:
            logger.exit_section("æ™ºèƒ½ç“¶é¢ˆå¯¼å‘å½¢æ€å‘ç”Ÿ")
    
    def _enhance_bottleneck_signatures_with_net2net(self, 
                                                   bottleneck_signatures: Dict,
                                                   net2net_analyses: Dict) -> Dict:
        """ä½¿ç”¨Net2Netåˆ†æç»“æœå¢å¼ºç“¶é¢ˆç­¾å"""
        
        enhanced_signatures = copy.deepcopy(bottleneck_signatures)
        
        for layer_name, signature in enhanced_signatures.items():
            if layer_name in net2net_analyses:
                net2net_analysis = net2net_analyses[layer_name]
                
                # è·å–å˜å¼‚é¢„æµ‹ä¿¡æ¯
                mutation_prediction = net2net_analysis.get('mutation_prediction', {})
                improvement_potential = mutation_prediction.get('improvement_potential', 0.0)
                risk_assessment = mutation_prediction.get('risk_assessment', {})
                
                # æ ¹æ®Net2Netåˆ†æè°ƒæ•´ç“¶é¢ˆä¸¥é‡ç¨‹åº¦
                original_severity = signature.severity
                net2net_adjustment = improvement_potential * 0.5  # Net2Netæ”¹è¿›æ½œåŠ›çš„æƒé‡
                
                # ç»¼åˆä¸¥é‡ç¨‹åº¦ = åŸå§‹ä¸¥é‡ç¨‹åº¦ + Net2Netæ”¹è¿›æ½œåŠ› - é£é™©è°ƒæ•´
                adjusted_severity = original_severity + net2net_adjustment - risk_assessment.get('overall_risk', 0) * 0.2
                signature.severity = max(0.0, min(1.0, adjusted_severity))
                
                # æ·»åŠ Net2Netç‰¹å®šä¿¡æ¯
                signature.net2net_improvement_potential = improvement_potential
                signature.net2net_risk = risk_assessment.get('overall_risk', 0.0)
                signature.net2net_recommended_strategy = net2net_analysis.get('recommendation', {}).get('recommended_strategy')
                
                logger.debug(f"å±‚{layer_name}: åŸå§‹ä¸¥é‡ç¨‹åº¦={original_severity:.3f} -> "
                           f"è°ƒæ•´åä¸¥é‡ç¨‹åº¦={signature.severity:.3f} "
                           f"(Net2Netæ½œåŠ›={improvement_potential:.3f})")
        
        return enhanced_signatures

    def update_performance_history(self, performance: float):
        """æ›´æ–°æ€§èƒ½å†å²"""
        self.performance_history.append(performance)
        
        # ä¿æŒå†å²é•¿åº¦
        if len(self.performance_history) > 100:
            self.performance_history = self.performance_history[-100:]
    
    def cache_activations_and_gradients(self, activations: Dict[str, torch.Tensor], 
                                       gradients: Dict[str, torch.Tensor]):
        """ç¼“å­˜æ¿€æ´»å€¼å’Œæ¢¯åº¦"""
        self.activation_cache = activations
        self.gradient_cache = gradients
    
    def get_morphogenesis_summary(self) -> Dict[str, Any]:
        """è·å–å½¢æ€å‘ç”Ÿæ€»ç»“"""
        if not self.morphogenesis_events:
            return {
                'total_events': 0,
                'total_parameters_added': 0,
                'morphogenesis_types': {},
                'events': []
            }
        
        # ç»Ÿè®¡å„ç§ç±»å‹çš„å½¢æ€å‘ç”Ÿ
        type_counts = defaultdict(int)
        for event in self.morphogenesis_events:
            type_counts[event.morphogenesis_type.value] += 1
        
        total_params = sum(event.parameters_added for event in self.morphogenesis_events)
        
        return {
            'total_events': len(self.morphogenesis_events),
            'total_parameters_added': total_params,
            'morphogenesis_types': dict(type_counts),
            'events': [
                {
                    'epoch': event.epoch,
                    'type': event.morphogenesis_type.value,
                    'location': event.location,
                    'parameters_added': event.parameters_added,
                    'confidence': event.confidence,
                    'expected_improvement': event.expected_improvement,
                    'reasoning': event.trigger_reason
                }
                for event in self.morphogenesis_events
            ]
        }