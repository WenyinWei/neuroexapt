#!/usr/bin/env python3
"""
Enhanced Dynamic Neural Morphogenesis (DNM) Framework - å¢å¼ºç‰ˆ

ğŸ§¬ æ ¸å¿ƒæ”¹è¿›ï¼š
1. å¤šç»´åº¦ç“¶é¢ˆåˆ†æ - æ·±åº¦ã€å®½åº¦ã€ä¿¡æ¯æµã€æ¢¯åº¦æµã€å®¹é‡ç“¶é¢ˆ
2. é«˜çº§å½¢æ€å‘ç”Ÿç­–ç•¥ - ä¸²è¡Œåˆ†è£‚ã€å¹¶è¡Œåˆ†è£‚ã€æ··åˆåˆ†è£‚
3. æ™ºèƒ½å†³ç­–åˆ¶å®š - åŸºäºç“¶é¢ˆç±»å‹çš„æœ€ä¼˜ç­–ç•¥é€‰æ‹©
4. æ€§èƒ½å¯¼å‘ - è¿½æ±‚æ›´é«˜çš„å‡†ç¡®ç‡çªç ´

ğŸ¯ ç›®æ ‡ï¼šå®ç°90%+çš„å‡†ç¡®ç‡ï¼Œæ¢ç´¢ç½‘ç»œç»“æ„çš„æ— é™å¯èƒ½
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from abc import ABC, abstractmethod
import math
from collections import defaultdict, deque
import copy
import traceback
import time
import os

# å¯¼å…¥é«˜çº§å½¢æ€å‘ç”Ÿæ¨¡å—
from .advanced_morphogenesis import (
    AdvancedBottleneckAnalyzer,
    AdvancedMorphogenesisExecutor,
    IntelligentMorphogenesisDecisionMaker,
    MorphogenesisType,
    MorphogenesisDecision
)

# å¯¼å…¥ç»Ÿä¸€çš„æ—¥å¿—ç³»ç»Ÿ
from .logging_utils import ConfigurableLogger, logger


# ä¿æŒå‘åå…¼å®¹æ€§çš„DebugPrinterç±»
class DebugPrinter:
    """å‘åå…¼å®¹çš„è°ƒè¯•æ‰“å°å™¨ï¼ˆå·²åºŸå¼ƒï¼Œå»ºè®®ä½¿ç”¨loggerï¼‰"""
    
    def __init__(self, enabled: bool = True):
        self.enabled = enabled
        self._logger = logger
        import warnings
        warnings.warn("DebugPrinter is deprecated, use logger instead", DeprecationWarning)
    
    def print_debug(self, message: str, level: str = "INFO"):
        """æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼ˆå…¼å®¹æ¥å£ï¼‰"""
        if not self.enabled:
            return
        getattr(self._logger, level.lower(), self._logger.info)(message)
    
    def enter_section(self, section_name: str):
        """è¿›å…¥è°ƒè¯•åŒºåŸŸ"""
        self._logger.enter_section(section_name)
    
        """é€€å‡ºè°ƒè¯•åŒºåŸŸ"""
        self._logger.exit_section(section_name)



# ä¿æŒå‘åå…¼å®¹æ€§çš„DebugPrinterç±»
class DebugPrinter:
    """å‘åå…¼å®¹çš„è°ƒè¯•æ‰“å°å™¨ï¼ˆå·²åºŸå¼ƒï¼Œå»ºè®®ä½¿ç”¨loggerï¼‰"""
    
    def __init__(self, enabled: bool = True):
        self.enabled = enabled
        self._logger = logger
        import warnings
        warnings.warn(
            "DebugPrinter is deprecated. Use the global 'logger' instance instead.",
            DeprecationWarning,
            stacklevel=2
        )
    
    def print_debug(self, message: str, level: str = "INFO"):
        if not self.enabled:
            return
        
        level_map = {
            "INFO": self._logger.info,
            "SUCCESS": self._logger.success,
            "WARNING": self._logger.warning,
            "ERROR": self._logger.error,
            "DEBUG": self._logger.debug
        }
        level_map.get(level, self._logger.info)(message)
    
    def enter_section(self, section_name: str):
        if self.enabled:
            self._logger.enter_section(section_name)
    
    def exit_section(self, section_name: str):
        if self.enabled:
            self._logger.exit_section(section_name)
    
    def print_tensor_info(self, tensor: torch.Tensor, name: str):
        if self.enabled:
            self._logger.log_tensor_info(tensor, name)
    
    def print_model_info(self, model: nn.Module, name: str = "Model"):
        if self.enabled:
            self._logger.log_model_info(model, name)



@dataclass
class EnhancedMorphogenesisEvent:
    """å¢å¼ºçš„å½¢æ€å‘ç”Ÿäº‹ä»¶è®°å½•"""
    epoch: int
    event_type: str  # 'width_expansion', 'serial_division', 'parallel_division', 'hybrid_division'
    location: str
    trigger_reason: str
    performance_before: float
    performance_after: Optional[float] = None
    parameters_added: int = 0
    complexity_change: float = 0.0
    morphogenesis_type: MorphogenesisType = MorphogenesisType.WIDTH_EXPANSION
    confidence: float = 0.0
    expected_improvement: float = 0.0

class EnhancedInformationTheoryTrigger:
    """å¢å¼ºçš„ä¿¡æ¯è®ºè§¦å‘å™¨"""
    
    def __init__(self, entropy_threshold: float = 0.1, complexity_threshold: float = 0.7):
        self.entropy_threshold = entropy_threshold
        self.complexity_threshold = complexity_threshold
        self.history = deque(maxlen=15)
        
    def should_trigger(self, context: Dict[str, Any]) -> Tuple[bool, str]:
        logger.enter_section("ä¿¡æ¯è®ºè§¦å‘å™¨æ£€æŸ¥")
        
        activations = context.get('activations', {})
        gradients = context.get('gradients', {})
        
        logger.debug(f"è¾“å…¥æ•°æ®: æ¿€æ´»å€¼å±‚æ•°={len(activations)}, æ¢¯åº¦å±‚æ•°={len(gradients)}")
        
        if not activations or not gradients:
            logger.warning("âŒ ç¼ºå°‘æ¿€æ´»å€¼æˆ–æ¢¯åº¦ä¿¡æ¯")
            logger.exit_section("ä¿¡æ¯è®ºè§¦å‘å™¨æ£€æŸ¥")
            return False, "ç¼ºå°‘æ¿€æ´»å€¼æˆ–æ¢¯åº¦ä¿¡æ¯"
            
        # è®¡ç®—ç»¼åˆå¤æ‚åº¦åˆ†æ•°
        complexity_score = self._compute_complexity_score(activations, gradients)
        
        logger.info(f"å¤æ‚åº¦åˆ†æ•°: {complexity_score:.4f} (é˜ˆå€¼: {self.complexity_threshold})")
        
        self.history.append({
            'complexity_score': complexity_score,
            'epoch': context.get('epoch', 0)
        })
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´å¤æ‚çš„ç»“æ„å˜å¼‚
        if complexity_score > self.complexity_threshold:
            logger.success(f"è§¦å‘æ¡ä»¶æ»¡è¶³: {complexity_score:.4f} > {self.complexity_threshold}")
            logger.exit_section("ä¿¡æ¯è®ºè§¦å‘å™¨æ£€æŸ¥")
            return True, f"å¤æ‚åº¦ç“¶é¢ˆæ£€æµ‹ï¼šåˆ†æ•°={complexity_score:.4f}"
            
        logger.info(f"âŒ æœªè¾¾åˆ°è§¦å‘æ¡ä»¶: {complexity_score:.4f} <= {self.complexity_threshold}")
        logger.exit_section("ä¿¡æ¯è®ºè§¦å‘å™¨æ£€æŸ¥")
        return False, "å¤æ‚åº¦æŒ‡æ ‡æœªè¾¾åˆ°è§¦å‘æ¡ä»¶"
    
    def _compute_complexity_score(self, activations: Dict[str, torch.Tensor], 
                                gradients: Dict[str, torch.Tensor]) -> float:
        """è®¡ç®—ç½‘ç»œå¤æ‚åº¦åˆ†æ•°"""
        logger.enter_section("å¤æ‚åº¦åˆ†æ•°è®¡ç®—")
        scores = []
        
        for name, activation in activations.items():
            if name not in gradients or gradients[name] is None:
                logger.warning(f"âš ï¸ è·³è¿‡å±‚ {name}: ç¼ºå°‘æ¢¯åº¦ä¿¡æ¯")
                continue
                
            gradient = gradients[name]
            logger.log_tensor_info(activation, f"æ¿€æ´»å€¼[{name}]")
            logger.log_tensor_info(gradient, f"æ¢¯åº¦[{name}]")
            
            # 1. ä¿¡æ¯ç†µåˆ†æ
            entropy = self._compute_entropy(activation)
            logger.debug(f"ä¿¡æ¯ç†µ[{name}]: {entropy:.4f}")
            
            # 2. æ¢¯åº¦å¤æ‚åº¦
            grad_complexity = self._compute_gradient_complexity(gradient)
            logger.debug(f"æ¢¯åº¦å¤æ‚åº¦[{name}]: {grad_complexity:.4f}")
            
            # 3. æ¿€æ´»æ¨¡å¼å¤æ‚åº¦
            activation_complexity = self._compute_activation_complexity(activation)
            logger.debug(f"æ¿€æ´»å¤æ‚åº¦[{name}]: {activation_complexity:.4f}")
            
            # ç»¼åˆåˆ†æ•°
            layer_score = 0.4 * entropy + 0.3 * grad_complexity + 0.3 * activation_complexity
            scores.append(layer_score)
            logger.debug(f"å±‚åˆ†æ•°[{name}]: {layer_score:.4f}")
        
        final_score = np.mean(scores) if scores else 0.0
        logger.info(f"æœ€ç»ˆå¤æ‚åº¦åˆ†æ•°: {final_score:.4f} (å…±{len(scores)}å±‚)")
        logger.exit_section("å¤æ‚åº¦åˆ†æ•°è®¡ç®—")
        return final_score
    
    def _compute_entropy(self, activation: torch.Tensor) -> float:
        """è®¡ç®—æ¿€æ´»å€¼ç†µ"""
        if activation.numel() == 0:
            return 0.0
            
        activation_flat = activation.flatten()
        activation_abs = torch.abs(activation_flat) + 1e-8
        probs = activation_abs / torch.sum(activation_abs)
        
        entropy = -torch.sum(probs * torch.log(probs + 1e-8))
        # å½’ä¸€åŒ–åˆ° [0, 1]
        max_entropy = math.log(len(probs))
        return min(entropy.item() / max_entropy, 1.0) if max_entropy > 0 else 0.0
    
    def _compute_gradient_complexity(self, gradient: torch.Tensor) -> float:
        """è®¡ç®—æ¢¯åº¦å¤æ‚åº¦"""
        if gradient is None or gradient.numel() == 0:
            return 0.0
            
        grad_flat = gradient.flatten()
        
        # æ¢¯åº¦çš„æ ‡å‡†å·®/å‡å€¼æ¯”ç‡
        grad_std = torch.std(grad_flat)
        grad_mean = torch.mean(torch.abs(grad_flat))
        
        if grad_mean > 1e-8:
            complexity = grad_std / grad_mean
            return min(complexity.item(), 1.0)
        else:
            return 0.0
    
    def _compute_activation_complexity(self, activation: torch.Tensor) -> float:
        """è®¡ç®—æ¿€æ´»æ¨¡å¼å¤æ‚åº¦"""
        if activation.numel() == 0 or len(activation.shape) < 2:
            return 0.0
            
        # è®¡ç®—æ¿€æ´»æ¨¡å¼çš„å˜å¼‚ç³»æ•°
        activation_flat = activation.view(activation.shape[0], -1)
        
        # æ‰¹æ¬¡é—´çš„å˜å¼‚æ€§
        batch_means = torch.mean(activation_flat, dim=1)
        batch_std = torch.std(batch_means)
        batch_mean_avg = torch.mean(batch_means)
        
        if batch_mean_avg > 1e-8:
            complexity = batch_std / batch_mean_avg
            return min(complexity.item(), 1.0)
        else:
            return 0.0

class EnhancedBiologicalPrinciplesTrigger:
    """å¢å¼ºçš„ç”Ÿç‰©å­¦åŸç†è§¦å‘å™¨"""
    
    def __init__(self, maturation_threshold: float = 0.6):
        self.maturation_threshold = maturation_threshold
        self.development_history = deque(maxlen=20)
        
    def should_trigger(self, context: Dict[str, Any]) -> Tuple[bool, str]:
        """æ£€æµ‹æ˜¯å¦å¤„äºå…³é”®å‘è‚²æœŸ"""
        logger.enter_section("ç”Ÿç‰©å­¦åŸç†è§¦å‘å™¨æ£€æŸ¥")
        
        performance_history = context.get('performance_history', [])
        epoch = context.get('epoch', 0)
        
        logger.debug(f"å½“å‰epoch: {epoch}, æ€§èƒ½å†å²é•¿åº¦: {len(performance_history)}")
        
        if len(performance_history) < 10:
            logger.warning("âŒ æ€§èƒ½å†å²æ•°æ®ä¸è¶³ (éœ€è¦è‡³å°‘10ä¸ªæ•°æ®ç‚¹)")
            logger.exit_section("ç”Ÿç‰©å­¦åŸç†è§¦å‘å™¨æ£€æŸ¥")
            return False, "æ€§èƒ½å†å²æ•°æ®ä¸è¶³"
        
        # æ£€æµ‹å‘è‚²é˜¶æ®µ
        logger.debug("è®¡ç®—å‘è‚²æˆç†Ÿåº¦åˆ†æ•°...")
        maturation_score = self._compute_maturation_score(performance_history)
        logger.info(f"æˆç†Ÿåº¦åˆ†æ•°: {maturation_score:.4f} (é˜ˆå€¼: {self.maturation_threshold})")
        
        self.development_history.append({
            'epoch': epoch,
            'maturation_score': maturation_score,
            'performance': performance_history[-1] if performance_history else 0.0
        })
        
        # æ£€æµ‹æ˜¯å¦éœ€è¦ç»“æ„åˆ†åŒ–
        differentiation_needed = self._detect_structural_differentiation_need(maturation_score)
        if differentiation_needed:
            logger.success(f"ç»“æ„åˆ†åŒ–éœ€æ±‚: âœ…éœ€è¦")
        else:
            logger.debug(f"ç»“æ„åˆ†åŒ–éœ€æ±‚: âŒä¸éœ€è¦")
        
        if differentiation_needed:
            logger.success(f"âœ… è§¦å‘æ¡ä»¶æ»¡è¶³: æˆç†Ÿåº¦={maturation_score:.3f}")
            logger.exit_section("ç”Ÿç‰©å­¦åŸç†è§¦å‘å™¨æ£€æŸ¥")
            return True, f"å…³é”®å‘è‚²æœŸæ£€æµ‹ï¼šæˆç†Ÿåº¦={maturation_score:.3f}ï¼Œé€‚åˆç»“æ„é‡ç»„"
            
        logger.info("âŒ æœªè¾¾åˆ°è§¦å‘æ¡ä»¶: æœªå¤„äºå…³é”®å‘è‚²æœŸ")
        logger.exit_section("ç”Ÿç‰©å­¦åŸç†è§¦å‘å™¨æ£€æŸ¥")
        return False, "æœªå¤„äºå…³é”®å‘è‚²æœŸ"
    
    def _compute_maturation_score(self, performance_history: List[float]) -> float:
        """è®¡ç®—å‘è‚²æˆç†Ÿåº¦åˆ†æ•°"""
        if len(performance_history) < 5:
            return 0.0
        
        recent_performances = performance_history[-10:]
        
        # 1. æ€§èƒ½ç¨³å®šæ€§
        stability = 1.0 - np.std(recent_performances[-5:]) / (np.mean(recent_performances[-5:]) + 1e-8)
        
        # 2. æ”¹è¿›é€Ÿåº¦
        if len(recent_performances) >= 5:
            early_avg = np.mean(recent_performances[:5])
            late_avg = np.mean(recent_performances[-5:])
            improvement_rate = (late_avg - early_avg) / (early_avg + 1e-8)
        else:
            improvement_rate = 0.0
        
        # 3. æ€§èƒ½é«˜åº¦
        performance_level = recent_performances[-1] if recent_performances else 0.0
        
        # ç»¼åˆæˆç†Ÿåº¦åˆ†æ•°
        maturation = (
            0.4 * min(stability, 1.0) +
            0.3 * min(improvement_rate, 1.0) +
            0.3 * performance_level
        )
        
        return max(0.0, min(maturation, 1.0))
    
    def _detect_structural_differentiation_need(self, maturation_score: float) -> bool:
        """æ£€æµ‹æ˜¯å¦éœ€è¦ç»“æ„åˆ†åŒ–"""
        return maturation_score > self.maturation_threshold

class EnhancedCognitiveScienceTrigger:
    """å¢å¼ºçš„è®¤çŸ¥ç§‘å­¦è§¦å‘å™¨"""
    
    def __init__(self, forgetting_threshold: float = 0.05):
        self.forgetting_threshold = forgetting_threshold
        self.learning_patterns = deque(maxlen=25)
        
    def should_trigger(self, context: Dict[str, Any]) -> Tuple[bool, str]:
        """æ£€æµ‹è®¤çŸ¥ç“¶é¢ˆå’Œç¾éš¾æ€§é—å¿˜"""
        logger.enter_section("è®¤çŸ¥ç§‘å­¦è§¦å‘å™¨æ£€æŸ¥")
        
        performance_history = context.get('performance_history', [])
        activations = context.get('activations', {})
        
        logger.debug(f"æ€§èƒ½å†å²é•¿åº¦: {len(performance_history)}, æ¿€æ´»å€¼å±‚æ•°: {len(activations)}")
        
        if len(performance_history) < 8:
            logger.warning("âŒ å­¦ä¹ å†å²æ•°æ®ä¸è¶³ (éœ€è¦è‡³å°‘8ä¸ªæ•°æ®ç‚¹)")
            logger.exit_section("è®¤çŸ¥ç§‘å­¦è§¦å‘å™¨æ£€æŸ¥")
            return False, "å­¦ä¹ å†å²æ•°æ®ä¸è¶³"
        
        # æ£€æµ‹ç¾éš¾æ€§é—å¿˜
        logger.debug("æ£€æµ‹ç¾éš¾æ€§é—å¿˜...")
        forgetting_detected = self._detect_catastrophic_forgetting(performance_history)
        if forgetting_detected:
            logger.warning(f"ç¾éš¾æ€§é—å¿˜æ£€æµ‹: âœ…å‘ç°")
        else:
            logger.debug(f"ç¾éš¾æ€§é—å¿˜æ£€æµ‹: âŒæœªå‘ç°")
        
        # æ£€æµ‹å­¦ä¹ é¥±å’Œ
        logger.debug("æ£€æµ‹å­¦ä¹ é¥±å’Œ...")
        saturation_detected = self._detect_learning_saturation(performance_history)
        if saturation_detected:
            logger.warning(f"å­¦ä¹ é¥±å’Œæ£€æµ‹: âœ…å‘ç°")
        else:
            logger.debug(f"å­¦ä¹ é¥±å’Œæ£€æµ‹: âŒæœªå‘ç°")
        
        # æ£€æµ‹ç‰¹å¾è¡¨ç¤ºå†²çª
        logger.debug("æ£€æµ‹ç‰¹å¾è¡¨ç¤ºå†²çª...")
        conflict_detected = self._detect_representation_conflict(activations)
        if conflict_detected:
            logger.warning(f"ç‰¹å¾è¡¨ç¤ºå†²çªæ£€æµ‹: âœ…å‘ç°")
        else:
            logger.debug(f"ç‰¹å¾è¡¨ç¤ºå†²çªæ£€æµ‹: âŒæœªå‘ç°")
        
        self.learning_patterns.append({
            'epoch': context.get('epoch', 0),
            'performance': performance_history[-1] if performance_history else 0.0,
            'forgetting_risk': forgetting_detected,
            'saturation_risk': saturation_detected,
            'conflict_risk': conflict_detected
        })
        
        if forgetting_detected or conflict_detected:
            reason = []
            if forgetting_detected:
                reason.append("ç¾éš¾æ€§é—å¿˜é£é™©")
            if conflict_detected:
                reason.append("ç‰¹å¾è¡¨ç¤ºå†²çª")
            logger.success(f"âœ… è§¦å‘æ¡ä»¶æ»¡è¶³: {', '.join(reason)}")
            logger.exit_section("è®¤çŸ¥ç§‘å­¦è§¦å‘å™¨æ£€æŸ¥")
            return True, f"è®¤çŸ¥ç“¶é¢ˆæ£€æµ‹ï¼š{', '.join(reason)}ï¼Œéœ€è¦åˆ†åŒ–ä¸“é—¨åŒ–ç¥ç»å…ƒ"
            
        logger.info("âŒ æœªè¾¾åˆ°è§¦å‘æ¡ä»¶: è®¤çŸ¥æŒ‡æ ‡æ­£å¸¸")
        logger.exit_section("è®¤çŸ¥ç§‘å­¦è§¦å‘å™¨æ£€æŸ¥")
        return False, "è®¤çŸ¥æŒ‡æ ‡æ­£å¸¸"
    
    def _detect_catastrophic_forgetting(self, performance_history: List[float]) -> bool:
        """æ£€æµ‹ç¾éš¾æ€§é—å¿˜"""
        if len(performance_history) < 8:
            return False
        
        # æ£€æŸ¥æœ€è¿‘æ€§èƒ½æ˜¯å¦æ˜¾è‘—ä¸‹é™
        recent_window = 5
        past_window = 5
        
        recent_avg = np.mean(performance_history[-recent_window:])
        past_avg = np.mean(performance_history[-(recent_window + past_window):-recent_window])
        
        if past_avg > 0:
            decline_ratio = (past_avg - recent_avg) / past_avg
            return decline_ratio > self.forgetting_threshold
        
        return False
    
    def _detect_learning_saturation(self, performance_history: List[float]) -> bool:
        """æ£€æµ‹å­¦ä¹ é¥±å’Œ"""
        if len(performance_history) < 10:
            return False
        
        # æ£€æŸ¥æœ€è¿‘10ä¸ªepochçš„æ”¹è¿›
        recent_performances = performance_history[-10:]
        improvements = [recent_performances[i] - recent_performances[i-1] 
                       for i in range(1, len(recent_performances))]
        
        avg_improvement = np.mean(improvements)
        return avg_improvement < 0.001  # æ”¹è¿›æå°
    
    def _detect_representation_conflict(self, activations: Dict[str, torch.Tensor]) -> bool:
        """æ£€æµ‹ç‰¹å¾è¡¨ç¤ºå†²çª"""
        if not activations:
            return False
        
        # ç®€åŒ–çš„å†²çªæ£€æµ‹ï¼šæ£€æŸ¥æ¿€æ´»æ¨¡å¼çš„ä¸€è‡´æ€§
        conflict_scores = []
        
        for name, activation in activations.items():
            if len(activation.shape) >= 2 and activation.shape[0] > 1:
                # è®¡ç®—æ‰¹æ¬¡å†…æ¿€æ´»æ¨¡å¼çš„ä¸€è‡´æ€§
                activation_flat = activation.view(activation.shape[0], -1)
                
                # è®¡ç®—æ ·æœ¬é—´çš„ç›¸å…³æ€§
                if activation_flat.shape[1] > 1:
                    try:
                        correlation_matrix = torch.corrcoef(activation_flat)
                        # å¯¹è§’çº¿å¤–çš„ç›¸å…³ç³»æ•°
                        mask = ~torch.eye(correlation_matrix.shape[0], dtype=torch.bool)
                        off_diagonal = correlation_matrix[mask]
                        
                        if len(off_diagonal) > 0:
                            consistency = torch.mean(torch.abs(off_diagonal))
                            conflict_scores.append(1.0 - consistency.item())
                    except:
                        continue
        
        if conflict_scores:
            avg_conflict = np.mean(conflict_scores)
            return avg_conflict > 0.7  # å†²çªé˜ˆå€¼
        
        return False

class EnhancedDNMFramework:
    """å¢å¼ºçš„åŠ¨æ€ç¥ç»å½¢æ€å‘ç”Ÿæ¡†æ¶
    
    ğŸ§¬ æ”¯æŒä¼ ç»Ÿå½¢æ€å‘ç”Ÿå’Œæ¿€è¿›å¤šç‚¹å˜å¼‚
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        # ç°æœ‰åˆå§‹åŒ–ä»£ç 
        default_config = {
            'trigger_interval': 8,
            'performance_monitoring_window': 10,
            'morphogenesis_budget': 5000,
            'enable_aggressive_mode': True,  # æ–°å¢ï¼šæ¿€è¿›æ¨¡å¼å¼€å…³
            'accuracy_plateau_threshold': 0.1,  # æ–°å¢ï¼šå‡†ç¡®ç‡åœæ»é˜ˆå€¼
            'plateau_detection_window': 5,  # æ–°å¢ï¼šåœæ»æ£€æµ‹çª—å£
            'aggressive_trigger_accuracy': 0.92,  # æ–°å¢ï¼šæ¿€è¿›æ¨¡å¼è§¦å‘å‡†ç¡®ç‡
            'max_concurrent_mutations': 3,  # æ–°å¢ï¼šæœ€å¤§å¹¶å‘å˜å¼‚æ•°
        }
        
        self.config = {**default_config, **(config or {})}
        
        # åŸæœ‰ç»„ä»¶
        self.triggers = {
            'information_theory': EnhancedInformationTheoryTrigger(),
            'biological_principle': EnhancedBiologicalPrinciplesTrigger(),
            'cognitive_science': EnhancedCognitiveScienceTrigger()
        }
        
        self.bottleneck_analyzer = AdvancedBottleneckAnalyzer()
        self.decision_maker = IntelligentMorphogenesisDecisionMaker()
        self.executor = AdvancedMorphogenesisExecutor()
        
        # æ–°å¢æ¿€è¿›å½¢æ€å‘ç”Ÿç»„ä»¶
        if self.config['enable_aggressive_mode']:
            from .aggressive_morphogenesis import (
                AggressiveMorphogenesisAnalyzer,
                MultiPointMutationPlanner,
                AggressiveMorphogenesisExecutor
            )
            from .net2net_subnetwork_analyzer import Net2NetSubnetworkAnalyzer
            
            self.aggressive_analyzer = AggressiveMorphogenesisAnalyzer(
                accuracy_plateau_threshold=self.config['accuracy_plateau_threshold'],
                plateau_window=self.config['plateau_detection_window']
            )
            self.mutation_planner = MultiPointMutationPlanner(
                max_concurrent_mutations=self.config['max_concurrent_mutations'],
                parameter_budget=self.config['morphogenesis_budget']
            )
            self.aggressive_executor = AggressiveMorphogenesisExecutor()
            self.net2net_analyzer = Net2NetSubnetworkAnalyzer()
        
        # è®°å½•å’Œç›‘æ§
        self.morphogenesis_events = []
        self.performance_history = []
        self.aggressive_mode_active = False

    def should_trigger_morphogenesis(self, 
                                   model: nn.Module,
                                   epoch: int,
                                   activations: Dict[str, torch.Tensor],
                                   gradients: Dict[str, torch.Tensor],
                                   performance_history: List[float]) -> Tuple[bool, List[str]]:
        """å¢å¼ºçš„å½¢æ€å‘ç”Ÿè§¦å‘æ£€æŸ¥ - æ”¯æŒæ¿€è¿›æ¨¡å¼"""
        logger.enter_section("å¢å¼ºå½¢æ€å‘ç”Ÿè§¦å‘æ£€æŸ¥")
        
        # æ£€æŸ¥å½“å‰å‡†ç¡®ç‡æ˜¯å¦è¾¾åˆ°æ¿€è¿›æ¨¡å¼é˜ˆå€¼
        current_accuracy = performance_history[-1] if performance_history else 0.0
        
        # æ¿€è¿›æ¨¡å¼æ¿€æ´»æ¡ä»¶
        aggressive_mode_triggered = False
        if (self.config['enable_aggressive_mode'] and 
            current_accuracy >= self.config['aggressive_trigger_accuracy']):
            
            # æ£€æµ‹å‡†ç¡®ç‡åœæ»
            is_plateau, stagnation_severity = self.aggressive_analyzer.detect_accuracy_plateau(performance_history)
            
            if is_plateau and stagnation_severity > 0.5:
                logger.warning(f"ğŸš¨ æ£€æµ‹åˆ°å‡†ç¡®ç‡åœæ»ï¼Œæ¿€æ´»æ¿€è¿›æ¨¡å¼ï¼åœæ»ä¸¥é‡ç¨‹åº¦: {stagnation_severity:.3f}")
                aggressive_mode_triggered = True
                self.aggressive_mode_active = True
        
        # å¦‚æœæ¿€è¿›æ¨¡å¼è¢«è§¦å‘ï¼Œä½¿ç”¨ä¸åŒçš„åˆ¤æ–­é€»è¾‘
        if aggressive_mode_triggered:
            logger.info("ğŸš€ ä½¿ç”¨æ¿€è¿›å½¢æ€å‘ç”Ÿç­–ç•¥")
            # æ¿€è¿›æ¨¡å¼ä¸‹æ›´é¢‘ç¹åœ°è§¦å‘ï¼Œä¸å—ä¼ ç»Ÿè§¦å‘é—´éš”é™åˆ¶
            trigger_reasons = [f"æ¿€è¿›æ¨¡å¼: å‡†ç¡®ç‡åœæ»(ä¸¥é‡ç¨‹åº¦={stagnation_severity:.3f})"]
            logger.exit_section("å¢å¼ºå½¢æ€å‘ç”Ÿè§¦å‘æ£€æŸ¥")
            return True, trigger_reasons
        
        # ä¼ ç»Ÿè§¦å‘é€»è¾‘
        logger.info(f"å½“å‰epoch: {epoch}, è§¦å‘é—´éš”: {self.config['trigger_interval']}")
        
        if epoch % self.config['trigger_interval'] != 0:
            logger.info(f"âŒ ä¸åœ¨è§¦å‘é—´éš”å†… ({epoch} % {self.config['trigger_interval']} != 0)")
            logger.exit_section("å¢å¼ºå½¢æ€å‘ç”Ÿè§¦å‘æ£€æŸ¥")
            return False, []
        
        logger.info("âœ… åœ¨è§¦å‘é—´éš”å†…ï¼Œæ£€æŸ¥å„è§¦å‘å™¨")
        
        # æ„å»ºåˆ†æä¸Šä¸‹æ–‡
        context = {
            'epoch': epoch,
            'activations': activations,
            'gradients': gradients,
            'performance_history': performance_history,
            'model': model
        }
        
        # æ£€æŸ¥å„ä¸ªè§¦å‘å™¨
        trigger_results = []
        trigger_reasons = []
        
        for name, trigger in self.triggers.items():
            try:
                logger.debug(f"æ£€æŸ¥è§¦å‘å™¨: {name}")
                should_trigger, reason = trigger.should_trigger(context)
                trigger_results.append(should_trigger)
                
                logger.info(f"è§¦å‘å™¨[{name}]: {'âœ…æ¿€æ´»' if should_trigger else 'âŒæœªæ¿€æ´»'} - {reason}")
                
                if should_trigger:
                    trigger_reasons.append(f"{name}: {reason}")
                    
            except Exception as e:
                logger.error(f"âŒ è§¦å‘å™¨ {name} æ‰§è¡Œå¤±è´¥: {e}")
                logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                trigger_results.append(False)
        
        should_trigger = any(trigger_results)
        
        logger.info(f"è§¦å‘å™¨æ±‡æ€»: {len([r for r in trigger_results if r])}/{len(trigger_results)} æ¿€æ´»")
        logger.info(f"æœ€ç»ˆå†³å®š: {'âœ…è§¦å‘å½¢æ€å‘ç”Ÿ' if should_trigger else 'âŒä¸è§¦å‘'}")
        
        logger.exit_section("å¢å¼ºå½¢æ€å‘ç”Ÿè§¦å‘æ£€æŸ¥")
        return should_trigger, trigger_reasons

    def execute_morphogenesis(self,
                            model: nn.Module,
                            activations_or_context,  # å…¼å®¹è€æ¥å£ï¼šå¯ä»¥æ˜¯context dictæˆ–activations dict
                            gradients: Optional[Dict[str, torch.Tensor]] = None,
                            performance_history: Optional[List[float]] = None,
                            epoch: Optional[int] = None,
                            targets: Optional[torch.Tensor] = None) -> Dict[str, Any]:
        """æ‰§è¡Œå½¢æ€å‘ç”Ÿ - æ”¯æŒä¼ ç»Ÿå’Œæ¿€è¿›æ¨¡å¼"""
        logger.enter_section("å¢å¼ºå½¢æ€å‘ç”Ÿæ‰§è¡Œ")
        logger.log_model_info(model, "è¾“å…¥æ¨¡å‹")
        
        # å…¼å®¹æ€§å¤„ç†ï¼šæ”¯æŒè€çš„contextæ¥å£å’Œæ–°çš„å‚æ•°æ¥å£
        if isinstance(activations_or_context, dict) and gradients is None:
            # è€æ¥å£ï¼šä¼ å…¥çš„æ˜¯contextå­—å…¸
            context = activations_or_context
            activations = context.get('activations', {})
            gradients = context.get('gradients', {})
            performance_history = context.get('performance_history', [])
            epoch = context.get('epoch', 0)
            targets = context.get('targets')
        else:
            # æ–°æ¥å£ï¼šç›´æ¥ä¼ å…¥å‚æ•°
            activations = activations_or_context
            if gradients is None or performance_history is None or epoch is None:
                logger.error("æ–°æ¥å£éœ€è¦æä¾›æ‰€æœ‰å¿…éœ€å‚æ•°ï¼šgradients, performance_history, epoch")
                return {
                    'model_modified': False,
                    'new_model': model,
                    'parameters_added': 0,
                    'morphogenesis_events': [],
                    'morphogenesis_type': 'error',
                    'trigger_reasons': [],
                    'error': 'missing_parameters'
                }
        
        try:
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³è§¦å‘æ¡ä»¶
            should_trigger, trigger_reasons = self.should_trigger_morphogenesis(
                model, epoch, activations, gradients, performance_history
            )
            
            if not should_trigger:
                logger.info("âŒ æœªæ»¡è¶³è§¦å‘æ¡ä»¶ï¼Œè·³è¿‡å½¢æ€å‘ç”Ÿ")
                logger.exit_section("å¢å¼ºå½¢æ€å‘ç”Ÿæ‰§è¡Œ")
                return {
                    'model_modified': False,
                    'new_model': model,
                    'parameters_added': 0,
                    'morphogenesis_events': [],
                    'morphogenesis_type': 'none',
                    'trigger_reasons': []
                }
            
            logger.success(f"æ»¡è¶³è§¦å‘æ¡ä»¶ï¼ŒåŸå› : {trigger_reasons}")
            
            # æ¿€è¿›æ¨¡å¼è·¯å¾„
            if self.aggressive_mode_active and self.config['enable_aggressive_mode']:
                return self._execute_aggressive_morphogenesis(
                    model, activations, gradients, targets, performance_history, epoch, trigger_reasons
                )
            
            # ä¼ ç»Ÿå½¢æ€å‘ç”Ÿè·¯å¾„
            return self._execute_traditional_morphogenesis(
                model, activations, gradients, performance_history, epoch, trigger_reasons
            )
            
        except Exception as e:
            logger.error(f"âŒ å½¢æ€å‘ç”Ÿæ‰§è¡Œå¤±è´¥: {e}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return {
                'model_modified': False,
                'new_model': model,
                'parameters_added': 0,
                'morphogenesis_events': [],
                'morphogenesis_type': 'error',
                'trigger_reasons': trigger_reasons,
                'error': str(e)
            }
        finally:
            logger.exit_section("å¢å¼ºå½¢æ€å‘ç”Ÿæ‰§è¡Œ")

    def _execute_aggressive_morphogenesis(self,
                                        model: nn.Module,
                                        activations: Dict[str, torch.Tensor],
                                        gradients: Dict[str, torch.Tensor],
                                        targets: Optional[torch.Tensor],
                                        performance_history: List[float],
                                        epoch: int,
                                        trigger_reasons: List[str]) -> Dict[str, Any]:
        """æ‰§è¡Œæ¿€è¿›å¤šç‚¹å½¢æ€å‘ç”Ÿ"""
        logger.enter_section("æ¿€è¿›å¤šç‚¹å½¢æ€å‘ç”Ÿ")
        
        try:
            # åå‘æ¢¯åº¦æŠ•å½±åˆ†æ
            if targets is None:
                # å¦‚æœæ²¡æœ‰æä¾›çœŸå®targetsï¼Œä½¿ç”¨æ¨¡æ‹Ÿtargets
                logger.warning("æœªæä¾›çœŸå®targetsï¼Œä½¿ç”¨æ¨¡æ‹Ÿtargetsè¿›è¡Œåˆ†æ")
                output_targets = torch.randint(0, 10, (128,))
            else:
                output_targets = targets
            
            bottleneck_signatures = self.aggressive_analyzer.analyze_reverse_gradient_projection(
                activations, gradients, output_targets
            )
            
            if not bottleneck_signatures:
                logger.warning("âŒ æœªæ£€æµ‹åˆ°ç“¶é¢ˆç­¾åï¼Œå›é€€åˆ°ä¼ ç»Ÿå½¢æ€å‘ç”Ÿ")
                return self._execute_traditional_morphogenesis(
                    model, activations, gradients, performance_history, epoch, trigger_reasons
                )
            
            # ä½¿ç”¨Net2Netåˆ†æå™¨è¿›ä¸€æ­¥åˆ†ææ¯ä¸ªç“¶é¢ˆå±‚çš„å˜å¼‚æ½œåŠ›
            logger.enter_section("Net2Netå­ç½‘ç»œæ½œåŠ›åˆ†æ")
            net2net_analyses = {}
            current_accuracy = performance_history[-1] if performance_history else 0.0
            
            for layer_name, signature in bottleneck_signatures.items():
                if signature.severity > 0.3:  # åªåˆ†æä¸¥é‡ç“¶é¢ˆ
                    try:
                        net2net_analysis = self.net2net_analyzer.analyze_layer_mutation_potential(
                            model, layer_name, activations, gradients, output_targets, current_accuracy
                        )
                        net2net_analyses[layer_name] = net2net_analysis
                        
                        # è®°å½•Net2Netåˆ†æç»“æœ
                        recommendation = net2net_analysis.get('recommendation', {})
                        logger.info(f"å±‚{layer_name}: {recommendation.get('action', 'unknown')} "
                                  f"(æ½œåŠ›={net2net_analysis.get('mutation_prediction', {}).get('improvement_potential', 0):.3f})")
                        
                    except Exception as e:
                        logger.warning(f"å±‚{layer_name}çš„Net2Netåˆ†æå¤±è´¥: {e}")
            
            logger.info(f"å®Œæˆ{len(net2net_analyses)}ä¸ªå±‚çš„Net2Netåˆ†æ")
            logger.exit_section("Net2Netå­ç½‘ç»œæ½œåŠ›åˆ†æ")
            
            # æ£€æµ‹åœæ»ä¸¥é‡ç¨‹åº¦
            _, stagnation_severity = self.aggressive_analyzer.detect_accuracy_plateau(performance_history)
            
            # åŸºäºNet2Netåˆ†æç»“æœæ”¹è¿›å˜å¼‚è§„åˆ’
            enhanced_bottleneck_signatures = self._enhance_bottleneck_signatures_with_net2net(
                bottleneck_signatures, net2net_analyses
            )
            
            # è§„åˆ’å¤šç‚¹å˜å¼‚
            mutations = self.mutation_planner.plan_aggressive_mutations(
                enhanced_bottleneck_signatures, performance_history, stagnation_severity
            )
            
            if not mutations:
                logger.warning("âŒ æœªç”Ÿæˆæœ‰æ•ˆçš„å˜å¼‚è®¡åˆ’ï¼Œå›é€€åˆ°ä¼ ç»Ÿå½¢æ€å‘ç”Ÿ")
                return self._execute_traditional_morphogenesis(
                    model, activations, gradients, performance_history, epoch, trigger_reasons
                )
            
            # æ‰§è¡Œæœ€ä½³å˜å¼‚ç­–ç•¥
            best_mutation = max(mutations, key=lambda m: m.expected_improvement - m.risk_assessment * 0.5)
            logger.info(f"é€‰æ‹©æœ€ä½³å˜å¼‚ç­–ç•¥: {best_mutation.coordination_strategy}, "
                       f"ç›®æ ‡ä½ç½®æ•°: {len(best_mutation.target_locations)}, "
                       f"æœŸæœ›æ”¹è¿›: {best_mutation.expected_improvement:.3f}")
            
            new_model, params_added, execution_result = self.aggressive_executor.execute_multi_point_mutation(
                model, best_mutation
            )
            
            # è®°å½•æ¿€è¿›å½¢æ€å‘ç”Ÿäº‹ä»¶
            morphogenesis_event = EnhancedMorphogenesisEvent(
                epoch=epoch,
                event_type='aggressive_multi_point',
                location=f"å¤šç‚¹({len(best_mutation.target_locations)}ä½ç½®)",
                trigger_reason='; '.join(trigger_reasons),
                performance_before=performance_history[-1] if performance_history else 0.0,
                parameters_added=params_added,
                morphogenesis_type=MorphogenesisType.HYBRID_DIVISION,  # ä»£è¡¨å¤šç‚¹å˜å¼‚
                confidence=1.0 - best_mutation.risk_assessment,
                expected_improvement=best_mutation.expected_improvement
            )
            
            self.morphogenesis_events.append(morphogenesis_event)
            
            logger.success(f"æ¿€è¿›å¤šç‚¹å½¢æ€å‘ç”Ÿå®Œæˆ: ç­–ç•¥={best_mutation.coordination_strategy}, "
                         f"æˆåŠŸå˜å¼‚={execution_result.get('successful_mutations', 0)}/"
                         f"{execution_result.get('total_mutations', 0)}, "
                         f"æ–°å¢å‚æ•°: {params_added:,}")
            
            # é‡ç½®æ¿€è¿›æ¨¡å¼çŠ¶æ€ï¼ˆç»™æ¨¡å‹å‡ ä¸ªepoché€‚åº”ï¼‰
            self.aggressive_mode_active = False
            
            return {
                'model_modified': params_added > 0,
                'new_model': new_model,
                'parameters_added': params_added,
                'morphogenesis_events': [morphogenesis_event],
                'morphogenesis_type': 'aggressive_multi_point',
                'trigger_reasons': trigger_reasons,
                'aggressive_details': {
                    'mutation_strategy': best_mutation.coordination_strategy,
                    'target_locations': best_mutation.target_locations,
                    'bottleneck_count': len(bottleneck_signatures),
                    'stagnation_severity': stagnation_severity,
                    'execution_result': execution_result,
                    'net2net_analyses': net2net_analyses  # åŒ…å«Net2Netåˆ†æç»“æœ
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ æ¿€è¿›å½¢æ€å‘ç”Ÿå¤±è´¥: {e}")
            logger.warning("å›é€€åˆ°ä¼ ç»Ÿå½¢æ€å‘ç”Ÿ")
            return self._execute_traditional_morphogenesis(
                model, activations, gradients, performance_history, epoch, trigger_reasons
            )
        finally:
            logger.exit_section("æ¿€è¿›å¤šç‚¹å½¢æ€å‘ç”Ÿ")

    def _execute_traditional_morphogenesis(self,
                                         model: nn.Module,
                                         activations: Dict[str, torch.Tensor],
                                         gradients: Dict[str, torch.Tensor],
                                         performance_history: List[float],
                                         epoch: int,
                                         trigger_reasons: List[str]) -> Dict[str, Any]:
        """æ‰§è¡Œä¼ ç»Ÿå•ç‚¹å½¢æ€å‘ç”Ÿ"""
        logger.enter_section("ä¼ ç»Ÿå½¢æ€å‘ç”Ÿ")
        
        try:
            # åŸæœ‰çš„ä¼ ç»Ÿå½¢æ€å‘ç”Ÿé€»è¾‘
            logger.info("æ‰§è¡Œä¼ ç»Ÿå•ç‚¹å½¢æ€å‘ç”Ÿç­–ç•¥")
            
            # ç“¶é¢ˆåˆ†æ
            logger.enter_section("ç“¶é¢ˆåˆ†æ")
            
            if not activations or not gradients:
                logger.error("âŒ ç¼ºå°‘æ¿€æ´»å€¼æˆ–æ¢¯åº¦ä¿¡æ¯ï¼Œè·³è¿‡å½¢æ€å‘ç”Ÿ")
                logger.exit_section("ç“¶é¢ˆåˆ†æ")
                logger.exit_section("ä¼ ç»Ÿå½¢æ€å‘ç”Ÿ")
                return {
                    'model_modified': False,
                    'new_model': model,
                    'parameters_added': 0,
                    'morphogenesis_events': [],
                    'morphogenesis_type': 'failed',
                    'trigger_reasons': trigger_reasons,
                    'error': 'missing_analysis_data'
                }
            
            logger.info(f"åˆ†ææ•°æ®: æ¿€æ´»å€¼{len(activations)}å±‚, æ¢¯åº¦{len(gradients)}å±‚")
            bottleneck_analysis = self.bottleneck_analyzer.analyze_network_bottlenecks(model, activations, gradients)
            
            logger.success(f"ç“¶é¢ˆåˆ†æå®Œæˆ: {len(bottleneck_analysis) if bottleneck_analysis else 0}ä¸ªç“¶é¢ˆ")
            logger.exit_section("ç“¶é¢ˆåˆ†æ")
            
            # å½¢æ€å‘ç”Ÿå†³ç­–
            logger.enter_section("å½¢æ€å‘ç”Ÿå†³ç­–")
            logger.info(f"æ€§èƒ½å†å²: {len(performance_history)}ä¸ªæ•°æ®ç‚¹")
            
            decision = self.decision_maker.make_decision(bottleneck_analysis, performance_history)
            if not decision:
                logger.warning("âŒ æœªå‘ç°éœ€è¦å½¢æ€å‘ç”Ÿçš„ç“¶é¢ˆ")
                logger.exit_section("å½¢æ€å‘ç”Ÿå†³ç­–")
                logger.exit_section("ä¼ ç»Ÿå½¢æ€å‘ç”Ÿ")
                return {
                    'model_modified': False,
                    'new_model': model,
                    'parameters_added': 0,
                    'morphogenesis_events': [],
                    'morphogenesis_type': 'none',
                    'trigger_reasons': trigger_reasons
                }
            
            logger.success(f"å†³ç­–åˆ¶å®šå®Œæˆ: {decision.morphogenesis_type.value} (ç½®ä¿¡åº¦: {decision.confidence:.3f})")
            logger.exit_section("å½¢æ€å‘ç”Ÿå†³ç­–")
            
            # å½¢æ€å‘ç”Ÿæ‰§è¡Œ
            logger.enter_section("å½¢æ€å‘ç”Ÿæ‰§è¡Œ")
            logger.info(f"æ‰§è¡Œç­–ç•¥: {decision.morphogenesis_type.value} åœ¨ {decision.target_location}")
            
            new_model, parameters_added = self.executor.execute_morphogenesis(model, decision)
            
            logger.info(f"å½¢æ€å‘ç”Ÿç»“æœ: æ–°å¢å‚æ•°={parameters_added}")
            logger.log_model_info(new_model, "æ–°æ¨¡å‹")
            logger.exit_section("å½¢æ€å‘ç”Ÿæ‰§è¡Œ")
            
            if parameters_added > 0:
                logger.success("âœ… å½¢æ€å‘ç”ŸæˆåŠŸï¼Œè®°å½•äº‹ä»¶")
                
                # è®°å½•å½¢æ€å‘ç”Ÿäº‹ä»¶
                morphogenesis_event = EnhancedMorphogenesisEvent(
                    epoch=epoch,
                    event_type=decision.morphogenesis_type.value,
                    location=decision.target_location,
                    trigger_reason='; '.join(trigger_reasons),
                    performance_before=performance_history[-1] if performance_history else 0.0,
                    parameters_added=parameters_added,
                    morphogenesis_type=decision.morphogenesis_type,
                    confidence=decision.confidence,
                    expected_improvement=decision.expected_improvement
                )
                
                self.morphogenesis_events.append(morphogenesis_event)
                
                logger.success(f"ä¼ ç»Ÿå½¢æ€å‘ç”Ÿå®Œæˆ: {decision.morphogenesis_type.value}, æ–°å¢å‚æ•°: {parameters_added:,}")
                
                return {
                    'model_modified': True,
                    'new_model': new_model,
                    'parameters_added': parameters_added,
                    'morphogenesis_events': [morphogenesis_event],
                    'morphogenesis_type': decision.morphogenesis_type.value,
                    'trigger_reasons': trigger_reasons
                }
            else:
                logger.warning("âŒ å½¢æ€å‘ç”Ÿæœªæ·»åŠ ä»»ä½•å‚æ•°")
                return {
                    'model_modified': False,
                    'new_model': model,
                    'parameters_added': 0,
                    'morphogenesis_events': [],
                    'morphogenesis_type': 'failed',
                    'trigger_reasons': trigger_reasons
                }
                
        except Exception as e:
            logger.error(f"âŒ ä¼ ç»Ÿå½¢æ€å‘ç”Ÿå¤±è´¥: {e}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return {
                'model_modified': False,
                'new_model': model,
                'parameters_added': 0,
                'morphogenesis_events': [],
                'morphogenesis_type': 'error',
                'trigger_reasons': trigger_reasons,
                'error': str(e)
            }
        finally:
            logger.exit_section("ä¼ ç»Ÿå½¢æ€å‘ç”Ÿ")
    
    def _enhance_bottleneck_signatures_with_net2net(self, 
                                                   bottleneck_signatures: Dict,
                                                   net2net_analyses: Dict) -> Dict:
        """ä½¿ç”¨Net2Netåˆ†æç»“æœå¢å¼ºç“¶é¢ˆç­¾å"""
        
        enhanced_signatures = copy.deepcopy(bottleneck_signatures)
        
        for layer_name, signature in enhanced_signatures.items():
            if layer_name in net2net_analyses:
                net2net_analysis = net2net_analyses[layer_name]
                
                # è·å–å˜å¼‚é¢„æµ‹ä¿¡æ¯
                mutation_prediction = net2net_analysis.get('mutation_prediction', {})
                improvement_potential = mutation_prediction.get('improvement_potential', 0.0)
                risk_assessment = mutation_prediction.get('risk_assessment', {})
                
                # æ ¹æ®Net2Netåˆ†æè°ƒæ•´ç“¶é¢ˆä¸¥é‡ç¨‹åº¦
                original_severity = signature.severity
                net2net_adjustment = improvement_potential * 0.5  # Net2Netæ”¹è¿›æ½œåŠ›çš„æƒé‡
                
                # ç»¼åˆä¸¥é‡ç¨‹åº¦ = åŸå§‹ä¸¥é‡ç¨‹åº¦ + Net2Netæ”¹è¿›æ½œåŠ› - é£é™©è°ƒæ•´
                adjusted_severity = original_severity + net2net_adjustment - risk_assessment.get('overall_risk', 0) * 0.2
                signature.severity = max(0.0, min(1.0, adjusted_severity))
                
                # æ·»åŠ Net2Netç‰¹å®šä¿¡æ¯
                signature.net2net_improvement_potential = improvement_potential
                signature.net2net_risk = risk_assessment.get('overall_risk', 0.0)
                signature.net2net_recommended_strategy = net2net_analysis.get('recommendation', {}).get('recommended_strategy')
                
                logger.debug(f"å±‚{layer_name}: åŸå§‹ä¸¥é‡ç¨‹åº¦={original_severity:.3f} -> "
                           f"è°ƒæ•´åä¸¥é‡ç¨‹åº¦={signature.severity:.3f} "
                           f"(Net2Netæ½œåŠ›={improvement_potential:.3f})")
        
        return enhanced_signatures

    def update_performance_history(self, performance: float):
        """æ›´æ–°æ€§èƒ½å†å²"""
        self.performance_history.append(performance)
        
        # ä¿æŒå†å²é•¿åº¦
        if len(self.performance_history) > 100:
            self.performance_history = self.performance_history[-100:]
    
    def cache_activations_and_gradients(self, activations: Dict[str, torch.Tensor], 
                                       gradients: Dict[str, torch.Tensor]):
        """ç¼“å­˜æ¿€æ´»å€¼å’Œæ¢¯åº¦"""
        self.activation_cache = activations
        self.gradient_cache = gradients
    
    def get_morphogenesis_summary(self) -> Dict[str, Any]:
        """è·å–å½¢æ€å‘ç”Ÿæ€»ç»“"""
        if not self.morphogenesis_events:
            return {
                'total_events': 0,
                'total_parameters_added': 0,
                'morphogenesis_types': {},
                'events': []
            }
        
        # ç»Ÿè®¡å„ç§ç±»å‹çš„å½¢æ€å‘ç”Ÿ
        type_counts = defaultdict(int)
        for event in self.morphogenesis_events:
            type_counts[event.morphogenesis_type.value] += 1
        
        total_params = sum(event.parameters_added for event in self.morphogenesis_events)
        
        return {
            'total_events': len(self.morphogenesis_events),
            'total_parameters_added': total_params,
            'morphogenesis_types': dict(type_counts),
            'events': [
                {
                    'epoch': event.epoch,
                    'type': event.morphogenesis_type.value,
                    'location': event.location,
                    'parameters_added': event.parameters_added,
                    'confidence': event.confidence,
                    'expected_improvement': event.expected_improvement,
                    'reasoning': event.trigger_reason
                }
                for event in self.morphogenesis_events
            ]
        }