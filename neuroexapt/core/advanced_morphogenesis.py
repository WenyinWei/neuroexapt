#!/usr/bin/env python3
"""
Advanced Morphogenesis Module - é«˜çº§å½¢æ€å‘ç”Ÿæ¨¡å—

ğŸ§¬ å®ç°æ›´å¤æ‚çš„ç»“æ„å˜å¼‚ç­–ç•¥ï¼š
1. ä¸²è¡Œåˆ†è£‚ (Serial Division) - å¢åŠ ç½‘ç»œæ·±åº¦
2. å¹¶è¡Œåˆ†è£‚ (Parallel Division) - åˆ›å»ºå¤šåˆ†æ”¯ç»“æ„  
3. æ··åˆåˆ†è£‚ (Hybrid Division) - ç»„åˆä¸åŒç±»å‹çš„å±‚
4. è·³è·ƒè¿æ¥ (Skip Connections) - å¢å¼ºä¿¡æ¯æµ
5. æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanisms) - æå‡ç‰¹å¾é€‰æ‹©èƒ½åŠ›

ğŸ¯ ç›®æ ‡ï¼šçªç ´ä¼ ç»Ÿæ¶æ„é™åˆ¶ï¼Œæ¢ç´¢æ›´é«˜æ€§èƒ½çš„ç½‘ç»œæ‹“æ‰‘
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass
from enum import Enum
import copy
import math
import time
import traceback
from collections import defaultdict

# é…ç½®è¯¦ç»†è°ƒè¯•æ—¥å¿—
logger = logging.getLogger(__name__)

class DebugPrinter:
    """è°ƒè¯•è¾“å‡ºç®¡ç†å™¨ - é«˜çº§å½¢æ€å‘ç”Ÿæ¨¡å—ä¸“ç”¨"""
    
    def __init__(self, enabled: bool = True):
        self.enabled = enabled
        self.indent_level = 0
        
    def print_debug(self, message: str, level: str = "INFO"):
        """æ‰“å°è°ƒè¯•ä¿¡æ¯"""
        if not self.enabled:
            return
            
        indent = "  " * self.indent_level
        timestamp = time.strftime("%H:%M:%S", time.localtime())
        
        # é¢œè‰²ç¼–ç 
        colors = {
            "INFO": "\033[36m",      # é’è‰²
            "SUCCESS": "\033[32m",   # ç»¿è‰² 
            "WARNING": "\033[33m",   # é»„è‰²
            "ERROR": "\033[31m",     # çº¢è‰²
            "DEBUG": "\033[35m",     # ç´«è‰²
        }
        color = colors.get(level, "\033[0m")
        reset = "\033[0m"
        
        print(f"{color}[{timestamp}] {indent}{level}: {message}{reset}")
        
    def enter_section(self, section_name: str):
        """è¿›å…¥æ–°çš„è°ƒè¯•åŒºåŸŸ"""
        self.print_debug(f"ğŸ” è¿›å…¥ {section_name}", "DEBUG")
        self.indent_level += 1
        
    def exit_section(self, section_name: str):
        """é€€å‡ºè°ƒè¯•åŒºåŸŸ"""
        self.indent_level = max(0, self.indent_level - 1)
        self.print_debug(f"âœ… å®Œæˆ {section_name}", "DEBUG")

# å…¨å±€è°ƒè¯•å™¨
morpho_debug = DebugPrinter(enabled=True)

class MorphogenesisType(Enum):
    """å½¢æ€å‘ç”Ÿç±»å‹æšä¸¾"""
    WIDTH_EXPANSION = "width_expansion"      # å®½åº¦æ‰©å±•
    SERIAL_DIVISION = "serial_division"      # ä¸²è¡Œåˆ†è£‚
    PARALLEL_DIVISION = "parallel_division"  # å¹¶è¡Œåˆ†è£‚
    HYBRID_DIVISION = "hybrid_division"      # æ··åˆåˆ†è£‚
    SKIP_CONNECTION = "skip_connection"      # è·³è·ƒè¿æ¥
    ATTENTION_INJECTION = "attention_injection"  # æ³¨æ„åŠ›æ³¨å…¥

@dataclass
class MorphogenesisDecision:
    """å½¢æ€å‘ç”Ÿå†³ç­–"""
    morphogenesis_type: MorphogenesisType
    target_location: str
    confidence: float
    expected_improvement: float
    complexity_cost: float
    parameters_added: int
    reasoning: str

class AdvancedBottleneckAnalyzer:
    """é«˜çº§ç“¶é¢ˆåˆ†æå™¨"""
    
    def __init__(self):
        self.analysis_history = []
        
    def analyze_network_bottlenecks(self, model: nn.Module, activations: Dict[str, torch.Tensor], 
                                  gradients: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """æ·±åº¦åˆ†æç½‘ç»œç“¶é¢ˆ"""
        morpho_debug.enter_section("ç½‘ç»œç“¶é¢ˆåˆ†æ")
        morpho_debug.print_debug(f"åˆ†æè¾“å…¥: æ¨¡å‹å±‚æ•°={len(list(model.named_modules()))}, æ¿€æ´»å€¼å±‚æ•°={len(activations)}, æ¢¯åº¦å±‚æ•°={len(gradients)}", "INFO")
        
        analysis = {}
        
        # åˆ†åˆ«åˆ†æå„ç±»ç“¶é¢ˆ
        morpho_debug.print_debug("1/5 åˆ†ææ·±åº¦ç“¶é¢ˆ", "DEBUG")
        analysis['depth_bottlenecks'] = self._analyze_depth_bottlenecks(activations, gradients)
        
        morpho_debug.print_debug("2/5 åˆ†æå®½åº¦ç“¶é¢ˆ", "DEBUG")
        analysis['width_bottlenecks'] = self._analyze_width_bottlenecks(activations, gradients)
        
        morpho_debug.print_debug("3/5 åˆ†æä¿¡æ¯æµç“¶é¢ˆ", "DEBUG")
        analysis['information_flow_bottlenecks'] = self._analyze_information_flow(activations)
        
        morpho_debug.print_debug("4/5 åˆ†ææ¢¯åº¦æµç“¶é¢ˆ", "DEBUG")
        analysis['gradient_flow_bottlenecks'] = self._analyze_gradient_flow(gradients)
        
        morpho_debug.print_debug("5/5 åˆ†æå®¹é‡ç“¶é¢ˆ", "DEBUG")
        analysis['capacity_bottlenecks'] = self._analyze_capacity_bottlenecks(model, activations)
        
        # è¾“å‡ºç“¶é¢ˆæ±‡æ€»
        morpho_debug.print_debug("ç“¶é¢ˆåˆ†ææ±‡æ€»:", "INFO")
        for bottleneck_type, results in analysis.items():
            if isinstance(results, dict):
                count = len([k for k, v in results.items() if v > 0.5])  # å‡è®¾0.5ä¸ºé«˜ç“¶é¢ˆé˜ˆå€¼
                morpho_debug.print_debug(f"  {bottleneck_type}: {count}ä¸ªé«˜ç“¶é¢ˆä½ç½®", "DEBUG")
        
        self.analysis_history.append(analysis)
        morpho_debug.exit_section("ç½‘ç»œç“¶é¢ˆåˆ†æ")
        return analysis
    
    def _analyze_depth_bottlenecks(self, 
                                   activations: Dict[str, torch.Tensor], 
                                   gradients: Dict[str, torch.Tensor],
                                   perform_gc: bool = False,
                                   memory_threshold_mb: Optional[int] = None) -> Dict[str, float]:
        """åˆ†ææ·±åº¦ç“¶é¢ˆ - éœ€è¦å¢åŠ å±‚æ•°çš„ä½ç½®
        
        Args:
            activations: å±‚ååˆ°æ¿€æ´»çš„æ˜ å°„
            gradients: å±‚ååˆ°æ¢¯åº¦çš„æ˜ å°„
            perform_gc: æ˜¯å¦åœ¨æ¯å±‚åæ‰§è¡Œåƒåœ¾å›æ”¶å’ŒCUDAç¼“å­˜æ¸…ç†
            memory_threshold_mb: ä»…å½“å†…å­˜ä½¿ç”¨è¶…è¿‡æ­¤é˜ˆå€¼ï¼ˆMBï¼‰æ—¶æ‰æ‰§è¡Œæ¸…ç†
        """
        import gc
        
        depth_scores = {}
        
        layer_names = list(activations.keys())
        for i, layer_name in enumerate(layer_names):
            if layer_name not in gradients:
                continue
                
            activation = activations[layer_name]
            gradient = gradients[layer_name]
            
            # 1. æ¿€æ´»é¥±å’Œåº¦åˆ†æ
            saturation_score = self._compute_activation_saturation(activation)
            
            # 2. æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸åˆ†æ
            gradient_health = self._compute_gradient_health(gradient)
            
            # 3. å±‚é—´ä¿¡æ¯æŸå¤±åˆ†æ
            if i > 0:
                prev_layer = layer_names[i-1]
                if prev_layer in activations:
                    info_loss = self._compute_information_loss(
                        activations[prev_layer], activation
                    )
                else:
                    info_loss = 0.0
            else:
                info_loss = 0.0
            
            # 4. è®¡ç®—æ·±åº¦ç“¶é¢ˆåˆ†æ•°
            depth_score = (
                0.4 * saturation_score +
                0.3 * (1.0 - gradient_health) +
                0.3 * info_loss
            )
            
            depth_scores[layer_name] = depth_score
            
            # å¯é€‰çš„åƒåœ¾å›æ”¶å’ŒCUDAç¼“å­˜æ¸…ç†
            do_cleanup = False
            if perform_gc:
                if memory_threshold_mb is not None:
                    # ä»…å½“å†…å­˜ä½¿ç”¨è¶…è¿‡é˜ˆå€¼æ—¶æ‰æ¸…ç†
                    if torch.cuda.is_available():
                        mem_mb = torch.cuda.memory_allocated() / 1024 / 1024
                        if mem_mb > memory_threshold_mb:
                            do_cleanup = True
                    else:
                        try:
                            import psutil
                            mem_mb = psutil.Process().memory_info().rss / 1024 / 1024
                            if mem_mb > memory_threshold_mb:
                                do_cleanup = True
                        except ImportError:
                            # psutil not available, fallback to always clean if requested
                            do_cleanup = True
                else:
                    do_cleanup = True
                    
            if do_cleanup:
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
        return depth_scores
    
    def _analyze_width_bottlenecks(self, activations: Dict[str, torch.Tensor], 
                                 gradients: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """åˆ†æå®½åº¦ç“¶é¢ˆ - éœ€è¦å¢åŠ ç¥ç»å…ƒæ•°é‡çš„ä½ç½®"""
        width_scores = {}
        
        for layer_name, activation in activations.items():
            if layer_name not in gradients:
                continue
                
            gradient = gradients[layer_name]
            
            # 1. ç¥ç»å…ƒåˆ©ç”¨ç‡åˆ†æ
            utilization = self._compute_neuron_utilization(activation)
            
            # 2. æ¢¯åº¦æ–¹å·®åˆ†æ
            gradient_variance = self._compute_gradient_variance(gradient)
            
            # 3. æ¿€æ´»æ¨¡å¼å¤šæ ·æ€§
            activation_diversity = self._compute_activation_diversity(activation)
            
            # 4. è®¡ç®—å®½åº¦ç“¶é¢ˆåˆ†æ•°
            width_score = (
                0.4 * (1.0 - utilization) +
                0.3 * gradient_variance +
                0.3 * (1.0 - activation_diversity)
            )
            
            width_scores[layer_name] = width_score
            
        return width_scores
    
    def _analyze_information_flow(self, activations: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """åˆ†æä¿¡æ¯æµç“¶é¢ˆ - éœ€è¦å¹¶è¡Œåˆ†æ”¯çš„ä½ç½®"""
        morpho_debug.enter_section("ä¿¡æ¯æµç“¶é¢ˆåˆ†æ")
        flow_scores = {}
        layer_names = list(activations.keys())
        
        morpho_debug.print_debug(f"åˆ†æ{len(layer_names)}å±‚çš„ä¿¡æ¯æµ", "INFO")
        
        for i, layer_name in enumerate(layer_names):
            morpho_debug.print_debug(f"åˆ†æå±‚ {i+1}/{len(layer_names)}: {layer_name}", "DEBUG")
            activation = activations[layer_name]
            
            # å†…å­˜æ£€æŸ¥
            if activation.numel() > 10**7:  # è¶…è¿‡1000ä¸‡å…ƒç´ 
                morpho_debug.print_debug(f"âš ï¸ å¤§å¼ é‡æ£€æµ‹: {activation.shape}, å…ƒç´ æ•°={activation.numel():,}", "WARNING")
            
            try:
                # 1. ä¿¡æ¯ç“¶é¢ˆåˆ†æ
                morpho_debug.print_debug(f"è®¡ç®—ç†µå€¼...", "DEBUG")
                entropy = self._compute_entropy(activation)
                
                # 2. ç‰¹å¾ç›¸å…³æ€§åˆ†æ
                morpho_debug.print_debug(f"è®¡ç®—ç‰¹å¾ç›¸å…³æ€§...", "DEBUG")
                feature_correlation = self._compute_feature_correlation(activation)
                
                # 3. ä¿¡æ¯å†—ä½™åˆ†æ
                morpho_debug.print_debug(f"è®¡ç®—ä¿¡æ¯å†—ä½™åº¦...", "DEBUG")
                redundancy = self._compute_information_redundancy(activation)
                
                # 4. è®¡ç®—ä¿¡æ¯æµç“¶é¢ˆåˆ†æ•°
                flow_score = (
                    0.3 * (1.0 - entropy) +
                    0.4 * feature_correlation +
                    0.3 * redundancy
                )
                
                flow_scores[layer_name] = flow_score
                morpho_debug.print_debug(f"å±‚{layer_name}: ç†µ={entropy:.3f}, ç›¸å…³æ€§={feature_correlation:.3f}, å†—ä½™={redundancy:.3f}, åˆ†æ•°={flow_score:.3f}", "DEBUG")
                
            except Exception as e:
                morpho_debug.print_debug(f"âŒ å±‚{layer_name}åˆ†æå¤±è´¥: {e}", "ERROR")
                flow_scores[layer_name] = 0.0
                
            # å¯é…ç½®çš„åƒåœ¾å›æ”¶ï¼Œä»…åœ¨éœ€è¦æ—¶æ‰§è¡Œä»¥é¿å…æ€§èƒ½æŸå¤±
            # æ³¨æ„ï¼šé¢‘ç¹çš„åƒåœ¾å›æ”¶å¯èƒ½å½±å“æ€§èƒ½ï¼Œå»ºè®®è®¾ç½®memory_threshold_mbå‚æ•°
            if i % 5 == 0 and getattr(self, 'enable_gc', False):  # æ¯5å±‚æ¸…ç†ä¸€æ¬¡ï¼Œé»˜è®¤å…³é—­
                import gc
                memory_threshold = getattr(self, 'gc_memory_threshold_mb', 1024)  # é»˜è®¤1GBé˜ˆå€¼
                
                do_cleanup = False
                if torch.cuda.is_available():
                    mem_mb = torch.cuda.memory_allocated() / 1024 / 1024
                    if mem_mb > memory_threshold:
                        do_cleanup = True
                else:
                    try:
                        import psutil
                        mem_mb = psutil.Process().memory_info().rss / 1024 / 1024
                        if mem_mb > memory_threshold:
                            do_cleanup = True
                    except ImportError:
                        do_cleanup = True  # fallback if psutil unavailable
                
                if do_cleanup:
                    gc.collect()
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    
        morpho_debug.print_debug(f"ä¿¡æ¯æµåˆ†æå®Œæˆï¼Œå…±{len(flow_scores)}å±‚", "SUCCESS")
        morpho_debug.exit_section("ä¿¡æ¯æµç“¶é¢ˆåˆ†æ")
        return flow_scores
    
    def _compute_activation_saturation(self, activation: torch.Tensor) -> float:
        """è®¡ç®—æ¿€æ´»é¥±å’Œåº¦"""
        if activation.numel() == 0:
            return 0.0
            
        # è®¡ç®—æ¿€æ´»å€¼æ¥è¿‘æå€¼çš„æ¯”ä¾‹
        activation_flat = activation.flatten()
        
        # å¯¹äºä¸åŒæ¿€æ´»å‡½æ•°çš„é¥±å’Œåº¦è®¡ç®—
        if torch.all(activation_flat >= 0):  # ReLUç±»æ¿€æ´»
            saturated = torch.sum(activation_flat == 0).float()
        else:  # Tanhç±»æ¿€æ´»
            saturated = torch.sum(torch.abs(activation_flat) > 0.9).float()
            
        saturation_ratio = saturated / activation_flat.numel()
        return saturation_ratio.item()
    
    def _compute_gradient_health(self, gradient: torch.Tensor) -> float:
        """è®¡ç®—æ¢¯åº¦å¥åº·åº¦"""
        if gradient is None or gradient.numel() == 0:
            return 0.0
            
        grad_norm = torch.norm(gradient).item()
        grad_mean = torch.mean(torch.abs(gradient)).item()
        
        # æ¢¯åº¦è¿‡å°æˆ–è¿‡å¤§éƒ½ä¸å¥åº·
        if grad_norm < 1e-7:  # æ¢¯åº¦æ¶ˆå¤±
            return 0.1
        elif grad_norm > 10.0:  # æ¢¯åº¦çˆ†ç‚¸
            return 0.2
        else:
            # ç†æƒ³çš„æ¢¯åº¦èŒƒå›´
            health = 1.0 / (1.0 + abs(math.log10(grad_mean + 1e-8)))
            return min(health, 1.0)
    
    def _compute_information_loss(self, prev_activation: torch.Tensor, 
                                curr_activation: torch.Tensor) -> float:
        """è®¡ç®—å±‚é—´ä¿¡æ¯æŸå¤±"""
        try:
            # ç®€åŒ–çš„ä¿¡æ¯æŸå¤±è®¡ç®—
            prev_entropy = self._compute_entropy(prev_activation)
            curr_entropy = self._compute_entropy(curr_activation)
            
            # ä¿¡æ¯æŸå¤± = (å‰ä¸€å±‚ç†µ - å½“å‰å±‚ç†µ) / å‰ä¸€å±‚ç†µ
            if prev_entropy > 1e-8:
                loss = max(0, (prev_entropy - curr_entropy) / prev_entropy)
                return min(loss, 1.0)
            else:
                return 0.0
        except:
            return 0.0
    
    def _compute_entropy(self, activation: torch.Tensor) -> float:
        """è®¡ç®—æ¿€æ´»å€¼ç†µ"""
        if activation.numel() == 0:
            return 0.0
            
        # å°†æ¿€æ´»å€¼è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        activation_flat = activation.flatten()
        activation_abs = torch.abs(activation_flat) + 1e-8
        probs = activation_abs / torch.sum(activation_abs)
        
        # è®¡ç®—ç†µ
        entropy = -torch.sum(probs * torch.log(probs + 1e-8))
        return entropy.item()
    
    def _compute_neuron_utilization(self, activation: torch.Tensor) -> float:
        """è®¡ç®—ç¥ç»å…ƒåˆ©ç”¨ç‡"""
        if activation.numel() == 0:
            return 0.0
            
        # è®¡ç®—æ¿€æ´»çš„ç¥ç»å…ƒæ¯”ä¾‹
        if len(activation.shape) >= 2:
            # å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œè®¡ç®—æ¿€æ´»çš„ç¥ç»å…ƒ
            batch_size = activation.shape[0]
            activation_reshaped = activation.view(batch_size, -1)
            active_neurons = torch.sum(activation_reshaped > 1e-6, dim=0)
            utilization = torch.mean((active_neurons > 0).float())
            return utilization.item()
        else:
            return 1.0
    
    def _compute_gradient_variance(self, gradient: torch.Tensor) -> float:
        """è®¡ç®—æ¢¯åº¦æ–¹å·®"""
        if gradient is None or gradient.numel() == 0:
            return 0.0
            
        grad_flat = gradient.flatten()
        variance = torch.var(grad_flat)
        
        # å½’ä¸€åŒ–æ–¹å·®
        mean_abs = torch.mean(torch.abs(grad_flat))
        if mean_abs > 1e-8:
            normalized_variance = variance / (mean_abs ** 2)
            return min(normalized_variance.item(), 1.0)
        else:
            return 0.0
    
    def _compute_activation_diversity(self, activation: torch.Tensor) -> float:
        """è®¡ç®—æ¿€æ´»æ¨¡å¼å¤šæ ·æ€§"""
        if activation.numel() == 0 or len(activation.shape) < 2:
            return 0.0
            
        batch_size = activation.shape[0]
        if batch_size < 2:
            return 0.0
            
        # è®¡ç®—æ‰¹æ¬¡å†…æ¿€æ´»æ¨¡å¼çš„ç›¸ä¼¼æ€§
        activation_flat = activation.view(batch_size, -1)
        
        # è®¡ç®—æ ·æœ¬é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = []
        for i in range(min(batch_size, 10)):  # é™åˆ¶è®¡ç®—é‡
            for j in range(i+1, min(batch_size, 10)):
                sim = F.cosine_similarity(
                    activation_flat[i:i+1], 
                    activation_flat[j:j+1], 
                    dim=1
                )
                similarities.append(sim.item())
        
        if similarities:
            avg_similarity = np.mean(similarities)
            diversity = 1.0 - avg_similarity  # ç›¸ä¼¼åº¦è¶Šä½ï¼Œå¤šæ ·æ€§è¶Šé«˜
            return max(0.0, diversity)
        else:
            return 0.0
    
    def _compute_feature_correlation(self, activation: torch.Tensor) -> float:
        """è®¡ç®—ç‰¹å¾ç›¸å…³æ€§ - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬"""
        if activation.numel() == 0 or len(activation.shape) < 2:
            return 0.0
            
        try:
            activation_flat = activation.view(activation.shape[0], -1)
            if activation_flat.shape[1] < 2:
                return 0.0
            
            # å†…å­˜ä¼˜åŒ–ï¼šé™åˆ¶ç‰¹å¾æ•°é‡ï¼Œä½¿ç”¨é‡‡æ ·
            max_features = 512  # æœ€å¤§ç‰¹å¾æ•°é™åˆ¶
            if activation_flat.shape[1] > max_features:
                # éšæœºé‡‡æ ·ç‰¹å¾
                indices = torch.randperm(activation_flat.shape[1])[:max_features]
                activation_flat = activation_flat[:, indices]
            
            # è¿›ä¸€æ­¥é™åˆ¶ï¼šå¦‚æœè¿˜æ˜¯å¤ªå¤§ï¼Œä½¿ç”¨æ›´å°çš„æ ·æœ¬
            if activation_flat.shape[0] > 64:
                indices = torch.randperm(activation_flat.shape[0])[:64]
                activation_flat = activation_flat[indices]
                
            # è®¡ç®—ç‰¹å¾é—´çš„ç›¸å…³ç³»æ•° - ä»…åœ¨å¯ç®¡ç†çš„å¤§å°æ—¶
            if activation_flat.shape[1] > 1024:
                # å¯¹äºéå¸¸å¤§çš„ç‰¹å¾ï¼Œä½¿ç”¨è¿‘ä¼¼æ–¹æ³•
                # éšæœºé€‰æ‹©ç‰¹å¾å¯¹è®¡ç®—ç›¸å…³æ€§
                num_pairs = min(100, activation_flat.shape[1] // 2)
                correlations = []
                
                for _ in range(num_pairs):
                    i = torch.randint(0, activation_flat.shape[1], (1,)).item()
                    j = torch.randint(0, activation_flat.shape[1], (1,)).item()
                    if i != j:
                        corr = torch.corrcoef(torch.stack([activation_flat[:, i], activation_flat[:, j]]))[0, 1]
                        if not torch.isnan(corr):
                            correlations.append(torch.abs(corr).item())
                
                return np.mean(correlations) if correlations else 0.0
            else:
                # æ ‡å‡†ç›¸å…³æ€§è®¡ç®—
                correlation_matrix = torch.corrcoef(activation_flat.T)
                
                # æ£€æŸ¥çŸ©é˜µæ˜¯å¦æœ‰æ•ˆ
                if torch.isnan(correlation_matrix).any():
                    return 0.0
                
                # è®¡ç®—å¹³å‡ç»å¯¹ç›¸å…³ç³»æ•°
                mask = torch.eye(correlation_matrix.shape[0], dtype=torch.bool)
                off_diagonal = correlation_matrix[~mask]
                
                if len(off_diagonal) > 0:
                    avg_correlation = torch.mean(torch.abs(off_diagonal))
                    return avg_correlation.item()
                else:
                    return 0.0
        except Exception as e:
            morpho_debug.print_debug(f"ç‰¹å¾ç›¸å…³æ€§è®¡ç®—å¤±è´¥: {e}", "WARNING")
            return 0.0
    
    def _compute_information_redundancy(self, activation: torch.Tensor) -> float:
        """è®¡ç®—ä¿¡æ¯å†—ä½™åº¦ - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬"""
        if activation.numel() == 0:
            return 0.0
            
        try:
            # å†…å­˜ä¼˜åŒ–ï¼šå¯¹äºå¤§å¼ é‡ä½¿ç”¨é‡‡æ ·
            activation_flat = activation.flatten()
            
            # é™åˆ¶åˆ†æçš„å…ƒç´ æ•°é‡
            max_elements = 100000  # æœ€å¤§åˆ†æ10ä¸‡ä¸ªå…ƒç´ 
            if len(activation_flat) > max_elements:
                # éšæœºé‡‡æ ·
                indices = torch.randperm(len(activation_flat))[:max_elements]
                activation_flat = activation_flat[indices]
            
            # è®¡ç®—é‡å¤å€¼çš„æ¯”ä¾‹
            unique_values = torch.unique(activation_flat)
            redundancy = 1.0 - (len(unique_values) / len(activation_flat))
            
            return redundancy
        except Exception as e:
            morpho_debug.print_debug(f"ä¿¡æ¯å†—ä½™åº¦è®¡ç®—å¤±è´¥: {e}", "WARNING")
            return 0.0
    
    def _analyze_gradient_flow(self, gradients: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """åˆ†ææ¢¯åº¦æµç“¶é¢ˆ"""
        flow_scores = {}
        
        layer_names = list(gradients.keys())
        grad_norms = []
        
        # è®¡ç®—æ¯å±‚çš„æ¢¯åº¦èŒƒæ•°
        for layer_name in layer_names:
            if gradients[layer_name] is not None:
                norm = torch.norm(gradients[layer_name]).item()
                grad_norms.append(norm)
            else:
                grad_norms.append(0.0)
        
        if not grad_norms:
            return flow_scores
            
        # è®¡ç®—æ¢¯åº¦èŒƒæ•°çš„å˜åŒ–ç‡
        for i, layer_name in enumerate(layer_names):
            if i == 0:
                flow_scores[layer_name] = 0.0
                continue
                
            prev_norm = grad_norms[i-1]
            curr_norm = grad_norms[i]
            
            # æ¢¯åº¦è¡°å‡ç‡
            if prev_norm > 1e-8:
                decay_rate = (prev_norm - curr_norm) / prev_norm
                flow_scores[layer_name] = max(0.0, decay_rate)
            else:
                flow_scores[layer_name] = 0.0
                
        return flow_scores
    
    def _analyze_capacity_bottlenecks(self, model: nn.Module, 
                                    activations: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """åˆ†æå®¹é‡ç“¶é¢ˆ"""
        capacity_scores = {}
        
        for name, module in model.named_modules():
            if name in activations and isinstance(module, (nn.Linear, nn.Conv2d)):
                activation = activations[name]
                
                # è®¡ç®—å±‚çš„ç†è®ºå®¹é‡
                if isinstance(module, nn.Linear):
                    theoretical_capacity = module.in_features * module.out_features
                    actual_capacity = self._compute_actual_capacity(activation)
                elif isinstance(module, nn.Conv2d):
                    theoretical_capacity = (module.in_channels * module.out_channels * 
                                          module.kernel_size[0] * module.kernel_size[1])
                    actual_capacity = self._compute_actual_capacity(activation)
                else:
                    continue
                
                # å®¹é‡åˆ©ç”¨ç‡
                if theoretical_capacity > 0:
                    utilization = actual_capacity / theoretical_capacity
                    capacity_scores[name] = 1.0 - utilization  # åˆ©ç”¨ç‡è¶Šä½ï¼Œç“¶é¢ˆåˆ†æ•°è¶Šé«˜
                else:
                    capacity_scores[name] = 0.0
        
        return capacity_scores
    
    def _compute_actual_capacity(self, activation: torch.Tensor) -> float:
        """è®¡ç®—å®é™…ä½¿ç”¨çš„å®¹é‡"""
        if activation.numel() == 0:
            return 0.0
            
        # è®¡ç®—æœ‰æ•ˆæ¿€æ´»çš„æ•°é‡
        effective_activations = torch.sum(torch.abs(activation) > 1e-6).item()
        return effective_activations

class AdvancedMorphogenesisExecutor:
    """é«˜çº§å½¢æ€å‘ç”Ÿæ‰§è¡Œå™¨"""
    
    def __init__(self):
        self.execution_history = []
        
    def execute_morphogenesis(self, model: nn.Module, decision: MorphogenesisDecision) -> Tuple[nn.Module, int]:
        """æ‰§è¡Œå½¢æ€å‘ç”Ÿ"""
        try:
            # è·å–æ¨¡å‹è®¾å¤‡
            device = next(model.parameters()).device
            
            if decision.morphogenesis_type == MorphogenesisType.SERIAL_DIVISION:
                new_model, params_added = self._execute_serial_division(model, decision.target_location)
            elif decision.morphogenesis_type == MorphogenesisType.PARALLEL_DIVISION:
                new_model, params_added = self._execute_parallel_division(model, decision.target_location)
            elif decision.morphogenesis_type == MorphogenesisType.HYBRID_DIVISION:
                new_model, params_added = self._execute_hybrid_division(model, decision.target_location)
            elif decision.morphogenesis_type == MorphogenesisType.SKIP_CONNECTION:
                new_model, params_added = self._execute_skip_connection(model, decision.target_location)
            elif decision.morphogenesis_type == MorphogenesisType.ATTENTION_INJECTION:
                new_model, params_added = self._execute_attention_injection(model, decision.target_location)
            else:
                # é»˜è®¤ä½¿ç”¨å®½åº¦æ‰©å±•
                new_model, params_added = self._execute_width_expansion(model, decision.target_location)
            
            # ç¡®ä¿æ–°æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            new_model = new_model.to(device)
            
            return new_model, params_added
                
        except Exception as e:
            logger.error(f"å½¢æ€å‘ç”Ÿæ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return model, 0
    
    def _execute_serial_division(self, model: nn.Module, target_location: str) -> Tuple[nn.Module, int]:
        """æ‰§è¡Œä¸²è¡Œåˆ†è£‚ - åœ¨ç›®æ ‡å±‚åæ’å…¥æ–°å±‚"""
        logger.info(f"æ‰§è¡Œä¸²è¡Œåˆ†è£‚: {target_location}")
        
        # åˆ›å»ºæ–°æ¨¡å‹
        new_model = copy.deepcopy(model)
        parameters_added = 0
        
        # æŸ¥æ‰¾ç›®æ ‡å±‚
        target_module = None
        target_parent = None
        target_attr = None
        
        for name, module in new_model.named_modules():
            if name == target_location:
                target_module = module
                # æ‰¾åˆ°çˆ¶æ¨¡å—
                if '.' in name:
                    parent_name = '.'.join(name.split('.')[:-1])
                    target_attr = name.split('.')[-1]
                    for pname, pmodule in new_model.named_modules():
                        if pname == parent_name:
                            target_parent = pmodule
                            break
                break
        
        if target_module is None:
            logger.warning(f"æœªæ‰¾åˆ°ç›®æ ‡å±‚: {target_location}")
            return model, 0
        
        if isinstance(target_module, nn.Linear):
            # è·å–è®¾å¤‡ä¿¡æ¯
            device = target_module.weight.device
            
            # åœ¨å…¨è¿æ¥å±‚åæ’å…¥æ–°çš„å…¨è¿æ¥å±‚
            hidden_size = max(target_module.out_features // 2, 64)
            
            # åˆ›å»ºæ–°çš„ä¸­é—´å±‚
            intermediate_layer = nn.Linear(target_module.out_features, hidden_size).to(device)
            output_layer = nn.Linear(hidden_size, target_module.out_features).to(device)
            
            # åˆå§‹åŒ–æƒé‡ä½¿æ–°å±‚ç»„åˆæ¥è¿‘åŸå±‚
            with torch.no_grad():
                # ä¸­é—´å±‚ï¼šå‹ç¼©è¡¨ç¤º
                nn.init.kaiming_normal_(intermediate_layer.weight)
                nn.init.zeros_(intermediate_layer.bias)
                
                # è¾“å‡ºå±‚ï¼šé‡æ„åˆ°åŸå§‹ç»´åº¦
                nn.init.kaiming_normal_(output_layer.weight)
                nn.init.zeros_(output_layer.bias)
            
            # åˆ›å»ºæ–°çš„åºåˆ—æ¨¡å—
            new_sequence = nn.Sequential(
                target_module,
                nn.ReLU(),
                intermediate_layer,
                nn.ReLU(),
                output_layer
            )
            
            # æ›¿æ¢åŸæ¨¡å—
            if target_parent is not None and target_attr is not None:
                setattr(target_parent, target_attr, new_sequence)
                parameters_added = (hidden_size * target_module.out_features + hidden_size +
                                  target_module.out_features * hidden_size + target_module.out_features)
                
        elif isinstance(target_module, nn.Conv2d):
            # è·å–è®¾å¤‡ä¿¡æ¯
            device = target_module.weight.device
            
            # åœ¨å·ç§¯å±‚åæ’å…¥æ–°çš„å·ç§¯å±‚
            intermediate_channels = max(target_module.out_channels // 2, 32)
            
            # åˆ›å»ºæ–°çš„å·ç§¯å±‚åºåˆ—
            intermediate_conv = nn.Conv2d(
                target_module.out_channels, 
                intermediate_channels,
                kernel_size=3, 
                padding=1
            ).to(device)
            output_conv = nn.Conv2d(
                intermediate_channels,
                target_module.out_channels,
                kernel_size=3,
                padding=1
            ).to(device)
            
            # åˆå§‹åŒ–æƒé‡
            with torch.no_grad():
                nn.init.kaiming_normal_(intermediate_conv.weight)
                nn.init.zeros_(intermediate_conv.bias)
                nn.init.kaiming_normal_(output_conv.weight)
                nn.init.zeros_(output_conv.bias)
            
            # åˆ›å»ºæ–°çš„åºåˆ—
            new_sequence = nn.Sequential(
                target_module,
                nn.ReLU(),
                intermediate_conv,
                nn.ReLU(),
                output_conv
            )
            
            # æ›¿æ¢åŸæ¨¡å—
            if target_parent is not None and target_attr is not None:
                setattr(target_parent, target_attr, new_sequence)
                parameters_added = (intermediate_channels * target_module.out_channels * 9 + intermediate_channels +
                                  target_module.out_channels * intermediate_channels * 9 + target_module.out_channels)
        
        self.execution_history.append({
            'type': 'serial_division',
            'location': target_location,
            'parameters_added': parameters_added
        })
        
        logger.info(f"ä¸²è¡Œåˆ†è£‚å®Œæˆï¼Œæ–°å¢å‚æ•°: {parameters_added}")
        return new_model, parameters_added
    
    def _execute_parallel_division(self, model: nn.Module, target_location: str) -> Tuple[nn.Module, int]:
        """æ‰§è¡Œå¹¶è¡Œåˆ†è£‚ - åˆ›å»ºå¤šåˆ†æ”¯ç»“æ„"""
        logger.info(f"æ‰§è¡Œå¹¶è¡Œåˆ†è£‚: {target_location}")
        
        new_model = copy.deepcopy(model)
        parameters_added = 0
        
        # æŸ¥æ‰¾ç›®æ ‡å±‚
        target_module = None
        target_parent = None
        target_attr = None
        
        for name, module in new_model.named_modules():
            if name == target_location:
                target_module = module
                if '.' in name:
                    parent_name = '.'.join(name.split('.')[:-1])
                    target_attr = name.split('.')[-1]
                    for pname, pmodule in new_model.named_modules():
                        if pname == parent_name:
                            target_parent = pmodule
                            break
                break
        
        if target_module is None:
            logger.warning(f"æœªæ‰¾åˆ°ç›®æ ‡å±‚: {target_location}")
            return model, 0
        
        if isinstance(target_module, nn.Linear):
            # è·å–è®¾å¤‡ä¿¡æ¯
            device = target_module.weight.device
            
            # åˆ›å»ºå¹¶è¡Œåˆ†æ”¯
            branch_size = target_module.out_features // 3
            
            # ä¸‰ä¸ªå¹¶è¡Œåˆ†æ”¯ï¼šä¸åŒçš„ç‰¹å¾æå–ç­–ç•¥
            branch1 = nn.Linear(target_module.in_features, branch_size).to(device)  # æ ‡å‡†çº¿æ€§å˜æ¢
            branch2 = nn.Sequential(  # æ·±åº¦åˆ†æ”¯
                nn.Linear(target_module.in_features, branch_size),
                nn.ReLU(),
                nn.Linear(branch_size, branch_size)
            ).to(device)
            branch3 = nn.Sequential(  # æ®‹å·®åˆ†æ”¯
                nn.Linear(target_module.in_features, branch_size),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(branch_size, branch_size)
            ).to(device)
            
            # èåˆå±‚
            fusion_layer = nn.Linear(branch_size * 3, target_module.out_features).to(device)
            
            # åˆå§‹åŒ–æƒé‡
            for branch in [branch1, branch2, branch3]:
                if isinstance(branch, nn.Linear):
                    nn.init.kaiming_normal_(branch.weight)
                    nn.init.zeros_(branch.bias)
                else:
                    for layer in branch:
                        if isinstance(layer, nn.Linear):
                            nn.init.kaiming_normal_(layer.weight)
                            nn.init.zeros_(layer.bias)
            
            nn.init.kaiming_normal_(fusion_layer.weight)
            nn.init.zeros_(fusion_layer.bias)
            
            # åˆ›å»ºå¹¶è¡Œæ¨¡å—
            class ParallelBranches(nn.Module):
                def __init__(self, branch1, branch2, branch3, fusion):
                    super().__init__()
                    self.branch1 = branch1
                    self.branch2 = branch2
                    self.branch3 = branch3
                    self.fusion = fusion
                
                def forward(self, x):
                    out1 = self.branch1(x)
                    out2 = self.branch2(x)
                    out3 = self.branch3(x)
                    combined = torch.cat([out1, out2, out3], dim=-1)
                    return self.fusion(combined)
            
            parallel_module = ParallelBranches(branch1, branch2, branch3, fusion_layer)
            
            # æ›¿æ¢åŸæ¨¡å—
            if target_parent is not None and target_attr is not None:
                setattr(target_parent, target_attr, parallel_module)
                
                # è®¡ç®—æ–°å¢å‚æ•°
                params1 = branch_size * target_module.in_features + branch_size
                params2 = (branch_size * target_module.in_features + branch_size + 
                          branch_size * branch_size + branch_size)
                params3 = (branch_size * target_module.in_features + branch_size + 
                          branch_size * branch_size + branch_size)
                params_fusion = target_module.out_features * (branch_size * 3) + target_module.out_features
                parameters_added = params1 + params2 + params3 + params_fusion
                
        elif isinstance(target_module, nn.Conv2d):
            # è·å–è®¾å¤‡ä¿¡æ¯
            device = target_module.weight.device
            
            # åˆ›å»ºå·ç§¯å¹¶è¡Œåˆ†æ”¯
            branch_channels = target_module.out_channels // 3
            
            # ä¸‰ä¸ªä¸åŒå°ºåº¦çš„å·ç§¯åˆ†æ”¯
            branch1 = nn.Conv2d(target_module.in_channels, branch_channels, 
                              kernel_size=1, padding=0).to(device)  # 1x1å·ç§¯
            branch2 = nn.Conv2d(target_module.in_channels, branch_channels, 
                              kernel_size=3, padding=1).to(device)  # 3x3å·ç§¯
            branch3 = nn.Conv2d(target_module.in_channels, branch_channels, 
                              kernel_size=5, padding=2).to(device)  # 5x5å·ç§¯
            
            # èåˆå·ç§¯
            fusion_conv = nn.Conv2d(branch_channels * 3, target_module.out_channels, 
                                  kernel_size=1, padding=0).to(device)
            
            # åˆå§‹åŒ–æƒé‡
            for branch in [branch1, branch2, branch3, fusion_conv]:
                nn.init.kaiming_normal_(branch.weight)
                nn.init.zeros_(branch.bias)
            
            # åˆ›å»ºå¹¶è¡Œå·ç§¯æ¨¡å—
            class ParallelConv(nn.Module):
                def __init__(self, branch1, branch2, branch3, fusion):
                    super().__init__()
                    self.branch1 = branch1
                    self.branch2 = branch2
                    self.branch3 = branch3
                    self.fusion = fusion
                
                def forward(self, x):
                    out1 = self.branch1(x)
                    out2 = self.branch2(x)
                    out3 = self.branch3(x)
                    combined = torch.cat([out1, out2, out3], dim=1)
                    return self.fusion(combined)
            
            parallel_module = ParallelConv(branch1, branch2, branch3, fusion_conv)
            
            # æ›¿æ¢åŸæ¨¡å—
            if target_parent is not None and target_attr is not None:
                setattr(target_parent, target_attr, parallel_module)
                
                # è®¡ç®—æ–°å¢å‚æ•°
                params1 = branch_channels * target_module.in_channels * 1 + branch_channels
                params2 = branch_channels * target_module.in_channels * 9 + branch_channels
                params3 = branch_channels * target_module.in_channels * 25 + branch_channels
                params_fusion = target_module.out_channels * (branch_channels * 3) + target_module.out_channels
                parameters_added = params1 + params2 + params3 + params_fusion
        
        self.execution_history.append({
            'type': 'parallel_division',
            'location': target_location,
            'parameters_added': parameters_added
        })
        
        logger.info(f"å¹¶è¡Œåˆ†è£‚å®Œæˆï¼Œæ–°å¢å‚æ•°: {parameters_added}")
        return new_model, parameters_added
    
    def _execute_hybrid_division(self, model: nn.Module, target_location: str) -> Tuple[nn.Module, int]:
        """æ‰§è¡Œæ··åˆåˆ†è£‚ - ç»„åˆä¸åŒç±»å‹çš„å±‚"""
        logger.info(f"æ‰§è¡Œæ··åˆåˆ†è£‚: {target_location}")
        
        new_model = copy.deepcopy(model)
        parameters_added = 0
        
        # æŸ¥æ‰¾ç›®æ ‡å±‚
        target_module = None
        target_parent = None
        target_attr = None
        
        for name, module in new_model.named_modules():
            if name == target_location:
                target_module = module
                if '.' in name:
                    parent_name = '.'.join(name.split('.')[:-1])
                    target_attr = name.split('.')[-1]
                    for pname, pmodule in new_model.named_modules():
                        if pname == parent_name:
                            target_parent = pmodule
                            break
                break
        
        if target_module is None:
            logger.warning(f"æœªæ‰¾åˆ°ç›®æ ‡å±‚: {target_location}")
            return model, 0
        
        if isinstance(target_module, nn.Linear):
            # è·å–è®¾å¤‡ä¿¡æ¯
            device = target_module.weight.device
            
            # åˆ›å»ºæ··åˆç»“æ„ï¼šçº¿æ€§å±‚ + æ³¨æ„åŠ›æœºåˆ¶ + æ®‹å·®è¿æ¥
            hidden_size = target_module.out_features
            
            # ä¸»è¦å˜æ¢
            main_transform = nn.Linear(target_module.in_features, hidden_size).to(device)
            
            # æ³¨æ„åŠ›æœºåˆ¶
            attention = nn.MultiheadAttention(
                embed_dim=target_module.in_features,
                num_heads=max(1, target_module.in_features // 64),
                batch_first=True
            ).to(device)
            attention_projection = nn.Linear(target_module.in_features, hidden_size).to(device)
            
            # è¾“å‡ºèåˆ
            output_layer = nn.Linear(hidden_size * 2, target_module.out_features).to(device)
            
            # åˆå§‹åŒ–
            nn.init.kaiming_normal_(main_transform.weight)
            nn.init.zeros_(main_transform.bias)
            nn.init.kaiming_normal_(attention_projection.weight)
            nn.init.zeros_(attention_projection.bias)
            nn.init.kaiming_normal_(output_layer.weight)
            nn.init.zeros_(output_layer.bias)
            
            class HybridLinear(nn.Module):
                def __init__(self, main_transform, attention, attention_projection, output_layer):
                    super().__init__()
                    self.main_transform = main_transform
                    self.attention = attention
                    self.attention_projection = attention_projection
                    self.output_layer = output_layer
                
                def forward(self, x):
                    # ä¸»è¦å˜æ¢
                    main_out = self.main_transform(x)
                    
                    # æ³¨æ„åŠ›åˆ†æ”¯
                    if len(x.shape) == 2:
                        x_unsqueezed = x.unsqueeze(1)  # æ·»åŠ åºåˆ—ç»´åº¦
                        attn_out, _ = self.attention(x_unsqueezed, x_unsqueezed, x_unsqueezed)
                        attn_out = attn_out.squeeze(1)  # ç§»é™¤åºåˆ—ç»´åº¦
                    else:
                        attn_out, _ = self.attention(x, x, x)
                    
                    attn_out = self.attention_projection(attn_out)
                    
                    # èåˆ
                    combined = torch.cat([main_out, attn_out], dim=-1)
                    return self.output_layer(combined)
            
            hybrid_module = HybridLinear(main_transform, attention, attention_projection, output_layer)
            
            # æ›¿æ¢åŸæ¨¡å—
            if target_parent is not None and target_attr is not None:
                setattr(target_parent, target_attr, hybrid_module)
                
                # è®¡ç®—å‚æ•°
                main_params = hidden_size * target_module.in_features + hidden_size
                attn_params = attention.in_proj_weight.numel() + attention.out_proj.weight.numel()
                proj_params = hidden_size * target_module.in_features + hidden_size
                output_params = target_module.out_features * (hidden_size * 2) + target_module.out_features
                parameters_added = main_params + attn_params + proj_params + output_params
        
        self.execution_history.append({
            'type': 'hybrid_division',
            'location': target_location,
            'parameters_added': parameters_added
        })
        
        logger.info(f"æ··åˆåˆ†è£‚å®Œæˆï¼Œæ–°å¢å‚æ•°: {parameters_added}")
        return new_model, parameters_added
    
    def _execute_skip_connection(self, model: nn.Module, target_location: str) -> Tuple[nn.Module, int]:
        """æ‰§è¡Œè·³è·ƒè¿æ¥æ·»åŠ """
        logger.warning(f"è·³è·ƒè¿æ¥åŠŸèƒ½å°šæœªå®ç°: {target_location}")
        # è·³è·ƒè¿æ¥å®ç°è¾ƒä¸ºå¤æ‚ï¼Œéœ€è¦ä¿®æ”¹æ¨¡å‹çš„forwardæ–¹æ³•
        # å½“å‰ç‰ˆæœ¬æš‚ä¸æ”¯æŒæ­¤åŠŸèƒ½
        raise NotImplementedError("Skip connection morphogenesis is not yet implemented. "
                                "This requires complex model architecture modification.")
    
    def _execute_attention_injection(self, model: nn.Module, target_location: str) -> Tuple[nn.Module, int]:
        """æ‰§è¡Œæ³¨æ„åŠ›æœºåˆ¶æ³¨å…¥"""
        logger.warning(f"æ³¨æ„åŠ›æ³¨å…¥åŠŸèƒ½å°šæœªå®ç°: {target_location}")
        # æ³¨æ„åŠ›æœºåˆ¶æ³¨å…¥éœ€è¦ä»”ç»†çš„æ¶æ„è®¾è®¡
        # å½“å‰ç‰ˆæœ¬æš‚ä¸æ”¯æŒæ­¤åŠŸèƒ½
        raise NotImplementedError("Attention injection morphogenesis is not yet implemented. "
                                "This requires careful attention mechanism design and integration.")
    
    def _execute_width_expansion(self, model: nn.Module, target_location: str) -> Tuple[nn.Module, int]:
        """æ‰§è¡Œå®½åº¦æ‰©å±•ï¼ˆå…œåº•ç­–ç•¥ï¼‰"""
        logger.info(f"æ‰§è¡Œå®½åº¦æ‰©å±•: {target_location}")
        
        new_model = copy.deepcopy(model)
        parameters_added = 0
        
        # æ‰¾åˆ°ç›®æ ‡å±‚å¹¶æ‰©å±•
        target_module = None
        for name, module in new_model.named_modules():
            if name == target_location:
                target_module = module
                break
        
        if target_module is None:
            logger.warning(f"æœªæ‰¾åˆ°ç›®æ ‡å±‚: {target_location}")
            return model, 0
        
        if isinstance(target_module, nn.Linear):
            # è·å–è®¾å¤‡ä¿¡æ¯
            device = target_module.weight.device
            
            old_out = target_module.out_features
            new_out = int(old_out * 1.2)
            expansion = new_out - old_out
            
            # æ‰©å±•å½“å‰å±‚çš„æƒé‡
            new_weight = torch.zeros(new_out, target_module.in_features, device=device)
            new_bias = torch.zeros(new_out, device=device) if target_module.bias is not None else None
            
            # å¤åˆ¶åŸæƒé‡
            new_weight[:old_out] = target_module.weight.data
            if new_bias is not None:
                new_bias[:old_out] = target_module.bias.data
                
            # åˆå§‹åŒ–æ–°æƒé‡
            nn.init.normal_(new_weight[old_out:], std=0.01)
            if new_bias is not None:
                nn.init.zeros_(new_bias[old_out:])
            
            # æ›´æ–°å½“å‰å±‚
            target_module.out_features = new_out
            target_module.weight = nn.Parameter(new_weight)
            if target_module.bias is not None:
                target_module.bias = nn.Parameter(new_bias)
            
            parameters_added += expansion * (target_module.in_features + 1)
            
            # æ‰¾åˆ°å¹¶æ›´æ–°ä¸‹ä¸€ä¸ªçº¿æ€§å±‚çš„è¾“å…¥ç»´åº¦
            all_modules = list(new_model.named_modules())
            target_index = None
            
            for i, (name, module) in enumerate(all_modules):
                if name == target_location:
                    target_index = i
                    break
            
            if target_index is not None:
                # å¯»æ‰¾ä¸‹ä¸€ä¸ªçº¿æ€§å±‚
                for i in range(target_index + 1, len(all_modules)):
                    next_name, next_module = all_modules[i]
                    if isinstance(next_module, nn.Linear):
                        # æ›´æ–°ä¸‹ä¸€å±‚çš„è¾“å…¥ç»´åº¦
                        old_in = next_module.in_features
                        new_in = new_out
                        
                        # æ‰©å±•ä¸‹ä¸€å±‚çš„æƒé‡
                        next_weight = torch.zeros(next_module.out_features, new_in, device=device)
                        
                        # å¤åˆ¶åŸæœ‰æƒé‡åˆ°å¯¹åº”ä½ç½®
                        next_weight[:, :old_in] = next_module.weight.data
                        
                        # ä¸ºæ–°å¢çš„è¾“å…¥ç»´åº¦åˆå§‹åŒ–æƒé‡
                        nn.init.normal_(next_weight[:, old_in:], std=0.01)
                        
                        # æ›´æ–°ä¸‹ä¸€å±‚
                        next_module.in_features = new_in
                        next_module.weight = nn.Parameter(next_weight)
                        
                        parameters_added += next_module.out_features * expansion
                        break
        
        return new_model, parameters_added

class IntelligentMorphogenesisDecisionMaker:
    """æ™ºèƒ½å½¢æ€å‘ç”Ÿå†³ç­–åˆ¶å®šå™¨"""
    
    def __init__(self):
        self.decision_history = []
        self.performance_tracker = {}
        
    def make_decision(self, bottleneck_analysis: Dict[str, Any], 
                     performance_history: List[float]) -> Optional[MorphogenesisDecision]:
        """åˆ¶å®šå½¢æ€å‘ç”Ÿå†³ç­–"""
        
        # åˆ†æä¸åŒç±»å‹çš„ç“¶é¢ˆ
        depth_bottlenecks = bottleneck_analysis['depth_bottlenecks']
        width_bottlenecks = bottleneck_analysis['width_bottlenecks']
        flow_bottlenecks = bottleneck_analysis['information_flow_bottlenecks']
        
        # æ‰¾åˆ°æœ€ä¸¥é‡çš„ç“¶é¢ˆ
        all_bottlenecks = []
        
        # æ·±åº¦ç“¶é¢ˆ -> ä¸²è¡Œåˆ†è£‚
        for layer, score in depth_bottlenecks.items():
            if score > 0.6:  # æ·±åº¦ç“¶é¢ˆé˜ˆå€¼
                all_bottlenecks.append({
                    'location': layer,
                    'type': MorphogenesisType.SERIAL_DIVISION,
                    'score': score,
                    'reasoning': f"æ·±åº¦ç“¶é¢ˆæ£€æµ‹ï¼Œåˆ†æ•°: {score:.3f}"
                })
        
        # ä¿¡æ¯æµç“¶é¢ˆ -> å¹¶è¡Œåˆ†è£‚
        for layer, score in flow_bottlenecks.items():
            if score > 0.5:  # ä¿¡æ¯æµç“¶é¢ˆé˜ˆå€¼
                all_bottlenecks.append({
                    'location': layer,
                    'type': MorphogenesisType.PARALLEL_DIVISION,
                    'score': score,
                    'reasoning': f"ä¿¡æ¯æµç“¶é¢ˆæ£€æµ‹ï¼Œåˆ†æ•°: {score:.3f}"
                })
        
        # å¤æ‚ç“¶é¢ˆ -> æ··åˆåˆ†è£‚
        for layer in set(depth_bottlenecks.keys()) & set(flow_bottlenecks.keys()):
            combined_score = (depth_bottlenecks[layer] + flow_bottlenecks[layer]) / 2
            if combined_score > 0.55:
                all_bottlenecks.append({
                    'location': layer,
                    'type': MorphogenesisType.HYBRID_DIVISION,
                    'score': combined_score,
                    'reasoning': f"å¤åˆç“¶é¢ˆæ£€æµ‹ï¼Œæ·±åº¦:{depth_bottlenecks[layer]:.3f}, æµåŠ¨:{flow_bottlenecks[layer]:.3f}"
                })
        
        # å®½åº¦ç“¶é¢ˆ -> å®½åº¦æ‰©å±•
        for layer, score in width_bottlenecks.items():
            if score > 0.4:  # å®½åº¦ç“¶é¢ˆé˜ˆå€¼ï¼ˆè¾ƒä½ï¼Œä½œä¸ºå¤‡é€‰ï¼‰
                all_bottlenecks.append({
                    'location': layer,
                    'type': MorphogenesisType.WIDTH_EXPANSION,
                    'score': score,
                    'reasoning': f"å®½åº¦ç“¶é¢ˆæ£€æµ‹ï¼Œåˆ†æ•°: {score:.3f}"
                })
        
        if not all_bottlenecks:
            return None
        
        # é€‰æ‹©åˆ†æ•°æœ€é«˜çš„ç“¶é¢ˆ
        best_bottleneck = max(all_bottlenecks, key=lambda x: x['score'])
        
        # ä¼°ç®—æ€§èƒ½æ”¹è¿›å’Œæˆæœ¬
        expected_improvement = self._estimate_improvement(best_bottleneck)
        complexity_cost = self._estimate_complexity_cost(best_bottleneck)
        parameters_added = self._estimate_parameters(best_bottleneck)
        
        decision = MorphogenesisDecision(
            morphogenesis_type=best_bottleneck['type'],
            target_location=best_bottleneck['location'],
            confidence=best_bottleneck['score'],
            expected_improvement=expected_improvement,
            complexity_cost=complexity_cost,
            parameters_added=parameters_added,
            reasoning=best_bottleneck['reasoning']
        )
        
        self.decision_history.append(decision)
        return decision
    
    def _estimate_improvement(self, bottleneck: Dict) -> float:
        """ä¼°ç®—æ€§èƒ½æ”¹è¿›"""
        score = bottleneck['score']
        morph_type = bottleneck['type']
        
        # åŸºäºç“¶é¢ˆç±»å‹å’Œä¸¥é‡ç¨‹åº¦ä¼°ç®—æ”¹è¿›
        type_multipliers = {
            MorphogenesisType.SERIAL_DIVISION: 1.5,      # ä¸²è¡Œåˆ†è£‚é€šå¸¸å¸¦æ¥æ›´å¤§æ”¹è¿›
            MorphogenesisType.PARALLEL_DIVISION: 1.3,    # å¹¶è¡Œåˆ†è£‚æä¾›å¤šæ ·æ€§
            MorphogenesisType.HYBRID_DIVISION: 1.4,      # æ··åˆåˆ†è£‚ç»¼åˆæ•ˆæœ
            MorphogenesisType.WIDTH_EXPANSION: 1.0       # å®½åº¦æ‰©å±•æ•ˆæœè¾ƒå°
        }
        
        base_improvement = score * 0.05  # åŸºç¡€æ”¹è¿›ï¼š5%
        type_bonus = type_multipliers.get(morph_type, 1.0)
        
        return base_improvement * type_bonus
    
    def _estimate_complexity_cost(self, bottleneck: Dict) -> float:
        """ä¼°ç®—å¤æ‚åº¦æˆæœ¬"""
        morph_type = bottleneck['type']
        
        complexity_costs = {
            MorphogenesisType.SERIAL_DIVISION: 0.3,      # å¢åŠ æ·±åº¦ï¼Œä¸­ç­‰æˆæœ¬
            MorphogenesisType.PARALLEL_DIVISION: 0.5,    # å¹¶è¡Œç»“æ„ï¼Œè¾ƒé«˜æˆæœ¬
            MorphogenesisType.HYBRID_DIVISION: 0.6,      # æ··åˆç»“æ„ï¼Œæœ€é«˜æˆæœ¬
            MorphogenesisType.WIDTH_EXPANSION: 0.2       # å®½åº¦æ‰©å±•ï¼Œä½æˆæœ¬
        }
        
        return complexity_costs.get(morph_type, 0.3)
    
    def _estimate_parameters(self, bottleneck: Dict) -> int:
        """ä¼°ç®—æ–°å¢å‚æ•°æ•°é‡"""
        morph_type = bottleneck['type']
        
        # åŸºäºç±»å‹çš„å‚æ•°ä¼°ç®—ï¼ˆç²—ç•¥ï¼‰
        base_params = {
            MorphogenesisType.SERIAL_DIVISION: 5000,
            MorphogenesisType.PARALLEL_DIVISION: 8000,
            MorphogenesisType.HYBRID_DIVISION: 10000,
            MorphogenesisType.WIDTH_EXPANSION: 3000
        }
        
        return base_params.get(morph_type, 3000)