"""
@defgroup group_checkpoint_manager Checkpoint Manager
@ingroup core
Checkpoint Manager module for NeuroExapt framework.

æ–­ç‚¹ç»­ç»ƒç®¡ç†å™¨ (Checkpoint Manager)

åƒä¸‹è½½è½¯ä»¶çš„æ–­ç‚¹ç»­ä¼ ä¸€æ ·ï¼Œæ”¯æŒè®­ç»ƒçš„æ–­ç‚¹ç»­ç»ƒï¼Œä»ä»»ä½•ä¸­æ–­ç‚¹æ— ç¼æ¢å¤è®­ç»ƒã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. è‡ªåŠ¨æ£€æµ‹è®­ç»ƒä¸­æ–­ç‚¹
2. æ™ºèƒ½æ¢å¤è®­ç»ƒçŠ¶æ€
3. æ¶æ„æ¼”è¿›å†å²è¿½è¸ª
4. å¤šç‰ˆæœ¬æ£€æŸ¥ç‚¹ç®¡ç†
5. è®­ç»ƒè¿›åº¦å¯è§†åŒ–
"""

import os
import json
import time
import torch
import logging
import shutil
import glob
from datetime import datetime
from typing import Dict, List, Optional, Union, Any, Tuple
from pathlib import Path
import pickle

logger = logging.getLogger(__name__)

class CheckpointManager:
    """
    æ–­ç‚¹ç»­ç»ƒç®¡ç†å™¨
    
    ç®¡ç†è®­ç»ƒçš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸï¼Œæ”¯æŒä»»æ„æ—¶åˆ»çš„ä¸­æ–­å’Œæ¢å¤
    """
    
    def __init__(self, checkpoint_dir: str, experiment_name: str = "aso_se_training",
                 max_checkpoints: int = 5, auto_save_interval: int = 5):
        """
        Args:
            checkpoint_dir: æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
            experiment_name: å®éªŒåç§°
            max_checkpoints: æœ€å¤§ä¿ç•™æ£€æŸ¥ç‚¹æ•°é‡
            auto_save_interval: è‡ªåŠ¨ä¿å­˜é—´éš”ï¼ˆepochsï¼‰
        """
        self.checkpoint_dir = Path(checkpoint_dir)
        self.experiment_name = experiment_name
        self.max_checkpoints = max_checkpoints
        self.auto_save_interval = auto_save_interval
        
        # åˆ›å»ºç›®å½•ç»“æ„
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.experiment_dir = self.checkpoint_dir / experiment_name
        self.experiment_dir.mkdir(exist_ok=True)
        
        # å­ç›®å½•
        self.models_dir = self.experiment_dir / "models"
        self.logs_dir = self.experiment_dir / "logs"
        self.metadata_dir = self.experiment_dir / "metadata"
        self.best_dir = self.experiment_dir / "best"
        
        for dir_path in [self.models_dir, self.logs_dir, self.metadata_dir, self.best_dir]:
            dir_path.mkdir(exist_ok=True)
        
        # è®­ç»ƒçŠ¶æ€
        self.training_session_id = self._generate_session_id()
        self.current_checkpoint = None
        self.checkpoint_history = []
        self.best_performance = 0.0
        self.best_checkpoint_path: Optional[str] = None
        
        # å…ƒæ•°æ®æ–‡ä»¶
        self.metadata_file = self.metadata_dir / "training_metadata.json"
        self.session_file = self.logs_dir / f"session_{self.training_session_id}.log"
        
        # åŠ è½½å†å²å…ƒæ•°æ®
        self._load_metadata()
        
        logger.info(f"ğŸ“ Checkpoint Manager initialized")
        logger.info(f"   Experiment: {experiment_name}")
        logger.info(f"   Directory: {self.experiment_dir}")
        logger.info(f"   Session ID: {self.training_session_id}")
        
    def _generate_session_id(self) -> str:
        """ç”Ÿæˆè®­ç»ƒä¼šè¯ID"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        return f"{timestamp}_{os.getpid()}"
    
    def _load_metadata(self):
        """åŠ è½½è®­ç»ƒå…ƒæ•°æ®"""
        if self.metadata_file.exists():
            try:
                with open(self.metadata_file, 'r') as f:
                    metadata = json.load(f)
                
                self.checkpoint_history = metadata.get('checkpoint_history', [])
                self.best_performance = metadata.get('best_performance', 0.0)
                self.best_checkpoint_path = metadata.get('best_checkpoint_path', None)
                
                logger.info(f"ğŸ“‚ Loaded metadata: {len(self.checkpoint_history)} checkpoints")
                
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to load metadata: {e}")
                self.checkpoint_history = []
    
    def _save_metadata(self):
        """ä¿å­˜è®­ç»ƒå…ƒæ•°æ®"""
        metadata = {
            'experiment_name': self.experiment_name,
            'training_session_id': self.training_session_id,
            'checkpoint_history': self.checkpoint_history,
            'best_performance': self.best_performance,
            'best_checkpoint_path': self.best_checkpoint_path,
            'last_updated': datetime.now().isoformat(),
            'total_checkpoints': len(self.checkpoint_history),
            'total_sessions': len(set(cp.get('session_id', '') for cp in self.checkpoint_history))
        }
        
        try:
            with open(self.metadata_file, 'w') as f:
                json.dump(metadata, f, indent=2, default=str)
        except Exception as e:
            logger.error(f"âŒ Failed to save metadata: {e}")
    
    def save_checkpoint(self, epoch: int, model_state: Dict, optimizer_states: Dict,
                       scheduler_states: Optional[Dict], training_stats: Dict,
                       framework_state: Dict, performance_metric: float,
                       architecture_info: Optional[Dict] = None) -> str:
        """
        ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹
        
        Args:
            epoch: å½“å‰epoch
            model_state: æ¨¡å‹çŠ¶æ€å­—å…¸
            optimizer_states: ä¼˜åŒ–å™¨çŠ¶æ€å­—å…¸
            scheduler_states: å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€
            training_stats: è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
            framework_state: ASO-SEæ¡†æ¶çŠ¶æ€
            performance_metric: æ€§èƒ½æŒ‡æ ‡ï¼ˆç”¨äºåˆ¤æ–­æœ€ä½³æ¨¡å‹ï¼‰
            architecture_info: æ¶æ„ä¿¡æ¯
            
        Returns:
            ä¿å­˜çš„æ£€æŸ¥ç‚¹è·¯å¾„
        """
        checkpoint_filename = f"checkpoint_epoch_{epoch:04d}.pth"
        checkpoint_path = self.models_dir / checkpoint_filename
        
        # å®Œæ•´çš„æ£€æŸ¥ç‚¹æ•°æ®
        checkpoint_data = {
            # åŸºæœ¬ä¿¡æ¯
            'epoch': epoch,
            'session_id': self.training_session_id,
            'timestamp': datetime.now().isoformat(),
            'performance_metric': performance_metric,
            
            # æ¨¡å‹çŠ¶æ€
            'model_state': model_state,
            'optimizer_states': optimizer_states,
            'scheduler_states': scheduler_states,
            
            # æ¡†æ¶çŠ¶æ€
            'framework_state': framework_state,
            'training_stats': training_stats,
            'architecture_info': architecture_info,
            
            # å…ƒä¿¡æ¯
            'checkpoint_version': '2.0',
            'pytorch_version': torch.__version__,
            'device_info': self._get_device_info()
        }
        
        try:
            # ä¿å­˜æ£€æŸ¥ç‚¹
            torch.save(checkpoint_data, checkpoint_path)
            
            # æ›´æ–°å†å²è®°å½•
            checkpoint_record = {
                'epoch': epoch,
                'path': str(checkpoint_path),
                'performance_metric': performance_metric,
                'timestamp': checkpoint_data['timestamp'],
                'session_id': self.training_session_id,
                'size_mb': checkpoint_path.stat().st_size / (1024 * 1024),
                'architecture_info': architecture_info
            }
            
            self.checkpoint_history.append(checkpoint_record)
            self.current_checkpoint = checkpoint_record
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ€§èƒ½
            if performance_metric > self.best_performance:
                self.best_performance = performance_metric
                self.best_checkpoint_path = str(checkpoint_path)
                self._save_best_model(checkpoint_data)
                logger.info(f"ğŸ† New best performance: {performance_metric:.4f}")
            
            # æ¸…ç†æ—§æ£€æŸ¥ç‚¹
            self._cleanup_old_checkpoints()
            
            # ä¿å­˜å…ƒæ•°æ®
            self._save_metadata()
            
            logger.info(f"ğŸ’¾ Checkpoint saved: epoch {epoch}, performance {performance_metric:.4f}")
            return str(checkpoint_path)
            
        except Exception as e:
            logger.error(f"âŒ Failed to save checkpoint: {e}")
            raise e
    
    def _save_best_model(self, checkpoint_data: Dict):
        """ä¿å­˜æœ€ä½³æ¨¡å‹å‰¯æœ¬"""
        best_path = self.best_dir / "best_model.pth"
        try:
            torch.save(checkpoint_data, best_path)
            
            # ä¿å­˜è½»é‡çº§æœ€ä½³æ¨¡å‹ï¼ˆåªåŒ…å«æ¨¡å‹æƒé‡ï¼‰
            lightweight_best = {
                'model_state': checkpoint_data['model_state'],
                'epoch': checkpoint_data['epoch'],
                'performance_metric': checkpoint_data['performance_metric'],
                'architecture_info': checkpoint_data.get('architecture_info'),
                'timestamp': checkpoint_data['timestamp']
            }
            
            lightweight_path = self.best_dir / "best_model_weights_only.pth"
            torch.save(lightweight_best, lightweight_path)
            
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to save best model: {e}")
    
    def load_latest_checkpoint(self) -> Optional[Dict]:
        """åŠ è½½æœ€æ–°çš„æ£€æŸ¥ç‚¹"""
        if not self.checkpoint_history:
            logger.info("ğŸ“‚ No checkpoint history found")
            return None
        
        # æŒ‰epochæ’åºï¼Œè·å–æœ€æ–°çš„
        latest_record = max(self.checkpoint_history, key=lambda x: x['epoch'])
        return self.load_checkpoint(latest_record['path'])
    
    def load_best_checkpoint(self) -> Optional[Dict]:
        """åŠ è½½æœ€ä½³æ€§èƒ½çš„æ£€æŸ¥ç‚¹"""
        if not self.best_checkpoint_path:
            logger.info("ğŸ† No best checkpoint found")
            return None
        
        return self.load_checkpoint(self.best_checkpoint_path)
    
    def load_checkpoint(self, checkpoint_path: str) -> Optional[Dict]:
        """åŠ è½½æŒ‡å®šçš„æ£€æŸ¥ç‚¹"""
        checkpoint_path = Path(checkpoint_path)
        
        if not checkpoint_path.exists():
            logger.error(f"âŒ Checkpoint not found: {checkpoint_path}")
            return None
        
        try:
            # PyTorch 2.6+ requires weights_only=False for complex checkpoints
            checkpoint_data = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
            
            logger.info(f"ğŸ“‚ Loaded checkpoint from epoch {checkpoint_data['epoch']}")
            logger.info(f"   Performance: {checkpoint_data['performance_metric']:.4f}")
            logger.info(f"   Timestamp: {checkpoint_data['timestamp']}")
            
            return checkpoint_data
            
        except Exception as e:
            logger.error(f"âŒ Failed to load checkpoint: {e}")
            return None
    
    def resume_training(self, model, optimizers: Dict, schedulers: Optional[Dict],
                       framework, preferred_checkpoint: str = "latest") -> Tuple[int, Dict]:
        """
        æ¢å¤è®­ç»ƒçŠ¶æ€
        
        Args:
            model: è¦æ¢å¤çš„æ¨¡å‹
            optimizers: ä¼˜åŒ–å™¨å­—å…¸ {'weight': optimizer1, 'arch': optimizer2}
            schedulers: å­¦ä¹ ç‡è°ƒåº¦å™¨å­—å…¸
            framework: ASO-SEæ¡†æ¶å®ä¾‹
            preferred_checkpoint: 'latest', 'best', æˆ–å…·ä½“è·¯å¾„
            
        Returns:
            (resume_epoch, training_stats)
        """
        logger.info(f"ğŸ”„ Attempting to resume training from {preferred_checkpoint} checkpoint...")
        
        # é€‰æ‹©æ£€æŸ¥ç‚¹
        if preferred_checkpoint == "latest":
            checkpoint_data = self.load_latest_checkpoint()
        elif preferred_checkpoint == "best":
            checkpoint_data = self.load_best_checkpoint()
        else:
            checkpoint_data = self.load_checkpoint(preferred_checkpoint)
        
        if checkpoint_data is None:
            logger.info("ğŸ†• No checkpoint found, starting fresh training")
            return 0, {}
        
        try:
            # æ¢å¤æ¨¡å‹çŠ¶æ€
            model.load_state_dict(checkpoint_data['model_state'])
            logger.info("âœ… Model state restored")
            
            # æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€
            optimizer_states = checkpoint_data.get('optimizer_states', {})
            for name, optimizer in optimizers.items():
                if name in optimizer_states:
                    optimizer.load_state_dict(optimizer_states[name])
                    logger.info(f"âœ… {name} optimizer state restored")
            
            # æ¢å¤å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€
            if schedulers and checkpoint_data.get('scheduler_states'):
                scheduler_states = checkpoint_data['scheduler_states']
                for name, scheduler in schedulers.items():
                    if name in scheduler_states:
                        scheduler.load_state_dict(scheduler_states[name])
                        logger.info(f"âœ… {name} scheduler state restored")
            
            # æ¢å¤ASO-SEæ¡†æ¶çŠ¶æ€
            framework_state = checkpoint_data.get('framework_state', {})
            if hasattr(framework, 'load_state'):
                framework.load_state(framework_state)
                logger.info("âœ… ASO-SE framework state restored")
            
            # æ¢å¤è®­ç»ƒç»Ÿè®¡
            training_stats = checkpoint_data.get('training_stats', {})
            resume_epoch = checkpoint_data['epoch'] + 1
            
            logger.info(f"ğŸ¯ Training resumed from epoch {resume_epoch}")
            logger.info(f"   Previous performance: {checkpoint_data['performance_metric']:.4f}")
            
            return resume_epoch, training_stats
            
        except Exception as e:
            logger.error(f"âŒ Failed to resume training: {e}")
            logger.info("ğŸ†• Starting fresh training due to resume failure")
            return 0, {}
    
    def _cleanup_old_checkpoints(self):
        """æ¸…ç†æ—§çš„æ£€æŸ¥ç‚¹æ–‡ä»¶"""
        if len(self.checkpoint_history) <= self.max_checkpoints:
            return
        
        # æŒ‰æ€§èƒ½æ’åºï¼Œä¿ç•™æœ€ä½³çš„checkpoints
        sorted_checkpoints = sorted(
            self.checkpoint_history, 
            key=lambda x: x['performance_metric'], 
            reverse=True
        )
        
        # ä¿ç•™æœ€ä½³çš„æ£€æŸ¥ç‚¹
        to_keep = sorted_checkpoints[:self.max_checkpoints]
        to_remove = [cp for cp in self.checkpoint_history if cp not in to_keep]
        
        for checkpoint in to_remove:
            try:
                checkpoint_path = Path(checkpoint['path'])
                if checkpoint_path.exists():
                    checkpoint_path.unlink()
                    logger.debug(f"ğŸ—‘ï¸ Removed old checkpoint: {checkpoint_path.name}")
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to remove checkpoint: {e}")
        
        # æ›´æ–°å†å²è®°å½•
        self.checkpoint_history = to_keep
    
    def _get_device_info(self) -> Dict:
        """è·å–è®¾å¤‡ä¿¡æ¯"""
        device_info = {'cpu_count': os.cpu_count()}
        
        if torch.cuda.is_available():
            device_info.update({
                'cuda_available': True,
                'cuda_device_count': torch.cuda.device_count(),
                'cuda_current_device': torch.cuda.current_device(),
                'cuda_device_name': torch.cuda.get_device_name(),
                'cuda_memory_total': torch.cuda.get_device_properties(0).total_memory
            })
        else:
            device_info['cuda_available'] = False
        
        return device_info
    
    def get_training_progress(self) -> Dict:
        """è·å–è®­ç»ƒè¿›åº¦ä¿¡æ¯"""
        if not self.checkpoint_history:
            return {
                'total_epochs': 0,
                'best_performance': 0.0,
                'total_checkpoints': 0,
                'latest_epoch': 0,
                'progress_trend': []
            }
        
        # æ€§èƒ½è¶‹åŠ¿
        progress_trend = [
            {
                'epoch': cp['epoch'],
                'performance': cp['performance_metric'],
                'timestamp': cp['timestamp']
            }
            for cp in sorted(self.checkpoint_history, key=lambda x: x['epoch'])
        ]
        
        latest_checkpoint = max(self.checkpoint_history, key=lambda x: x['epoch'])
        
        return {
            'total_epochs': latest_checkpoint['epoch'],
            'best_performance': self.best_performance,
            'total_checkpoints': len(self.checkpoint_history),
            'latest_epoch': latest_checkpoint['epoch'],
            'latest_performance': latest_checkpoint['performance_metric'],
            'progress_trend': progress_trend,
            'experiment_name': self.experiment_name,
            'checkpoint_directory': str(self.experiment_dir)
        }
    
    def create_training_report(self) -> str:
        """åˆ›å»ºè®­ç»ƒæŠ¥å‘Š"""
        progress = self.get_training_progress()
        
        report = f"""
# ASO-SE Training Report

## Experiment Information
- **Experiment Name**: {self.experiment_name}
- **Session ID**: {self.training_session_id}
- **Checkpoint Directory**: {self.experiment_dir}

## Training Progress
- **Total Epochs**: {progress['total_epochs']}
- **Total Checkpoints**: {progress['total_checkpoints']}
- **Best Performance**: {progress['best_performance']:.4f}
- **Latest Performance**: {progress.get('latest_performance', 0.0):.4f}

## Architecture Evolution
"""
        
        # æ·»åŠ æ¶æ„æ¼”è¿›ä¿¡æ¯
        arch_evolution = []
        for cp in sorted(self.checkpoint_history, key=lambda x: x['epoch']):
            if cp.get('architecture_info'):
                arch_info = cp['architecture_info']
                arch_evolution.append(f"- **Epoch {cp['epoch']}**: {arch_info}")
        
        if arch_evolution:
            report += "\n".join(arch_evolution)
        else:
            report += "- No architecture evolution recorded"
        
        report += f"""

## Performance Trend
"""
        
        # æ·»åŠ æ€§èƒ½è¶‹åŠ¿
        for trend in progress['progress_trend'][-10:]:  # æœ€è¿‘10ä¸ªæ£€æŸ¥ç‚¹
            report += f"- **Epoch {trend['epoch']}**: {trend['performance']:.4f}\n"
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = self.logs_dir / f"training_report_{self.training_session_id}.md"
        with open(report_path, 'w') as f:
            f.write(report)
        
        logger.info(f"ğŸ“Š Training report saved: {report_path}")
        return str(report_path)
    
    def should_auto_save(self, epoch: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è‡ªåŠ¨ä¿å­˜"""
        return epoch % self.auto_save_interval == 0

# å…¨å±€æ£€æŸ¥ç‚¹ç®¡ç†å™¨å®ä¾‹
_global_checkpoint_manager = None

def get_checkpoint_manager(checkpoint_dir: str = "./checkpoints", 
                          experiment_name: str = "aso_se_training") -> CheckpointManager:
    """è·å–å…¨å±€æ£€æŸ¥ç‚¹ç®¡ç†å™¨"""
    global _global_checkpoint_manager
    
    if _global_checkpoint_manager is None:
        _global_checkpoint_manager = CheckpointManager(checkpoint_dir, experiment_name)
    
    return _global_checkpoint_manager

def reset_checkpoint_manager():
    """é‡ç½®å…¨å±€æ£€æŸ¥ç‚¹ç®¡ç†å™¨"""
    global _global_checkpoint_manager
    _global_checkpoint_manager = None

def test_checkpoint_manager():
    """æµ‹è¯•æ£€æŸ¥ç‚¹ç®¡ç†å™¨åŠŸèƒ½"""
    print("ğŸ§ª Testing Checkpoint Manager...")
    
    # åˆ›å»ºä¸´æ—¶æµ‹è¯•ç›®å½•
    import tempfile
    with tempfile.TemporaryDirectory() as temp_dir:
        cm = CheckpointManager(temp_dir, "test_experiment")
        
        # æ¨¡æ‹Ÿæ¨¡å‹å’Œä¼˜åŒ–å™¨
        import torch.nn as nn
        model = nn.Linear(10, 1)
        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
        
        # æµ‹è¯•ä¿å­˜æ£€æŸ¥ç‚¹
        model_state = model.state_dict()
        optimizer_states = {'weight': optimizer.state_dict()}
        training_stats = {'loss': 0.5, 'accuracy': 85.0}
        framework_state = {'current_cycle': 1, 'current_phase': 'warmup'}
        
        checkpoint_path = cm.save_checkpoint(
            epoch=10,
            model_state=model_state,
            optimizer_states=optimizer_states,
            scheduler_states=None,
            training_stats=training_stats,
            framework_state=framework_state,
            performance_metric=85.0,
            architecture_info={'mutation_count': 2}
        )
        
        print(f"âœ… Checkpoint saved: {checkpoint_path}")
        
        # æµ‹è¯•åŠ è½½æ£€æŸ¥ç‚¹
        loaded_data = cm.load_checkpoint(checkpoint_path)
        assert loaded_data is not None
        assert loaded_data['epoch'] == 10
        assert loaded_data['performance_metric'] == 85.0
        
        print(f"âœ… Checkpoint loaded successfully")
        
        # æµ‹è¯•è¿›åº¦æŠ¥å‘Š
        progress = cm.get_training_progress()
        assert progress['total_epochs'] == 10
        assert progress['best_performance'] == 85.0
        
        print(f"âœ… Progress tracking works")
        
        # æµ‹è¯•è®­ç»ƒæŠ¥å‘Š
        report_path = cm.create_training_report()
        assert os.path.exists(report_path)
        
        print(f"âœ… Training report created: {report_path}")
    
    print("ğŸ‰ Checkpoint Manager tests passed!")

if __name__ == "__main__":
    test_checkpoint_manager() 