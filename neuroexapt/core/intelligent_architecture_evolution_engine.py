"""
æ™ºèƒ½æ¶æ„è¿›åŒ–å¼•æ“
æ•´åˆç“¶é¢ˆæ£€æµ‹ã€å˜å¼‚è§„åˆ’å’Œå‚æ•°è¿ç§»çš„å®Œæ•´æ¶æ„è¿›åŒ–ç³»ç»Ÿ
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Tuple, Optional, Any, Callable
import logging
from dataclasses import dataclass
from enum import Enum
import time
import copy

from .intelligent_bottleneck_detector import IntelligentBottleneckDetector, BottleneckReport
from .intelligent_mutation_planner import IntelligentMutationPlanner, MutationPlan
from .advanced_net2net_transfer import AdvancedNet2NetTransfer

logger = logging.getLogger(__name__)


@dataclass
class EvolutionConfig:
    """è¿›åŒ–é…ç½®"""
    # æ£€æµ‹å‚æ•°
    confidence_threshold: float = 0.7
    max_mutations_per_iteration: int = 3
    
    # è¿ç§»å‚æ•°
    risk_tolerance: float = 0.7
    preserve_function: bool = True
    
    # è¿›åŒ–å‚æ•°
    max_iterations: int = 10
    patience: int = 3  # è¿ç»­æ— æ”¹è¿›çš„å®¹å¿è½®æ•°
    min_improvement: float = 0.01  # æœ€å°æ”¹è¿›é˜ˆå€¼
    
    # ä»»åŠ¡å‚æ•°
    task_type: str = 'vision'  # 'vision', 'nlp', 'graph'
    
    # è¯„ä¼°å‚æ•°
    evaluation_samples: int = 1000
    evaluation_metric: str = 'accuracy'  # 'accuracy', 'loss', 'f1'


@dataclass
class EvolutionIteration:
    """å•æ¬¡è¿›åŒ–è¿­ä»£ç»“æœ"""
    iteration: int
    bottleneck_reports: List[BottleneckReport]
    mutation_plans: List[MutationPlan]
    transfer_reports: List[Dict[str, Any]]
    
    # æ€§èƒ½æŒ‡æ ‡
    performance_before: float
    performance_after: float
    improvement: float
    
    # æ¨¡å‹å¤æ‚åº¦
    parameters_before: int
    parameters_after: int
    parameter_growth: float
    
    # æ—¶é—´å¼€é”€
    detection_time: float
    planning_time: float
    transfer_time: float
    total_time: float


class IntelligentArchitectureEvolutionEngine:
    """
    æ™ºèƒ½æ¶æ„è¿›åŒ–å¼•æ“
    
    æ ¸å¿ƒç†å¿µï¼š
    1. ç§‘å­¦çš„è¿›åŒ–æµç¨‹ï¼šæ£€æµ‹ -> è§„åˆ’ -> è¿ç§» -> è¯„ä¼° -> è¿­ä»£
    2. æ™ºèƒ½çš„å†³ç­–æœºåˆ¶ï¼šåŸºäºäº’ä¿¡æ¯å’Œè´å¶æ–¯ä¸ç¡®å®šæ€§çš„ç²¾ç¡®å®šä½
    3. ç¨³å¥çš„å‚æ•°è¿ç§»ï¼šä¿è¯åŠŸèƒ½ç­‰ä»·æ€§å’Œè®­ç»ƒç¨³å®šæ€§
    4. è‡ªé€‚åº”çš„è¿›åŒ–ç­–ç•¥ï¼šæ ¹æ®æ”¹è¿›æ•ˆæœåŠ¨æ€è°ƒæ•´ç­–ç•¥
    """
    
    def __init__(self, config: EvolutionConfig = None):
        self.config = config or EvolutionConfig()
        
        # æ ¸å¿ƒç»„ä»¶
        self.bottleneck_detector = IntelligentBottleneckDetector()
        self.mutation_planner = IntelligentMutationPlanner()
        self.transfer_engine = AdvancedNet2NetTransfer()
        
        # è¿›åŒ–å†å²
        self.evolution_history: List[EvolutionIteration] = []
        self.best_model = None
        self.best_performance = float('-inf')
        
        # æ€§èƒ½è·Ÿè¸ª
        self.performance_plateau_count = 0
        self.convergence_detected = False
        
    def evolve(self,
              model: nn.Module,
              data_loader,
              evaluation_fn: Callable[[nn.Module], float],
              feature_extractor_fn: Optional[Callable] = None,
              gradient_extractor_fn: Optional[Callable] = None) -> Tuple[nn.Module, List[EvolutionIteration]]:
        """
        æ‰§è¡Œæ™ºèƒ½æ¶æ„è¿›åŒ–
        
        Args:
            model: åˆå§‹æ¨¡å‹
            data_loader: æ•°æ®åŠ è½½å™¨
            evaluation_fn: è¯„ä¼°å‡½æ•°ï¼Œè¿”å›æ€§èƒ½åˆ†æ•°
            feature_extractor_fn: ç‰¹å¾æå–å‡½æ•°ï¼Œè¿”å›å„å±‚ç‰¹å¾
            gradient_extractor_fn: æ¢¯åº¦æå–å‡½æ•°ï¼Œè¿”å›å„å±‚æ¢¯åº¦
            
        Returns:
            (æœ€ä½³æ¨¡å‹, è¿›åŒ–å†å²)
        """
        logger.info("ğŸš€ å¯åŠ¨æ™ºèƒ½æ¶æ„è¿›åŒ–")
        
        current_model = copy.deepcopy(model)
        current_performance = evaluation_fn(current_model)
        
        self.best_model = copy.deepcopy(current_model)
        self.best_performance = current_performance
        
        logger.info(f"åˆå§‹æ€§èƒ½: {current_performance:.4f}")
        
        # è¿­ä»£è¿›åŒ–
        for iteration in range(self.config.max_iterations):
            if self.convergence_detected:
                logger.info(f"åœ¨ç¬¬{iteration}è½®æ£€æµ‹åˆ°æ”¶æ•›ï¼Œåœæ­¢è¿›åŒ–")
                break
                
            logger.info(f"\nğŸ”„ å¼€å§‹ç¬¬{iteration + 1}è½®è¿›åŒ–")
            
            # æ‰§è¡Œå•æ¬¡è¿›åŒ–è¿­ä»£
            evolution_result = self._execute_evolution_iteration(
                current_model, data_loader, evaluation_fn,
                feature_extractor_fn, gradient_extractor_fn, iteration
            )
            
            self.evolution_history.append(evolution_result)
            
            # æ›´æ–°å½“å‰æ¨¡å‹å’Œæ€§èƒ½
            if evolution_result.improvement > self.config.min_improvement:
                current_model = self._load_evolved_model(current_model, evolution_result)
                current_performance = evolution_result.performance_after
                
                # æ›´æ–°æœ€ä½³æ¨¡å‹
                if current_performance > self.best_performance:
                    self.best_model = copy.deepcopy(current_model)
                    self.best_performance = current_performance
                    self.performance_plateau_count = 0
                    logger.info(f"ğŸ‰ å‘ç°æ›´ä½³æ¨¡å‹ï¼Œæ€§èƒ½: {current_performance:.4f}")
                else:
                    self.performance_plateau_count += 1
            else:
                self.performance_plateau_count += 1
                logger.info(f"æœ¬è½®æ”¹è¿›ä¸è¶³ ({evolution_result.improvement:.4f})")
            
            # æ£€æŸ¥æ”¶æ•›
            if self.performance_plateau_count >= self.config.patience:
                self.convergence_detected = True
                logger.info("è¾¾åˆ°æ€§èƒ½å¹³å°æœŸï¼Œæ ‡è®°æ”¶æ•›")
        
        logger.info(f"ğŸ è¿›åŒ–å®Œæˆï¼Œæœ€ä½³æ€§èƒ½: {self.best_performance:.4f}")
        return self.best_model, self.evolution_history
    
    def _execute_evolution_iteration(self,
                                   model: nn.Module,
                                   data_loader,
                                   evaluation_fn: Callable,
                                   feature_extractor_fn: Optional[Callable],
                                   gradient_extractor_fn: Optional[Callable],
                                   iteration: int) -> EvolutionIteration:
        """æ‰§è¡Œå•æ¬¡è¿›åŒ–è¿­ä»£"""
        
        start_time = time.time()
        
        # 1. æå–ç‰¹å¾å’Œæ¢¯åº¦
        feature_dict, labels, gradient_dict = self._extract_features_and_gradients(
            model, data_loader, feature_extractor_fn, gradient_extractor_fn
        )
        
        # 2. ç“¶é¢ˆæ£€æµ‹
        detection_start = time.time()
        bottleneck_reports = self.bottleneck_detector.detect_bottlenecks(
            model=model,
            feature_dict=feature_dict,
            labels=labels,
            gradient_dict=gradient_dict,
            num_classes=self._infer_num_classes(labels),
            confidence_threshold=self.config.confidence_threshold
        )
        detection_time = time.time() - detection_start
        
        logger.info(f"æ£€æµ‹åˆ° {len(bottleneck_reports)} ä¸ªç“¶é¢ˆï¼Œè€—æ—¶ {detection_time:.2f}s")
        
        # 3. å˜å¼‚è§„åˆ’
        planning_start = time.time()
        mutation_plans = self.mutation_planner.plan_mutations(
            bottleneck_reports=bottleneck_reports,
            model=model,
            task_type=self.config.task_type,
            max_mutations=self.config.max_mutations_per_iteration,
            risk_tolerance=self.config.risk_tolerance
        )
        planning_time = time.time() - planning_start
        
        logger.info(f"ç”Ÿæˆ {len(mutation_plans)} ä¸ªå˜å¼‚è®¡åˆ’ï¼Œè€—æ—¶ {planning_time:.2f}s")
        
        # 4. å‚æ•°è¿ç§»
        transfer_start = time.time()
        evolved_model = copy.deepcopy(model)
        transfer_reports = []
        
        if mutation_plans:
            evolved_model, transfer_reports = self.transfer_engine.batch_transfer(
                evolved_model, mutation_plans
            )
        
        transfer_time = time.time() - transfer_start
        
        logger.info(f"æ‰§è¡Œå‚æ•°è¿ç§»ï¼Œè€—æ—¶ {transfer_time:.2f}s")
        
        # 5. æ€§èƒ½è¯„ä¼°
        performance_before = evaluation_fn(model)
        performance_after = evaluation_fn(evolved_model) if mutation_plans else performance_before
        improvement = performance_after - performance_before
        
        # 6. å¤æ‚åº¦åˆ†æ
        params_before = sum(p.numel() for p in model.parameters())
        params_after = sum(p.numel() for p in evolved_model.parameters())
        param_growth = (params_after - params_before) / params_before if params_before > 0 else 0.0
        
        total_time = time.time() - start_time
        
        logger.info(f"æ€§èƒ½å˜åŒ–: {performance_before:.4f} -> {performance_after:.4f} "
                   f"(+{improvement:.4f}), å‚æ•°å¢é•¿: {param_growth:.2%}")
        
        return EvolutionIteration(
            iteration=iteration,
            bottleneck_reports=bottleneck_reports,
            mutation_plans=mutation_plans,
            transfer_reports=transfer_reports,
            performance_before=performance_before,
            performance_after=performance_after,
            improvement=improvement,
            parameters_before=params_before,
            parameters_after=params_after,
            parameter_growth=param_growth,
            detection_time=detection_time,
            planning_time=planning_time,
            transfer_time=transfer_time,
            total_time=total_time
        )
    
    def _extract_features_and_gradients(self,
                                      model: nn.Module,
                                      data_loader,
                                      feature_extractor_fn: Optional[Callable],
                                      gradient_extractor_fn: Optional[Callable]) -> Tuple[Dict, torch.Tensor, Dict]:
        """æå–ç‰¹å¾å’Œæ¢¯åº¦"""
        
        if feature_extractor_fn:
            # ä½¿ç”¨è‡ªå®šä¹‰ç‰¹å¾æå–å‡½æ•°
            feature_dict, labels = feature_extractor_fn(model, data_loader)
        else:
            # ä½¿ç”¨é»˜è®¤ç‰¹å¾æå–
            feature_dict, labels = self._default_feature_extraction(model, data_loader)
        
        gradient_dict = {}
        if gradient_extractor_fn:
            gradient_dict = gradient_extractor_fn(model, data_loader)
        
        return feature_dict, labels, gradient_dict
    
    def _default_feature_extraction(self, model: nn.Module, data_loader) -> Tuple[Dict[str, torch.Tensor], torch.Tensor]:
        """é»˜è®¤ç‰¹å¾æå–"""
        feature_dict = {}
        all_labels = []
        
        # æ³¨å†Œhookæ¥æ”¶é›†ç‰¹å¾
        def get_hook(name):
            def hook(module, input, output):
                if name not in feature_dict:
                    feature_dict[name] = []
                feature_dict[name].append(output.detach().cpu())
            return hook
        
        # ä¸ºä¸»è¦å±‚æ³¨å†Œhook
        hooks = []
        for name, module in model.named_modules():
            if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d)):
                hook = module.register_forward_hook(get_hook(name))
                hooks.append(hook)
        
        model.eval()
        with torch.no_grad():
            for batch_idx, (data, target) in enumerate(data_loader):
                if batch_idx >= 3:  # åªå¤„ç†å‡ ä¸ªæ‰¹æ¬¡
                    break
                    
                data = data.to(next(model.parameters()).device)
                _ = model(data)
                all_labels.append(target)
        
        # æ¸…ç†hooks
        for hook in hooks:
            hook.remove()
        
        # åˆå¹¶ç‰¹å¾
        for name in feature_dict:
            if feature_dict[name]:
                feature_dict[name] = torch.cat(feature_dict[name], dim=0)
        
        labels = torch.cat(all_labels, dim=0) if all_labels else torch.tensor([])
        
        return feature_dict, labels
    
    def _infer_num_classes(self, labels: torch.Tensor) -> Optional[int]:
        """æ¨æ–­ç±»åˆ«æ•°é‡"""
        if labels.numel() == 0:
            return None
        
        if labels.dtype in [torch.long, torch.int]:
            return int(labels.max().item()) + 1
        
        return None
    
    def _load_evolved_model(self, current_model: nn.Module, evolution_result: EvolutionIteration) -> nn.Module:
        """åŠ è½½è¿›åŒ–åçš„æ¨¡å‹"""
        # è¿™é‡Œåº”è¯¥æ ¹æ®transfer_reportsé‡æ–°æ„å»ºæ¨¡å‹
        # ç®€åŒ–å®ç°ï¼šå¦‚æœæœ‰æˆåŠŸçš„è¿ç§»ï¼Œè¿”å›æ–°æ¨¡å‹
        if evolution_result.transfer_reports and any(r.get('quality_score', 0) > 0.5 for r in evolution_result.transfer_reports):
            # é‡æ–°æ‰§è¡Œè¿ç§»ä»¥è·å¾—æ–°æ¨¡å‹
            evolved_model = copy.deepcopy(current_model)
            evolved_model, _ = self.transfer_engine.batch_transfer(evolved_model, evolution_result.mutation_plans)
            return evolved_model
        
        return current_model
    
    def get_evolution_summary(self) -> Dict[str, Any]:
        """è·å–è¿›åŒ–æ‘˜è¦"""
        if not self.evolution_history:
            return {'status': 'no_evolution', 'message': 'å°šæœªæ‰§è¡Œè¿›åŒ–'}
        
        total_iterations = len(self.evolution_history)
        successful_iterations = sum(1 for iter_result in self.evolution_history 
                                  if iter_result.improvement > self.config.min_improvement)
        
        total_improvement = self.best_performance - self.evolution_history[0].performance_before
        total_param_growth = (self.evolution_history[-1].parameters_after - 
                            self.evolution_history[0].parameters_before) / self.evolution_history[0].parameters_before
        
        avg_detection_time = np.mean([iter_result.detection_time for iter_result in self.evolution_history])
        avg_planning_time = np.mean([iter_result.planning_time for iter_result in self.evolution_history])
        avg_transfer_time = np.mean([iter_result.transfer_time for iter_result in self.evolution_history])
        
        return {
            'status': 'completed',
            'total_iterations': total_iterations,
            'successful_iterations': successful_iterations,
            'success_rate': successful_iterations / total_iterations,
            'initial_performance': self.evolution_history[0].performance_before,
            'final_performance': self.evolution_history[-1].performance_after,
            'best_performance': self.best_performance,
            'total_improvement': total_improvement,
            'total_parameter_growth': total_param_growth,
            'average_times': {
                'detection': avg_detection_time,
                'planning': avg_planning_time,
                'transfer': avg_transfer_time
            },
            'convergence_detected': self.convergence_detected
        }
    
    def visualize_evolution(self) -> str:
        """å¯è§†åŒ–è¿›åŒ–è¿‡ç¨‹"""
        if not self.evolution_history:
            return "ğŸ“Š å°šæœªæ‰§è¡Œè¿›åŒ–"
        
        visualization = "ğŸ“Š æ™ºèƒ½æ¶æ„è¿›åŒ–æŠ¥å‘Š\n" + "="*60 + "\n"
        
        # æ€»ä½“æ‘˜è¦
        summary = self.get_evolution_summary()
        visualization += f"\nğŸ¯ è¿›åŒ–æ‘˜è¦:\n"
        visualization += f"   æ€»è½®æ•°: {summary['total_iterations']}\n"
        visualization += f"   æˆåŠŸè½®æ•°: {summary['successful_iterations']} (æˆåŠŸç‡: {summary['success_rate']:.1%})\n"
        visualization += f"   æ€§èƒ½æå‡: {summary['initial_performance']:.4f} -> {summary['best_performance']:.4f} "
        visualization += f"(+{summary['total_improvement']:.4f})\n"
        visualization += f"   å‚æ•°å¢é•¿: {summary['total_parameter_growth']:.1%}\n"
        
        # è¯¦ç»†è¿­ä»£å†å²
        visualization += f"\nğŸ“ˆ è¿­ä»£å†å²:\n"
        for i, iter_result in enumerate(self.evolution_history, 1):
            status_icon = "âœ…" if iter_result.improvement > self.config.min_improvement else "â¸ï¸"
            
            visualization += f"\n{status_icon} ç¬¬{i}è½®:\n"
            visualization += f"   ç“¶é¢ˆ: {len(iter_result.bottleneck_reports)}ä¸ª | "
            visualization += f"å˜å¼‚: {len(iter_result.mutation_plans)}ä¸ª\n"
            visualization += f"   æ€§èƒ½: {iter_result.performance_before:.4f} -> {iter_result.performance_after:.4f} "
            visualization += f"({iter_result.improvement:+.4f})\n"
            visualization += f"   å‚æ•°: {iter_result.parameter_growth:+.1%} | "
            visualization += f"è€—æ—¶: {iter_result.total_time:.1f}s\n"
        
        # æ€§èƒ½è¶‹åŠ¿
        performances = [iter_result.performance_after for iter_result in self.evolution_history]
        if len(performances) > 1:
            trend = "ğŸ“ˆ ä¸Šå‡" if performances[-1] > performances[0] else "ğŸ“‰ ä¸‹é™"
            visualization += f"\n{trend} æ€§èƒ½è¶‹åŠ¿: "
            visualization += " -> ".join([f"{p:.3f}" for p in performances[:5]])
            if len(performances) > 5:
                visualization += " -> ..."
        
        return visualization
    
    def export_best_model(self, save_path: str):
        """å¯¼å‡ºæœ€ä½³æ¨¡å‹"""
        if self.best_model is not None:
            torch.save({
                'model_state_dict': self.best_model.state_dict(),
                'performance': self.best_performance,
                'evolution_history': self.evolution_history,
                'config': self.config
            }, save_path)
            logger.info(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
        else:
            logger.warning("æ— æœ€ä½³æ¨¡å‹å¯å¯¼å‡º")
    
    def clear_history(self):
        """æ¸…ç†å†å²è®°å½•"""
        self.evolution_history.clear()
        self.best_model = None
        self.best_performance = float('-inf')
        self.performance_plateau_count = 0
        self.convergence_detected = False
        
        # æ¸…ç†å­ç»„ä»¶ç¼“å­˜
        self.bottleneck_detector.clear_cache()
        
        logger.info("å·²æ¸…ç†è¿›åŒ–å†å²")