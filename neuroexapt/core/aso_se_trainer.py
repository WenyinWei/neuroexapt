"""
ASO-SE ç¨³å®šè®­ç»ƒå™¨
é‡æ–°è®¾è®¡çš„å››é˜¶æ®µè®­ç»ƒæµç¨‹
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
from tqdm import tqdm
import time
import math
from .aso_se_architecture import ProgressiveArchitectureNetwork


class StableASO_SETrainer:
    """ç¨³å®šçš„ASO-SEè®­ç»ƒå™¨"""
    
    def __init__(self, config=None):
        self.config = config or self._default_config()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.current_phase = 'warmup'
        self.phase_epochs = 0
        self.best_accuracy = 0.0
        self.training_history = []
        
        # æ•°æ®åŠ è½½å™¨
        self.train_loader = None
        self.test_loader = None
        
        # æ¨¡å‹å’Œä¼˜åŒ–å™¨
        self.network = None
        self.weight_optimizer = None
        self.arch_optimizer = None
        self.scheduler = None
        
        print(f"ğŸš€ ç¨³å®šASO-SEè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   é…ç½®: {self.config}")
    
    def _default_config(self):
        """é»˜è®¤é…ç½®"""
        return {
            'dataset': 'CIFAR-10',
            'batch_size': 128,
            'num_epochs': 100,
            'init_channels': 32,
            'init_depth': 4,
            'max_depth': 8,
            
            # å­¦ä¹ ç‡è®¾ç½®
            'weight_lr': 0.025,
            'arch_lr': 3e-4,
            'momentum': 0.9,
            'weight_decay': 3e-4,
            
            # é˜¶æ®µè®¾ç½®
            'warmup_epochs': 15,
            'search_epochs': 30,
            'growth_epochs': 35,
            'optimize_epochs': 20,
            
            # æœç´¢æ§åˆ¶
            'arch_update_freq': 5,  # æ¯5ä¸ªbatchæ›´æ–°ä¸€æ¬¡æ¶æ„
            'growth_patience': 8,   # æ€§èƒ½åœæ»8ä¸ªepochåç”Ÿé•¿
            'growth_threshold': 0.01,  # æ€§èƒ½æå‡é˜ˆå€¼
        }
    
    def setup_data(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        if self.config['dataset'] == 'CIFAR-10':
            # æ•°æ®å¢å¼º
            train_transform = transforms.Compose([
                transforms.RandomCrop(32, padding=4),
                transforms.RandomHorizontalFlip(),
                transforms.ToTensor(),
                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
            ])
            
            test_transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
            ])
            
            train_dataset = torchvision.datasets.CIFAR10(
                root='./data', train=True, download=True, transform=train_transform
            )
            test_dataset = torchvision.datasets.CIFAR10(
                root='./data', train=False, download=True, transform=test_transform
            )
            
            self.train_loader = DataLoader(
                train_dataset, batch_size=self.config['batch_size'], 
                shuffle=True, num_workers=2, pin_memory=True
            )
            self.test_loader = DataLoader(
                test_dataset, batch_size=self.config['batch_size'], 
                shuffle=False, num_workers=2, pin_memory=True
            )
            
            print(f"ğŸ“Š CIFAR-10æ•°æ®åŠ è½½å®Œæˆ: è®­ç»ƒé›† {len(train_dataset)}, æµ‹è¯•é›† {len(test_dataset)}")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {self.config['dataset']}")
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹"""
        self.network = ProgressiveArchitectureNetwork(
            input_channels=3,
            init_channels=self.config['init_channels'],
            num_classes=10,
            init_depth=self.config['init_depth'],
            max_depth=self.config['max_depth']
        ).to(self.device)
        
        # ä½¿ç”¨skip_biasedåˆå§‹åŒ–
        self.network.arch_manager.init_strategy = 'skip_biased'
        
        print(f"ğŸ—ï¸ ç½‘ç»œåˆå§‹åŒ–å®Œæˆ:")
        info = self.network.get_architecture_info()
        print(f"   æ·±åº¦: {info['depth']}")
        print(f"   å‚æ•°é‡: {info['parameters']:,}")
        print(f"   åˆå§‹æ¶æ„: {info['architecture']}")
    
    def setup_optimizers(self):
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        # åˆ†ç¦»æƒé‡å‚æ•°å’Œæ¶æ„å‚æ•°
        weight_params = []
        arch_params = []
        
        for name, param in self.network.named_parameters():
            if 'arch_manager.alpha' in name:
                arch_params.append(param)
            else:
                weight_params.append(param)
        
        # æƒé‡ä¼˜åŒ–å™¨
        self.weight_optimizer = optim.SGD(
            weight_params,
            lr=self.config['weight_lr'],
            momentum=self.config['momentum'],
            weight_decay=self.config['weight_decay']
        )
        
        # æ¶æ„ä¼˜åŒ–å™¨
        self.arch_optimizer = optim.Adam(
            arch_params,
            lr=self.config['arch_lr'],
            betas=(0.5, 0.999),
            weight_decay=1e-3
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.weight_optimizer,
            T_max=self.config['num_epochs'],
            eta_min=1e-4
        )
        
        print(f"âš™ï¸ ä¼˜åŒ–å™¨è®¾ç½®å®Œæˆ:")
        print(f"   æƒé‡å‚æ•°: {len(weight_params)}")
        print(f"   æ¶æ„å‚æ•°: {len(arch_params)}")
    
    def train_epoch(self):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.network.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        pbar = tqdm(self.train_loader, desc=f'Epoch {self.current_epoch+1}')
        
        for batch_idx, (data, targets) in enumerate(pbar):
            data, targets = data.to(self.device), targets.to(self.device)
            
            # æ ¹æ®è®­ç»ƒé˜¶æ®µé€‰æ‹©ä¼˜åŒ–ç­–ç•¥
            if self.current_phase == 'warmup':
                # warmupé˜¶æ®µåªä¼˜åŒ–æƒé‡
                self._optimize_weights(data, targets)
            
            elif self.current_phase in ['search', 'growth']:
                # æœç´¢å’Œç”Ÿé•¿é˜¶æ®µäº¤æ›¿ä¼˜åŒ–
                if batch_idx % self.config['arch_update_freq'] == 0:
                    # ä¼˜åŒ–æ¶æ„å‚æ•°
                    self._optimize_architecture(data, targets)
                else:
                    # ä¼˜åŒ–æƒé‡å‚æ•°
                    self._optimize_weights(data, targets)
            
            elif self.current_phase == 'optimize':
                # ä¼˜åŒ–é˜¶æ®µåªä¼˜åŒ–æƒé‡
                self._optimize_weights(data, targets)
            
            # ç»Ÿè®¡
            with torch.no_grad():
                outputs = self.network(data)
                loss = F.cross_entropy(outputs, targets)
                total_loss += loss.item()
                
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
            
            # æ›´æ–°è¿›åº¦æ¡
            accuracy = 100. * correct / total
            arch_info = self.network.get_architecture_info()
            
            pbar.set_postfix({
                'Loss': f'{total_loss/(batch_idx+1):.3f}',
                'Acc': f'{accuracy:.2f}%',
                'Phase': self.current_phase,
                'Temp': f'{arch_info["temperature"]:.3f}',
                'Entropy': f'{arch_info["entropy"]:.2f}'
            })
        
        return total_loss / len(self.train_loader), accuracy
    
    def _optimize_weights(self, data, targets):
        """ä¼˜åŒ–æƒé‡å‚æ•°"""
        self.weight_optimizer.zero_grad()
        outputs = self.network(data)
        loss = F.cross_entropy(outputs, targets)
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.network.parameters(), 5.0)
        
        self.weight_optimizer.step()
        return loss.item()
    
    def _optimize_architecture(self, data, targets):
        """ä¼˜åŒ–æ¶æ„å‚æ•°"""
        self.arch_optimizer.zero_grad()
        outputs = self.network(data)
        loss = F.cross_entropy(outputs, targets)
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        arch_params = [p for name, p in self.network.named_parameters() if 'arch_manager.alpha' in name]
        torch.nn.utils.clip_grad_norm_(arch_params, 5.0)
        
        self.arch_optimizer.step()
        
        # æ¸©åº¦é€€ç«
        self.network.arch_manager.anneal_temperature()
        
        return loss.item()
    
    def evaluate(self):
        """è¯„ä¼°æ¨¡å‹"""
        self.network.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, targets in self.test_loader:
                data, targets = data.to(self.device), targets.to(self.device)
                outputs = self.network(data)
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
        
        accuracy = 100. * correct / total
        return accuracy
    
    def update_phase(self):
        """æ›´æ–°è®­ç»ƒé˜¶æ®µ"""
        self.phase_epochs += 1
        old_phase = self.current_phase
        
        # é˜¶æ®µè½¬æ¢é€»è¾‘
        if (self.current_phase == 'warmup' and 
            self.phase_epochs >= self.config['warmup_epochs']):
            self.current_phase = 'search'
            self.phase_epochs = 0
            self.network.set_training_phase('search')
            self.network.arch_manager.smooth_transition_to_search()
            print(f"ğŸ”„ è¿›å…¥æœç´¢é˜¶æ®µï¼Œæ¸©åº¦: {self.network.arch_manager.sampler.tau:.3f}")
        
        elif (self.current_phase == 'search' and 
              self.phase_epochs >= self.config['search_epochs']):
            self.current_phase = 'growth'
            self.phase_epochs = 0
            self.network.set_training_phase('growth')
            print(f"ğŸ”„ è¿›å…¥ç”Ÿé•¿é˜¶æ®µ")
        
        elif (self.current_phase == 'growth' and 
              self.phase_epochs >= self.config['growth_epochs']):
            self.current_phase = 'optimize'
            self.phase_epochs = 0
            self.network.set_training_phase('optimize')
            print(f"ğŸ”„ è¿›å…¥ä¼˜åŒ–é˜¶æ®µ")
        
        # å¦‚æœé˜¶æ®µå‘ç”Ÿå˜åŒ–ï¼Œæ‰“å°æ¶æ„ä¿¡æ¯
        if old_phase != self.current_phase:
            self._print_architecture_analysis()
    
    def _print_architecture_analysis(self):
        """æ‰“å°æ¶æ„åˆ†æ"""
        info = self.network.get_architecture_info()
        print(f"\nğŸ” æ¶æ„åˆ†æ:")
        print(f"   æ·±åº¦: {info['depth']}")
        print(f"   å‚æ•°é‡: {info['parameters']:,}")
        print(f"   æ¶æ„ç†µ: {info['entropy']:.3f}")
        print(f"   ç½®ä¿¡åº¦: {info['confidence']:.3f}")
        print(f"   å½“å‰æ¶æ„: {info['architecture']}")
    
    def _should_grow(self, current_accuracy):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç”Ÿé•¿"""
        if len(self.training_history) < self.config['growth_patience']:
            return False
        
        # æ£€æŸ¥æœ€è¿‘å‡ ä¸ªepochçš„æ€§èƒ½
        recent_accuracies = [h['test_acc'] for h in self.training_history[-self.config['growth_patience']:]]
        improvement = max(recent_accuracies) - min(recent_accuracies)
        
        return improvement < self.config['growth_threshold']
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"\nğŸ”§ å¼€å§‹ASO-SEè®­ç»ƒ")
        print(f"{'='*60}")
        
        # è®¾ç½®
        self.setup_data()
        self.setup_model()
        self.setup_optimizers()
        
        start_time = time.time()
        
        for epoch in range(self.config['num_epochs']):
            self.current_epoch = epoch
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch()
            
            # è¯„ä¼°
            test_acc = self.evaluate()
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            
            # è®°å½•å†å²
            epoch_info = {
                'epoch': epoch,
                'phase': self.current_phase,
                'train_loss': train_loss,
                'train_acc': train_acc,
                'test_acc': test_acc,
                'architecture': self.network.get_architecture_info()
            }
            self.training_history.append(epoch_info)
            
            # ç”Ÿé•¿æ§åˆ¶
            if (self.current_phase == 'growth' and 
                self._should_grow(test_acc) and 
                self.network.current_depth < self.network.max_depth):
                
                print(f"ğŸŒ± è§¦å‘ç½‘ç»œç”Ÿé•¿")
                self.network.grow_depth(1)
                self._update_optimizers_after_growth()
            
            # æ›´æ–°æœ€ä½³ç²¾åº¦
            if test_acc > self.best_accuracy:
                self.best_accuracy = test_acc
                self._save_checkpoint('best')
            
            # æ›´æ–°é˜¶æ®µ
            self.update_phase()
            
            # å®šæœŸæ±‡æŠ¥
            if (epoch + 1) % 5 == 0:
                elapsed = time.time() - start_time
                print(f"\nğŸ“Š Epoch {epoch+1}/{self.config['num_epochs']} | é˜¶æ®µ: {self.current_phase}")
                print(f"   è®­ç»ƒæŸå¤±: {train_loss:.4f} | è®­ç»ƒç²¾åº¦: {train_acc:.2f}%")
                print(f"   æµ‹è¯•ç²¾åº¦: {test_acc:.2f}% | æœ€ä½³: {self.best_accuracy:.2f}%")
                print(f"   è€—æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ")
                
                if self.current_phase == 'search':
                    self._print_architecture_analysis()
        
        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        print(f"   æœ€ä½³ç²¾åº¦: {self.best_accuracy:.2f}%")
        print(f"   æ€»è€—æ—¶: {total_time/60:.1f}åˆ†é’Ÿ")
        print(f"   æœ€ç»ˆæ¶æ„: {self.network.get_architecture_info()['architecture']}")
        
        return self.training_history, self.best_accuracy
    
    def _update_optimizers_after_growth(self):
        """ç”Ÿé•¿åæ›´æ–°ä¼˜åŒ–å™¨"""
        try:
            # é‡æ–°è®¾ç½®ä¼˜åŒ–å™¨ä»¥åŒ…å«æ–°å‚æ•°
            self.setup_optimizers()
            print(f"âœ… ä¼˜åŒ–å™¨å·²æ›´æ–°")
        except Exception as e:
            print(f"âš ï¸ ä¼˜åŒ–å™¨æ›´æ–°å¤±è´¥: {e}")
    
    def _save_checkpoint(self, name):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.current_epoch,
            'phase': self.current_phase,
            'model_state_dict': self.network.state_dict(),
            'optimizer_state_dict': self.weight_optimizer.state_dict(),
            'arch_optimizer_state_dict': self.arch_optimizer.state_dict(),
            'best_accuracy': self.best_accuracy,
            'config': self.config,
            'training_history': self.training_history
        }
        
        torch.save(checkpoint, f'aso_se_{name}.pth')
    
    def load_checkpoint(self, path):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(path, map_location=self.device)
        
        self.network.load_state_dict(checkpoint['model_state_dict'])
        self.weight_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.arch_optimizer.load_state_dict(checkpoint['arch_optimizer_state_dict'])
        
        self.current_epoch = checkpoint['epoch']
        self.current_phase = checkpoint['phase']
        self.best_accuracy = checkpoint['best_accuracy']
        self.training_history = checkpoint['training_history']
        
        print(f"âœ… æ£€æŸ¥ç‚¹åŠ è½½å®Œæˆ: {path}") 