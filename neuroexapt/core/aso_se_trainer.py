"""
ASO-SEè®­ç»ƒå™¨ (ASO-SE Trainer)

é‡æ„åçš„ASO-SEè®­ç»ƒå™¨ï¼ŒåŸºäºæ–°çš„ASO-SEæ¡†æ¶å®ç°ã€‚
ä¿æŒå‘åå…¼å®¹æ€§ï¼ŒåŒæ—¶æä¾›å¢å¼ºçš„åŠŸèƒ½å’Œæ›´å¥½çš„æ¶æ„è®¾è®¡ã€‚
"""

import torch
import torch.nn as nn
try:
    import torch.nn.functional as F
except ImportError:
    from torch.nn import functional as F
import numpy as np
import logging
from typing import Dict, List, Optional, Tuple, Union, Any

from .model import Network as SearchNetwork
from .evolvable_model import EvolvableNetwork
from .genotypes import Genotype, PRIMITIVES
from .aso_se_framework import ASOSEFramework, ASOSEConfig
from .function_preserving_init import FunctionPreservingInitializer
from .gumbel_softmax_explorer import GumbelSoftmaxExplorer
from .architecture_mutator import ArchitectureMutator

logger = logging.getLogger(__name__)

def _derive_genotype(alphas_normal, alphas_reduce, steps=4):
    """
    ä»è¿ç»­çš„alphaå‚æ•°å¯¼å‡ºç¦»æ•£åŸºå› å‹ï¼ˆä½¿ç”¨argmaxï¼‰
    å…¼å®¹æ—§æ¥å£çš„è¾…åŠ©å‡½æ•°
    """
    
    def _parse(weights):
        gene = []
        n = 2
        start = 0
        for i in range(steps):
            end = start + n
            W = weights[start:end].copy()
            
            # ä¸ºå½“å‰èŠ‚ç‚¹æ‰¾åˆ°æœ€å¥½çš„2æ¡è¾¹
            edges = sorted(range(i + 2), 
                         key=lambda x: -max(W[x][k] for k in range(len(W[x])) 
                                           if k != PRIMITIVES.index('none')))[:2]
            
            # ä¸ºé€‰ä¸­çš„2æ¡è¾¹å„è‡ªæ‰¾åˆ°æœ€å¥½çš„æ“ä½œ
            for j in edges:
                k_best = None
                for k in range(len(W[j])):
                    if k != PRIMITIVES.index('none'):
                        if k_best is None or W[j][k] > W[j][k_best]:
                            k_best = k
                gene.append((PRIMITIVES[k_best], j))
            start = end
            n += 1
        return gene

    gene_normal = _parse(F.softmax(alphas_normal, dim=-1).data.cpu().numpy())
    gene_reduce = _parse(F.softmax(alphas_reduce, dim=-1).data.cpu().numpy())
    
    concat = range(2, 2 + steps)
    
    genotype = Genotype(
        normal=gene_normal, normal_concat=concat,
        reduce=gene_reduce, reduce_concat=concat
    )
    return genotype

class ASOSETrainer:
    """
    é‡æ„çš„ASO-SEè®­ç»ƒå™¨
    
    åŸºäºæ–°çš„ASO-SEæ¡†æ¶ï¼Œæä¾›å››é˜¶æ®µè®­ç»ƒæµç¨‹ï¼š
    1. æƒé‡é¢„çƒ­ (W-Training)
    2. æ¶æ„å‚æ•°å­¦ä¹  (Î±-Training)  
    3. æ¶æ„çªå˜ä¸ç¨³å®š (Architecture Mutation & Stabilization)
    4. æƒé‡å†é€‚åº” (W-Retraining)
    """
    
    def __init__(self, search_model_args: Dict, model_args: Dict, training_args: Dict):
        """
        Args:
            search_model_args: æœç´¢æ¨¡å‹å‚æ•°
            model_args: å¯è¿›åŒ–æ¨¡å‹å‚æ•°
            training_args: è®­ç»ƒå‚æ•°
        """
        # 1. åˆ›å»ºæœç´¢æ¨¡å‹
        self.search_model = SearchNetwork(**search_model_args)
        
        # 2. åˆ›å»ºASO-SEé…ç½®
        self.config = self._create_config_from_args(training_args)
        
        # 3. åˆå§‹åŒ–ASO-SEæ¡†æ¶
        self.framework = ASOSEFramework(self.search_model, self.config)
        
        # 4. ä¿å­˜å‚æ•°ä»¥ä¾¿å…¼å®¹æ€§
        self.model_args = model_args
        self.training_args = training_args
        
        # 5. å‘åå…¼å®¹çš„å±æ€§
        self.criterion = nn.CrossEntropyLoss()
        self.w_optimizer = None
        self.alpha_optimizer = None
        
        # 6. å½“å‰çŠ¶æ€ï¼ˆå‘åå…¼å®¹ï¼‰
        self.current_genotype = None
        self.evolvable_model = None
        
        # 7. è®­ç»ƒç»Ÿè®¡
        self.training_stats = {
            "epoch_stats": [],
            "phase_transitions": [],
            "best_accuracy": 0.0
        }
        
        logger.info(f"ğŸš€ ASO-SE Trainer initialized with framework integration")
        logger.info(f"   Config: {self.config.total_cycles} cycles, "
                   f"warmup={self.config.warmup_epochs}, "
                   f"arch={self.config.arch_training_epochs}")
    
    def _create_config_from_args(self, training_args: Dict) -> ASOSEConfig:
        """ä»è®­ç»ƒå‚æ•°åˆ›å»ºASO-SEé…ç½®"""
        return ASOSEConfig(
            # ä»training_argsæå–å‚æ•°ï¼Œæä¾›é»˜è®¤å€¼
            warmup_epochs=int(training_args.get('warmup_epochs', 10)),
            arch_training_epochs=int(training_args.get('arch_epochs', 3)),
            weight_training_epochs=int(training_args.get('weight_epochs', 8)),
            total_cycles=int(training_args.get('total_cycles', 5)),
            
            # Gumbel-Softmaxå‚æ•°
            initial_temp=training_args.get('initial_temp', 5.0),
            min_temp=training_args.get('min_temp', 0.1),
            anneal_rate=training_args.get('temp_annealing_rate', 0.98),
            
            # æ¶æ„çªå˜å‚æ•°
            mutation_strength=training_args.get('mutation_strength', 0.3),
            mutation_frequency=training_args.get('mutation_frequency', 2),
            
            # ä¼˜åŒ–å™¨å‚æ•°
            weight_lr=training_args.get('learning_rate', 0.025),
            arch_lr=training_args.get('arch_learning_rate', 3e-4),
            weight_momentum=training_args.get('momentum', 0.9),
            weight_decay=training_args.get('weight_decay', 3e-4)
        )
    
    def initialize_optimizers(self):
        """åˆå§‹åŒ–ä¼˜åŒ–å™¨ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
        self.framework.initialize_optimizers()
        
        # ä¸ºå‘åå…¼å®¹æ€§æä¾›è®¿é—®
        self.w_optimizer = self.framework.weight_optimizer
        self.alpha_optimizer = self.framework.arch_optimizer
        
        logger.info("âœ… Optimizers initialized through framework")
    
    def train_epoch(self, train_loader, valid_loader, epoch: int) -> Dict[str, float]:
        """
        è®­ç»ƒä¸€ä¸ªepochï¼ˆæ–°çš„ç»Ÿä¸€æ¥å£ï¼‰
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            valid_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            epoch: å½“å‰epoch
            
        Returns:
            è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        """
        if self.w_optimizer is None:
            self.initialize_optimizers()
        
        # ä½¿ç”¨æ¡†æ¶è¿›è¡Œè®­ç»ƒ
        stats = self.framework.train_cycle(train_loader, valid_loader, self.criterion, epoch)
        
        # æ›´æ–°å‘åå…¼å®¹çš„çŠ¶æ€
        self._update_legacy_state()
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        self.training_stats["epoch_stats"].append(stats)
        
        # æ›´æ–°æœ€ä½³å‡†ç¡®ç‡
        if "valid_accuracy" in stats:
            self.training_stats["best_accuracy"] = max(
                self.training_stats["best_accuracy"], 
                stats["valid_accuracy"]
            )
        
        return stats
    
    def train_weights(self, train_queue, epoch: int):
        """
        é˜¶æ®µ1ï¼šæƒé‡è®­ç»ƒï¼ˆå‘åå…¼å®¹æ¥å£ï¼‰
        
        Args:
            train_queue: è®­ç»ƒæ•°æ®é˜Ÿåˆ—
            epoch: å½“å‰epoch
        """
        logger.info(f"ğŸ”¥ Epoch {epoch}: [W-Training] Training weights of current model")
        
        # å¦‚æœæ¡†æ¶å¤„äºæƒé‡è®­ç»ƒé˜¶æ®µï¼Œè¿›è¡Œè®­ç»ƒ
        if self.framework.current_phase in ["warmup", "weight_retraining"]:
            if hasattr(train_queue, '__iter__'):
                # å¦‚æœæ˜¯æ•°æ®åŠ è½½å™¨ï¼Œç›´æ¥ä½¿ç”¨
                train_loader = train_queue
                valid_loader = train_queue  # ç®€åŒ–ï¼Œå®é™…åº”è¯¥æœ‰å•ç‹¬çš„éªŒè¯é›†
                
                stats = self.framework.train_cycle(train_loader, valid_loader, self.criterion, epoch)
                return stats
        
        logger.warning(f"Weight training called but framework is in {self.framework.current_phase} phase")
    
    def train_alphas(self, valid_queue, epoch: int):
        """
        é˜¶æ®µ2ï¼šæ¶æ„å‚æ•°è®­ç»ƒï¼ˆå‘åå…¼å®¹æ¥å£ï¼‰
        
        Args:
            valid_queue: éªŒè¯æ•°æ®é˜Ÿåˆ—
            epoch: å½“å‰epoch
        """
        logger.info(f"ğŸ” Epoch {epoch}: [Î±-Training] Searching for better architecture")
        
        # å¦‚æœæ¡†æ¶å¤„äºæ¶æ„è®­ç»ƒé˜¶æ®µï¼Œè¿›è¡Œè®­ç»ƒ
        if self.framework.current_phase == "arch_training":
            if hasattr(valid_queue, '__iter__'):
                train_loader = valid_queue  # ç®€åŒ–ï¼Œå®é™…åº”è¯¥æœ‰å•ç‹¬çš„è®­ç»ƒé›†
                valid_loader = valid_queue
                
                stats = self.framework.train_cycle(train_loader, valid_loader, self.criterion, epoch)
                return stats
        
        logger.warning(f"Alpha training called but framework is in {self.framework.current_phase} phase")
    
    def mutate_architecture(self) -> Genotype:
        """
        é˜¶æ®µ3ï¼šæ¶æ„çªå˜ï¼ˆå‘åå…¼å®¹æ¥å£ï¼‰
        
        Returns:
            æ–°çš„åŸºå› å‹
        """
        logger.info("ğŸ§¬ [Mutation] Performing architecture mutation using enhanced ASO-SE")
        
        # è§¦å‘æ¡†æ¶çš„çªå˜é˜¶æ®µ
        if self.framework.current_phase == "mutation":
            # æ¡†æ¶ä¼šè‡ªåŠ¨å¤„ç†çªå˜
            new_genotype = self.framework.current_genotype or self.derive_best_genotype(use_gumbel=True)
        else:
            # æ‰‹åŠ¨è§¦å‘çªå˜ï¼ˆå¦‚æœéœ€è¦ï¼‰
            new_genotype = self.derive_best_genotype(use_gumbel=True)
        
        # æ›´æ–°å½“å‰çŠ¶æ€
        self.current_genotype = new_genotype
        self.evolvable_model = self.framework.evolvable_model
        
        logger.info(f"âœ… [Stabilization] Architecture mutated successfully")
        return new_genotype
    
    def derive_best_genotype(self, use_gumbel: bool = False) -> Genotype:
        """
        ä»æœç´¢æ¨¡å‹çš„alphaså¯¼å‡ºæœ€ä½³åŸºå› å‹
        
        Args:
            use_gumbel: æ˜¯å¦ä½¿ç”¨Gumbel-Softmaxé‡‡æ ·
            
        Returns:
            å¯¼å‡ºçš„åŸºå› å‹
        """
        if use_gumbel and hasattr(self.framework, 'explorer'):
            # ä½¿ç”¨æ¡†æ¶çš„Gumbel-Softmaxæ¢ç´¢å™¨
            try:
                return self.framework._gumbel_sample_architecture()
            except Exception as e:
                logger.warning(f"Gumbel sampling failed: {e}, falling back to argmax")
        
        # å›é€€åˆ°ç¡®å®šæ€§argmaxå¯¼å‡º
        return _derive_genotype(
            self.search_model.alphas_normal,
            self.search_model.alphas_reduce
        )
    
    def run_training_loop(self, train_queue, valid_queue, epochs: int, 
                         w_epochs: int = None, alpha_epochs: int = None):
        """
        ä¸»è¦çš„ASO-SEè®­ç»ƒå¾ªç¯ï¼ˆå‘åå…¼å®¹æ¥å£ï¼‰
        
        Args:
            train_queue: è®­ç»ƒæ•°æ®é˜Ÿåˆ—
            valid_queue: éªŒè¯æ•°æ®é˜Ÿåˆ—
            epochs: æ€»epochæ•°
            w_epochs: æƒé‡è®­ç»ƒepochæ•°ï¼ˆå¯é€‰ï¼Œä¼šä½¿ç”¨é…ç½®ä¸­çš„å€¼ï¼‰
            alpha_epochs: æ¶æ„è®­ç»ƒepochæ•°ï¼ˆå¯é€‰ï¼Œä¼šä½¿ç”¨é…ç½®ä¸­çš„å€¼ï¼‰
        """
        logger.info(f"ğŸš€ Starting ASO-SE training loop for {epochs} epochs")
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        if self.w_optimizer is None:
            self.initialize_optimizers()
        
        # è¿è¡Œè®­ç»ƒå¾ªç¯
        for epoch in range(epochs):
            try:
                # ä½¿ç”¨æ–°çš„ç»Ÿä¸€è®­ç»ƒæ¥å£
                stats = self.train_epoch(train_queue, valid_queue, epoch)
                
                # æ—¥å¿—è®°å½•
                self._log_epoch_stats(epoch, stats)
                
                # æ£€æŸ¥æ—©åœ
                if self.framework.should_early_stop():
                    logger.info(f"ğŸ›‘ Early stopping at epoch {epoch}")
                    break
                    
            except Exception as e:
                logger.error(f"âŒ Error in epoch {epoch}: {e}")
                break
        
        # è®­ç»ƒå®Œæˆåçš„æ€»ç»“
        self._log_training_summary()
    
    def _create_evolvable_model(self, genotype: Genotype) -> EvolvableNetwork:
        """
        åŸºäºåŸºå› å‹åˆ›å»ºå¯è¿›åŒ–æ¨¡å‹ï¼ˆå‘åå…¼å®¹ï¼‰
        
        Args:
            genotype: ç›®æ ‡åŸºå› å‹
            
        Returns:
            å¯è¿›åŒ–ç½‘ç»œæ¨¡å‹
        """
        return EvolvableNetwork(**self.model_args, genotype=genotype)
    
    def _update_legacy_state(self):
        """æ›´æ–°å‘åå…¼å®¹çš„çŠ¶æ€å˜é‡"""
        # ä»æ¡†æ¶åŒæ­¥çŠ¶æ€
        self.current_genotype = self.framework.current_genotype
        self.evolvable_model = self.framework.evolvable_model
    
    def _log_epoch_stats(self, epoch: int, stats: Dict[str, float]):
        """è®°å½•epochç»Ÿè®¡ä¿¡æ¯"""
        phase = stats.get("phase", "unknown")
        
        if "train_accuracy" in stats and "valid_accuracy" in stats:
            logger.info(f"ğŸ“Š Epoch {epoch:3d} [{phase:>12s}] "
                       f"Train: {stats['train_accuracy']:.2f}% "
                       f"Valid: {stats['valid_accuracy']:.2f}%")
        elif "valid_accuracy" in stats:
            logger.info(f"ğŸ“Š Epoch {epoch:3d} [{phase:>12s}] "
                       f"Valid: {stats['valid_accuracy']:.2f}%")
        
        # è®°å½•é˜¶æ®µè½¬æ¢
        if "phase" in stats:
            last_phase = (self.training_stats["epoch_stats"][-1]["phase"] 
                         if self.training_stats["epoch_stats"] else None)
            if phase != last_phase:
                self.training_stats["phase_transitions"].append({
                    "epoch": epoch,
                    "phase": phase
                })
    
    def _log_training_summary(self):
        """è®°å½•è®­ç»ƒæ€»ç»“"""
        logger.info("=" * 60)
        logger.info("ğŸ‰ ASO-SE Training Completed!")
        logger.info(f"ğŸ“ˆ Best Accuracy: {self.training_stats['best_accuracy']:.2f}%")
        
        # è·å–æ¡†æ¶æŠ¥å‘Š
        framework_report = self.framework.get_training_report()
        logger.info(f"ğŸ”¬ Total Cycles: {framework_report['current_cycle']}")
        logger.info(f"ğŸ§¬ Total Mutations: {framework_report['total_mutations']}")
        
        # æ¢ç´¢æŠ¥å‘Š
        exploration_report = framework_report.get("exploration_report", {})
        if "current_temperature" in exploration_report:
            logger.info(f"ğŸŒ¡ï¸ Final Temperature: {exploration_report['current_temperature']:.3f}")
        
        logger.info("=" * 60)
    
    # æ–°å¢çš„ä¾¿åˆ©æ–¹æ³•
    
    def get_current_architecture(self) -> Optional[Genotype]:
        """è·å–å½“å‰æ¶æ„"""
        return self.current_genotype
    
    def get_search_model(self) -> nn.Module:
        """è·å–æœç´¢æ¨¡å‹"""
        return self.search_model
    
    def get_evolvable_model(self) -> Optional[nn.Module]:
        """è·å–å¯è¿›åŒ–æ¨¡å‹"""
        return self.evolvable_model
    
    def get_training_stats(self) -> Dict:
        """è·å–è®­ç»ƒç»Ÿè®¡"""
        return self.training_stats
    
    def save_checkpoint(self, filepath: str):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        self.framework.save_checkpoint(filepath)
        logger.info(f"ğŸ’¾ Trainer checkpoint saved to {filepath}")
    
    def load_checkpoint(self, filepath: str):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        self.framework.load_checkpoint(filepath)
        self._update_legacy_state()
        logger.info(f"ğŸ“‚ Trainer checkpoint loaded from {filepath}")
    
    def get_framework_report(self) -> Dict:
        """è·å–æ¡†æ¶è¯¦ç»†æŠ¥å‘Š"""
        return self.framework.get_training_report()

# å‘åå…¼å®¹çš„å·¥å‚å‡½æ•°
def create_aso_se_trainer(search_model_args: Dict, model_args: Dict, 
                         training_args: Dict) -> ASOSETrainer:
    """
    åˆ›å»ºASO-SEè®­ç»ƒå™¨çš„å·¥å‚å‡½æ•°
    
    Args:
        search_model_args: æœç´¢æ¨¡å‹å‚æ•°
        model_args: å¯è¿›åŒ–æ¨¡å‹å‚æ•°
        training_args: è®­ç»ƒå‚æ•°
        
    Returns:
        é…ç½®å¥½çš„ASO-SEè®­ç»ƒå™¨
    """
    trainer = ASOSETrainer(search_model_args, model_args, training_args)
    return trainer 