#!/usr/bin/env python3
"""
é«˜æ•ˆæ“ä½œæ¨¡å—

å®ç°å‚æ•°é«˜æ•ˆçš„è‡ªé€‚åº”æ¶æ„æœç´¢ç­–ç•¥ï¼š
1. å‚æ•°å…±äº«MixedOp - ç›¸åŒæ“ä½œè·¨å±‚å…±äº«å‚æ•°
2. åŠ¨æ€æ“ä½œå‰ªæ - å®æ—¶å‰ªé™¤ä½æƒé‡æ“ä½œ  
3. è½»é‡çº§å€™é€‰é›† - å‡å°‘å€™é€‰æ“ä½œæ•°é‡
4. æ¸è¿›å¼æœç´¢ - ä»ç®€å•åˆ°å¤æ‚çš„æœç´¢ç­–ç•¥
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple
import math

from .operations import OPS, SepConv, DilConv, Identity, Zero
from .genotypes import PRIMITIVES

# è½»é‡çº§å€™é€‰æ“ä½œé›†ï¼ˆå‡å°‘å‚æ•°é‡ï¼‰
EFFICIENT_PRIMITIVES = [
    'none',
    'skip_connect', 
    'sep_conv_3x3',
    'sep_conv_5x5',
    'avg_pool_3x3',
    'max_pool_3x3'
]

class SharedOperationPool(nn.Module):
    """
    å…±äº«æ“ä½œæ±  - æ‰€æœ‰MixedOpå…±äº«ç›¸åŒçš„æ“ä½œå®ä¾‹
    
    è¿™æ ·å¯ä»¥å¤§å¹…å‡å°‘å‚æ•°é‡ï¼Œå› ä¸ºç›¸åŒç±»å‹çš„æ“ä½œåœ¨ä¸åŒä½ç½®å…±äº«å‚æ•°
    """
    
    def __init__(self, C: int, stride: int = 1):
        super().__init__()
        self.C = C
        self.stride = stride
        
        # åˆ›å»ºå…±äº«çš„æ“ä½œå®ä¾‹
        self.shared_ops = nn.ModuleDict()
        
        for primitive in EFFICIENT_PRIMITIVES:
            if primitive == 'none':
                self.shared_ops[primitive] = Zero(stride)
            elif primitive == 'skip_connect':
                if stride == 1:
                    self.shared_ops[primitive] = Identity()
                else:
                    self.shared_ops[primitive] = nn.Sequential(
                        nn.AvgPool2d(1, stride=stride, padding=0),
                        nn.Conv2d(C, C, 1, stride=1, padding=0, bias=False),
                        nn.BatchNorm2d(C, affine=False)
                    )
            else:
                op = OPS[primitive](C, stride, False)
                if 'pool' in primitive:
                    op = nn.Sequential(op, nn.BatchNorm2d(C, affine=False))
                self.shared_ops[primitive] = op
    
    def forward(self, x: torch.Tensor, operation: str) -> torch.Tensor:
        """æ‰§è¡ŒæŒ‡å®šæ“ä½œ"""
        return self.shared_ops[operation](x)

class EfficientMixedOp(nn.Module):
    """
    é«˜æ•ˆæ··åˆæ“ä½œ
    
    ç‰¹ç‚¹ï¼š
    1. ä½¿ç”¨å…±äº«æ“ä½œæ± å‡å°‘å‚æ•°
    2. æ”¯æŒåŠ¨æ€å‰ªæ
    3. å¯é…ç½®çš„å€™é€‰æ“ä½œé›†
    """
    
    def __init__(self, C: int, stride: int, operation_pool: SharedOperationPool, 
                 enable_pruning: bool = True, pruning_threshold: float = 0.01):
        super().__init__()
        self.C = C
        self.stride = stride
        self.operation_pool = operation_pool
        self.enable_pruning = enable_pruning
        self.pruning_threshold = pruning_threshold
        
        # æ“ä½œå€™é€‰åˆ—è¡¨
        self.primitives = EFFICIENT_PRIMITIVES
        self.num_ops = len(self.primitives)
        
        # æ´»è·ƒæ“ä½œæ©ç ï¼ˆç”¨äºåŠ¨æ€å‰ªæï¼‰
        self.register_buffer('active_mask', torch.ones(self.num_ops, dtype=torch.bool))
        
        # æ“ä½œä½¿ç”¨ç»Ÿè®¡ï¼ˆç”¨äºåˆ†æï¼‰
        self.register_buffer('op_usage_count', torch.zeros(self.num_ops))
    
    def forward(self, x: torch.Tensor, weights: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥å¼ é‡ [B, C, H, W]
            weights: æ“ä½œæƒé‡ [num_ops]
        """
        # åŠ¨æ€å‰ªæï¼šåªè®¡ç®—æƒé‡å¤§äºé˜ˆå€¼çš„æ“ä½œ
        if self.enable_pruning and self.training:
            active_indices = torch.where(weights > self.pruning_threshold)[0]
            if len(active_indices) == 0:
                # å¦‚æœæ‰€æœ‰æƒé‡éƒ½å¤ªå°ï¼Œä¿ç•™æƒé‡æœ€å¤§çš„æ“ä½œ
                active_indices = torch.argmax(weights).unsqueeze(0)
        else:
            active_indices = torch.arange(self.num_ops, device=weights.device)
        
        # è®¡ç®—æ´»è·ƒæ“ä½œçš„è¾“å‡º
        outputs = []
        active_weights = []
        
        for i in active_indices:
            primitive = self.primitives[i]
            op_output = self.operation_pool(x, primitive)
            outputs.append(op_output)
            active_weights.append(weights[i])
            
            # æ›´æ–°ä½¿ç”¨ç»Ÿè®¡
            self.op_usage_count[i] += 1
        
        if len(outputs) == 1:
            return outputs[0] * active_weights[0]
        else:
            # å½’ä¸€åŒ–æƒé‡
            active_weights = torch.stack(active_weights)
            active_weights = F.softmax(active_weights, dim=0)
            
            # åŠ æƒæ±‚å’Œ
            result = outputs[0] * active_weights[0]
            for i in range(1, len(outputs)):
                result = result + outputs[i] * active_weights[i]
            
            return result
    
    def get_active_operations(self, weights: torch.Tensor) -> List[str]:
        """è·å–å½“å‰æ´»è·ƒçš„æ“ä½œåˆ—è¡¨"""
        active_indices = torch.where(weights > self.pruning_threshold)[0]
        return [self.primitives[i] for i in active_indices]
    
    def get_operation_stats(self) -> Dict[str, int]:
        """è·å–æ“ä½œä½¿ç”¨ç»Ÿè®¡"""
        stats = {}
        for i, primitive in enumerate(self.primitives):
            stats[primitive] = self.op_usage_count[i].item()
        return stats

class EfficientCell(nn.Module):
    """
    é«˜æ•ˆCellå®ç°
    
    ä½¿ç”¨å‚æ•°å…±äº«å’ŒåŠ¨æ€å‰ªææ¥å‡å°‘è®¡ç®—å¼€é”€
    """
    
    def __init__(self, steps: int, block_multiplier: int, C_prev_prev: int, 
                 C_prev: int, C: int, reduction: bool, reduction_prev: bool,
                 shared_normal_pool: SharedOperationPool, shared_reduce_pool: SharedOperationPool):
        super().__init__()
        self.reduction = reduction
        self.steps = steps
        self.block_multiplier = block_multiplier
        
        # é¢„å¤„ç†å±‚ï¼ˆä»éœ€è¦ç‹¬ç«‹å‚æ•°ï¼‰
        if reduction_prev:
            self.preprocess0 = nn.Sequential(
                nn.AvgPool2d(1, stride=2, padding=0),
                nn.Conv2d(C_prev_prev, C, 1, stride=1, padding=0, bias=False),
                nn.BatchNorm2d(C, affine=False)
            )
        else:
            self.preprocess0 = nn.Sequential(
                nn.Conv2d(C_prev_prev, C, 1, stride=1, padding=0, bias=False),
                nn.BatchNorm2d(C, affine=False)
            )
        
        self.preprocess1 = nn.Sequential(
            nn.Conv2d(C_prev, C, 1, stride=1, padding=0, bias=False),
            nn.BatchNorm2d(C, affine=False)
        )
        
        # é«˜æ•ˆMixedOpï¼ˆä½¿ç”¨å…±äº«æ“ä½œæ± ï¼‰
        self._ops = nn.ModuleList()
        for i in range(self.steps):
            for j in range(2 + i):
                stride = 2 if reduction and j < 2 else 1
                if reduction:
                    op = EfficientMixedOp(C, stride, shared_reduce_pool)
                else:
                    op = EfficientMixedOp(C, stride, shared_normal_pool)
                self._ops.append(op)
    
    def forward(self, s0: torch.Tensor, s1: torch.Tensor, weights: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        s0 = self.preprocess0(s0)
        s1 = self.preprocess1(s1)
        
        states = [s0, s1]
        offset = 0
        
        for i in range(self.steps):
            s = 0
            for j in range(2 + i):
                op = self._ops[offset + j]
                h = op(states[j], weights[offset + j])
                s = s + h
            
            offset += len(states)
            states.append(s)
        
        return torch.cat(states[-self.block_multiplier:], dim=1)

class ProgressiveArchitectureSearch:
    """
    æ¸è¿›å¼æ¶æ„æœç´¢
    
    ä»ç®€å•æ¶æ„å¼€å§‹ï¼Œé€æ­¥å¢åŠ å¤æ‚åº¦ï¼š
    1. ç¬¬1é˜¶æ®µï¼šåªä½¿ç”¨åŸºç¡€æ“ä½œï¼ˆskip, poolï¼‰
    2. ç¬¬2é˜¶æ®µï¼šæ·»åŠ 3x3å·ç§¯
    3. ç¬¬3é˜¶æ®µï¼šæ·»åŠ 5x5å·ç§¯å’Œdilatedå·ç§¯
    """
    
    def __init__(self):
        self.stage = 1
        self.stage_epochs = [5, 10, 15]  # æ¯ä¸ªé˜¶æ®µçš„epochæ•°
        self.current_epoch = 0
        
        # å„é˜¶æ®µçš„æ“ä½œé›†
        self.stage_primitives = {
            1: ['none', 'skip_connect', 'avg_pool_3x3', 'max_pool_3x3'],
            2: ['none', 'skip_connect', 'avg_pool_3x3', 'max_pool_3x3', 'sep_conv_3x3'],
            3: EFFICIENT_PRIMITIVES
        }
    
    def update_epoch(self, epoch: int):
        """æ›´æ–°å½“å‰epochï¼Œè‡ªåŠ¨åˆ‡æ¢æœç´¢é˜¶æ®µ"""
        self.current_epoch = epoch
        
        if epoch < self.stage_epochs[0]:
            self.stage = 1
        elif epoch < self.stage_epochs[1]:
            self.stage = 2
        else:
            self.stage = 3
    
    def get_current_primitives(self) -> List[str]:
        """è·å–å½“å‰é˜¶æ®µçš„æ“ä½œé›†"""
        return self.stage_primitives[self.stage]
    
    def should_expand_search(self) -> bool:
        """æ˜¯å¦åº”è¯¥æ‰©å±•æœç´¢ç©ºé—´"""
        return self.current_epoch in self.stage_epochs

def create_efficient_network(C: int, num_classes: int, layers: int, 
                           use_progressive_search: bool = True) -> Tuple[nn.Module, Dict]:
    """
    åˆ›å»ºå‚æ•°é«˜æ•ˆçš„è‡ªé€‚åº”ç½‘ç»œ
    
    Returns:
        model: ç½‘ç»œæ¨¡å‹
        optimization_info: ä¼˜åŒ–ä¿¡æ¯
    """
    # åˆ›å»ºå…±äº«æ“ä½œæ± 
    shared_normal_pool = SharedOperationPool(C, stride=1)
    shared_reduce_pool = SharedOperationPool(C, stride=2)
    
    # è®¡ç®—å‚æ•°èŠ‚çœ
    traditional_params = layers * 4 * 2 * len(PRIMITIVES) * (C * C * 9)  # ç²—ç•¥ä¼°ç®—
    efficient_params = len(EFFICIENT_PRIMITIVES) * (C * C * 9) * 2  # å…±äº«æ± å‚æ•°
    param_reduction = (traditional_params - efficient_params) / traditional_params
    
    optimization_info = {
        'parameter_reduction': param_reduction,
        'shared_operations': len(EFFICIENT_PRIMITIVES),
        'traditional_operations': layers * 4 * 2 * len(PRIMITIVES),
        'efficiency_ratio': (layers * 4 * 2 * len(PRIMITIVES)) / len(EFFICIENT_PRIMITIVES)
    }
    
    print(f"ğŸ’¡ å‚æ•°æ•ˆç‡ä¼˜åŒ–:")
    print(f"   ä¼ ç»Ÿæ–¹æ³•æ“ä½œæ•°: {optimization_info['traditional_operations']}")
    print(f"   é«˜æ•ˆæ–¹æ³•æ“ä½œæ•°: {optimization_info['shared_operations']}")
    print(f"   å‚æ•°å‡å°‘ä¼°ç®—: {param_reduction*100:.1f}%")
    print(f"   æ•ˆç‡æå‡æ¯”: {optimization_info['efficiency_ratio']:.1f}x")
    
    return None, optimization_info  # æš‚æ—¶è¿”å›Noneï¼Œç¨åå®ç°å®Œæ•´ç½‘ç»œ

if __name__ == "__main__":
    # æµ‹è¯•å‚æ•°æ•ˆç‡
    info = create_efficient_network(16, 10, 6)
    print(f"ä¼˜åŒ–ä¿¡æ¯: {info}") 