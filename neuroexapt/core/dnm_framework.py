#!/usr/bin/env python3
"""
Dynamic Neural Morphogenesis (DNM) Framework - é‡æ„ç‰ˆæœ¬

ğŸ§¬ æ ¸å¿ƒç†è®ºæ”¯æ’‘ï¼š
1. ä¿¡æ¯è®º (Information Theory) - ä¿¡æ¯ç“¶é¢ˆå’Œç†µåˆ†æ
2. ç”Ÿç‰©å­¦åŸç† (Biological Principles) - ç¥ç»å‘è‚²å’Œçªè§¦å¯å¡‘æ€§
3. åŠ¨åŠ›å­¦ç³»ç»Ÿ (Dynamical Systems) - æ¢¯åº¦æµå’Œæ”¶æ•›æ€§åˆ†æ  
4. è®¤çŸ¥ç§‘å­¦ (Cognitive Science) - å­¦ä¹ æ›²çº¿å’Œè®°å¿†å·©å›º
5. ç½‘ç»œç§‘å­¦ (Network Science) - è¿æ¥æ¨¡å¼å’Œæ‹“æ‰‘åˆ†æ

ğŸ¯ ç›®æ ‡ï¼šçªç ´ä¼ ç»Ÿç¥ç»ç½‘ç»œçš„æ€§èƒ½ç“¶é¢ˆï¼Œå®ç°çœŸæ­£çš„æ™ºèƒ½å½¢æ€å‘ç”Ÿ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from abc import ABC, abstractmethod
import math
from collections import defaultdict, deque
import copy

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

@dataclass
class MorphogenesisEvent:
    """å½¢æ€å‘ç”Ÿäº‹ä»¶è®°å½•"""
    epoch: int
    event_type: str  # 'neuron_division', 'connection_growth', 'pruning', 'topology_change'
    location: str    # å±‚åç§°æˆ–ä½ç½®
    trigger_reason: str
    performance_before: float
    performance_after: Optional[float] = None
    parameters_added: int = 0
    complexity_change: float = 0.0
    
class MorphogenesisTrigger(ABC):
    """å½¢æ€å‘ç”Ÿè§¦å‘å™¨æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    def should_trigger(self, context: Dict[str, Any]) -> Tuple[bool, str]:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è§¦å‘å½¢æ€å‘ç”Ÿ"""
        pass
    
    @abstractmethod
    def get_priority(self) -> float:
        """è·å–è§¦å‘å™¨ä¼˜å…ˆçº§"""
        pass

class InformationTheoryTrigger(MorphogenesisTrigger):
    """åŸºäºä¿¡æ¯è®ºçš„è§¦å‘å™¨"""
    
    def __init__(self, entropy_threshold: float = 0.1, mi_threshold: float = 0.05):
        self.entropy_threshold = entropy_threshold
        self.mi_threshold = mi_threshold
        self.history = deque(maxlen=10)
        
    def should_trigger(self, context: Dict[str, Any]) -> Tuple[bool, str]:
        activations = context.get('activations', {})
        gradients = context.get('gradients', {})
        
        if not activations or not gradients:
            return False, "ç¼ºå°‘æ¿€æ´»å€¼æˆ–æ¢¯åº¦ä¿¡æ¯"
            
        # è®¡ç®—ä¿¡æ¯ç†µå˜åŒ–
        entropy_changes = self._compute_entropy_changes(activations)
        
        # è®¡ç®—äº’ä¿¡æ¯
        mutual_info = self._compute_mutual_information(activations)
        
        # æ¢¯åº¦æ–¹å·®åˆ†æ
        gradient_variance = self._analyze_gradient_variance(gradients)
        
        self.history.append({
            'entropy_changes': entropy_changes,
            'mutual_info': mutual_info,
            'gradient_variance': gradient_variance
        })
        
        # ä¿¡æ¯ç“¶é¢ˆæ£€æµ‹
        if self._detect_information_bottleneck():
            return True, f"ä¿¡æ¯ç“¶é¢ˆæ£€æµ‹ï¼šç†µå˜åŒ–={entropy_changes:.4f}, äº’ä¿¡æ¯={mutual_info:.4f}"
            
        return False, "ä¿¡æ¯è®ºæŒ‡æ ‡æœªè¾¾åˆ°è§¦å‘æ¡ä»¶"
    
    def _compute_entropy_changes(self, activations: Dict[str, torch.Tensor]) -> float:
        """è®¡ç®—æ¿€æ´»å€¼ç†µçš„å˜åŒ–"""
        total_entropy_change = 0.0
        count = 0
        
        for name, activation in activations.items():
            if len(activation.shape) >= 2:
                # è®¡ç®—æ¯ä¸ªç¥ç»å…ƒçš„ç†µ
                activation_flat = activation.view(activation.size(0), -1)
                probs = F.softmax(activation_flat, dim=-1) + 1e-8
                entropy = -torch.sum(probs * torch.log(probs), dim=-1).mean()
                
                if len(self.history) > 0:
                    prev_entropy = self.history[-1].get('entropy_changes', 0)
                    entropy_change = abs(entropy.item() - prev_entropy)
                    total_entropy_change += entropy_change
                    count += 1
                    
        return total_entropy_change / max(count, 1)
    
    def _compute_mutual_information(self, activations: Dict[str, torch.Tensor]) -> float:
        """è®¡ç®—å±‚é—´äº’ä¿¡æ¯"""
        layer_names = list(activations.keys())
        if len(layer_names) < 2:
            return 0.0
            
        # ç®€åŒ–çš„äº’ä¿¡æ¯ä¼°è®¡
        mi_sum = 0.0
        pairs = 0
        
        for i in range(len(layer_names) - 1):
            for j in range(i + 1, min(i + 3, len(layer_names))):  # åªè€ƒè™‘ç›¸é‚»å±‚
                act1 = activations[layer_names[i]].flatten()
                act2 = activations[layer_names[j]].flatten()
                
                # ä½¿ç”¨ç›¸å…³ç³»æ•°è¿‘ä¼¼äº’ä¿¡æ¯
                if len(act1) == len(act2):
                    correlation = torch.corrcoef(torch.stack([act1, act2]))[0, 1]
                    mi = -0.5 * torch.log(1 - correlation**2 + 1e-8)
                    mi_sum += mi.item()
                    pairs += 1
                    
        return mi_sum / max(pairs, 1)
    
    def _analyze_gradient_variance(self, gradients: Dict[str, torch.Tensor]) -> float:
        """åˆ†ææ¢¯åº¦æ–¹å·®"""
        total_variance = 0.0
        count = 0
        
        for name, grad in gradients.items():
            if grad is not None:
                variance = torch.var(grad).item()
                total_variance += variance
                count += 1
                
        return total_variance / max(count, 1)
    
    def _detect_information_bottleneck(self) -> bool:
        """æ£€æµ‹ä¿¡æ¯ç“¶é¢ˆ"""
        if len(self.history) < 5:
            return False
            
        recent_entropies = [h['entropy_changes'] for h in list(self.history)[-5:]]
        recent_mis = [h['mutual_info'] for h in list(self.history)[-5:]]
        
        # ç†µå˜åŒ–è¶‹äºç¨³å®šä¸”äº’ä¿¡æ¯è¾ƒä½
        entropy_stability = np.std(recent_entropies) < self.entropy_threshold
        low_mi = np.mean(recent_mis) < self.mi_threshold
        
        return entropy_stability and low_mi
    
    def get_priority(self) -> float:
        return 0.8

class BiologicalPrinciplesTrigger(MorphogenesisTrigger):
    """åŸºäºç”Ÿç‰©å­¦åŸç†çš„è§¦å‘å™¨"""
    
    def __init__(self, learning_rate_threshold: float = 1e-4, saturation_threshold: float = 0.95):
        self.learning_rate_threshold = learning_rate_threshold
        self.saturation_threshold = saturation_threshold
        self.performance_history = deque(maxlen=20)
        
    def should_trigger(self, context: Dict[str, Any]) -> Tuple[bool, str]:
        current_performance = context.get('current_performance', 0.0)
        learning_rate = context.get('learning_rate', 1e-3)
        activations = context.get('activations', {})
        
        self.performance_history.append(current_performance)
        
        # æ¨¡æ‹Ÿç¥ç»å¯å¡‘æ€§ - Hebbianå­¦ä¹ åŸç†
        if self._detect_hebbian_potential(activations):
            return True, f"Hebbianå¯å¡‘æ€§æ£€æµ‹ï¼šæ€§èƒ½={current_performance:.4f}"
            
        # æ¨¡æ‹Ÿçªè§¦ç¨³æ€ - æ€§èƒ½å¹³å°æœŸæ£€æµ‹
        if self._detect_homeostatic_imbalance():
            return True, f"ç¨³æ€å¤±è¡¡æ£€æµ‹ï¼šæ€§èƒ½åœæ»ï¼Œå»ºè®®ç»“æ„è°ƒæ•´"
            
        # æ¨¡æ‹Ÿç¥ç»å‘è‚²çš„å…³é”®æœŸ
        if self._detect_critical_period():
            return True, f"å…³é”®å‘è‚²æœŸæ£€æµ‹ï¼šé€‚åˆç»“æ„é‡ç»„"
            
        return False, "ç”Ÿç‰©å­¦æŒ‡æ ‡æœªè¾¾åˆ°è§¦å‘æ¡ä»¶"
    
    def _detect_hebbian_potential(self, activations: Dict[str, torch.Tensor]) -> bool:
        """æ£€æµ‹Hebbianå¯å¡‘æ€§æ½œåŠ›"""
        if not activations:
            return False
            
        # åˆ†ææ¿€æ´»æ¨¡å¼çš„ç›¸å…³æ€§
        correlation_strengths = []
        
        for name, activation in activations.items():
            if len(activation.shape) >= 2:
                # è®¡ç®—ç¥ç»å…ƒé—´çš„ç›¸å…³æ€§
                act_flat = activation.view(activation.size(0), -1)
                if act_flat.size(1) > 1:
                    corr_matrix = torch.corrcoef(act_flat.T)
                    # ç§»é™¤å¯¹è§’çº¿å…ƒç´ 
                    mask = ~torch.eye(corr_matrix.size(0), dtype=bool)
                    corr_values = corr_matrix[mask]
                    avg_correlation = torch.mean(torch.abs(corr_values)).item()
                    correlation_strengths.append(avg_correlation)
        
        if correlation_strengths:
            mean_correlation = np.mean(correlation_strengths)
            return mean_correlation > 0.7  # é«˜ç›¸å…³æ€§è¡¨æ˜å¯ä»¥åˆ†è£‚
            
        return False
    
    def _detect_homeostatic_imbalance(self) -> bool:
        """æ£€æµ‹ç¨³æ€å¤±è¡¡"""
        if len(self.performance_history) < 10:
            return False
            
        recent_performance = list(self.performance_history)[-10:]
        performance_std = np.std(recent_performance)
        performance_trend = np.polyfit(range(len(recent_performance)), recent_performance, 1)[0]
        
        # æ€§èƒ½åœæ»ä¸”æ— æ˜æ˜¾ä¸Šå‡è¶‹åŠ¿
        return performance_std < 0.01 and abs(performance_trend) < 0.001
    
    def _detect_critical_period(self) -> bool:
        """æ£€æµ‹å…³é”®å‘è‚²æœŸ"""
        if len(self.performance_history) < 15:
            return False
            
        # æ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»ç½‘ç»œçš„å…³é”®æœŸ
        recent_performance = list(self.performance_history)[-15:]
        
        # æŸ¥æ‰¾æ€§èƒ½å¿«é€Ÿä¸Šå‡åçš„å¹³å°æœŸ
        for i in range(5, len(recent_performance)):
            early_avg = np.mean(recent_performance[:i-5])
            recent_avg = np.mean(recent_performance[i-5:i])
            latest_avg = np.mean(recent_performance[i:])
            
            # å¿«é€Ÿä¸Šå‡ååœæ»
            if (recent_avg - early_avg > 0.05) and (abs(latest_avg - recent_avg) < 0.01):
                return True
                
        return False
    
    def get_priority(self) -> float:
        return 0.9

class DynamicalSystemsTrigger(MorphogenesisTrigger):
    """åŸºäºåŠ¨åŠ›å­¦ç³»ç»Ÿçš„è§¦å‘å™¨"""
    
    def __init__(self):
        self.gradient_history = deque(maxlen=15)
        self.loss_history = deque(maxlen=20)
        
    def should_trigger(self, context: Dict[str, Any]) -> Tuple[bool, str]:
        gradients = context.get('gradients', {})
        current_loss = context.get('current_loss', float('inf'))
        
        self.loss_history.append(current_loss)
        
        if gradients:
            gradient_norm = self._compute_gradient_norm(gradients)
            self.gradient_history.append(gradient_norm)
        
        # æ£€æµ‹æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
        if self._detect_gradient_pathology():
            return True, "æ¢¯åº¦ç—…ç†æ£€æµ‹ï¼šéœ€è¦ç»“æ„è°ƒæ•´æ”¹å–„æ¢¯åº¦æµ"
            
        # æ£€æµ‹æŸå¤±å‡½æ•°çš„åŠ¨åŠ›å­¦ç‰¹æ€§
        if self._detect_loss_dynamics_anomaly():
            return True, "æŸå¤±åŠ¨åŠ›å­¦å¼‚å¸¸ï¼šå»ºè®®å¢åŠ æ¨¡å‹å®¹é‡"
            
        # æ£€æµ‹æ”¶æ•›æ€§é—®é¢˜
        if self._detect_convergence_issues():
            return True, "æ”¶æ•›æ€§é—®é¢˜æ£€æµ‹ï¼šæ¨¡å‹å¯èƒ½æ¬ æ‹Ÿåˆ"
            
        return False, "åŠ¨åŠ›å­¦ç³»ç»ŸæŒ‡æ ‡æ­£å¸¸"
    
    def _compute_gradient_norm(self, gradients: Dict[str, torch.Tensor]) -> float:
        """è®¡ç®—æ¢¯åº¦èŒƒæ•°"""
        total_norm = 0.0
        for grad in gradients.values():
            if grad is not None:
                total_norm += torch.norm(grad).item() ** 2
        return math.sqrt(total_norm)
    
    def _detect_gradient_pathology(self) -> bool:
        """æ£€æµ‹æ¢¯åº¦ç—…ç†"""
        if len(self.gradient_history) < 10:
            return False
            
        recent_grads = list(self.gradient_history)[-10:]
        
        # æ¢¯åº¦æ¶ˆå¤±
        if np.mean(recent_grads) < 1e-6:
            return True
            
        # æ¢¯åº¦çˆ†ç‚¸
        if np.max(recent_grads) > 100:
            return True
            
        # æ¢¯åº¦æŒ¯è¡
        grad_diff = np.diff(recent_grads)
        if len(grad_diff) > 5:
            oscillation = np.sum(np.diff(np.sign(grad_diff)) != 0) / len(grad_diff)
            if oscillation > 0.7:
                return True
                
        return False
    
    def _detect_loss_dynamics_anomaly(self) -> bool:
        """æ£€æµ‹æŸå¤±åŠ¨åŠ›å­¦å¼‚å¸¸"""
        if len(self.loss_history) < 15:
            return False
            
        recent_losses = list(self.loss_history)[-15:]
        
        # æŸå¤±åœæ»
        loss_std = np.std(recent_losses)
        if loss_std < 0.001:
            return True
            
        # æŸå¤±æŒ¯è¡è€Œä¸æ”¶æ•›
        loss_trend = np.polyfit(range(len(recent_losses)), recent_losses, 1)[0]
        if abs(loss_trend) < 0.001 and loss_std > 0.01:
            return True
            
        return False
    
    def _detect_convergence_issues(self) -> bool:
        """æ£€æµ‹æ”¶æ•›æ€§é—®é¢˜"""
        if len(self.loss_history) < 15 or len(self.gradient_history) < 10:
            return False
            
        # æŸå¤±ä¸‹é™ç¼“æ…¢ä¸”æ¢¯åº¦å¾ˆå°
        recent_losses = list(self.loss_history)[-10:]
        recent_grads = list(self.gradient_history)[-5:]
        
        loss_improvement = recent_losses[0] - recent_losses[-1]
        avg_grad = np.mean(recent_grads)
        
        # æŸå¤±æ”¹å–„å¾ˆå°ä¸”æ¢¯åº¦å¾ˆå°ï¼Œä½†ä¸æ˜¯è¿‡æ‹Ÿåˆ
        if loss_improvement < 0.01 and avg_grad < 0.01 and recent_losses[-1] > 0.5:
            return True
            
        return False
    
    def get_priority(self) -> float:
        return 0.85

class CognitiveScienceTrigger(MorphogenesisTrigger):
    """åŸºäºè®¤çŸ¥ç§‘å­¦çš„è§¦å‘å™¨"""
    
    def __init__(self):
        self.learning_curve = deque(maxlen=50)
        self.forgetting_events = []
        
    def should_trigger(self, context: Dict[str, Any]) -> Tuple[bool, str]:
        train_acc = context.get('train_accuracy', 0.0)
        val_acc = context.get('val_accuracy', 0.0)
        epoch = context.get('epoch', 0)
        
        self.learning_curve.append({
            'epoch': epoch,
            'train_acc': train_acc,
            'val_acc': val_acc,
            'generalization_gap': train_acc - val_acc
        })
        
        # æ£€æµ‹å­¦ä¹ é«˜åŸæœŸ
        if self._detect_learning_plateau():
            return True, "å­¦ä¹ é«˜åŸæœŸæ£€æµ‹ï¼šéœ€è¦å¢åŠ è®¤çŸ¥å¤æ‚æ€§"
            
        # æ£€æµ‹é—å¿˜ç°è±¡
        if self._detect_catastrophic_forgetting():
            return True, "ç¾éš¾æ€§é—å¿˜æ£€æµ‹ï¼šéœ€è¦åˆ†åŒ–ä¸“é—¨åŒ–ç¥ç»å…ƒ"
            
        # æ£€æµ‹è®¤çŸ¥è´Ÿè·è¿‡è½½
        if self._detect_cognitive_overload():
            return True, "è®¤çŸ¥è´Ÿè·è¿‡è½½ï¼šå»ºè®®åˆ†è§£ä»»åŠ¡å¤æ‚æ€§"
            
        return False, "è®¤çŸ¥ç§‘å­¦æŒ‡æ ‡æ­£å¸¸"
    
    def _detect_learning_plateau(self) -> bool:
        """æ£€æµ‹å­¦ä¹ é«˜åŸæœŸ"""
        if len(self.learning_curve) < 20:
            return False
            
        recent_curves = list(self.learning_curve)[-20:]
        train_accs = [c['train_acc'] for c in recent_curves]
        val_accs = [c['val_acc'] for c in recent_curves]
        
        # è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡éƒ½åœæ»
        train_improvement = max(train_accs) - min(train_accs)
        val_improvement = max(val_accs) - min(val_accs)
        
        return train_improvement < 0.02 and val_improvement < 0.02
    
    def _detect_catastrophic_forgetting(self) -> bool:
        """æ£€æµ‹ç¾éš¾æ€§é—å¿˜"""
        if len(self.learning_curve) < 10:
            return False
            
        recent_curves = list(self.learning_curve)[-10:]
        
        # æ£€æµ‹éªŒè¯å‡†ç¡®ç‡å¤§å¹…ä¸‹é™
        for i in range(1, len(recent_curves)):
            val_drop = recent_curves[i-1]['val_acc'] - recent_curves[i]['val_acc']
            if val_drop > 0.05:  # å‡†ç¡®ç‡ä¸‹é™è¶…è¿‡5%
                self.forgetting_events.append(recent_curves[i]['epoch'])
                return True
                
        return False
    
    def _detect_cognitive_overload(self) -> bool:
        """æ£€æµ‹è®¤çŸ¥è´Ÿè·è¿‡è½½"""
        if len(self.learning_curve) < 15:
            return False
            
        recent_curves = list(self.learning_curve)[-15:]
        gaps = [c['generalization_gap'] for c in recent_curves]
        
        # æ³›åŒ–å·®è·æŒç»­å¢å¤§
        gap_trend = np.polyfit(range(len(gaps)), gaps, 1)[0]
        avg_gap = np.mean(gaps)
        
        return gap_trend > 0.002 and avg_gap > 0.15
    
    def get_priority(self) -> float:
        return 0.75

class NetworkScienceTrigger(MorphogenesisTrigger):
    """åŸºäºç½‘ç»œç§‘å­¦çš„è§¦å‘å™¨"""
    
    def __init__(self):
        self.connectivity_history = deque(maxlen=10)
        
    def should_trigger(self, context: Dict[str, Any]) -> Tuple[bool, str]:
        model = context.get('model')
        activations = context.get('activations', {})
        
        if model is None:
            return False, "ç¼ºå°‘æ¨¡å‹ä¿¡æ¯"
            
        # åˆ†æç½‘ç»œæ‹“æ‰‘ç‰¹æ€§
        connectivity_metrics = self._analyze_network_topology(model, activations)
        self.connectivity_history.append(connectivity_metrics)
        
        # æ£€æµ‹ç½‘ç»œç“¶é¢ˆ
        if self._detect_network_bottleneck(connectivity_metrics):
            return True, f"ç½‘ç»œç“¶é¢ˆæ£€æµ‹ï¼šä¸­å¿ƒæ€§è¿‡é«˜={connectivity_metrics.get('centrality', 0):.3f}"
            
        # æ£€æµ‹è¿æ¥ä¸å¹³è¡¡
        if self._detect_connectivity_imbalance(connectivity_metrics):
            return True, "è¿æ¥ä¸å¹³è¡¡æ£€æµ‹ï¼šéœ€è¦é‡æ–°åˆ†å¸ƒç½‘ç»œè¿æ¥"
            
        return False, "ç½‘ç»œç§‘å­¦æŒ‡æ ‡æ­£å¸¸"
    
    def _analyze_network_topology(self, model: nn.Module, activations: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """åˆ†æç½‘ç»œæ‹“æ‰‘ç‰¹æ€§"""
        metrics = {}
        
        # è®¡ç®—å±‚é—´è¿æ¥å¼ºåº¦
        layer_connections = self._compute_layer_connections(model)
        metrics['avg_connection_strength'] = np.mean(list(layer_connections.values()))
        
        # è®¡ç®—ç½‘ç»œä¸­å¿ƒæ€§
        centrality = self._compute_network_centrality(activations)
        metrics['centrality'] = centrality
        
        # è®¡ç®—èšç±»ç³»æ•°
        clustering = self._compute_clustering_coefficient(activations)
        metrics['clustering'] = clustering
        
        return metrics
    
    def _compute_layer_connections(self, model: nn.Module) -> Dict[str, float]:
        """è®¡ç®—å±‚é—´è¿æ¥å¼ºåº¦"""
        connections = {}
        
        for name, module in model.named_modules():
            if isinstance(module, (nn.Linear, nn.Conv2d)):
                if hasattr(module, 'weight') and module.weight is not None:
                    weight_norm = torch.norm(module.weight).item()
                    connections[name] = weight_norm
                    
        return connections
    
    def _compute_network_centrality(self, activations: Dict[str, torch.Tensor]) -> float:
        """è®¡ç®—ç½‘ç»œä¸­å¿ƒæ€§"""
        if len(activations) < 2:
            return 0.0
            
        # ç®€åŒ–çš„ä¸­å¿ƒæ€§è®¡ç®—
        activation_norms = {}
        for name, activation in activations.items():
            activation_norms[name] = torch.norm(activation).item()
            
        norm_values = list(activation_norms.values())
        if not norm_values:
            return 0.0
            
        # è®¡ç®—æ ‡å‡†åŒ–çš„ä¸­å¿ƒæ€§
        max_norm = max(norm_values)
        avg_norm = np.mean(norm_values)
        
        return max_norm / (avg_norm + 1e-8)
    
    def _compute_clustering_coefficient(self, activations: Dict[str, torch.Tensor]) -> float:
        """è®¡ç®—èšç±»ç³»æ•°"""
        if len(activations) < 3:
            return 0.0
            
        # ç®€åŒ–çš„èšç±»ç³»æ•°è®¡ç®—
        layer_names = list(activations.keys())
        correlations = []
        
        for i in range(len(layer_names)):
            for j in range(i+1, len(layer_names)):
                act1 = activations[layer_names[i]].flatten()
                act2 = activations[layer_names[j]].flatten()
                
                if len(act1) == len(act2) and len(act1) > 1:
                    corr = torch.corrcoef(torch.stack([act1, act2]))[0, 1]
                    correlations.append(abs(corr.item()))
                    
        return np.mean(correlations) if correlations else 0.0
    
    def _detect_network_bottleneck(self, metrics: Dict[str, float]) -> bool:
        """æ£€æµ‹ç½‘ç»œç“¶é¢ˆ"""
        centrality = metrics.get('centrality', 0)
        return centrality > 3.0  # ä¸­å¿ƒæ€§è¿‡é«˜è¡¨æ˜å­˜åœ¨ç“¶é¢ˆ
    
    def _detect_connectivity_imbalance(self, metrics: Dict[str, float]) -> bool:
        """æ£€æµ‹è¿æ¥ä¸å¹³è¡¡"""
        if len(self.connectivity_history) < 5:
            return False
            
        recent_metrics = list(self.connectivity_history)[-5:]
        connection_strengths = [m.get('avg_connection_strength', 0) for m in recent_metrics]
        
        # è¿æ¥å¼ºåº¦æ–¹å·®è¿‡å¤§
        strength_std = np.std(connection_strengths)
        return strength_std > 0.5
    
    def get_priority(self) -> float:
        return 0.7

class NeuronDivisionExecutor:
    """ç¥ç»å…ƒåˆ†è£‚æ‰§è¡Œå™¨"""
    
    def __init__(self):
        self.division_history = []
        
    def execute_division(self, model: nn.Module, layer_name: str, division_type: str = 'width_expansion') -> Tuple[nn.Module, int]:
        """æ‰§è¡Œç¥ç»å…ƒåˆ†è£‚"""
        try:
            if division_type == 'width_expansion':
                return self._expand_layer_width(model, layer_name)
            elif division_type == 'depth_expansion':
                return self._expand_network_depth(model, layer_name)
            elif division_type == 'branch_creation':
                return self._create_branch(model, layer_name)
            else:
                logger.warning(f"æœªçŸ¥çš„åˆ†è£‚ç±»å‹: {division_type}")
                return model, 0
                
        except Exception as e:
            logger.error(f"ç¥ç»å…ƒåˆ†è£‚æ‰§è¡Œå¤±è´¥: {e}")
            return model, 0
    
    def _expand_layer_width(self, model: nn.Module, layer_name: str) -> Tuple[nn.Module, int]:
        """æ‰©å±•å±‚å®½åº¦ï¼ˆå¢åŠ ç¥ç»å…ƒæ•°é‡ï¼‰"""
        new_model = copy.deepcopy(model)
        parameters_added = 0
        
        # æ‰¾åˆ°ç›®æ ‡å±‚
        target_module = None
        for name, module in new_model.named_modules():
            if name == layer_name:
                target_module = module
                break
                
        if target_module is None:
            logger.warning(f"æœªæ‰¾åˆ°ç›®æ ‡å±‚: {layer_name}")
            return model, 0
            
        if isinstance(target_module, nn.Linear):
            # æ‰©å±•å…¨è¿æ¥å±‚
            old_out_features = target_module.out_features
            new_out_features = int(old_out_features * 1.2)  # å¢åŠ 20%
            expansion_size = new_out_features - old_out_features
            
            # åˆ›å»ºæ–°çš„æƒé‡å’Œåç½®
            new_weight = torch.zeros(new_out_features, target_module.in_features)
            new_bias = torch.zeros(new_out_features) if target_module.bias is not None else None
            
            # å¤åˆ¶åŸæœ‰æƒé‡
            new_weight[:old_out_features] = target_module.weight.data
            if new_bias is not None:
                new_bias[:old_out_features] = target_module.bias.data
                
            # åˆå§‹åŒ–æ–°å¢çš„ç¥ç»å…ƒ
            with torch.no_grad():
                # ä½¿ç”¨å°çš„éšæœºå€¼åˆå§‹åŒ–æ–°ç¥ç»å…ƒ
                nn.init.normal_(new_weight[old_out_features:], mean=0, std=0.01)
                if new_bias is not None:
                    nn.init.zeros_(new_bias[old_out_features:])
                    
            # æ›´æ–°æ¨¡å—
            target_module.out_features = new_out_features
            target_module.weight = nn.Parameter(new_weight)
            if target_module.bias is not None:
                target_module.bias = nn.Parameter(new_bias)
                
            parameters_added = expansion_size * (target_module.in_features + 1)
            
        elif isinstance(target_module, nn.Conv2d):
            # æ‰©å±•å·ç§¯å±‚
            old_out_channels = target_module.out_channels
            new_out_channels = int(old_out_channels * 1.15)  # å¢åŠ 15%
            expansion_size = new_out_channels - old_out_channels
            
            # åˆ›å»ºæ–°çš„å·ç§¯å±‚
            new_conv = nn.Conv2d(
                target_module.in_channels,
                new_out_channels,
                target_module.kernel_size,
                target_module.stride,
                target_module.padding,
                target_module.dilation,
                target_module.groups,
                target_module.bias is not None
            )
            
            # å¤åˆ¶åŸæœ‰æƒé‡
            with torch.no_grad():
                new_conv.weight.data[:old_out_channels] = target_module.weight.data
                if target_module.bias is not None:
                    new_conv.bias.data[:old_out_channels] = target_module.bias.data
                    
                # åˆå§‹åŒ–æ–°å¢çš„é€šé“
                nn.init.kaiming_normal_(new_conv.weight.data[old_out_channels:])
                if new_conv.bias is not None:
                    nn.init.zeros_(new_conv.bias.data[old_out_channels:])
                    
            # æ›¿æ¢æ¨¡å—
            parent_name = '.'.join(layer_name.split('.')[:-1])
            child_name = layer_name.split('.')[-1]
            
            if parent_name:
                parent_module = new_model
                for part in parent_name.split('.'):
                    parent_module = getattr(parent_module, part)
                setattr(parent_module, child_name, new_conv)
            else:
                setattr(new_model, child_name, new_conv)
                
            parameters_added = expansion_size * target_module.in_channels * \
                             target_module.kernel_size[0] * target_module.kernel_size[1]
            
        self.division_history.append({
            'layer': layer_name,
            'type': 'width_expansion',
            'parameters_added': parameters_added
        })
        
        logger.info(f"æ‰§è¡Œå®½åº¦æ‰©å±•: {layer_name}, æ–°å¢å‚æ•°: {parameters_added}")
        return new_model, parameters_added
    
    def _expand_network_depth(self, model: nn.Module, layer_name: str) -> Tuple[nn.Module, int]:
        """æ‰©å±•ç½‘ç»œæ·±åº¦ï¼ˆæ·»åŠ æ–°å±‚ï¼‰"""
        # æ·±åº¦æ‰©å±•çš„å®ç°è¾ƒä¸ºå¤æ‚ï¼Œè¿™é‡Œæä¾›åŸºç¡€æ¡†æ¶
        logger.info(f"æ·±åº¦æ‰©å±•åŠŸèƒ½å¾…å®ç°: {layer_name}")
        return model, 0
    
    def _create_branch(self, model: nn.Module, layer_name: str) -> Tuple[nn.Module, int]:
        """åˆ›å»ºåˆ†æ”¯ç»“æ„"""
        # åˆ†æ”¯åˆ›å»ºçš„å®ç°è¾ƒä¸ºå¤æ‚ï¼Œè¿™é‡Œæä¾›åŸºç¡€æ¡†æ¶
        logger.info(f"åˆ†æ”¯åˆ›å»ºåŠŸèƒ½å¾…å®ç°: {layer_name}")
        return model, 0

class DNMFramework:
    """Dynamic Neural Morphogenesis Framework - ä¸»æ¡†æ¶"""
    
    def __init__(self, model: nn.Module, config: Optional[Dict[str, Any]] = None):
        self.model = model
        self.config = config or {}
        
        # åˆå§‹åŒ–è§¦å‘å™¨
        self.triggers = [
            InformationTheoryTrigger(),
            BiologicalPrinciplesTrigger(), 
            DynamicalSystemsTrigger(),
            CognitiveScienceTrigger(),
            NetworkScienceTrigger()
        ]
        
        # æ‰§è¡Œå™¨
        self.executor = NeuronDivisionExecutor()
        
        # çŠ¶æ€è¿½è¸ª
        self.morphogenesis_events = []
        self.performance_history = deque(maxlen=100)
        self.activation_cache = {}
        self.gradient_cache = {}
        
        # é…ç½®å‚æ•°
        self.morphogenesis_interval = self.config.get('morphogenesis_interval', 4)
        self.max_morphogenesis_per_epoch = self.config.get('max_morphogenesis_per_epoch', 2)
        self.performance_improvement_threshold = self.config.get('performance_improvement_threshold', 0.02)
        
    def should_trigger_morphogenesis(self, epoch: int, train_metrics: Dict[str, float], 
                                   val_metrics: Dict[str, float]) -> Tuple[bool, List[str]]:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è§¦å‘å½¢æ€å‘ç”Ÿ"""
        
        # æ£€æŸ¥è§¦å‘é—´éš”
        if epoch % self.morphogenesis_interval != 0:
            return False, []
            
        # å‡†å¤‡ä¸Šä¸‹æ–‡ä¿¡æ¯
        context = {
            'epoch': epoch,
            'train_accuracy': train_metrics.get('accuracy', 0.0),
            'val_accuracy': val_metrics.get('accuracy', 0.0),
            'current_loss': train_metrics.get('loss', float('inf')),
            'current_performance': val_metrics.get('accuracy', 0.0),
            'learning_rate': train_metrics.get('learning_rate', 1e-3),
            'model': self.model,
            'activations': self.activation_cache,
            'gradients': self.gradient_cache
        }
        
        # æ£€æŸ¥æ‰€æœ‰è§¦å‘å™¨
        trigger_results = []
        triggered_reasons = []
        
        for trigger in self.triggers:
            should_trigger, reason = trigger.should_trigger(context)
            if should_trigger:
                trigger_results.append((trigger, reason))
                triggered_reasons.append(f"{trigger.__class__.__name__}: {reason}")
                
        # æ ¹æ®ä¼˜å…ˆçº§æ’åº
        trigger_results.sort(key=lambda x: x[0].get_priority(), reverse=True)
        
        # è‡³å°‘æœ‰ä¸€ä¸ªé«˜ä¼˜å…ˆçº§è§¦å‘å™¨æ¿€æ´»
        if trigger_results and trigger_results[0][0].get_priority() >= 0.8:
            return True, triggered_reasons
            
        # æˆ–è€…æœ‰å¤šä¸ªä¸­ç­‰ä¼˜å…ˆçº§è§¦å‘å™¨æ¿€æ´»
        if len(trigger_results) >= 2 and all(t[0].get_priority() >= 0.7 for t in trigger_results[:2]):
            return True, triggered_reasons
            
        return False, []
    
    def execute_morphogenesis(self, epoch: int) -> Dict[str, Any]:
        """æ‰§è¡Œå½¢æ€å‘ç”Ÿ"""
        logger.info(f"ğŸ”„ Triggering morphogenesis analysis...")
        
        results = {
            'neuron_divisions': 0,
            'connection_growths': 0,
            'optimizations': 0,
            'parameters_added': 0,
            'events': []
        }
        
        # åˆ†ææœ€ä½³åˆ†è£‚ä½ç½®
        best_layers = self._identify_optimal_division_layers()
        
        divisions_executed = 0
        for layer_name, score in best_layers[:self.max_morphogenesis_per_epoch]:
            # æ‰§è¡Œç¥ç»å…ƒåˆ†è£‚
            new_model, params_added = self.executor.execute_division(
                self.model, layer_name, 'width_expansion'
            )
            
            if params_added > 0:
                self.model = new_model
                divisions_executed += 1
                results['parameters_added'] += params_added
                
                # è®°å½•äº‹ä»¶
                event = MorphogenesisEvent(
                    epoch=epoch,
                    event_type='neuron_division',
                    location=layer_name,
                    trigger_reason=f"ä¼˜åŒ–åˆ†æ•°: {score:.4f}",
                    performance_before=self.performance_history[-1] if self.performance_history else 0.0,
                    parameters_added=params_added
                )
                
                self.morphogenesis_events.append(event)
                results['events'].append(event)
                
        results['neuron_divisions'] = divisions_executed
        
        logger.info(f"DNM Neuron Division completed: {divisions_executed} splits executed")
        
        return results
    
    def _identify_optimal_division_layers(self) -> List[Tuple[str, float]]:
        """è¯†åˆ«æœ€ä½³åˆ†è£‚å±‚"""
        layer_scores = []
        
        for name, module in self.model.named_modules():
            if isinstance(module, (nn.Linear, nn.Conv2d)):
                score = self._compute_division_score(name, module)
                layer_scores.append((name, score))
                
        # æŒ‰åˆ†æ•°æ’åº
        layer_scores.sort(key=lambda x: x[1], reverse=True)
        return layer_scores
    
    def _compute_division_score(self, layer_name: str, module: nn.Module) -> float:
        """è®¡ç®—å±‚çš„åˆ†è£‚åˆ†æ•°"""
        score = 0.0
        
        # åŸºäºæƒé‡åˆ†æ
        if hasattr(module, 'weight') and module.weight is not None:
            weight = module.weight.data
            
            # æƒé‡æ–¹å·®ï¼ˆé«˜æ–¹å·®è¡¨æ˜ç¥ç»å…ƒåˆ†åŒ–ç¨‹åº¦é«˜ï¼‰
            weight_var = torch.var(weight).item()
            score += weight_var * 0.3
            
            # æƒé‡èŒƒæ•°ï¼ˆé€‚ä¸­çš„èŒƒæ•°æœ€ä½³ï¼‰
            weight_norm = torch.norm(weight).item()
            normalized_norm = weight_norm / weight.numel()
            score += (1.0 - abs(normalized_norm - 0.1)) * 0.2
            
        # åŸºäºæ¿€æ´»å€¼åˆ†æ
        if layer_name in self.activation_cache:
            activation = self.activation_cache[layer_name]
            
            # æ¿€æ´»å€¼å¤šæ ·æ€§
            act_std = torch.std(activation).item()
            score += act_std * 0.3
            
            # æ¿€æ´»å€¼é¥±å’Œåº¦
            saturation = torch.mean((activation > 0.9).float()).item()
            score += (1.0 - saturation) * 0.2
            
        return score
    
    def update_caches(self, activations: Dict[str, torch.Tensor], 
                      gradients: Dict[str, torch.Tensor]):
        """æ›´æ–°æ¿€æ´»å€¼å’Œæ¢¯åº¦ç¼“å­˜"""
        self.activation_cache = {k: v.detach().clone() for k, v in activations.items()}
        self.gradient_cache = {k: v.detach().clone() if v is not None else None 
                              for k, v in gradients.items()}
    
    def record_performance(self, performance: float):
        """è®°å½•æ€§èƒ½"""
        self.performance_history.append(performance)
    
    def get_morphogenesis_summary(self) -> Dict[str, Any]:
        """è·å–å½¢æ€å‘ç”Ÿæ‘˜è¦"""
        if not self.morphogenesis_events:
            return {
                'total_events': 0,
                'total_neuron_divisions': 0,
                'total_parameters_added': 0,
                'performance_improvement': 0.0
            }
            
        total_events = len(self.morphogenesis_events)
        neuron_divisions = sum(1 for e in self.morphogenesis_events 
                              if e.event_type == 'neuron_division')
        total_params = sum(e.parameters_added for e in self.morphogenesis_events)
        
        # è®¡ç®—æ€§èƒ½æ”¹å–„
        if len(self.performance_history) >= 2:
            initial_perf = self.performance_history[0]
            final_perf = self.performance_history[-1]
            performance_improvement = final_perf - initial_perf
        else:
            performance_improvement = 0.0
            
        return {
            'total_events': total_events,
            'total_neuron_divisions': neuron_divisions,
            'total_parameters_added': total_params,
            'performance_improvement': performance_improvement,
            'events_detail': [
                {
                    'epoch': e.epoch,
                    'type': e.event_type,
                    'location': e.location,
                    'params_added': e.parameters_added,
                    'reason': e.trigger_reason
                }
                for e in self.morphogenesis_events
            ]
        }