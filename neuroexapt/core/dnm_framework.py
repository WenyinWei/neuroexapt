#!/usr/bin/env python3
"""
Dynamic Neural Morphogenesis (DNM) æ¡†æ¶ä¸»æ§åˆ¶å™¨

æ•´åˆä¸‰å¤§åˆ›æ–°æ¨¡å—ï¼š
1. ä¿¡æ¯ç†µé©±åŠ¨çš„ç¥ç»å…ƒåˆ†è£‚ (DNMNeuronDivision)
2. æ¢¯åº¦å¼•å¯¼çš„è¿æ¥ç”Ÿé•¿ (DNMConnectionGrowth)  
3. å¤šç›®æ ‡è¿›åŒ–ä¼˜åŒ– (DNMMultiObjectiveOptimization)

æä¾›ç»Ÿä¸€çš„è®­ç»ƒæ¥å£ï¼Œå®ç°çœŸæ­£çš„ç¥ç»ç½‘ç»œè‡ªé€‚åº”ç”Ÿé•¿
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import time
import copy
from typing import Dict, List, Tuple, Optional, Any, Callable
from collections import defaultdict, deque
import logging

# å¯¼å…¥DNMæ ¸å¿ƒæ¨¡å—
from .dnm_neuron_division import DNMNeuronDivision
from .dnm_connection_growth import DNMConnectionGrowth
from ..math.pareto_optimization import DNMMultiObjectiveOptimization

logger = logging.getLogger(__name__)


class DNMFramework:
    """
    Dynamic Neural Morphogenesis ä¸»æ¡†æ¶
    
    é›†æˆæ‰€æœ‰DNMç»„ä»¶ï¼Œæä¾›å®Œæ•´çš„è‡ªé€‚åº”ç¥ç»ç½‘ç»œç”Ÿé•¿åŠŸèƒ½
    """
    
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or self._default_config()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆå§‹åŒ–DNMæ ¸å¿ƒç»„ä»¶
        self.neuron_division = DNMNeuronDivision(self.config.get('neuron_division'))
        self.connection_growth = DNMConnectionGrowth(self.config.get('connection_growth'))
        self.multi_objective = DNMMultiObjectiveOptimization(self.config.get('multi_objective'))
        
        # è®­ç»ƒçŠ¶æ€
        self.training_active = False
        self.current_epoch = 0
        self.model_population = []
        
        # æ€§èƒ½å’Œæ¼”åŒ–è¿½è¸ª
        self.performance_history = []
        self.morphogenesis_events = []
        self.architecture_snapshots = []
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.statistics = {
            'total_neuron_splits': 0,
            'total_connections_grown': 0,
            'total_optimizations': 0,
            'performance_improvements': 0,
            'architecture_complexity_growth': 0.0
        }
        
        logger.info(f"ğŸ§¬ DNM Framework initialized on {self.device}")
        logger.info(f"   Configuration: {self.config}")
    
    def _default_config(self) -> Dict:
        """é»˜è®¤é…ç½®"""
        return {
            'neuron_division': {
                'splitter': {
                    'entropy_threshold': 0.7,
                    'overload_threshold': 0.6,
                    'split_probability': 0.4,
                    'max_splits_per_layer': 3,
                    'inheritance_noise': 0.1
                },
                'monitoring': {
                    'target_layers': ['conv', 'linear'],
                    'analysis_frequency': 5,
                    'min_epoch_before_split': 10
                }
            },
            'connection_growth': {
                'analyzer': {
                    'correlation_threshold': 0.15,
                    'history_length': 8
                },
                'growth': {
                    'max_new_connections': 3,
                    'min_correlation_threshold': 0.1,
                    'growth_frequency': 8,
                    'connection_types': ['skip_connection', 'attention_connection']
                },
                'filtering': {
                    'min_layer_distance': 2,
                    'max_layer_distance': 6,
                    'avoid_redundant_connections': True
                }
            },
            'multi_objective': {
                'evolution': {
                    'population_size': 12,
                    'max_generations': 15,
                    'mutation_rate': 0.3,
                    'crossover_rate': 0.7,
                    'elitism_ratio': 0.15
                },
                'optimization': {
                    'trigger_frequency': 20,
                    'performance_plateau_threshold': 0.01,
                    'min_improvement_epochs': 5
                }
            },
            'framework': {
                'morphogenesis_frequency': 5,  # æ¯5ä¸ªepochæ£€æŸ¥ä¸€æ¬¡å½¢æ€å‘ç”Ÿ
                'performance_tracking_window': 10,  # æ€§èƒ½è¿½è¸ªçª—å£
                'early_stopping_patience': 15,
                'target_accuracy_threshold': 95.0,
                'enable_architecture_snapshots': True,
                'adaptive_morphogenesis': True  # è‡ªé€‚åº”å½¢æ€å‘ç”Ÿ
            }
        }
    
    def train_with_morphogenesis(self, 
                                model: nn.Module,
                                train_loader: DataLoader,
                                val_loader: DataLoader,
                                epochs: int,
                                optimizer: Optional[torch.optim.Optimizer] = None,
                                criterion: Optional[nn.Module] = None,
                                callbacks: Optional[List[Callable]] = None) -> Dict[str, Any]:
        """
        ä½¿ç”¨DNMè¿›è¡Œè®­ç»ƒ
        
        Args:
            model: åˆå§‹æ¨¡å‹
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            epochs: è®­ç»ƒè½®æ•°
            optimizer: ä¼˜åŒ–å™¨ï¼ˆå¯é€‰ï¼‰
            criterion: æŸå¤±å‡½æ•°ï¼ˆå¯é€‰ï¼‰
            callbacks: å›è°ƒå‡½æ•°åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        logger.info(f"ğŸš€ Starting DNM training for {epochs} epochs")
        logger.info(f"   Initial model parameters: {sum(p.numel() for p in model.parameters()):,}")
        
        # åˆå§‹åŒ–
        self.training_active = True
        self.current_epoch = 0
        model = model.to(self.device)
        
        # é»˜è®¤ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        if optimizer is None:
            optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)
        if criterion is None:
            criterion = nn.CrossEntropyLoss()
        
        # æ³¨å†Œç¥ç»å…ƒåˆ†è£‚ç›‘æ§
        self.neuron_division.register_model_hooks(model)
        
        # åˆå§‹åŒ–æ¨¡å‹ç§ç¾¤ï¼ˆç”¨äºå¤šç›®æ ‡ä¼˜åŒ–ï¼‰
        self.model_population = [copy.deepcopy(model) for _ in range(3)]
        
        # è®°å½•åˆå§‹æ¶æ„å¿«ç…§
        if self.config['framework']['enable_architecture_snapshots']:
            self._take_architecture_snapshot(model, epoch=0)
        
        best_val_acc = 0.0
        patience_counter = 0
        
        try:
            for epoch in range(epochs):
                self.current_epoch = epoch
                epoch_start_time = time.time()
                
                logger.info(f"\nğŸ§¬ Epoch {epoch+1}/{epochs} - Dynamic Morphogenesis")
                
                # 1. æ ‡å‡†è®­ç»ƒæ­¥éª¤
                train_loss, train_acc = self._train_epoch(model, train_loader, optimizer, criterion)
                val_loss, val_acc = self._validate_epoch(model, val_loader, criterion)
                
                # 2. æ”¶é›†æ¢¯åº¦ç”¨äºè¿æ¥ç”Ÿé•¿åˆ†æ
                self.connection_growth.collect_and_analyze_gradients(model)
                
                # 3. è®°å½•æ€§èƒ½
                epoch_record = {
                    'epoch': epoch,
                    'train_loss': train_loss,
                    'train_acc': train_acc,
                    'val_loss': val_loss,
                    'val_acc': val_acc,
                    'model_params': sum(p.numel() for p in model.parameters()),
                    'epoch_time': time.time() - epoch_start_time
                }
                self.performance_history.append(epoch_record)
                
                # 4. å½¢æ€å‘ç”Ÿæ£€æŸ¥å’Œæ‰§è¡Œ
                morphogenesis_triggered = False
                
                if self._should_trigger_morphogenesis(epoch, val_acc):
                    logger.info("  ğŸ”„ Triggering morphogenesis analysis...")
                    
                    # ç¥ç»å…ƒåˆ†è£‚
                    neuron_result = self.neuron_division.analyze_and_split(model, epoch)
                    
                    # è¿æ¥ç”Ÿé•¿
                    connection_result = self.connection_growth.analyze_and_grow_connections(model, epoch)
                    
                    # å¤šç›®æ ‡ä¼˜åŒ–
                    optimization_result = self.multi_objective.optimize_architecture_population(
                        self.model_population, train_loader, val_loader, epoch
                    )
                    
                    # è®°å½•å½¢æ€å‘ç”Ÿäº‹ä»¶
                    if (neuron_result.get('splits_executed', 0) > 0 or 
                        connection_result.get('connections_grown', 0) > 0 or
                        optimization_result.get('optimized', False)):
                        
                        morphogenesis_event = {
                            'epoch': epoch,
                            'neuron_splits': neuron_result.get('splits_executed', 0),
                            'connections_grown': connection_result.get('connections_grown', 0),
                            'optimization_triggered': optimization_result.get('optimized', False),
                            'performance_before': val_acc,
                            'details': {
                                'neuron_division': neuron_result,
                                'connection_growth': connection_result,
                                'multi_objective': optimization_result
                            }
                        }
                        self.morphogenesis_events.append(morphogenesis_event)
                        morphogenesis_triggered = True
                        
                        # æ›´æ–°ç»Ÿè®¡
                        self.statistics['total_neuron_splits'] += neuron_result.get('splits_executed', 0)
                        self.statistics['total_connections_grown'] += connection_result.get('connections_grown', 0)
                        if optimization_result.get('optimized', False):
                            self.statistics['total_optimizations'] += 1
                        
                        # æ›´æ–°ä¼˜åŒ–å™¨ä»¥åŒ…å«æ–°å‚æ•°
                        self._update_optimizer(optimizer, model)
                        
                        # æ›´æ–°æ¨¡å‹ç§ç¾¤
                        if optimization_result.get('optimized', False) and optimization_result.get('best_models'):
                            self.model_population = optimization_result['best_models'][:3]
                            # é€‰æ‹©æœ€å¥½çš„æ¨¡å‹ç»§ç»­è®­ç»ƒ
                            model = self._select_best_model_for_training(optimization_result['best_models'], 
                                                                        optimization_result['best_fitness'])
                            self._update_optimizer(optimizer, model)
                
                # 5. è¾“å‡ºçŠ¶æ€
                current_params = sum(p.numel() for p in model.parameters())
                if epoch == 0:
                    initial_params = current_params
                else:
                    param_growth = (current_params - initial_params) / initial_params * 100
                
                logger.info(f"  ğŸ“Š Train: {train_acc:.2f}% | Val: {val_acc:.2f}% | "
                           f"Params: {current_params:,} | "
                           f"{'ğŸ§¬ Morphogenesis' if morphogenesis_triggered else ''}")
                
                # 6. æ—©æœŸåœæ­¢å’Œæ€§èƒ½è¿½è¸ª
                if val_acc > best_val_acc:
                    best_val_acc = val_acc
                    patience_counter = 0
                    self.statistics['performance_improvements'] += 1
                else:
                    patience_counter += 1
                
                if val_acc >= self.config['framework']['target_accuracy_threshold']:
                    logger.info(f"  ğŸ¯ Reached target accuracy: {val_acc:.2f}%!")
                    break
                
                if patience_counter >= self.config['framework']['early_stopping_patience']:
                    logger.info(f"  ğŸ›‘ Early stopping triggered (patience: {patience_counter})")
                    break
                
                # 7. æ‰§è¡Œå›è°ƒ
                if callbacks:
                    for callback in callbacks:
                        callback(self, model, epoch_record)
                
                # 8. æ¶æ„å¿«ç…§
                if (self.config['framework']['enable_architecture_snapshots'] and 
                    epoch % 10 == 0 and morphogenesis_triggered):
                    self._take_architecture_snapshot(model, epoch)
        
        except KeyboardInterrupt:
            logger.info("  â¹ï¸ Training interrupted by user")
        
        finally:
            # æ¸…ç†
            self.training_active = False
            self.neuron_division.remove_model_hooks(model)
        
        # ç”Ÿæˆè®­ç»ƒæ€»ç»“
        training_summary = self._generate_training_summary(model, best_val_acc)
        
        logger.info("âœ… DNM training completed")
        logger.info(f"   Final accuracy: {val_acc:.2f}% | Best: {best_val_acc:.2f}%")
        logger.info(f"   Morphogenesis events: {len(self.morphogenesis_events)}")
        
        return {
            'model': model,
            'best_val_accuracy': best_val_acc,
            'final_val_accuracy': val_acc,
            'performance_history': self.performance_history,
            'morphogenesis_events': self.morphogenesis_events,
            'architecture_snapshots': self.architecture_snapshots,
            'statistics': self.statistics,
            'training_summary': training_summary
        }
    
    def _should_trigger_morphogenesis(self, epoch: int, current_performance: float) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è§¦å‘å½¢æ€å‘ç”Ÿ"""
        
        # åŸºç¡€é¢‘ç‡æ£€æŸ¥
        if epoch % self.config['framework']['morphogenesis_frequency'] != 0:
            return False
        
        # æœ€å°epochæ£€æŸ¥
        if epoch < 10:
            return False
        
        # è‡ªé€‚åº”å½¢æ€å‘ç”Ÿ
        if self.config['framework']['adaptive_morphogenesis']:
            # æ£€æŸ¥æœ€è¿‘çš„æ€§èƒ½å¹³å°æœŸ
            recent_history = self.performance_history[-self.config['framework']['performance_tracking_window']:]
            if len(recent_history) >= 5:
                recent_accs = [h['val_acc'] for h in recent_history]
                performance_std = np.std(recent_accs)
                
                # å¦‚æœæ€§èƒ½å˜åŒ–å¾ˆå°ï¼Œæ›´å¯èƒ½è§¦å‘å½¢æ€å‘ç”Ÿ
                if performance_std < self.config['multi_objective']['optimization']['performance_plateau_threshold']:
                    return True
        
        return True
    
    def _train_epoch(self, model: nn.Module, train_loader: DataLoader, 
                    optimizer: torch.optim.Optimizer, criterion: nn.Module) -> Tuple[float, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            
            optimizer.step()
            
            total_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
        
        avg_loss = total_loss / len(train_loader)
        accuracy = 100.0 * correct / total
        return avg_loss, accuracy
    
    def _validate_epoch(self, model: nn.Module, val_loader: DataLoader, 
                       criterion: nn.Module) -> Tuple[float, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = model(data)
                loss = criterion(output, target)
                
                total_loss += loss.item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)
        
        avg_loss = total_loss / len(val_loader)
        accuracy = 100.0 * correct / total
        return avg_loss, accuracy
    
    def _update_optimizer(self, optimizer: torch.optim.Optimizer, model: nn.Module) -> None:
        """æ›´æ–°ä¼˜åŒ–å™¨å‚æ•°ç»„ä»¥åŒ…å«æ–°å‚æ•°"""
        # ç®€å•é‡å»ºä¼˜åŒ–å™¨
        state_dict = optimizer.state_dict()
        lr = optimizer.param_groups[0]['lr']
        
        # åˆ›å»ºæ–°ä¼˜åŒ–å™¨
        new_optimizer = type(optimizer)(model.parameters(), lr=lr)
        
        # å°è¯•æ¢å¤çŠ¶æ€ï¼ˆå¯¹äºæ–°å‚æ•°ï¼ŒçŠ¶æ€å°†ä¸ºç©ºï¼‰
        try:
            new_optimizer.load_state_dict(state_dict)
        except Exception as e:
            logger.warning(f"Could not restore optimizer state: {e}")
        
        # æ›´æ–°åŸä¼˜åŒ–å™¨çš„å¼•ç”¨
        optimizer.param_groups = new_optimizer.param_groups
        optimizer.state = new_optimizer.state
    
    def _select_best_model_for_training(self, models: List[nn.Module], fitness_list: List) -> nn.Module:
        """ä»å€™é€‰æ¨¡å‹ä¸­é€‰æ‹©æœ€ä½³çš„ç”¨äºç»§ç»­è®­ç»ƒ"""
        if not models or not fitness_list:
            return models[0] if models else None
        
        # é€‰æ‹©å‡†ç¡®ç‡æœ€é«˜çš„æ¨¡å‹
        best_idx = 0
        best_accuracy = fitness_list[0].accuracy
        
        for i, fitness in enumerate(fitness_list):
            if fitness.accuracy > best_accuracy:
                best_accuracy = fitness.accuracy
                best_idx = i
        
        logger.info(f"  ğŸ¯ Selected model {best_idx} with accuracy {best_accuracy:.2f}% for continued training")
        return copy.deepcopy(models[best_idx])
    
    def _take_architecture_snapshot(self, model: nn.Module, epoch: int) -> None:
        """æ‹æ‘„æ¶æ„å¿«ç…§"""
        snapshot = {
            'epoch': epoch,
            'model_structure': str(model),
            'parameter_count': sum(p.numel() for p in model.parameters()),
            'layer_info': []
        }
        
        for name, module in model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)):
                layer_info = {
                    'name': name,
                    'type': type(module).__name__,
                    'parameters': sum(p.numel() for p in module.parameters())
                }
                
                if isinstance(module, nn.Conv2d):
                    layer_info.update({
                        'in_channels': module.in_channels,
                        'out_channels': module.out_channels,
                        'kernel_size': module.kernel_size
                    })
                elif isinstance(module, nn.Linear):
                    layer_info.update({
                        'in_features': module.in_features,
                        'out_features': module.out_features
                    })
                
                snapshot['layer_info'].append(layer_info)
        
        self.architecture_snapshots.append(snapshot)
        logger.debug(f"Architecture snapshot taken at epoch {epoch}")
    
    def _generate_training_summary(self, model: nn.Module, best_val_acc: float) -> Dict[str, Any]:
        """ç”Ÿæˆè®­ç»ƒæ€»ç»“"""
        if not self.performance_history:
            return {}
        
        initial_params = self.performance_history[0]['model_params']
        final_params = self.performance_history[-1]['model_params']
        param_growth = (final_params - initial_params) / initial_params * 100
        
        summary = {
            'training_epochs': len(self.performance_history),
            'best_validation_accuracy': best_val_acc,
            'final_validation_accuracy': self.performance_history[-1]['val_acc'],
            'accuracy_improvement': self.performance_history[-1]['val_acc'] - self.performance_history[0]['val_acc'],
            'parameter_growth': param_growth,
            'initial_parameters': initial_params,
            'final_parameters': final_params,
            'morphogenesis_events': len(self.morphogenesis_events),
            'total_neuron_splits': self.statistics['total_neuron_splits'],
            'total_connections_grown': self.statistics['total_connections_grown'],
            'total_optimizations': self.statistics['total_optimizations'],
            'avg_epoch_time': np.mean([h['epoch_time'] for h in self.performance_history]),
            'total_training_time': sum(h['epoch_time'] for h in self.performance_history)
        }
        
        return summary
    
    def get_morphogenesis_summary(self) -> Dict[str, Any]:
        """è·å–å½¢æ€å‘ç”Ÿæ€»ç»“"""
        return {
            'framework_statistics': self.statistics,
            'neuron_division_summary': self.neuron_division.get_split_summary(),
            'connection_growth_summary': self.connection_growth.get_growth_summary(),
            'multi_objective_summary': self.multi_objective.get_optimization_summary(),
            'morphogenesis_events': self.morphogenesis_events,
            'architecture_snapshots': self.architecture_snapshots[-5:] if self.architecture_snapshots else []
        }
    
    def export_evolved_model(self, filepath: str, model: nn.Module) -> None:
        """å¯¼å‡ºæ¼”åŒ–åçš„æ¨¡å‹"""
        save_dict = {
            'model_state_dict': model.state_dict(),
            'model_structure': str(model),
            'morphogenesis_summary': self.get_morphogenesis_summary(),
            'performance_history': self.performance_history,
            'config': self.config
        }
        
        torch.save(save_dict, filepath)
        logger.info(f"Evolved model exported to {filepath}")


# ä¾¿æ·å‡½æ•°
def train_with_dnm(model: nn.Module,
                   train_loader: DataLoader,
                   val_loader: DataLoader,
                   epochs: int = 100,
                   config: Optional[Dict] = None,
                   **kwargs) -> Dict[str, Any]:
    """
    ä¾¿æ·çš„DNMè®­ç»ƒå‡½æ•°
    
    Args:
        model: æ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®
        val_loader: éªŒè¯æ•°æ®
        epochs: è®­ç»ƒè½®æ•°
        config: DNMé…ç½®
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        è®­ç»ƒç»“æœ
    """
    dnm = DNMFramework(config)
    return dnm.train_with_morphogenesis(model, train_loader, val_loader, epochs, **kwargs)


# æµ‹è¯•å‡½æ•°
def test_dnm_framework():
    """æµ‹è¯•DNMæ¡†æ¶"""
    print("ğŸ§¬ Testing Complete DNM Framework")
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    class TestEvolvableCNN(nn.Module):
        def __init__(self, num_classes=10):
            super().__init__()
            self.features = nn.Sequential(
                nn.Conv2d(3, 32, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(2),
                nn.Conv2d(32, 64, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(2),
                nn.Conv2d(64, 128, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.AdaptiveAvgPool2d((4, 4))
            )
            
            self.classifier = nn.Sequential(
                nn.Flatten(),
                nn.Linear(128 * 16, 256),
                nn.ReLU(inplace=True),
                nn.Dropout(0.5),
                nn.Linear(256, 128),
                nn.ReLU(inplace=True),
                nn.Linear(128, num_classes)
            )
        
        def forward(self, x):
            x = self.features(x)
            x = self.classifier(x)
            return x
    
    # åˆ›å»ºè™šæ‹Ÿæ•°æ®é›†
    from torch.utils.data import TensorDataset
    
    train_data = torch.randn(500, 3, 32, 32)
    train_labels = torch.randint(0, 10, (500,))
    val_data = torch.randn(100, 3, 32, 32)
    val_labels = torch.randint(0, 10, (100,))
    
    train_dataset = TensorDataset(train_data, train_labels)
    val_dataset = TensorDataset(val_data, val_labels)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹
    model = TestEvolvableCNN()
    
    # ä½¿ç”¨DNMè®­ç»ƒ
    result = train_with_dnm(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        epochs=30
    )
    
    print(f"Training completed: {result['training_summary']}")
    print(f"Morphogenesis events: {len(result['morphogenesis_events'])}")
    
    return result


if __name__ == "__main__":
    test_dnm_framework()