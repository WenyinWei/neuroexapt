"""
é‡æ„åçš„è´å¶æ–¯å½¢æ€å‘ç”Ÿå¼•æ“

ä½¿ç”¨ç»„ä»¶åŒ–æ¶æ„ï¼Œæé«˜å¯ç»´æŠ¤æ€§å’Œå¯æµ‹è¯•æ€§
"""

import torch
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
import logging

logger = logging.getLogger(__name__)


class RefactoredBayesianMorphogenesisEngine:
    """
    é‡æ„åçš„è´å¶æ–¯å½¢æ€å‘ç”Ÿå¼•æ“
    
    ç»„ä»¶åŒ–è®¾è®¡ï¼š
    1. ç‰¹å¾æå–å™¨ - æå–æ¶æ„ç‰¹å¾
    2. å€™é€‰ç‚¹æ£€æµ‹å™¨ - å‘ç°å˜å¼‚å€™é€‰ç‚¹
    3. è´å¶æ–¯æ¨ç†å™¨ - è¿›è¡Œè´å¶æ–¯æ¨ç†
    4. æ•ˆç”¨è¯„ä¼°å™¨ - è¯„ä¼°å˜å¼‚æ•ˆç”¨
    5. å†³ç­–åˆ¶å®šå™¨ - åˆ¶å®šæœ€ç»ˆå†³ç­–
    """
    
    def __init__(self, config=None, feature_extractor=None, candidate_detector=None):
        # é…ç½®ç®¡ç†
        from .bayesian_prediction.bayesian_config import BayesianConfigManager
        self.config_manager = BayesianConfigManager()
        self.config = self.config_manager.get_config()
        
        if config:
            self.config_manager.update_config(config)
        
        # ç»„ä»¶æ³¨å…¥
        self.feature_extractor = feature_extractor or self._create_feature_extractor()
        self.candidate_detector = candidate_detector or self._create_candidate_detector()
        self.bayesian_inference = self._create_bayesian_inference()
        self.utility_evaluator = self._create_utility_evaluator()
        self.decision_maker = self._create_decision_maker()
        
        # å†å²è®°å½•
        self.mutation_history = []
        self.performance_history = []
        
    def _create_feature_extractor(self):
        """åˆ›å»ºç‰¹å¾æå–å™¨"""
        from .bayesian_prediction.feature_extractor import ArchitectureFeatureExtractor
        return ArchitectureFeatureExtractor()
    
    def _create_candidate_detector(self):
        """åˆ›å»ºå€™é€‰ç‚¹æ£€æµ‹å™¨"""
        from .bayesian_prediction.candidate_detector import BayesianCandidateDetector
        return BayesianCandidateDetector(self.config)
    
    def _create_bayesian_inference(self):
        """åˆ›å»ºè´å¶æ–¯æ¨ç†å™¨"""
        return BayesianInferenceEngine(self.config)
    
    def _create_utility_evaluator(self):
        """åˆ›å»ºæ•ˆç”¨è¯„ä¼°å™¨"""
        return UtilityEvaluator(self.config)
    
    def _create_decision_maker(self):
        """åˆ›å»ºå†³ç­–åˆ¶å®šå™¨"""
        return DecisionMaker(self.config)
    
    def bayesian_morphogenesis_analysis(self, model: torch.nn.Module, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œè´å¶æ–¯å½¢æ€å‘ç”Ÿåˆ†æ"""
        
        logger.info("ğŸ§  å¼€å§‹è´å¶æ–¯å½¢æ€å‘ç”Ÿåˆ†æ")
        
        try:
            # 1. ç‰¹å¾æå–
            features = self.feature_extractor.extract_features(model, context)
            logger.info(f"âœ… ç‰¹å¾æå–å®Œæˆ: {len(features)}ä¸ªç‰¹å¾ç»´åº¦")
            
            # 2. å€™é€‰ç‚¹æ£€æµ‹
            candidates = self.candidate_detector.detect_candidates(features)
            logger.info(f"ğŸ” å€™é€‰ç‚¹æ£€æµ‹å®Œæˆ: å‘ç°{len(candidates)}ä¸ªå€™é€‰ç‚¹")
            
            if not candidates:
                return self._create_no_candidates_result()
            
            # 3. è´å¶æ–¯æ¨ç†
            inference_results = self.bayesian_inference.analyze_candidates(candidates, features, self.mutation_history)
            logger.info(f"ğŸ¯ è´å¶æ–¯æ¨ç†å®Œæˆ: åˆ†æäº†{len(candidates)}ä¸ªå€™é€‰ç‚¹")
            
            # 4. æ•ˆç”¨è¯„ä¼°
            utility_results = self.utility_evaluator.evaluate_utilities(candidates, inference_results, features)
            logger.info(f"ğŸ’° æ•ˆç”¨è¯„ä¼°å®Œæˆ: è®¡ç®—äº†{len(candidates)}ä¸ªå€™é€‰ç‚¹çš„æ•ˆç”¨")
            
            # 5. å†³ç­–åˆ¶å®š
            decisions = self.decision_maker.make_decisions(candidates, inference_results, utility_results)
            logger.info(f"ğŸ² å†³ç­–åˆ¶å®šå®Œæˆ: ç”Ÿæˆäº†{len(decisions.get('optimal_decisions', []))}ä¸ªæœ€ä¼˜å†³ç­–")
            
            # 6. æ„å»ºç»“æœ
            result = self._build_analysis_result(features, candidates, inference_results, utility_results, decisions)
            
            logger.info(f"âœ… è´å¶æ–¯åˆ†æå®Œæˆ: {len(result.get('optimal_decisions', []))}ä¸ªæœ€ä¼˜å†³ç­–")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ è´å¶æ–¯åˆ†æå¤±è´¥: {e}")
            return self._create_error_result(str(e))
    
    def _create_no_candidates_result(self) -> Dict[str, Any]:
        """åˆ›å»ºæ— å€™é€‰ç‚¹ç»“æœ"""
        return {
            'optimal_decisions': [],
            'execution_plan': {'execute': False, 'reason': 'no_candidates_found'},
            'bayesian_analysis': {
                'candidates_found': 0,
                'decision_confidence': 0.0,
                'analysis_status': 'no_candidates'
            },
            'bayesian_insights': {
                'most_promising_mutation': None,
                'expected_performance_gain': 0.0,
                'risk_assessment': {'overall_risk': 0.0, 'risk_factors': []}
            }
        }
    
    def _create_error_result(self, error_message: str) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        return {
            'optimal_decisions': [],
            'execution_plan': {'execute': False, 'reason': 'analysis_error', 'error': error_message},
            'bayesian_analysis': {
                'candidates_found': 0,
                'decision_confidence': 0.0,
                'analysis_status': 'error'
            },
            'bayesian_insights': {
                'error': error_message,
                'expected_performance_gain': 0.0,
                'risk_assessment': {'overall_risk': 1.0, 'risk_factors': ['analysis_error']}
            }
        }
    
    def _build_analysis_result(self, 
                             features: Dict[str, Any],
                             candidates: List[Dict[str, Any]],
                             inference_results: Dict[str, Any],
                             utility_results: Dict[str, Any],
                             decisions: Dict[str, Any]) -> Dict[str, Any]:
        """æ„å»ºåˆ†æç»“æœ"""
        
        optimal_decisions = decisions.get('optimal_decisions', [])
        
        return {
            'optimal_decisions': optimal_decisions,
            'execution_plan': self._build_execution_plan(optimal_decisions, decisions),
            'bayesian_analysis': {
                'candidates_found': len(candidates),
                'decision_confidence': decisions.get('overall_confidence', 0.0),
                'analysis_status': 'success',
                'inference_summary': inference_results.get('summary', {}),
                'utility_summary': utility_results.get('summary', {})
            },
            'bayesian_insights': self._build_insights(optimal_decisions, inference_results, utility_results),
            'detailed_analysis': {
                'features': features,
                'candidates': candidates,
                'inference_results': inference_results,
                'utility_results': utility_results
            }
        }
    
    def _build_execution_plan(self, optimal_decisions: List[Dict[str, Any]], decisions: Dict[str, Any]) -> Dict[str, Any]:
        """æ„å»ºæ‰§è¡Œè®¡åˆ’"""
        
        overall_confidence = decisions.get('overall_confidence', 0.0)
        confidence_threshold = self.config.dynamic_thresholds['confidence_threshold']
        
        # ä¿®å¤ç½®ä¿¡åº¦è®¡ç®—é—®é¢˜ - å¦‚æœæœ‰å†³ç­–ä½†ç½®ä¿¡åº¦ä¸º0ï¼Œä½¿ç”¨å†³ç­–æœ¬èº«çš„ç½®ä¿¡åº¦
        if len(optimal_decisions) > 0 and overall_confidence == 0.0:
            decision_confidences = [d.get('decision_confidence', 0.0) for d in optimal_decisions]
            if decision_confidences:
                overall_confidence = max(decision_confidences)  # ä½¿ç”¨æœ€é«˜çš„å†³ç­–ç½®ä¿¡åº¦
                logger.info(f"ğŸ”§ ä¿®æ­£æ‰§è¡Œç½®ä¿¡åº¦: {overall_confidence:.3f} (æ¥è‡ªæœ€ä½³å†³ç­–)")
        
        should_execute = len(optimal_decisions) > 0 and overall_confidence > confidence_threshold
        
        # è®¡ç®—æ€»æœŸæœ›æ”¹è¿›
        total_expected_improvement = sum(d.get('expected_improvement', 0.0) for d in optimal_decisions)
        
        plan = {
            'execute': should_execute,
            'reason': decisions.get('execution_reason', 'bayesian_analysis'),
            'confidence': overall_confidence,
            'expected_improvements': [],
            'total_expected_improvement': total_expected_improvement,
            'decisions_count': len(optimal_decisions)
        }
        
        if should_execute:
            for decision in optimal_decisions:
                plan['expected_improvements'].append({
                    'layer': decision.get('layer_name', ''),
                    'mutation': decision.get('mutation_type', ''),
                    'expected_gain': decision.get('expected_improvement', 0.0)
                })
        
        return plan
    
    def _build_insights(self, 
                       optimal_decisions: List[Dict[str, Any]],
                       inference_results: Dict[str, Any],
                       utility_results: Dict[str, Any]) -> Dict[str, Any]:
        """æ„å»ºè´å¶æ–¯æ´å¯Ÿ"""
        
        insights = {
            'most_promising_mutation': None,
            'expected_performance_gain': 0.0,
            'risk_assessment': {'overall_risk': 0.0, 'risk_factors': []},
            'confidence_levels': {},
            'mutation_recommendations': []
        }
        
        if optimal_decisions:
            # æœ€æœ‰å‰æ™¯çš„å˜å¼‚
            best_decision = max(optimal_decisions, key=lambda x: x.get('expected_utility', 0))
            insights['most_promising_mutation'] = {
                'layer_name': best_decision.get('layer_name', ''),
                'mutation_type': best_decision.get('mutation_type', ''),
                'expected_utility': best_decision.get('expected_utility', 0.0),
                'success_probability': best_decision.get('success_probability', 0.0)
            }
            
            # æœŸæœ›æ€§èƒ½å¢ç›Š
            insights['expected_performance_gain'] = sum(
                d.get('expected_improvement', 0.0) for d in optimal_decisions
            )
            
            # é£é™©è¯„ä¼°
            risks = [d.get('risk_metrics', {}).get('overall_risk', 0.0) for d in optimal_decisions]
            insights['risk_assessment'] = {
                'overall_risk': np.mean(risks) if risks else 0.0,
                'risk_factors': [],
                'risk_distribution': risks
            }
            
            # ç½®ä¿¡åº¦æ°´å¹³
            confidences = [d.get('decision_confidence', 0.0) for d in optimal_decisions]
            insights['confidence_levels'] = {
                'average': np.mean(confidences) if confidences else 0.0,
                'minimum': np.min(confidences) if confidences else 0.0,
                'maximum': np.max(confidences) if confidences else 0.0
            }
        
        return insights
    
    def update_mutation_outcome(self, mutation_info: Dict[str, Any], success: bool, improvement: float):
        """æ›´æ–°å˜å¼‚ç»“æœï¼ˆåœ¨çº¿å­¦ä¹ ï¼‰"""
        
        outcome = {
            'mutation_info': mutation_info,
            'success': success,
            'improvement': improvement,
            'timestamp': len(self.mutation_history)
        }
        
        self.mutation_history.append(outcome)
        
        # é™åˆ¶å†å²é•¿åº¦
        if len(self.mutation_history) > self.config.max_mutation_history:
            self.mutation_history = self.mutation_history[-self.config.max_mutation_history:]
        
        # æ›´æ–°è´å¶æ–¯å…ˆéªŒ
        self.bayesian_inference.update_priors(mutation_info, success, improvement)
        
        logger.info(f"ğŸ“ˆ æ›´æ–°å˜å¼‚ç»“æœ: æˆåŠŸ={success}, æ”¹è¿›={improvement:.4f}")
    
    def set_aggressive_mode(self):
        """è®¾ç½®ç§¯ææ¨¡å¼"""
        self.config_manager.reset_to_aggressive_mode()
        self.config = self.config_manager.get_config()
        logger.info("ğŸš€ è´å¶æ–¯å¼•æ“è®¾ç½®ä¸ºç§¯ææ¨¡å¼")
    
    def set_conservative_mode(self):
        """è®¾ç½®ä¿å®ˆæ¨¡å¼"""
        self.config_manager.reset_to_conservative_mode()
        self.config = self.config_manager.get_config()
        logger.info("ğŸ›¡ï¸ è´å¶æ–¯å¼•æ“è®¾ç½®ä¸ºä¿å®ˆæ¨¡å¼")
    
    def get_analysis_summary(self) -> Dict[str, Any]:
        """è·å–åˆ†ææ‘˜è¦"""
        return {
            'mutation_history_length': len(self.mutation_history),
            'performance_history_length': len(self.performance_history),
            'current_mode': 'aggressive' if self.config.dynamic_thresholds['confidence_threshold'] < 0.3 else 'conservative',
            'config_summary': {
                'confidence_threshold': self.config.dynamic_thresholds['confidence_threshold'],
                'min_expected_improvement': self.config.dynamic_thresholds['min_expected_improvement'],
                'mc_samples': self.config.mc_samples
            }
        }


class BayesianInferenceEngine:
    """è´å¶æ–¯æ¨ç†å¼•æ“"""
    
    def __init__(self, config):
        self.config = config
        self.mutation_priors = config.mutation_priors.copy()
    
    def analyze_candidates(self, candidates: List[Dict[str, Any]], features: Dict[str, Any], history: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æå€™é€‰ç‚¹çš„è´å¶æ–¯æ¨ç†"""
        
        results = {'candidate_analyses': [], 'summary': {}}
        
        for candidate in candidates:
            analysis = self._analyze_single_candidate(candidate, features, history)
            results['candidate_analyses'].append(analysis)
        
        # æ„å»ºæ‘˜è¦
        if results['candidate_analyses']:
            success_probs = [a['success_probability'] for a in results['candidate_analyses']]
            results['summary'] = {
                'avg_success_probability': np.mean(success_probs),
                'max_success_probability': np.max(success_probs),
                'min_success_probability': np.min(success_probs),
                'total_candidates_analyzed': len(candidates)
            }
        
        return results
    
    def _analyze_single_candidate(self, candidate: Dict[str, Any], features: Dict[str, Any], history: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªå€™é€‰ç‚¹"""
        
        # è·å–å»ºè®®çš„å˜å¼‚ç±»å‹
        suggested_mutations = candidate.get('suggested_mutations', ['width_expansion'])
        mutation_type = suggested_mutations[0] if suggested_mutations else 'width_expansion'
        
        # è·å–å…ˆéªŒåˆ†å¸ƒå‚æ•°
        prior = self.mutation_priors.get(mutation_type, {'alpha': 10, 'beta': 10})
        
        # æ ¹æ®å†å²æ›´æ–°åéªŒ
        alpha, beta = self._update_posterior_from_history(mutation_type, history, prior['alpha'], prior['beta'])
        
        # è®¡ç®—æˆåŠŸæ¦‚ç‡
        success_probability = alpha / (alpha + beta)
        
        # ä¼°è®¡æœŸæœ›æ”¹è¿›
        expected_improvement = self._estimate_expected_improvement(candidate, features, success_probability)
        
        # æ›´å¥½çš„ç½®ä¿¡åº¦è®¡ç®—
        # åŸºäºè´å¶æ–¯åéªŒåˆ†å¸ƒçš„ä¸ç¡®å®šæ€§
        total_observations = alpha + beta
        if total_observations > 0:
            # ä½¿ç”¨è´å¡”åˆ†å¸ƒçš„æ–¹å·®æ¥è®¡ç®—ç½®ä¿¡åº¦
            variance = (alpha * beta) / ((alpha + beta) ** 2 * (alpha + beta + 1))
            confidence = min(1.0, 1.0 - variance * 10)  # æ–¹å·®è¶Šå°ï¼Œç½®ä¿¡åº¦è¶Šé«˜
        else:
            confidence = success_probability * 0.5  # æ— è§‚æµ‹æ•°æ®æ—¶çš„ä½ç½®ä¿¡åº¦
        
        # ç¡®ä¿æœ€å°ç½®ä¿¡åº¦
        if expected_improvement > 0:
            confidence = max(confidence, 0.3)  # å¦‚æœæœ‰æœŸæœ›æ”¹è¿›ï¼Œæœ€å°‘30%ç½®ä¿¡åº¦
        
        return {
            'candidate': candidate,
            'mutation_type': mutation_type,
            'prior_alpha': prior['alpha'],
            'prior_beta': prior['beta'],
            'posterior_alpha': alpha,
            'posterior_beta': beta,
            'success_probability': success_probability,
            'expected_improvement': expected_improvement,
            'confidence': confidence
        }
    
    def _update_posterior_from_history(self, mutation_type: str, history: List[Dict[str, Any]], alpha: float, beta: float) -> Tuple[float, float]:
        """æ ¹æ®å†å²æ›´æ–°åéªŒåˆ†å¸ƒ"""
        
        for outcome in history:
            mut_info = outcome.get('mutation_info', {})
            if mut_info.get('mutation_type') == mutation_type:
                if outcome.get('success', False):
                    alpha += 1
                else:
                    beta += 1
        
        return alpha, beta
    
    def _estimate_expected_improvement(self, candidate: Dict[str, Any], features: Dict[str, Any], success_prob: float) -> float:
        """ä¼°è®¡æœŸæœ›æ”¹è¿›"""
        
        # åŸºç¡€æ”¹è¿›ä¼°è®¡
        base_improvement = 0.02  # 2%åŸºç¡€æ”¹è¿›
        
        # æ ¹æ®å€™é€‰ç‚¹ä¼˜å…ˆçº§è°ƒæ•´
        priority = candidate.get('priority', 0.5)
        priority_factor = 0.5 + priority
        
        # æ ¹æ®æ£€æµ‹æ–¹æ³•è°ƒæ•´
        method_factors = {
            'gradient_vanishing': 1.5,
            'gradient_explosion': 1.3,
            'low_activation': 1.2,
            'performance_degradation': 2.0,
            'performance_stagnation': 0.8
        }
        
        detection_method = candidate.get('detection_method', '')
        method_factor = method_factors.get(detection_method, 1.0)
        
        # è®¡ç®—æœŸæœ›æ”¹è¿›
        expected_improvement = base_improvement * priority_factor * method_factor * success_prob
        
        return min(expected_improvement, 0.1)  # é™åˆ¶æœ€å¤§æ”¹è¿›ä¸º10%
    
    def update_priors(self, mutation_info: Dict[str, Any], success: bool, improvement: float):
        """æ›´æ–°å…ˆéªŒåˆ†å¸ƒ"""
        
        mutation_type = mutation_info.get('mutation_type', '')
        if mutation_type in self.mutation_priors:
            if success:
                self.mutation_priors[mutation_type]['alpha'] += 1
            else:
                self.mutation_priors[mutation_type]['beta'] += 1


class UtilityEvaluator:
    """æ•ˆç”¨è¯„ä¼°å™¨"""
    
    def __init__(self, config):
        self.config = config
        self.utility_params = config.utility_params
    
    def evaluate_utilities(self, candidates: List[Dict[str, Any]], inference_results: Dict[str, Any], features: Dict[str, Any]) -> Dict[str, Any]:
        """è¯„ä¼°å€™é€‰ç‚¹æ•ˆç”¨"""
        
        candidate_analyses = inference_results.get('candidate_analyses', [])
        utilities = []
        
        for analysis in candidate_analyses:
            utility = self._calculate_utility(analysis, features)
            utilities.append(utility)
        
        return {
            'utilities': utilities,
            'summary': {
                'max_utility': max(utilities) if utilities else 0.0,
                'avg_utility': np.mean(utilities) if utilities else 0.0,
                'total_evaluated': len(utilities)
            }
        }
    
    def _calculate_utility(self, analysis: Dict[str, Any], features: Dict[str, Any]) -> float:
        """è®¡ç®—å•ä¸ªå€™é€‰ç‚¹çš„æ•ˆç”¨"""
        
        # åŸºç¡€æ•ˆç”¨ç»„ä»¶
        accuracy_gain = analysis.get('expected_improvement', 0.0) * self.utility_params['accuracy_weight']
        success_bonus = analysis.get('success_probability', 0.0) * 0.1
        exploration_bonus = self.utility_params['exploration_bonus']
        
        # é£é™©æƒ©ç½š
        risk_penalty = (1 - analysis.get('success_probability', 0.0)) * self.utility_params['risk_aversion']
        
        # è®¡ç®—æ€»æ•ˆç”¨
        total_utility = accuracy_gain + success_bonus + exploration_bonus - risk_penalty
        
        # ç¡®ä¿æ•ˆç”¨å€¼åˆç†ï¼Œé¿å…å…¨ä¸º0çš„æƒ…å†µ
        if total_utility <= 0 and analysis.get('expected_improvement', 0.0) > 0:
            # å¦‚æœè®¡ç®—å‡ºçš„æ•ˆç”¨ä¸º0ä½†æœ‰æœŸæœ›æ”¹è¿›ï¼Œç»™ä¸€ä¸ªæœ€å°å€¼
            total_utility = analysis.get('expected_improvement', 0.0) * 0.5
        
        return max(0.0, total_utility)


class DecisionMaker:
    """å†³ç­–åˆ¶å®šå™¨"""
    
    def __init__(self, config):
        self.config = config
        self.thresholds = config.dynamic_thresholds
    
    def make_decisions(self, candidates: List[Dict[str, Any]], inference_results: Dict[str, Any], utility_results: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ¶å®šæœ€ç»ˆå†³ç­–"""
        
        candidate_analyses = inference_results.get('candidate_analyses', [])
        utilities = utility_results.get('utilities', [])
        
        if not candidate_analyses or not utilities:
            return {'optimal_decisions': [], 'overall_confidence': 0.0, 'execution_reason': 'no_viable_candidates'}
        
        # åˆå¹¶åˆ†æå’Œæ•ˆç”¨
        combined_data = []
        for analysis, utility in zip(candidate_analyses, utilities):
            # æ›´çµæ´»çš„é˜ˆå€¼æ£€æŸ¥
            expected_improvement = analysis.get('expected_improvement', 0)
            success_probability = analysis.get('success_probability', 0)
            confidence = analysis.get('confidence', 0)
            
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³é˜ˆå€¼ï¼ˆä½¿ç”¨ORé€»è¾‘ï¼Œæ›´å®½æ¾ï¼‰
            meets_improvement = expected_improvement >= self.thresholds['min_expected_improvement']
            meets_probability = success_probability >= self.thresholds['confidence_threshold'] 
            meets_confidence = confidence >= self.thresholds['confidence_threshold']
            meets_utility = utility >= self.thresholds.get('min_utility', 0.01)
            
            # å¦‚æœæ»¡è¶³ä»»æ„ä¸¤ä¸ªæ¡ä»¶å°±è®¤ä¸ºé€šè¿‡ï¼ˆæ›´å®½æ¾çš„ç­–ç•¥ï¼‰
            conditions_met = sum([meets_improvement, meets_probability, meets_confidence, meets_utility])
            meets_threshold = conditions_met >= 2
            
            combined_data.append({
                'analysis': analysis,
                'utility': utility,
                'meets_threshold': meets_threshold,
                'conditions_met': conditions_met
            })
        
        # ç­›é€‰æ»¡è¶³é˜ˆå€¼çš„å€™é€‰ç‚¹
        viable_candidates = [data for data in combined_data if data['meets_threshold']]
        
        if not viable_candidates:
            return {'optimal_decisions': [], 'overall_confidence': 0.0, 'execution_reason': 'no_candidates_meet_thresholds'}
        
        # æŒ‰æ•ˆç”¨æ’åº
        viable_candidates.sort(key=lambda x: x['utility'], reverse=True)
        
        # é€‰æ‹©æœ€ä¼˜å†³ç­–ï¼ˆæœ€å¤š3ä¸ªï¼‰
        max_decisions = min(3, len(viable_candidates))
        selected_candidates = viable_candidates[:max_decisions]
        
        optimal_decisions = []
        for data in selected_candidates:
            analysis = data['analysis']
            candidate = analysis['candidate']
            
            # è°ƒè¯•æ—¥å¿—ï¼šæ£€æŸ¥å€™é€‰ç‚¹å†…å®¹
            layer_name = candidate.get('layer_name', '')
            logger.info(f"ğŸ” æ„å»ºå†³ç­– - å€™é€‰ç‚¹ layer_name: '{layer_name}', å€™é€‰ç‚¹å†…å®¹: {candidate}")
            
            decision = {
                'layer_name': layer_name,
                'target_layer': layer_name,  # æ·»åŠ å¤‡ç”¨å­—æ®µä¿æŒä¸€è‡´æ€§
                'mutation_type': analysis.get('mutation_type', ''),
                'success_probability': analysis.get('success_probability', 0.0),
                'expected_improvement': analysis.get('expected_improvement', 0.0),
                'expected_utility': data['utility'],
                'decision_confidence': analysis.get('confidence', 0.0),
                'rationale': candidate.get('rationale', ''),
                'risk_metrics': {
                    'overall_risk': 1.0 - analysis.get('success_probability', 0.0),
                    'value_at_risk': analysis.get('expected_improvement', 0.0) * 0.5  # ç®€åŒ–çš„VaR
                },
                'source': 'bayesian_analysis'
            }
            optimal_decisions.append(decision)
        
        # è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦
        confidences = [d['decision_confidence'] for d in optimal_decisions]
        overall_confidence = np.mean(confidences) if confidences else 0.0
        
        # ç¡®ä¿ç½®ä¿¡åº¦ä¸ä¸º0ï¼ˆå¦‚æœæœ‰å†³ç­–çš„è¯ï¼‰
        if overall_confidence == 0.0 and len(optimal_decisions) > 0:
            # ä½¿ç”¨æœŸæœ›æ•ˆç”¨ä½œä¸ºå¤‡ç”¨ç½®ä¿¡åº¦æŒ‡æ ‡
            utilities = [d.get('expected_utility', 0.0) for d in optimal_decisions]
            if utilities and max(utilities) > 0:
                overall_confidence = min(0.8, max(utilities) * 10)  # å°†æ•ˆç”¨è½¬æ¢ä¸ºç½®ä¿¡åº¦
                logger.info(f"ğŸ”§ ä½¿ç”¨æ•ˆç”¨è®¡ç®—ç½®ä¿¡åº¦: {overall_confidence:.3f}")
        
        return {
            'optimal_decisions': optimal_decisions,
            'overall_confidence': overall_confidence,
            'execution_reason': 'bayesian_optimization',
            'candidates_evaluated': len(candidate_analyses),
            'candidates_viable': len(viable_candidates),
            'decisions_selected': len(optimal_decisions)
        }