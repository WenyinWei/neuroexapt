"""
ASO-SEæ¡†æ¶ (Alternating Stable Optimization with Stochastic Exploration)

äº¤æ›¿å¼ç¨³å®šä¼˜åŒ–ä¸éšæœºæ¢ç´¢æ¡†æ¶çš„å®Œæ•´å®ç°ï¼ŒåŒ…å«ï¼š

å››é˜¶æ®µè®­ç»ƒæµç¨‹ï¼š
1. é˜¶æ®µä¸€ï¼šç½‘ç»œæƒé‡é¢„çƒ­ (W-Training) - ç¨³å®šåŒ–åŸºç¡€æƒé‡
2. é˜¶æ®µäºŒï¼šæ¶æ„å‚æ•°å­¦ä¹  (Î±-Training) - æœç´¢æœ€ä¼˜æ¶æ„é…ç½®  
3. é˜¶æ®µä¸‰ï¼šæ¶æ„çªå˜ä¸ç¨³å®š (Architecture Mutation & Stabilization) - å‡½æ•°ä¿æŒçªå˜
4. é˜¶æ®µå››ï¼šæƒé‡å†é€‚åº” (W-Retraining) - åœ¨æ–°æ¶æ„ä¸Šç»§ç»­ä¼˜åŒ–

æ ¸å¿ƒç‰¹æ€§ï¼š
- å‡½æ•°ä¿æŒåˆå§‹åŒ–ç¡®ä¿æ¶æ„å˜åŒ–æ—¶çš„å¹³æ»‘è¿‡æ¸¡
- Gumbel-Softmaxå¼•å¯¼å¼æ¢ç´¢é¿å…å±€éƒ¨æœ€ä¼˜
- è‡ªé€‚åº”æ¸©åº¦æ§åˆ¶å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨
- æ¸è¿›å¼æ¶æ„ç”Ÿé•¿é¿å…å‰§çƒˆå˜åŒ–
- è®¾å¤‡ä¸€è‡´æ€§ç®¡ç†å’Œå†…å­˜ä¼˜åŒ–
- å®Œæ•´çš„æ£€æŸ¥ç‚¹ä¿å­˜å’Œæ¢å¤æœºåˆ¶
"""

import torch
import torch.nn as nn
try:
    import torch.optim as optim
    import torch.nn.functional as F
except ImportError:
    from torch.nn import functional as F
    from torch import optim
import numpy as np
import logging
import time
import os
from typing import Dict, List, Optional, Tuple, Union, Any, Callable
from dataclasses import dataclass

from .function_preserving_init import FunctionPreservingInitializer
from .gumbel_softmax_explorer import GumbelSoftmaxExplorer, create_annealing_schedule
from .architecture_mutator import ArchitectureMutator, GradualArchitectureGrowth
from .genotypes import Genotype, PRIMITIVES
from .device_manager import DeviceManager, get_device_manager

logger = logging.getLogger(__name__)

@dataclass
class ASOSEConfig:
    """ASO-SEæ¡†æ¶é…ç½®"""
    # è®­ç»ƒé˜¶æ®µé…ç½®
    warmup_epochs: int = 10
    arch_training_epochs: int = 3
    weight_training_epochs: int = 8
    total_cycles: int = 5
    
    # Gumbel-Softmaxæ¢ç´¢é…ç½®
    initial_temp: float = 5.0
    min_temp: float = 0.1
    anneal_rate: float = 0.98
    exploration_factor: float = 1.0
    
    # æ¶æ„çªå˜é…ç½®
    mutation_strength: float = 0.3
    mutation_frequency: int = 2  # æ¯å‡ ä¸ªcycleè¿›è¡Œä¸€æ¬¡çªå˜
    preserve_function: bool = True
    
    # ä¼˜åŒ–å™¨é…ç½®
    weight_lr: float = 0.025
    weight_momentum: float = 0.9
    weight_decay: float = 3e-4
    arch_lr: float = 3e-4
    
    # ç”Ÿé•¿ç­–ç•¥é…ç½®
    enable_gradual_growth: bool = True
    growth_schedule: Optional[List[Dict]] = None
    
    # ç›‘æ§é…ç½®
    early_stopping_patience: int = 10
    performance_threshold: float = 0.01
    
    # è®¾å¤‡å’Œå†…å­˜é…ç½®
    device: Optional[str] = None
    memory_fraction: float = 0.9
    
    # æ£€æŸ¥ç‚¹é…ç½®
    save_checkpoints: bool = True
    checkpoint_frequency: int = 5  # æ¯å‡ ä¸ªepochä¿å­˜ä¸€æ¬¡
    max_checkpoints: int = 3  # æœ€å¤šä¿ç•™å‡ ä¸ªæ£€æŸ¥ç‚¹

class ASOSEFramework:
    """
    ASO-SEæ¡†æ¶ä¸»ç±»
    
    ç»Ÿä¸€ç®¡ç†å››é˜¶æ®µè®­ç»ƒæµç¨‹å’Œå„ä¸ªç»„ä»¶ï¼ŒåŒ…å«è®¾å¤‡ç®¡ç†å’Œå†…å­˜ä¼˜åŒ–
    """
    
    def __init__(self, search_model: nn.Module, config: ASOSEConfig):
        """
        Args:
            search_model: æœç´¢æ¨¡å‹ï¼ˆå¸¦æ¶æ„å‚æ•°çš„ç½‘ç»œï¼‰
            config: ASO-SEé…ç½®
        """
        self.config = config
        
        # åˆå§‹åŒ–è®¾å¤‡ç®¡ç†å™¨
        self.device_manager = get_device_manager(config.device)
        
        # æ³¨å†Œæœç´¢æ¨¡å‹åˆ°è®¾å¤‡ç®¡ç†å™¨
        self.search_model = self.device_manager.register_model("search_model", search_model)
        
        # æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–
        self.initializer = FunctionPreservingInitializer()
        self.explorer = GumbelSoftmaxExplorer(
            initial_temp=config.initial_temp,
            min_temp=config.min_temp,
            anneal_rate=config.anneal_rate,
            exploration_factor=config.exploration_factor
        )
        self.mutator = ArchitectureMutator(
            preserve_function=config.preserve_function,
            mutation_strength=config.mutation_strength
        )
        
        # å½“å‰çŠ¶æ€
        self.current_cycle = 0
        self.current_phase = "warmup"  # warmup, arch_training, mutation, weight_retraining
        self.current_genotype = None
        self.evolvable_model = None
        
        # è®­ç»ƒå†å²
        self.training_history = {
            "loss_history": [],
            "accuracy_history": [],
            "architecture_history": [],
            "mutation_history": [],
            "phase_transitions": [],
            "device_stats": []
        }
        
        # ä¼˜åŒ–å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self.weight_optimizer = None
        self.arch_optimizer = None
        
        # æ€§èƒ½ç›‘æ§
        self.best_performance = 0.0
        self.patience_counter = 0
        
        # æ¸è¿›å¼ç”Ÿé•¿
        if config.enable_gradual_growth:
            self.growth_manager = GradualArchitectureGrowth(
                self.mutator, config.growth_schedule
            )
        else:
            self.growth_manager = None
        
        # æ£€æŸ¥ç‚¹ç®¡ç†
        self.checkpoint_dir = None
        self.checkpoint_counter = 0
        
        logger.info(f"ğŸš€ ASO-SE Framework initialized: "
                   f"{config.total_cycles} cycles, "
                   f"warmup={config.warmup_epochs}, "
                   f"arch={config.arch_training_epochs}, "
                   f"weight={config.weight_training_epochs}")
        
        # è®°å½•è®¾å¤‡ä¿¡æ¯
        device_report = self.device_manager.get_device_report()
        logger.info(f"ğŸ”§ Device: {device_report['device']}")
        
        # å†…å­˜ç›‘æ§
        self._log_memory_usage("initialization")
    
    def setup_checkpoint_dir(self, checkpoint_dir: str):
        """è®¾ç½®æ£€æŸ¥ç‚¹ç›®å½•"""
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(checkpoint_dir, exist_ok=True)
        logger.info(f"ğŸ’¾ Checkpoint directory: {checkpoint_dir}")
    
    def initialize_optimizers(self):
        """åˆå§‹åŒ–ä¼˜åŒ–å™¨"""
        # åˆ†ç¦»å‚æ•°
        arch_params = list(self.search_model.arch_parameters()) if hasattr(self.search_model, 'arch_parameters') else []
        weight_params = [p for p in self.search_model.parameters() 
                        if not any(p is ap for ap in arch_params)]
        
        self.weight_optimizer = optim.SGD(
            weight_params,
            lr=self.config.weight_lr,
            momentum=self.config.weight_momentum,
            weight_decay=self.config.weight_decay
        )
        
        if arch_params:
            self.arch_optimizer = optim.Adam(
                arch_params,
                lr=self.config.arch_lr,
                weight_decay=1e-3
            )
        
        logger.info(f"ğŸ“Š Optimizers initialized: "
                   f"weight_params={len(weight_params)}, "
                   f"arch_params={len(arch_params)}")
    
    def train_cycle(self, train_loader, valid_loader, 
                   criterion: nn.Module, epoch: int) -> Dict[str, float]:
        """
        æ‰§è¡Œä¸€ä¸ªå®Œæ•´çš„ASO-SEè®­ç»ƒå‘¨æœŸ
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            valid_loader: éªŒè¯æ•°æ®åŠ è½½å™¨  
            criterion: æŸå¤±å‡½æ•°
            epoch: å½“å‰epoch
            
        Returns:
            è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        """
        # åŒ…è£…æ•°æ®åŠ è½½å™¨ä»¥è‡ªåŠ¨è½¬ç§»è®¾å¤‡
        train_loader = self.device_manager.create_data_loader_wrapper(train_loader)
        valid_loader = self.device_manager.create_data_loader_wrapper(valid_loader)
        
        # ç¡®ä¿æŸå¤±å‡½æ•°åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        criterion = criterion.to(self.device_manager.device)
        
        cycle_stats = {}
        
        # ç¡®å®šå½“å‰é˜¶æ®µ
        phase = self._determine_current_phase(epoch)
        
        if phase != self.current_phase:
            self._transition_to_phase(phase, epoch)
        
        # æ ¹æ®é˜¶æ®µæ‰§è¡Œå¯¹åº”çš„è®­ç»ƒ
        try:
            if phase == "warmup":
                stats = self._warmup_phase(train_loader, valid_loader, criterion, epoch)
            elif phase == "arch_training":
                stats = self._arch_training_phase(train_loader, valid_loader, criterion, epoch)
            elif phase == "mutation":
                stats = self._mutation_phase(train_loader, valid_loader, criterion, epoch)
            elif phase == "weight_retraining":
                stats = self._weight_retraining_phase(train_loader, valid_loader, criterion, epoch)
            else:
                raise ValueError(f"Unknown phase: {phase}")
            
            # è®°å½•è®¾å¤‡ç»Ÿè®¡
            device_stats = self.device_manager.get_memory_stats()
            # å°†è®¾å¤‡ç»Ÿè®¡è½¬æ¢ä¸ºæ•°å€¼æ ¼å¼
            if isinstance(device_stats, dict) and 'device' in device_stats:
                # CPUæƒ…å†µä¸‹è®°å½•è®¾å¤‡ç±»å‹ä¸ºæ•°å€¼ï¼ˆ0è¡¨ç¤ºCPUï¼‰
                stats['device_type'] = 0.0
            else:
                # GPUæƒ…å†µä¸‹è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ
                allocated = device_stats.get('allocated_mb', 0.0)
                utilization = device_stats.get('utilization', 0.0)
                stats['memory_allocated_mb'] = float(allocated) if isinstance(allocated, (int, float)) else 0.0
                stats['memory_utilization'] = float(utilization) if isinstance(utilization, (int, float)) else 0.0
            
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                logger.error(f"ğŸš¨ OOM error in epoch {epoch}, attempting recovery...")
                self.device_manager.optimize_memory()
                
                # é‡è¯•ä¸€æ¬¡
                try:
                    if phase == "warmup":
                        stats = self._warmup_phase(train_loader, valid_loader, criterion, epoch)
                    elif phase == "arch_training":
                        stats = self._arch_training_phase(train_loader, valid_loader, criterion, epoch)
                    elif phase == "mutation":
                        stats = self._mutation_phase(train_loader, valid_loader, criterion, epoch)
                    elif phase == "weight_retraining":
                        stats = self._weight_retraining_phase(train_loader, valid_loader, criterion, epoch)
                except RuntimeError as e2:
                    logger.error(f"âŒ Memory recovery failed: {e2}")
                    stats = {"error": "out_of_memory", "phase": phase}
            else:
                logger.error(f"âŒ Training error in epoch {epoch}: {e}")
                stats = {"error": str(e), "phase": phase}
        
        # æ›´æ–°å†å²è®°å½•
        self._update_training_history(stats, epoch, phase)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡Œæ¸è¿›å¼ç”Ÿé•¿
        if self.growth_manager and self.growth_manager.should_grow(epoch):
            self._perform_gradual_growth(epoch)
        
        # æ›´æ–°æ¢ç´¢æ¸©åº¦
        self._update_exploration_temperature(stats.get("valid_accuracy", 0.0))
        
        # å®šæœŸå†…å­˜ä¼˜åŒ–
        if epoch % 10 == 0:
            self.device_manager.optimize_memory()
            self._log_memory_usage(f"epoch_{epoch}")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if (self.config.save_checkpoints and 
            epoch % self.config.checkpoint_frequency == 0 and 
            self.checkpoint_dir):
            self._save_checkpoint(epoch, stats)
        
        return stats
    
    def _determine_current_phase(self, epoch: int) -> str:
        """ç¡®å®šå½“å‰è®­ç»ƒé˜¶æ®µ"""
        cycle_length = (self.config.warmup_epochs + 
                       self.config.arch_training_epochs + 
                       1 +  # mutation phase
                       self.config.weight_training_epochs)
        
        position_in_cycle = epoch % cycle_length
        
        if position_in_cycle < self.config.warmup_epochs:
            return "warmup"
        elif position_in_cycle < (self.config.warmup_epochs + self.config.arch_training_epochs):
            return "arch_training"
        elif position_in_cycle == (self.config.warmup_epochs + self.config.arch_training_epochs):
            return "mutation"
        else:
            return "weight_retraining"
    
    def _transition_to_phase(self, new_phase: str, epoch: int):
        """é˜¶æ®µè½¬æ¢å¤„ç†"""
        logger.info(f"ğŸ”„ Phase transition: {self.current_phase} â†’ {new_phase} at epoch {epoch}")
        
        # ä¿å­˜é˜¶æ®µè½¬æ¢å‰çš„æ£€æŸ¥ç‚¹
        if (self.config.save_checkpoints and self.checkpoint_dir and 
            new_phase == "mutation"):
            self._save_pre_mutation_checkpoint(epoch)
        
        self.training_history["phase_transitions"].append({
            "epoch": epoch,
            "from_phase": self.current_phase,
            "to_phase": new_phase
        })
        
        self.current_phase = new_phase
        
        # é˜¶æ®µç‰¹å®šçš„å¤„ç†
        if new_phase == "arch_training":
            self._prepare_arch_training()
        elif new_phase == "mutation":
            self._prepare_mutation()
        elif new_phase == "weight_retraining":
            self._prepare_weight_retraining()
    
    def _warmup_phase(self, train_loader, valid_loader, criterion, epoch: int) -> Dict[str, float]:
        """é˜¶æ®µä¸€ï¼šç½‘ç»œæƒé‡é¢„çƒ­"""
        logger.debug(f"ğŸ”¥ Warmup phase - Epoch {epoch}")
        
        # å†»ç»“æ¶æ„å‚æ•°
        if self.arch_optimizer:
            for param_group in self.arch_optimizer.param_groups:
                for param in param_group['params']:
                    param.requires_grad = False
        
        train_loss, train_acc = self._train_weights(train_loader, criterion)
        valid_loss, valid_acc = self._validate(valid_loader, criterion)
        
        # æ¢å¤æ¶æ„å‚æ•°çš„æ¢¯åº¦
        if self.arch_optimizer:
            for param_group in self.arch_optimizer.param_groups:
                for param in param_group['params']:
                    param.requires_grad = True
        
        return {
            "train_loss": train_loss,
            "train_accuracy": train_acc,
            "valid_loss": valid_loss,
            "valid_accuracy": valid_acc,
            "phase": "warmup"
        }
    
    def _arch_training_phase(self, train_loader, valid_loader, criterion, epoch: int) -> Dict[str, float]:
        """é˜¶æ®µäºŒï¼šæ¶æ„å‚æ•°å­¦ä¹ """
        logger.debug(f"ğŸ” Architecture training phase - Epoch {epoch}")
        
        # å†»ç»“æƒé‡å‚æ•°
        for param_group in self.weight_optimizer.param_groups:
            for param in param_group['params']:
                param.requires_grad = False
        
        arch_loss, arch_acc = self._train_architecture(valid_loader, criterion)
        valid_loss, valid_acc = self._validate(valid_loader, criterion)
        
        # æ¢å¤æƒé‡å‚æ•°çš„æ¢¯åº¦
        for param_group in self.weight_optimizer.param_groups:
            for param in param_group['params']:
                param.requires_grad = True
        
        return {
            "arch_loss": arch_loss,
            "arch_accuracy": arch_acc,
            "valid_loss": valid_loss,
            "valid_accuracy": valid_acc,
            "phase": "arch_training"
        }
    
    def _mutation_phase(self, train_loader, valid_loader, criterion, epoch: int) -> Dict[str, float]:
        """é˜¶æ®µä¸‰ï¼šæ¶æ„çªå˜ä¸ç¨³å®š"""
        logger.info(f"ğŸ§¬ Architecture mutation phase - Epoch {epoch}")
        
        # ä»å½“å‰æ¶æ„å‚æ•°å¯¼å‡ºåŸºå› å‹
        if hasattr(self.search_model, 'genotype'):
            current_genotype = self.search_model.genotype()
        else:
            # ä½¿ç”¨Gumbel-Softmaxé‡‡æ ·ç”ŸæˆåŸºå› å‹
            current_genotype = self._sample_genotype_from_search_model()
        
        # è®°å½•çªå˜å‰çš„æ€§èƒ½
        pre_mutation_loss, pre_mutation_acc = self._validate(valid_loader, criterion)
        
        # ä½¿ç”¨Gumbel-Softmaxé‡‡æ ·æ–°æ¶æ„ï¼ˆè€Œä¸æ˜¯è´ªå©ªé€‰æ‹©ï¼‰
        new_genotype = self._gumbel_sample_architecture()
        
        # å¦‚æœé…ç½®å…è®¸ï¼Œæ‰§è¡ŒåŸºå› å‹çªå˜
        if self.current_cycle % self.config.mutation_frequency == 0:
            new_genotype = self.mutator.mutate_genotype(new_genotype, "conservative")
            logger.info("ğŸ§¬ Applied genotype mutation")
        
        # åˆ›å»ºæ–°çš„å¯è¿›åŒ–æ¨¡å‹ï¼ˆä½¿ç”¨è®¾å¤‡ç®¡ç†å™¨ï¼‰
        old_evolvable_model = self.evolvable_model
        try:
            self.evolvable_model = self._create_evolvable_model(new_genotype)
            
            # ä½¿ç”¨è®¾å¤‡ç®¡ç†å™¨è¿›è¡Œæ¨¡å‹åˆ‡æ¢
            if old_evolvable_model is not None:
                self.evolvable_model = self.device_manager.context_switch_model(
                    old_evolvable_model, self.evolvable_model
                )
            else:
                self.evolvable_model = self.device_manager.register_model(
                    "evolvable_model", self.evolvable_model
                )
            
            # å‡½æ•°ä¿æŒå‚æ•°ä¼ é€’
            if self.config.preserve_function:
                self._transfer_parameters_with_function_preservation()
            
        except Exception as e:
            logger.error(f"âŒ Failed to create evolvable model: {e}")
            # ä¿æŒæ—§æ¨¡å‹
            self.evolvable_model = old_evolvable_model
            new_genotype = current_genotype
        
        self.current_genotype = new_genotype
        
        # è®°å½•çªå˜åçš„æ€§èƒ½ï¼ˆåº”è¯¥ä¸çªå˜å‰åŸºæœ¬ä¸€è‡´ï¼‰
        post_mutation_loss, post_mutation_acc = self._validate(
            valid_loader, criterion, use_evolvable=True
        )
        
        # è®°å½•çªå˜å†å²
        mutation_record = {
            "epoch": epoch,
            "cycle": self.current_cycle,
            "pre_mutation_acc": pre_mutation_acc,
            "post_mutation_acc": post_mutation_acc,
            "genotype": str(new_genotype),
            "performance_preservation": abs(pre_mutation_acc - post_mutation_acc) < 0.05
        }
        self.training_history["mutation_history"].append(mutation_record)
        
        return {
            "pre_mutation_loss": pre_mutation_loss,
            "pre_mutation_accuracy": pre_mutation_acc,
            "post_mutation_loss": post_mutation_loss,
            "post_mutation_accuracy": post_mutation_acc,
            "genotype": str(new_genotype),
            "phase": "mutation"
        }
    
    def _weight_retraining_phase(self, train_loader, valid_loader, criterion, epoch: int) -> Dict[str, float]:
        """é˜¶æ®µå››ï¼šæƒé‡å†é€‚åº”"""
        logger.debug(f"ğŸ”§ Weight retraining phase - Epoch {epoch}")
        
        if self.evolvable_model is None:
            logger.warning("No evolvable model available, skipping weight retraining")
            return {"phase": "weight_retraining", "error": "no_evolvable_model"}
        
        # ä½¿ç”¨å¯è¿›åŒ–æ¨¡å‹è¿›è¡Œæƒé‡è®­ç»ƒ
        train_loss, train_acc = self._train_evolvable_weights(train_loader, criterion)
        valid_loss, valid_acc = self._validate(valid_loader, criterion, use_evolvable=True)
        
        return {
            "train_loss": train_loss,
            "train_accuracy": train_acc,
            "valid_loss": valid_loss,
            "valid_accuracy": valid_acc,
            "phase": "weight_retraining"
        }
    
    def _gumbel_sample_architecture(self) -> Genotype:
        """ä½¿ç”¨Gumbel-Softmaxé‡‡æ ·æ¶æ„"""
        if not hasattr(self.search_model, 'arch_parameters'):
            raise ValueError("Search model must have arch_parameters method")
        
        arch_params = self.search_model.arch_parameters()
        
        # ç®€åŒ–ï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªæ¶æ„å‚æ•°è¿›è¡Œé‡‡æ ·
        if len(arch_params) >= 2:
            normal_weights = arch_params[0]  # æ­£å¸¸è¾¹çš„æƒé‡
            reduce_weights = arch_params[1]  # å‡å°‘è¾¹çš„æƒé‡
            
            # Gumbel-Softmaxé‡‡æ ·
            normal_samples, _ = self.explorer.sample_architecture(normal_weights, hard=True)
            reduce_samples, _ = self.explorer.sample_architecture(reduce_weights, hard=True)
            
            # è½¬æ¢ä¸ºåŸºå› å‹æ ¼å¼
            normal_genotype = self._samples_to_genotype_edges(normal_samples)
            reduce_genotype = self._samples_to_genotype_edges(reduce_samples)
            
            return Genotype(
                normal=normal_genotype,
                normal_concat=list(range(2, 6)),  # å‡è®¾4ä¸ªä¸­é—´èŠ‚ç‚¹
                reduce=reduce_genotype,
                reduce_concat=list(range(2, 6))
            )
        else:
            logger.warning("Insufficient architecture parameters for Gumbel sampling")
            return self._default_genotype()
    
    def _samples_to_genotype_edges(self, samples: torch.Tensor) -> List[Tuple[str, int]]:
        """å°†é‡‡æ ·ç»“æœè½¬æ¢ä¸ºåŸºå› å‹è¾¹æ ¼å¼"""
        edges = []
        edge_idx = 0
        
        # å‡è®¾æ¯ä¸ªèŠ‚ç‚¹ä»å‰é¢æ‰€æœ‰èŠ‚ç‚¹é€‰æ‹©ä¸¤æ¡è¾¹
        for node in range(4):  # 4ä¸ªä¸­é—´èŠ‚ç‚¹
            node_edges = []
            for _ in range(2):  # æ¯ä¸ªèŠ‚ç‚¹2æ¡è¾¹
                if edge_idx < len(samples):
                    op_idx = torch.argmax(samples[edge_idx]).item()
                    op_name = PRIMITIVES[op_idx] if op_idx < len(PRIMITIVES) else 'none'
                    predecessor = edge_idx % (node + 2)  # è¿æ¥åˆ°çš„å‰é©±èŠ‚ç‚¹
                    node_edges.append((op_name, predecessor))
                    edge_idx += 1
            edges.extend(node_edges)
        
        return edges
    
    def _train_weights(self, train_loader, criterion) -> Tuple[float, float]:
        """è®­ç»ƒæƒé‡å‚æ•°"""
        self.search_model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            # æ•°æ®å·²ç»é€šè¿‡è®¾å¤‡ç®¡ç†å™¨è‡ªåŠ¨è½¬ç§»åˆ°æ­£ç¡®è®¾å¤‡
            
            self.weight_optimizer.zero_grad()
            output = self.search_model(data)
            loss = criterion(output, target)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.weight_optimizer.param_groups[0]['params'], 5.0)
            
            self.weight_optimizer.step()
            
            total_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
        
        avg_loss = total_loss / len(train_loader)
        accuracy = 100. * correct / total
        return avg_loss, accuracy
    
    def _train_architecture(self, valid_loader, criterion) -> Tuple[float, float]:
        """è®­ç»ƒæ¶æ„å‚æ•°"""
        self.search_model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(valid_loader):
            # æ•°æ®å·²ç»é€šè¿‡è®¾å¤‡ç®¡ç†å™¨è‡ªåŠ¨è½¬ç§»åˆ°æ­£ç¡®è®¾å¤‡
            
            if self.arch_optimizer:
                self.arch_optimizer.zero_grad()
                output = self.search_model(data)
                loss = criterion(output, target)
                loss.backward()
                self.arch_optimizer.step()
                
                total_loss += loss.item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)
        
        avg_loss = total_loss / len(valid_loader) if len(valid_loader) > 0 else 0.0
        accuracy = 100. * correct / total if total > 0 else 0.0
        return avg_loss, accuracy
    
    def _validate(self, valid_loader, criterion, use_evolvable: bool = False) -> Tuple[float, float]:
        """éªŒè¯æ€§èƒ½"""
        model = self.evolvable_model if use_evolvable and self.evolvable_model else self.search_model
        model.eval()
        
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in valid_loader:
                # æ•°æ®å·²ç»é€šè¿‡è®¾å¤‡ç®¡ç†å™¨è‡ªåŠ¨è½¬ç§»åˆ°æ­£ç¡®è®¾å¤‡
                
                output = model(data)
                loss = criterion(output, target)
                
                total_loss += loss.item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)
        
        avg_loss = total_loss / len(valid_loader) if len(valid_loader) > 0 else 0.0
        accuracy = 100. * correct / total if total > 0 else 0.0
        return avg_loss, accuracy
    
    def _train_evolvable_weights(self, train_loader, criterion) -> Tuple[float, float]:
        """è®­ç»ƒå¯è¿›åŒ–æ¨¡å‹çš„æƒé‡"""
        if self.evolvable_model is None:
            return 0.0, 0.0
        
        # ä¸ºå¯è¿›åŒ–æ¨¡å‹åˆ›å»ºä¼˜åŒ–å™¨
        evolvable_optimizer = optim.SGD(
            self.evolvable_model.parameters(),
            lr=self.config.weight_lr,
            momentum=self.config.weight_momentum,
            weight_decay=self.config.weight_decay
        )
        
        self.evolvable_model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            # æ•°æ®å·²ç»é€šè¿‡è®¾å¤‡ç®¡ç†å™¨è‡ªåŠ¨è½¬ç§»åˆ°æ­£ç¡®è®¾å¤‡
            
            evolvable_optimizer.zero_grad()
            output = self.evolvable_model(data)
            loss = criterion(output, target)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.evolvable_model.parameters(), 5.0)
            
            evolvable_optimizer.step()
            
            total_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
        
        avg_loss = total_loss / len(train_loader)
        accuracy = 100. * correct / total
        return avg_loss, accuracy
    
    def _create_evolvable_model(self, genotype: Genotype) -> nn.Module:
        """åˆ›å»ºå¯è¿›åŒ–æ¨¡å‹"""
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„ç½‘ç»œç»“æ„åˆ›å»º
        # ç®€åŒ–ç‰ˆæœ¬ï¼šè¿”å›ä¸€ä¸ªåŒ…è£…ç±»
        from .evolvable_model import EvolvableNetwork
        
        # ä»æœç´¢æ¨¡å‹è·å–åŸºæœ¬å‚æ•°
        model_args = {
            'C': getattr(self.search_model, '_C', 16),
            'num_classes': getattr(self.search_model, '_num_classes', 10),
            'layers': getattr(self.search_model, '_layers', 8)
        }
        
        # ä½¿ç”¨è®¾å¤‡ç®¡ç†å™¨å®‰å…¨åˆ›å»ºæ¨¡å‹
        return self.device_manager.safe_model_creation(
            EvolvableNetwork, **model_args, genotype=genotype
        )
    
    def _transfer_parameters_with_function_preservation(self):
        """ä½¿ç”¨å‡½æ•°ä¿æŒçš„å‚æ•°ä¼ é€’"""
        if self.evolvable_model is None:
            return
        
        try:
            # ä½¿ç”¨è®¾å¤‡ç®¡ç†å™¨è¿›è¡Œå‚æ•°ä¼ é€’
            self.evolvable_model = self.device_manager.transfer_model_state(
                self.search_model, self.evolvable_model
            )
            logger.info("âœ… Parameters transferred with function preservation")
        except Exception as e:
            logger.warning(f"âš ï¸ Parameter transfer failed: {e}")
    
    def _update_training_history(self, stats: Dict[str, Any], epoch: int, phase: str):
        """æ›´æ–°è®­ç»ƒå†å²"""
        stats['epoch'] = epoch
        stats['phase'] = phase
        
        for key, value in stats.items():
            if key not in self.training_history:
                self.training_history[key] = []
            self.training_history[key].append(value)
    
    def _update_exploration_temperature(self, current_performance: float):
        """æ›´æ–°æ¢ç´¢æ¸©åº¦"""
        performance_gain = current_performance - self.best_performance
        self.explorer.update_temperature(performance_gain)
        
        if current_performance > self.best_performance:
            self.best_performance = current_performance
            self.patience_counter = 0
        else:
            self.patience_counter += 1
    
    def _perform_gradual_growth(self, epoch: int):
        """æ‰§è¡Œæ¸è¿›å¼ç”Ÿé•¿"""
        if self.growth_manager:
            grown_model = self.growth_manager.perform_growth(self.search_model, epoch)
            if grown_model is not None:
                # ä½¿ç”¨è®¾å¤‡ç®¡ç†å™¨è¿›è¡Œæ¨¡å‹æ›´æ–°
                self.search_model = self.device_manager.context_switch_model(
                    self.search_model, grown_model
                )
                self.device_manager.registered_models["search_model"] = self.search_model
                logger.info(f"ğŸŒ± Performed gradual growth at epoch {epoch}")
    
    def _default_genotype(self) -> Genotype:
        """é»˜è®¤åŸºå› å‹"""
        return Genotype(
            normal=[('sep_conv_3x3', 0), ('sep_conv_3x3', 1), ('sep_conv_3x3', 0), ('sep_conv_3x3', 1)],
            normal_concat=[2, 3, 4, 5],
            reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('max_pool_3x3', 1)],
            reduce_concat=[2, 3, 4, 5]
        )
    
    def _sample_genotype_from_search_model(self) -> Genotype:
        """ä»æœç´¢æ¨¡å‹é‡‡æ ·åŸºå› å‹"""
        if hasattr(self.search_model, 'genotype'):
            return self.search_model.genotype()
        else:
            return self._default_genotype()
    
    def _prepare_arch_training(self):
        """å‡†å¤‡æ¶æ„è®­ç»ƒé˜¶æ®µ"""
        logger.debug("ğŸ” Preparing architecture training phase")
    
    def _prepare_mutation(self):
        """å‡†å¤‡çªå˜é˜¶æ®µ"""
        logger.debug("ğŸ§¬ Preparing mutation phase")
        self.current_cycle += 1
    
    def _prepare_weight_retraining(self):
        """å‡†å¤‡æƒé‡å†è®­ç»ƒé˜¶æ®µ"""
        logger.debug("ğŸ”§ Preparing weight retraining phase")
    
    def _log_memory_usage(self, context: str):
        """è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        memory_stats = self.device_manager.get_memory_stats()
        logger.debug(f"ğŸ’¾ Memory usage ({context}): {memory_stats}")
        
        # è®°å½•åˆ°å†å²
        self.training_history["device_stats"].append({
            "context": context,
            "stats": memory_stats
        })
    
    def _save_checkpoint(self, epoch: int, stats: Dict[str, float]):
        """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹"""
        try:
            checkpoint_path = os.path.join(
                self.checkpoint_dir, 
                f"aso_se_checkpoint_epoch_{epoch}.pth"
            )
            
            checkpoint = {
                "epoch": epoch,
                "search_model_state": self.search_model.state_dict(),
                "evolvable_model_state": self.evolvable_model.state_dict() if self.evolvable_model else None,
                "weight_optimizer_state": self.weight_optimizer.state_dict() if self.weight_optimizer else None,
                "arch_optimizer_state": self.arch_optimizer.state_dict() if self.arch_optimizer else None,
                "current_genotype": self.current_genotype,
                "current_cycle": self.current_cycle,
                "current_phase": self.current_phase,
                "best_performance": self.best_performance,
                "explorer_state": {
                    "current_temp": self.explorer.current_temp,
                    "step_count": self.explorer.step_count
                },
                "training_history": self.training_history,
                "config": self.config,
                "stats": stats
            }
            
            torch.save(checkpoint, checkpoint_path)
            self.checkpoint_counter += 1
            
            logger.info(f"ğŸ’¾ Checkpoint saved: {checkpoint_path}")
            
            # æ¸…ç†æ—§æ£€æŸ¥ç‚¹
            self._cleanup_old_checkpoints()
            
        except Exception as e:
            logger.error(f"âŒ Failed to save checkpoint: {e}")
    
    def _save_pre_mutation_checkpoint(self, epoch: int):
        """ä¿å­˜çªå˜å‰çš„ç‰¹æ®Šæ£€æŸ¥ç‚¹"""
        if not self.checkpoint_dir:
            return
            
        try:
            checkpoint_path = os.path.join(
                self.checkpoint_dir, 
                f"pre_mutation_checkpoint_epoch_{epoch}.pth"
            )
            
            checkpoint = {
                "epoch": epoch,
                "search_model_state": self.search_model.state_dict(),
                "weight_optimizer_state": self.weight_optimizer.state_dict() if self.weight_optimizer else None,
                "arch_optimizer_state": self.arch_optimizer.state_dict() if self.arch_optimizer else None,
                "current_genotype": self.current_genotype,
                "best_performance": self.best_performance,
                "pre_mutation": True
            }
            
            torch.save(checkpoint, checkpoint_path)
            logger.info(f"ğŸ’¾ Pre-mutation checkpoint saved: {checkpoint_path}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to save pre-mutation checkpoint: {e}")
    
    def _cleanup_old_checkpoints(self):
        """æ¸…ç†æ—§çš„æ£€æŸ¥ç‚¹æ–‡ä»¶"""
        if not self.checkpoint_dir or self.config.max_checkpoints <= 0:
            return
        
        try:
            # è·å–æ‰€æœ‰æ£€æŸ¥ç‚¹æ–‡ä»¶
            checkpoint_files = []
            for filename in os.listdir(self.checkpoint_dir):
                if filename.startswith("aso_se_checkpoint_epoch_") and filename.endswith(".pth"):
                    filepath = os.path.join(self.checkpoint_dir, filename)
                    checkpoint_files.append((filepath, os.path.getctime(filepath)))
            
            # æŒ‰åˆ›å»ºæ—¶é—´æ’åº
            checkpoint_files.sort(key=lambda x: x[1], reverse=True)
            
            # åˆ é™¤è¶…å‡ºé™åˆ¶çš„æ–‡ä»¶
            for filepath, _ in checkpoint_files[self.config.max_checkpoints:]:
                os.remove(filepath)
                logger.debug(f"ğŸ—‘ï¸ Removed old checkpoint: {filepath}")
                
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to cleanup checkpoints: {e}")
    
    def should_early_stop(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ"""
        return self.patience_counter >= self.config.early_stopping_patience
    
    def get_training_report(self) -> Dict:
        """è·å–è®­ç»ƒæŠ¥å‘Š"""
        return {
            "current_cycle": self.current_cycle,
            "current_phase": self.current_phase,
            "best_performance": self.best_performance,
            "total_mutations": len(self.training_history["mutation_history"]),
            "exploration_report": self.explorer.get_exploration_report(),
            "mutation_report": self.mutator.get_mutation_report(),
            "device_report": self.device_manager.get_device_report(),
            "training_history": self.training_history
        }
    
    def save_checkpoint(self, filepath: str):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            "search_model_state": self.search_model.state_dict(),
            "evolvable_model_state": self.evolvable_model.state_dict() if self.evolvable_model else None,
            "current_genotype": self.current_genotype,
            "current_cycle": self.current_cycle,
            "current_phase": self.current_phase,
            "training_history": self.training_history,
            "config": self.config
        }
        torch.save(checkpoint, filepath)
        logger.info(f"ğŸ’¾ Checkpoint saved to {filepath}")
    
    def load_checkpoint(self, filepath: str):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(filepath, map_location=self.device_manager.device)
        
        self.search_model.load_state_dict(checkpoint["search_model_state"])
        
        if checkpoint["evolvable_model_state"] and self.evolvable_model:
            self.evolvable_model.load_state_dict(checkpoint["evolvable_model_state"])
        
        self.current_genotype = checkpoint["current_genotype"]
        self.current_cycle = checkpoint["current_cycle"]
        self.current_phase = checkpoint["current_phase"]
        self.training_history = checkpoint["training_history"]
        
        logger.info(f"ğŸ“‚ Checkpoint loaded from {filepath}")

def test_aso_se_framework():
    """æµ‹è¯•ASO-SEæ¡†æ¶åŠŸèƒ½"""
    print("ğŸ§ª Testing ASO-SE Framework...")
    
    # åˆ›å»ºé…ç½®
    config = ASOSEConfig(
        warmup_epochs=2,
        arch_training_epochs=1,
        weight_training_epochs=2,
        total_cycles=2,
        save_checkpoints=False  # æµ‹è¯•æ—¶ä¸ä¿å­˜æ£€æŸ¥ç‚¹
    )
    
    # åˆ›å»ºç®€å•çš„æœç´¢æ¨¡å‹
    class SimpleSearchModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
            self.conv2 = nn.Conv2d(16, 32, 3, padding=1)
            self.fc = nn.Linear(32 * 8 * 8, 10)
            
            # æ¨¡æ‹Ÿæ¶æ„å‚æ•°
            self.alphas_normal = nn.Parameter(torch.randn(4, 8))
            self.alphas_reduce = nn.Parameter(torch.randn(4, 8))
            
            self._C = 16
            self._num_classes = 10
            self._layers = 8
        
        def forward(self, x):
            x = F.relu(self.conv1(x))
            x = F.max_pool2d(x, 2)
            x = F.relu(self.conv2(x))
            x = F.max_pool2d(x, 2)
            x = x.view(x.size(0), -1)
            x = self.fc(x)
            return x
        
        def arch_parameters(self):
            return [self.alphas_normal, self.alphas_reduce]
        
        def genotype(self):
            from .genotypes import Genotype
            return Genotype(
                normal=[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)],
                normal_concat=[2],
                reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1)],
                reduce_concat=[2]
            )
    
    model = SimpleSearchModel()
    framework = ASOSEFramework(model, config)
    framework.initialize_optimizers()
    
    print(f"âœ… Framework initialized with {framework.config.total_cycles} cycles")
    print(f"âœ… Device: {framework.device_manager.device}")
    
    # æµ‹è¯•é˜¶æ®µç¡®å®š
    phase = framework._determine_current_phase(0)
    print(f"âœ… Phase determination: epoch 0 -> {phase}")
    
    phase = framework._determine_current_phase(3)
    print(f"âœ… Phase determination: epoch 3 -> {phase}")
    
    # æµ‹è¯•å†…å­˜ç®¡ç†
    memory_stats = framework.device_manager.get_memory_stats()
    print(f"âœ… Memory stats: {memory_stats}")
    
    # æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ
    report = framework.get_training_report()
    print(f"âœ… Training report generated: {report['current_cycle']} cycles completed")
    
    print("ğŸ‰ ASO-SE Framework tests passed!")

if __name__ == "__main__":
    test_aso_se_framework() 