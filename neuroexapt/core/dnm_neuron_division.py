#!/usr/bin/env python3
"""
Dynamic Neural Morphogenesis - ç¥ç»å…ƒåˆ†è£‚æ¨¡å—

åŸºäºä¿¡æ¯ç†µçš„ç¥ç»å…ƒåŠ¨æ€åˆ†è£‚æœºåˆ¶ï¼š
1. å®æ—¶ç›‘æ§æ¯ä¸ªç¥ç»å…ƒçš„ä¿¡æ¯æ‰¿è½½é‡
2. è¯†åˆ«ä¿¡æ¯è¿‡è½½çš„é«˜ç†µç¥ç»å…ƒ
3. æ‰§è¡Œæ™ºèƒ½åˆ†è£‚ï¼Œç»§æ‰¿æƒé‡å¹¶æ·»åŠ é€‚åº”æ€§å˜å¼‚
4. æ”¯æŒCNNå’Œå…¨è¿æ¥å±‚çš„åˆ†è£‚æ“ä½œ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
from typing import Dict, List, Tuple, Optional, Any
from collections import defaultdict
import logging

logger = logging.getLogger(__name__)


class NeuronInformationAnalyzer:
    """ç¥ç»å…ƒä¿¡æ¯åˆ†æå™¨ - è®¡ç®—ä¿¡æ¯ç†µå’Œè´Ÿè½½"""
    
    def __init__(self, bins=32, smoothing_factor=1e-8):
        self.bins = bins
        self.smoothing_factor = smoothing_factor
        self.activation_history = defaultdict(list)
        
    def analyze_activation_entropy(self, activations: torch.Tensor, layer_name: str) -> torch.Tensor:
        """
        åˆ†ææ¿€æ´»çš„ä¿¡æ¯ç†µ
        
        Args:
            activations: ç¥ç»å…ƒæ¿€æ´»å€¼ [batch_size, channels, ...] æˆ– [batch_size, neurons]
            layer_name: å±‚åç§°
            
        Returns:
            æ¯ä¸ªç¥ç»å…ƒ/é€šé“çš„ä¿¡æ¯ç†µ
        """
        # å®‰å…¨å¤„ç†æ¿€æ´»å¼ é‡
        if not isinstance(activations, torch.Tensor) or activations.numel() == 0:
            return torch.zeros(1, device=activations.device if isinstance(activations, torch.Tensor) else torch.device('cpu'))
        
        # ç¡®ä¿æ¿€æ´»å¼ é‡æ˜¯è¿ç»­çš„
        activations = activations.contiguous()
        
        if len(activations.shape) == 4:  # Conv2Då±‚
            return self._analyze_conv_entropy(activations)
        elif len(activations.shape) == 2:  # Linearå±‚
            return self._analyze_linear_entropy(activations)
        elif len(activations.shape) == 3:  # å¯èƒ½æ˜¯å±•å¹³åçš„å·ç§¯å±‚
            # å°è¯•å°†å…¶è§†ä¸º2Dè¿›è¡Œå¤„ç†
            reshaped = activations.view(activations.shape[0], -1)
            return self._analyze_linear_entropy(reshaped)
        else:
            logger.warning(f"Unsupported activation shape: {activations.shape} for layer {layer_name}")
            # è¿”å›å®‰å…¨çš„é»˜è®¤å€¼
            return torch.zeros(activations.shape[1] if len(activations.shape) > 1 else 1, 
                             device=activations.device)
    
    def _analyze_conv_entropy(self, activations: torch.Tensor) -> torch.Tensor:
        """åˆ†æå·ç§¯å±‚çš„é€šé“ç†µ"""
        if len(activations.shape) < 4:
            # å¤„ç†ä¸è§„åˆ™çš„æ¿€æ´»å½¢çŠ¶
            return torch.zeros(activations.shape[1] if len(activations.shape) > 1 else 1, device=activations.device)
        
        B, C, H, W = activations.shape
        channel_entropies = []
        
        for c in range(C):
            try:
                # å®‰å…¨åœ°è·å–è¯¥é€šé“çš„æ‰€æœ‰æ¿€æ´»å€¼
                channel_data = activations[:, c, :, :].contiguous().view(-1)
                
                # è®¡ç®—ä¿¡æ¯ç†µ
                entropy = self._calculate_entropy(channel_data)
                channel_entropies.append(entropy)
            except Exception as e:
                # å¦‚æœå‡ºç°é”™è¯¯ï¼Œä½¿ç”¨é»˜è®¤ç†µå€¼
                channel_entropies.append(0.0)
        
        return torch.tensor(channel_entropies, device=activations.device)
    
    def _analyze_linear_entropy(self, activations: torch.Tensor) -> torch.Tensor:
        """åˆ†æå…¨è¿æ¥å±‚çš„ç¥ç»å…ƒç†µ"""
        B, N = activations.shape
        neuron_entropies = []
        
        for n in range(N):
            # è·å–è¯¥ç¥ç»å…ƒçš„æ‰€æœ‰æ¿€æ´»å€¼
            neuron_data = activations[:, n]
            
            # è®¡ç®—ä¿¡æ¯ç†µ
            entropy = self._calculate_entropy(neuron_data)
            neuron_entropies.append(entropy)
        
        return torch.tensor(neuron_entropies, device=activations.device)
    
    def _calculate_entropy(self, data: torch.Tensor) -> float:
        """
        è®¡ç®—æ•°æ®çš„ä¿¡æ¯ç†µ
        
        ä¿¡æ¯ç†µå…¬å¼: H(X) = -Î£ p(x) * log2(p(x))
        é«˜ç†µè¡¨ç¤ºä¿¡æ¯åˆ†å¸ƒå‡åŒ€ï¼Œä½ç†µè¡¨ç¤ºä¿¡æ¯é›†ä¸­
        """
        if len(data) == 0:
            return 0.0
        
        # æ•°æ®é¢„å¤„ç†
        data = data.detach().cpu().float()
        
        # å¤„ç†å¸¸æ•°æ•°æ®
        if torch.std(data) < self.smoothing_factor:
            return 0.0
        
        # æ•°æ®å½’ä¸€åŒ–åˆ°[0,1]èŒƒå›´
        data_min, data_max = torch.min(data), torch.max(data)
        if data_max - data_min > self.smoothing_factor:
            normalized_data = (data - data_min) / (data_max - data_min)
        else:
            normalized_data = data
        
        # åˆ›å»ºç›´æ–¹å›¾
        hist = torch.histc(normalized_data, bins=self.bins, min=0.0, max=1.0)
        
        # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
        prob = hist / (hist.sum() + self.smoothing_factor)
        prob = prob[prob > 0]  # åªè€ƒè™‘éé›¶æ¦‚ç‡
        
        # è®¡ç®—ä¿¡æ¯ç†µ
        if len(prob) > 1:
            entropy = -torch.sum(prob * torch.log2(prob + self.smoothing_factor))
            return entropy.item()
        else:
            return 0.0
    
    def calculate_information_load(self, activations: torch.Tensor) -> Dict[str, float]:
        """
        è®¡ç®—ç¥ç»å…ƒçš„ä¿¡æ¯è´Ÿè½½æŒ‡æ ‡
        
        è¿”å›å¤šç»´åº¦çš„ä¿¡æ¯è´Ÿè½½åˆ†æï¼š
        - entropy: ä¿¡æ¯ç†µ
        - variance: æ–¹å·® (ä¿¡æ¯æ•£å¸ƒç¨‹åº¦)
        - sparsity: ç¨€ç–åº¦ (éé›¶æ¿€æ´»æ¯”ä¾‹)
        - dynamic_range: åŠ¨æ€èŒƒå›´ (æœ€å¤§å€¼-æœ€å°å€¼)
        """
        data = activations.detach().cpu().float()
        
        # åŸºç¡€ç»Ÿè®¡
        entropy = self._calculate_entropy(data)
        variance = torch.var(data).item()
        mean_abs = torch.mean(torch.abs(data)).item()
        
        # ç¨€ç–åº¦åˆ†æ
        threshold = mean_abs * 0.1  # 10%å¹³å‡å€¼ä½œä¸ºæ¿€æ´»é˜ˆå€¼
        active_ratio = torch.mean((torch.abs(data) > threshold).float()).item()
        
        # åŠ¨æ€èŒƒå›´
        dynamic_range = (torch.max(data) - torch.min(data)).item()
        
        # ä¿¡æ¯å¯†åº¦ (ç»“åˆç†µå’Œæ–¹å·®)
        information_density = entropy * math.sqrt(variance + self.smoothing_factor)
        
        return {
            'entropy': entropy,
            'variance': variance,
            'sparsity': 1.0 - active_ratio,  # ç¨€ç–åº¦ = 1 - æ¿€æ´»æ¯”ä¾‹
            'dynamic_range': dynamic_range,
            'information_density': information_density,
            'overload_score': self._calculate_overload_score(entropy, variance, active_ratio)
        }
    
    def _calculate_overload_score(self, entropy: float, variance: float, active_ratio: float) -> float:
        """
        è®¡ç®—ç¥ç»å…ƒè¿‡è½½è¯„åˆ†
        
        ç»¼åˆè€ƒè™‘ï¼š
        - é«˜ç†µ (ä¿¡æ¯å¤æ‚)
        - é«˜æ–¹å·® (æ¿€æ´»ä¸ç¨³å®š)  
        - é«˜æ¿€æ´»ç‡ (ç¥ç»å…ƒç¹å¿™)
        """
        # æ ‡å‡†åŒ–å„é¡¹æŒ‡æ ‡
        normalized_entropy = min(entropy / math.log2(self.bins), 1.0)
        normalized_variance = min(variance / 10.0, 1.0)  # å‡è®¾æ–¹å·®ä¸Šé™ä¸º10
        
        # åŠ æƒç»¼åˆè¯„åˆ†
        overload_score = (
            0.5 * normalized_entropy +      # ä¿¡æ¯å¤æ‚åº¦æƒé‡50%
            0.3 * normalized_variance +     # æ¿€æ´»ä¸ç¨³å®šæ€§æƒé‡30%
            0.2 * active_ratio             # ç¥ç»å…ƒç¹å¿™åº¦æƒé‡20%
        )
        
        return min(overload_score, 1.0)


class IntelligentNeuronSplitter:
    """æ™ºèƒ½ç¥ç»å…ƒåˆ†è£‚å™¨"""
    
    def __init__(self, 
                 entropy_threshold: float = 0.7,
                 overload_threshold: float = 0.6,
                 split_probability: float = 0.4,
                 max_splits_per_layer: int = 3,
                 inheritance_noise: float = 0.1):
        
        self.entropy_threshold = entropy_threshold
        self.overload_threshold = overload_threshold
        self.split_probability = split_probability
        self.max_splits_per_layer = max_splits_per_layer
        self.inheritance_noise = inheritance_noise
        
        self.analyzer = NeuronInformationAnalyzer()
        self.split_history = defaultdict(list)
        
        # å¯¼å…¥Net2Netå˜æ¢å™¨
        try:
            from .dnm_net2net import Net2NetTransformer, DNMArchitectureMutator
            self.net2net_transformer = Net2NetTransformer(noise_scale=inheritance_noise)
            self.architecture_mutator = DNMArchitectureMutator(self.net2net_transformer)
        except ImportError:
            logger.warning("Net2Net transformer not available, using simple splitting")
            self.net2net_transformer = None
            self.architecture_mutator = None
        
    def decide_split_candidates(self, 
                              activations: torch.Tensor, 
                              layer_name: str) -> List[int]:
        """
        å†³å®šéœ€è¦åˆ†è£‚çš„ç¥ç»å…ƒå€™é€‰
        
        Args:
            activations: ç¥ç»å…ƒæ¿€æ´»å€¼
            layer_name: å±‚åç§°
            
        Returns:
            éœ€è¦åˆ†è£‚çš„ç¥ç»å…ƒ/é€šé“ç´¢å¼•åˆ—è¡¨
        """
        split_candidates = []
        
        if len(activations.shape) == 4:  # Convå±‚
            num_channels = activations.shape[1]
            
            for c in range(num_channels):
                channel_data = activations[:, c, :, :]
                info_load = self.analyzer.calculate_information_load(channel_data)
                
                # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†è£‚
                if self._should_split_neuron(info_load, c, layer_name):
                    split_candidates.append(c)
                    
        elif len(activations.shape) == 2:  # Linearå±‚
            num_neurons = activations.shape[1]
            
            for n in range(num_neurons):
                neuron_data = activations[:, n]
                info_load = self.analyzer.calculate_information_load(neuron_data)
                
                # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†è£‚
                if self._should_split_neuron(info_load, n, layer_name):
                    split_candidates.append(n)
        
        # é™åˆ¶åˆ†è£‚æ•°é‡
        if len(split_candidates) > self.max_splits_per_layer:
            # æŒ‰ç…§è¿‡è½½è¯„åˆ†æ’åºï¼Œé€‰æ‹©æœ€éœ€è¦åˆ†è£‚çš„
            candidate_scores = []
            for idx in split_candidates:
                if len(activations.shape) == 4:
                    data = activations[:, idx, :, :]
                else:
                    data = activations[:, idx]
                
                info_load = self.analyzer.calculate_information_load(data)
                candidate_scores.append((idx, info_load['overload_score']))
            
            # æ’åºå¹¶é€‰æ‹©å‰Nä¸ª
            candidate_scores.sort(key=lambda x: x[1], reverse=True)
            split_candidates = [idx for idx, _ in candidate_scores[:self.max_splits_per_layer]]
        
        # è®°å½•åˆ†è£‚å†å²
        if split_candidates:
            self.split_history[layer_name].extend(split_candidates)
            logger.info(f"Layer {layer_name}: Selected {len(split_candidates)} neurons for splitting")
        
        return split_candidates
    
    def _should_split_neuron(self, info_load: Dict[str, float], neuron_idx: int, layer_name: str) -> bool:
        """åˆ¤æ–­ç¥ç»å…ƒæ˜¯å¦åº”è¯¥åˆ†è£‚"""
        
        # æ¡ä»¶1: ä¿¡æ¯ç†µè¶…è¿‡é˜ˆå€¼
        entropy_condition = info_load['entropy'] > self.entropy_threshold
        
        # æ¡ä»¶2: è¿‡è½½è¯„åˆ†è¶…è¿‡é˜ˆå€¼
        overload_condition = info_load['overload_score'] > self.overload_threshold
        
        # æ¡ä»¶3: éšæœºæ¦‚ç‡
        random_condition = torch.rand(1).item() < self.split_probability
        
        # æ¡ä»¶4: é¿å…é‡å¤åˆ†è£‚åŒä¸€ç¥ç»å…ƒ
        recent_splits = self.split_history.get(layer_name, [])
        not_recently_split = neuron_idx not in recent_splits[-10:]  # æœ€è¿‘10æ¬¡åˆ†è£‚ä¸­ä¸åŒ…å«
        
        # ç»¼åˆåˆ¤æ–­
        should_split = (entropy_condition and overload_condition and 
                       random_condition and not_recently_split)
        
        if should_split:
            logger.debug(f"Neuron {neuron_idx} in {layer_name} marked for splitting: "
                        f"entropy={info_load['entropy']:.3f}, "
                        f"overload={info_load['overload_score']:.3f}")
        
        return should_split
    
    def execute_conv_split(self, conv_layer: nn.Conv2d, split_indices: List[int]) -> nn.Conv2d:
        """æ‰§è¡Œå·ç§¯å±‚é€šé“åˆ†è£‚"""
        if not split_indices:
            return conv_layer
        
        # åˆ›å»ºæ–°çš„å·ç§¯å±‚
        new_out_channels = conv_layer.out_channels + len(split_indices)
        new_conv = nn.Conv2d(
            in_channels=conv_layer.in_channels,
            out_channels=new_out_channels,
            kernel_size=conv_layer.kernel_size,
            stride=conv_layer.stride,
            padding=conv_layer.padding,
            dilation=conv_layer.dilation,
            groups=conv_layer.groups,
            bias=conv_layer.bias is not None,
            padding_mode=conv_layer.padding_mode
        ).to(conv_layer.weight.device)
        
        # æƒé‡ç»§æ‰¿å’Œåˆ†è£‚
        with torch.no_grad():
            # å¤åˆ¶åŸå§‹æƒé‡
            new_conv.weight[:conv_layer.out_channels] = conv_layer.weight.data
            if conv_layer.bias is not None:
                new_conv.bias[:conv_layer.out_channels] = conv_layer.bias.data
            
            # ä¸ºåˆ†è£‚çš„é€šé“åˆå§‹åŒ–æƒé‡
            for i, split_idx in enumerate(split_indices):
                new_idx = conv_layer.out_channels + i
                
                # ç»§æ‰¿çˆ¶é€šé“æƒé‡ + è‡ªé€‚åº”å™ªå£°
                parent_weight = conv_layer.weight.data[split_idx]
                noise_scale = self.inheritance_noise * torch.std(parent_weight)
                noise = torch.randn_like(parent_weight) * noise_scale
                
                new_conv.weight[new_idx] = parent_weight + noise
                
                if conv_layer.bias is not None:
                    parent_bias = conv_layer.bias.data[split_idx]
                    bias_noise = torch.randn(1, device=parent_bias.device) * noise_scale
                    new_conv.bias[new_idx] = parent_bias + bias_noise
        
        logger.info(f"Conv layer split: {conv_layer.out_channels} -> {new_out_channels} channels")
        return new_conv
    
    def execute_linear_split(self, linear_layer: nn.Linear, split_indices: List[int]) -> nn.Linear:
        """æ‰§è¡Œå…¨è¿æ¥å±‚ç¥ç»å…ƒåˆ†è£‚"""
        if not split_indices:
            return linear_layer
        
        # åˆ›å»ºæ–°çš„å…¨è¿æ¥å±‚
        new_out_features = linear_layer.out_features + len(split_indices)
        new_linear = nn.Linear(
            in_features=linear_layer.in_features,
            out_features=new_out_features,
            bias=linear_layer.bias is not None
        ).to(linear_layer.weight.device)
        
        # æƒé‡ç»§æ‰¿å’Œåˆ†è£‚
        with torch.no_grad():
            # å¤åˆ¶åŸå§‹æƒé‡
            new_linear.weight[:linear_layer.out_features] = linear_layer.weight.data
            if linear_layer.bias is not None:
                new_linear.bias[:linear_layer.out_features] = linear_layer.bias.data
            
            # ä¸ºåˆ†è£‚çš„ç¥ç»å…ƒåˆå§‹åŒ–æƒé‡
            for i, split_idx in enumerate(split_indices):
                new_idx = linear_layer.out_features + i
                
                # ç»§æ‰¿çˆ¶ç¥ç»å…ƒæƒé‡ + è‡ªé€‚åº”å™ªå£°
                parent_weight = linear_layer.weight.data[split_idx]
                noise_scale = self.inheritance_noise * torch.std(parent_weight)
                noise = torch.randn_like(parent_weight) * noise_scale
                
                new_linear.weight[new_idx] = parent_weight + noise
                
                if linear_layer.bias is not None:
                    parent_bias = linear_layer.bias.data[split_idx]
                    bias_noise = torch.randn(1, device=parent_bias.device) * noise_scale
                    new_linear.bias[new_idx] = parent_bias + bias_noise
        
        logger.info(f"Linear layer split: {linear_layer.out_features} -> {new_out_features} neurons")
        return new_linear


class DNMNeuronDivision:
    """DNMç¥ç»å…ƒåˆ†è£‚ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or self._default_config()
        self.splitter = IntelligentNeuronSplitter(**self.config['splitter'])
        self.activation_hooks = {}
        self.split_statistics = defaultdict(int)
        
    def _default_config(self) -> Dict:
        """é»˜è®¤é…ç½®"""
        return {
            'splitter': {
                'entropy_threshold': 0.7,
                'overload_threshold': 0.6,
                'split_probability': 0.4,
                'max_splits_per_layer': 3,
                'inheritance_noise': 0.1
            },
            'monitoring': {
                'target_layers': ['conv', 'linear'],  # ç›‘æ§çš„å±‚ç±»å‹
                'analysis_frequency': 5,  # æ¯5ä¸ªepochåˆ†æä¸€æ¬¡
                'min_epoch_before_split': 10  # æœ€å°è®­ç»ƒepochåæ‰å¼€å§‹åˆ†è£‚
            }
        }
    
    def register_model_hooks(self, model: nn.Module) -> None:
        """ä¸ºæ¨¡å‹æ³¨å†Œæ¿€æ´»ç›‘æ§hooks"""
        self.activation_cache = {}
        hooks = []
        
        def create_hook(name):
            def hook_fn(module, input, output):
                if isinstance(output, torch.Tensor):
                    self.activation_cache[name] = output.detach().clone()
            return hook_fn
        
        for name, module in model.named_modules():
            if self._should_monitor_layer(module, name):
                hook = module.register_forward_hook(create_hook(name))
                hooks.append(hook)
                logger.debug(f"Registered hook for layer: {name}")
        
        self.activation_hooks[id(model)] = hooks
    
    def remove_model_hooks(self, model: nn.Module) -> None:
        """ç§»é™¤æ¨¡å‹çš„hooks"""
        model_id = id(model)
        if model_id in self.activation_hooks:
            for hook in self.activation_hooks[model_id]:
                hook.remove()
            del self.activation_hooks[model_id]
            logger.debug("Removed all activation hooks")
    
    def _should_monitor_layer(self, module: nn.Module, name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç›‘æ§è¯¥å±‚"""
        target_types = {
            'conv': nn.Conv2d,
            'linear': nn.Linear
        }
        
        for layer_type in self.config['monitoring']['target_layers']:
            if isinstance(module, target_types.get(layer_type)):
                return True
        return False
    
    def analyze_and_split(self, model: nn.Module, epoch: int) -> Dict[str, Any]:
        """åˆ†ææ¨¡å‹å¹¶æ‰§è¡Œç¥ç»å…ƒåˆ†è£‚"""
        
        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³åˆ†è£‚æ¡ä»¶
        if epoch < self.config['monitoring']['min_epoch_before_split']:
            return {'splits_executed': 0, 'message': 'Too early for splitting'}
        
        if epoch % self.config['monitoring']['analysis_frequency'] != 0:
            return {'splits_executed': 0, 'message': 'Not analysis epoch'}
        
        split_decisions = {}
        
        # åˆ†æå„å±‚çš„æ¿€æ´»å¹¶å†³å®šåˆ†è£‚ç­–ç•¥
        for layer_name, activations in self.activation_cache.items():
            try:
                split_candidates = self.splitter.decide_split_candidates(
                    activations, layer_name
                )
                
                if split_candidates:
                    split_decisions[layer_name] = split_candidates
                    logger.info(f"Layer {layer_name}: {len(split_candidates)} neurons marked for splitting")
                    
            except Exception as e:
                logger.warning(f"Failed to analyze layer {layer_name}: {e}")
                continue
        
        # æ‰§è¡Œåˆ†è£‚æ“ä½œ
        splits_executed = self._execute_splits(model, split_decisions)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        for layer_name, candidates in split_decisions.items():
            self.split_statistics[layer_name] += len(candidates)
        
        # æ¸…ç†æ¿€æ´»ç¼“å­˜
        self.activation_cache.clear()
        
        result = {
            'splits_executed': splits_executed,
            'split_decisions': split_decisions,
            'total_splits_per_layer': dict(self.split_statistics),
            'message': f'Successfully executed {splits_executed} splits'
        }
        
        logger.info(f"DNM Neuron Division completed: {splits_executed} splits executed")
        return result
    
    def _execute_splits(self, model: nn.Module, split_decisions: Dict[str, List[int]]) -> int:
        """æ‰§è¡Œå…·ä½“çš„åˆ†è£‚æ“ä½œ"""
        total_splits = 0
        
        for layer_name, split_indices in split_decisions.items():
            try:
                # è·å–ç›®æ ‡å±‚
                target_module = self._get_module_by_name(model, layer_name)
                if target_module is None:
                    logger.warning(f"Could not find module: {layer_name}")
                    continue
                
                # æ ¹æ®å±‚ç±»å‹æ‰§è¡Œåˆ†è£‚
                if isinstance(target_module, nn.Conv2d):
                    new_module = self.splitter.execute_conv_split(target_module, split_indices)
                elif isinstance(target_module, nn.Linear):
                    new_module = self.splitter.execute_linear_split(target_module, split_indices)
                else:
                    logger.warning(f"Unsupported layer type for splitting: {type(target_module)}")
                    continue
                
                # æ›¿æ¢æ¨¡å‹ä¸­çš„å±‚
                self._replace_module_in_model(model, layer_name, new_module)
                
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šåŒæ­¥æ›´æ–°ç›¸å…³BatchNormå±‚å’Œä¸‹æ¸¸å±‚
                if isinstance(target_module, nn.Conv2d):
                    self._sync_batchnorm_after_conv_split(model, layer_name, target_module.out_channels, new_module.out_channels, split_indices)
                    # ğŸš€ æ–°å¢ï¼šçº§è”æ›´æ–°ä¸‹æ¸¸Convå±‚çš„è¾“å…¥é€šé“
                    self._sync_downstream_conv_input_channels(model, layer_name, target_module.out_channels, new_module.out_channels, split_indices)
                    # ğŸ¯ æœ€ç»ˆä¿®å¤ï¼šçº§è”æ›´æ–°ä¸‹æ¸¸Linearå±‚çš„è¾“å…¥ç‰¹å¾
                    self._sync_downstream_linear_input_features(model, layer_name, target_module.out_channels, new_module.out_channels, split_indices)
                    # ğŸ”— æ®‹å·®è¿æ¥ä¿®å¤ï¼šæ›´æ–°ResidualBlockçš„shortcutå±‚
                    self._sync_residual_shortcut_channels(model, layer_name, target_module.out_channels, new_module.out_channels, split_indices)
                
                total_splits += len(split_indices)
                
                logger.info(f"Successfully split layer {layer_name}: {len(split_indices)} new neurons/channels")
                
            except Exception as e:
                logger.error(f"Failed to split layer {layer_name}: {e}")
                continue
        
        return total_splits
    
    def _get_module_by_name(self, model: nn.Module, module_name: str) -> Optional[nn.Module]:
        """æ ¹æ®åç§°è·å–æ¨¡å—"""
        try:
            return model.get_submodule(module_name)
        except AttributeError:
            # å…¼å®¹è€ç‰ˆæœ¬PyTorch
            parts = module_name.split('.')
            module = model
            for part in parts:
                module = getattr(module, part)
            return module
        except Exception:
            return None
    
    def _replace_module_in_model(self, model: nn.Module, module_name: str, new_module: nn.Module) -> None:
        """åœ¨æ¨¡å‹ä¸­æ›¿æ¢æ¨¡å—"""
        parts = module_name.split('.')
        parent = model
        
        # æ‰¾åˆ°çˆ¶æ¨¡å—
        for part in parts[:-1]:
            parent = getattr(parent, part)
        
        # æ›¿æ¢ç›®æ ‡æ¨¡å—
        setattr(parent, parts[-1], new_module)
    
    def _sync_batchnorm_after_conv_split(self, model: nn.Module, conv_layer_name: str, 
                                        old_channels: int, new_channels: int, split_indices: List[int]) -> None:
        """
        ğŸ”§ å…³é”®ä¿®å¤ï¼šConvå±‚åˆ†è£‚ååŒæ­¥ç›¸å…³BatchNormå±‚
        
        è¿™æ˜¯æœ€å®¹æ˜“å¿½ç•¥ä½†æå…¶é‡è¦çš„æ­¥éª¤ï¼
        å½“Convå±‚é€šé“æ•°æ”¹å˜æ—¶ï¼Œå¯¹åº”çš„BatchNormå±‚å¿…é¡»åŒæ­¥æ›´æ–°ï¼š
        - num_features
        - running_mean
        - running_var  
        - weight (gamma)
        - bias (beta)
        """
        # æŸ¥æ‰¾å¯¹åº”çš„BatchNormå±‚
        bn_layer_name = self._find_corresponding_batchnorm(model, conv_layer_name)
        if not bn_layer_name:
            logger.warning(f"No corresponding BatchNorm found for {conv_layer_name}")
            return
        
        try:
            bn_module = self._get_module_by_name(model, bn_layer_name)
            if not isinstance(bn_module, (nn.BatchNorm2d, nn.BatchNorm1d)):
                return
            
            logger.info(f"Syncing BatchNorm {bn_layer_name}: {old_channels} -> {new_channels} features")
            
            # åˆ›å»ºæ–°çš„BatchNormå±‚
            if isinstance(bn_module, nn.BatchNorm2d):
                new_bn = nn.BatchNorm2d(
                    num_features=new_channels,
                    eps=bn_module.eps,
                    momentum=bn_module.momentum,
                    affine=bn_module.affine,
                    track_running_stats=bn_module.track_running_stats
                ).to(bn_module.weight.device if bn_module.weight is not None else 'cpu')
            else:  # BatchNorm1d
                new_bn = nn.BatchNorm1d(
                    num_features=new_channels,
                    eps=bn_module.eps,
                    momentum=bn_module.momentum,
                    affine=bn_module.affine,
                    track_running_stats=bn_module.track_running_stats
                ).to(bn_module.weight.device if bn_module.weight is not None else 'cpu')
            
            # ç»§æ‰¿åŸå§‹å‚æ•°
            with torch.no_grad():
                if bn_module.affine:
                    # å¤åˆ¶åŸå§‹weight (gamma) å’Œ bias (beta)
                    new_bn.weight[:old_channels] = bn_module.weight.data
                    new_bn.bias[:old_channels] = bn_module.bias.data
                    
                    # ä¸ºæ–°é€šé“åˆå§‹åŒ–å‚æ•°
                    for i, split_idx in enumerate(split_indices):
                        new_idx = old_channels + i
                        # gammaç»§æ‰¿çˆ¶é€šé“å€¼
                        new_bn.weight[new_idx] = bn_module.weight.data[split_idx]
                        # betaç»§æ‰¿çˆ¶é€šé“å€¼
                        new_bn.bias[new_idx] = bn_module.bias.data[split_idx]
                
                if bn_module.track_running_stats:
                    # å¤åˆ¶running_meanå’Œrunning_var
                    new_bn.running_mean[:old_channels] = bn_module.running_mean
                    new_bn.running_var[:old_channels] = bn_module.running_var
                    new_bn.num_batches_tracked = bn_module.num_batches_tracked
                    
                    # ä¸ºæ–°é€šé“åˆå§‹åŒ–running stats
                    for i, split_idx in enumerate(split_indices):
                        new_idx = old_channels + i
                        new_bn.running_mean[new_idx] = bn_module.running_mean[split_idx]
                        new_bn.running_var[new_idx] = bn_module.running_var[split_idx]
            
            # æ›¿æ¢BatchNormå±‚
            self._replace_module_in_model(model, bn_layer_name, new_bn)
            logger.info(f"âœ… BatchNorm {bn_layer_name} successfully synced!")
            
        except Exception as e:
            logger.error(f"Failed to sync BatchNorm {bn_layer_name}: {e}")
    
    def _sync_downstream_conv_input_channels(self, model: nn.Module, conv_layer_name: str,
                                           old_out_channels: int, new_out_channels: int, 
                                           split_indices: List[int]) -> None:
        """
        ğŸš€ çº§è”åŒæ­¥ï¼šæ›´æ–°ä¸‹æ¸¸Convå±‚çš„è¾“å…¥é€šé“
        
        å½“ä¸€ä¸ªConvå±‚çš„è¾“å‡ºé€šé“å¢åŠ æ—¶ï¼Œæ‰€æœ‰ä»¥å®ƒä¸ºè¾“å…¥çš„Convå±‚éƒ½éœ€è¦ç›¸åº”æ›´æ–°è¾“å…¥é€šé“
        è¿™æ˜¯è§£å†³ "weight of size [69, 64, 3, 3], expected input to have 64 channels, but got 69" çš„å…³é”®ï¼
        """
        logger.debug(f"ğŸ” Finding downstream Conv layers for {conv_layer_name}")
        
        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½å—å½±å“çš„ä¸‹æ¸¸Convå±‚
        downstream_conv_layers = self._find_downstream_conv_layers(model, conv_layer_name)
        
        for downstream_name in downstream_conv_layers:
            try:
                downstream_conv = self._get_module_by_name(model, downstream_name)
                if not isinstance(downstream_conv, nn.Conv2d):
                    continue
                
                # æ£€æŸ¥è¾“å…¥é€šé“æ˜¯å¦åŒ¹é…
                if downstream_conv.in_channels == old_out_channels:
                    logger.info(f"ğŸ”„ Updating downstream Conv {downstream_name}: in_channels {old_out_channels} -> {new_out_channels}")
                    
                    # åˆ›å»ºæ–°çš„Convå±‚ï¼Œæ‰©å±•è¾“å…¥é€šé“
                    new_downstream_conv = self._expand_conv_input_channels(
                        downstream_conv, old_out_channels, new_out_channels, split_indices
                    )
                    
                    # æ›¿æ¢æ¨¡å‹ä¸­çš„å±‚
                    self._replace_module_in_model(model, downstream_name, new_downstream_conv)
                    logger.info(f"âœ… Successfully updated downstream Conv {downstream_name}")
                    
            except Exception as e:
                logger.error(f"Failed to update downstream Conv {downstream_name}: {e}")
    
    def _find_downstream_conv_layers(self, model: nn.Module, conv_layer_name: str) -> List[str]:
        """æŸ¥æ‰¾å¯èƒ½å—å½±å“çš„ä¸‹æ¸¸Convå±‚"""
        downstream_layers = []
        
        # ç®€å•çš„å¯å‘å¼æ–¹æ³•ï¼šæŸ¥æ‰¾åç»­çš„Convå±‚
        conv_parts = conv_layer_name.split('.')
        
        for name, module in model.named_modules():
            if isinstance(module, nn.Conv2d) and name != conv_layer_name:
                name_parts = name.split('.')
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºåºåˆ—ä¸­çš„ä¸‹ä¸€å±‚
                if self._is_likely_downstream_layer(conv_parts, name_parts):
                    downstream_layers.append(name)
        
        logger.debug(f"Found potential downstream Conv layers: {downstream_layers}")
        return downstream_layers
    
    def _is_likely_downstream_layer(self, upstream_parts: List[str], downstream_parts: List[str]) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºä¸‹æ¸¸å±‚"""
        # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®è¯†åˆ«è·¨blockçš„è¿æ¥æ¨¡å¼
        
        # stem.0 -> block1.main_path.0 æˆ– block1.shortcut.0
        if upstream_parts[0] == 'stem' and len(downstream_parts) >= 2:
            if downstream_parts[0] == 'block1':
                return True
        
        # stem.0 -> layer1.0 è¿™ç§æ¨¡å¼ï¼ˆæ—§çš„å‘½åï¼‰
        if len(upstream_parts) == 2 and len(downstream_parts) == 2:
            if upstream_parts[0] == 'stem' and downstream_parts[0] == 'layer1':
                return True
        
        # blocké—´çš„è¿æ¥: block1 -> block2, block2 -> block3, etc.
        if len(upstream_parts) >= 2 and len(downstream_parts) >= 2:
            if upstream_parts[0].startswith('block') and downstream_parts[0].startswith('block'):
                try:
                    up_block_num = int(upstream_parts[0].replace('block', ''))
                    down_block_num = int(downstream_parts[0].replace('block', ''))
                    # è¿ç»­çš„block
                    if down_block_num == up_block_num + 1:
                        return True
                except ValueError:
                    pass
        
        # Sequentialå±‚å†…çš„è¿æ¥: block1.main_path.0 -> block1.main_path.3 (è·³è¿‡BNå’ŒReLU)
        if len(upstream_parts) == len(downstream_parts) and len(upstream_parts) >= 3:
            # åŒä¸€ä¸ªæ¨¡å—å†…çš„åç»­å±‚
            if upstream_parts[:-1] == downstream_parts[:-1]:
                try:
                    up_idx = int(upstream_parts[-1])
                    down_idx = int(downstream_parts[-1])
                    # è€ƒè™‘ä¸­é—´å¯èƒ½æœ‰BNå’ŒReLUï¼Œæ‰€ä»¥å…è®¸é—´éš”
                    if down_idx > up_idx and down_idx - up_idx <= 6:
                        return True
                except ValueError:
                    pass
        
        return False
    
    def _expand_conv_input_channels(self, conv_layer: nn.Conv2d, old_in_channels: int, 
                                  new_in_channels: int, split_indices: List[int]) -> nn.Conv2d:
        """æ‰©å±•Convå±‚çš„è¾“å…¥é€šé“"""
        # åˆ›å»ºæ–°çš„Convå±‚
        new_conv = nn.Conv2d(
            in_channels=new_in_channels,
            out_channels=conv_layer.out_channels,
            kernel_size=conv_layer.kernel_size,
            stride=conv_layer.stride,
            padding=conv_layer.padding,
            dilation=conv_layer.dilation,
            groups=conv_layer.groups,
            bias=conv_layer.bias is not None,
            padding_mode=conv_layer.padding_mode
        ).to(conv_layer.weight.device)
        
        # æƒé‡ç»§æ‰¿ç­–ç•¥
        with torch.no_grad():
            # å¤åˆ¶åŸå§‹æƒé‡ [out_channels, in_channels, kernel_h, kernel_w]
            new_conv.weight[:, :old_in_channels, :, :] = conv_layer.weight.data
            
            # ä¸ºæ–°çš„è¾“å…¥é€šé“åˆå§‹åŒ–æƒé‡ï¼ˆç»§æ‰¿è‡ªåˆ†è£‚çš„çˆ¶é€šé“ï¼‰
            for i, split_idx in enumerate(split_indices):
                new_in_idx = old_in_channels + i
                # ç»§æ‰¿çˆ¶é€šé“çš„æƒé‡
                new_conv.weight[:, new_in_idx, :, :] = conv_layer.weight.data[:, split_idx, :, :]
            
            # å¤åˆ¶bias
            if conv_layer.bias is not None:
                new_conv.bias.data = conv_layer.bias.data
        
        return new_conv
    
    def _sync_downstream_linear_input_features(self, model: nn.Module, conv_layer_name: str,
                                             old_out_channels: int, new_out_channels: int,
                                             split_indices: List[int]) -> None:
        """
        ğŸ¯ æœ€ç»ˆä¿®å¤ï¼šæ›´æ–°ä¸‹æ¸¸Linearå±‚çš„è¾“å…¥ç‰¹å¾æ•°
        
        å½“æœ€åä¸€ä¸ªConvå±‚é€šé“å¢åŠ æ—¶ï¼Œåç»­çš„Linearå±‚(classifier)éœ€è¦ç›¸åº”æ›´æ–°è¾“å…¥ç‰¹å¾æ•°
        è§£å†³: "mat1 and mat2 shapes cannot be multiplied (4x69 and 64x15)"
        """
        logger.debug(f"ğŸ” Finding downstream Linear layers for {conv_layer_name}")
        
        # æŸ¥æ‰¾æ‰€æœ‰Linearå±‚
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                # æ£€æŸ¥è¾“å…¥ç‰¹å¾æ˜¯å¦åŒ¹é…ï¼ˆè€ƒè™‘å¯èƒ½é€šè¿‡Global Average Poolingï¼‰
                if module.in_features == old_out_channels:
                    logger.info(f"ğŸ”„ Updating downstream Linear {name}: in_features {old_out_channels} -> {new_out_channels}")
                    
                    try:
                        # åˆ›å»ºæ–°çš„Linearå±‚ï¼Œæ‰©å±•è¾“å…¥ç‰¹å¾
                        new_linear = self._expand_linear_input_features(
                            module, old_out_channels, new_out_channels, split_indices
                        )
                        
                        # æ›¿æ¢æ¨¡å‹ä¸­çš„å±‚
                        self._replace_module_in_model(model, name, new_linear)
                        logger.info(f"âœ… Successfully updated downstream Linear {name}")
                        
                    except Exception as e:
                        logger.error(f"Failed to update downstream Linear {name}: {e}")
    
    def _sync_residual_shortcut_channels(self, model: nn.Module, conv_layer_name: str,
                                       old_out_channels: int, new_out_channels: int,
                                       split_indices: List[int]) -> None:
        """
        ğŸ”— æ®‹å·®è¿æ¥ä¿®å¤ï¼šæ›´æ–°ResidualBlockçš„shortcutå±‚
        
        å½“main_pathä¸­çš„Convå±‚é€šé“å‘ç”Ÿå˜åŒ–æ—¶ï¼Œå¯¹åº”çš„shortcutå±‚ä¹Ÿéœ€è¦ç›¸åº”æ›´æ–°
        ä»¥ç¡®ä¿æ®‹å·®ç›¸åŠ æ—¶é€šé“æ•°åŒ¹é…
        """
        logger.debug(f"ğŸ” Checking residual shortcut for {conv_layer_name}")
        
        # è§£æå±‚åä»¥æ‰¾åˆ°å¯¹åº”çš„ResidualBlock
        parts = conv_layer_name.split('.')
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ResidualBlockå†…çš„main_pathå±‚
        if len(parts) >= 3 and parts[-2] == 'main_path':
            # æ„é€ å¯¹åº”çš„shortcutå±‚å
            block_name = '.'.join(parts[:-2])  # ä¾‹å¦‚ï¼šblock1
            shortcut_layer_name = f"{block_name}.shortcut.0"
            
            try:
                shortcut_conv = self._get_module_by_name(model, shortcut_layer_name)
                
                # å¦‚æœshortcutæ˜¯Convå±‚ä¸”è¾“å‡ºé€šé“åŒ¹é…ï¼Œéœ€è¦æ›´æ–°
                if isinstance(shortcut_conv, nn.Conv2d) and shortcut_conv.out_channels == old_out_channels:
                    logger.info(f"ğŸ”„ Updating residual shortcut {shortcut_layer_name}: out_channels {old_out_channels} -> {new_out_channels}")
                    
                    # åˆ›å»ºæ–°çš„shortcut Convå±‚
                    new_shortcut_conv = self._expand_conv_output_channels(
                        shortcut_conv, old_out_channels, new_out_channels, split_indices
                    )
                    
                    # æ›¿æ¢æ¨¡å‹ä¸­çš„å±‚
                    self._replace_module_in_model(model, shortcut_layer_name, new_shortcut_conv)
                    
                    # åŒæ­¥å¯¹åº”çš„BatchNorm
                    shortcut_bn_name = f"{block_name}.shortcut.1"
                    self._sync_batchnorm_after_conv_split(model, shortcut_layer_name, old_out_channels, new_out_channels, split_indices)
                    
                    logger.info(f"âœ… Successfully updated residual shortcut {shortcut_layer_name}")
                    
            except Exception as e:
                logger.error(f"Failed to update residual shortcut for {conv_layer_name}: {e}")
    
    def _expand_conv_output_channels(self, conv_layer: nn.Conv2d, old_out_channels: int,
                                   new_out_channels: int, split_indices: List[int]) -> nn.Conv2d:
        """æ‰©å±•Convå±‚çš„è¾“å‡ºé€šé“ï¼ˆç”¨äºshortcutå±‚æ›´æ–°ï¼‰"""
        # åˆ›å»ºæ–°çš„Convå±‚
        new_conv = nn.Conv2d(
            in_channels=conv_layer.in_channels,
            out_channels=new_out_channels,
            kernel_size=conv_layer.kernel_size,
            stride=conv_layer.stride,
            padding=conv_layer.padding,
            dilation=conv_layer.dilation,
            groups=conv_layer.groups,
            bias=conv_layer.bias is not None,
            padding_mode=conv_layer.padding_mode
        ).to(conv_layer.weight.device)
        
        # æƒé‡ç»§æ‰¿ç­–ç•¥
        with torch.no_grad():
            # å¤åˆ¶åŸå§‹æƒé‡ [out_channels, in_channels, kernel_h, kernel_w]
            new_conv.weight[:old_out_channels, :, :, :] = conv_layer.weight.data
            
            # ä¸ºæ–°çš„è¾“å‡ºé€šé“åˆå§‹åŒ–æƒé‡ï¼ˆç»§æ‰¿è‡ªåˆ†è£‚çš„çˆ¶é€šé“ï¼‰
            for i, split_idx in enumerate(split_indices):
                new_out_idx = old_out_channels + i
                # ç»§æ‰¿çˆ¶é€šé“çš„æƒé‡å¹¶æ·»åŠ å°‘é‡å™ªå£°
                parent_weight = conv_layer.weight.data[split_idx, :, :, :]
                noise_scale = 0.01 * torch.std(parent_weight)
                noise = torch.randn_like(parent_weight) * noise_scale
                new_conv.weight[new_out_idx, :, :, :] = parent_weight + noise
            
            # å¤åˆ¶bias
            if conv_layer.bias is not None:
                new_conv.bias[:old_out_channels] = conv_layer.bias.data
                # ä¸ºæ–°é€šé“åˆå§‹åŒ–bias
                for i, split_idx in enumerate(split_indices):
                    new_out_idx = old_out_channels + i
                    new_conv.bias[new_out_idx] = conv_layer.bias.data[split_idx]
        
        return new_conv
    
    def _expand_linear_input_features(self, linear_layer: nn.Linear, old_in_features: int,
                                    new_in_features: int, split_indices: List[int]) -> nn.Linear:
        """æ‰©å±•Linearå±‚çš„è¾“å…¥ç‰¹å¾æ•°"""
        # åˆ›å»ºæ–°çš„Linearå±‚
        new_linear = nn.Linear(
            in_features=new_in_features,
            out_features=linear_layer.out_features,
            bias=linear_layer.bias is not None
        ).to(linear_layer.weight.device)
        
        # æƒé‡ç»§æ‰¿ç­–ç•¥
        with torch.no_grad():
            # å¤åˆ¶åŸå§‹æƒé‡ [out_features, in_features]
            new_linear.weight[:, :old_in_features] = linear_layer.weight.data
            
            # ä¸ºæ–°çš„è¾“å…¥ç‰¹å¾åˆå§‹åŒ–æƒé‡ï¼ˆç»§æ‰¿è‡ªåˆ†è£‚çš„çˆ¶ç‰¹å¾ï¼‰
            for i, split_idx in enumerate(split_indices):
                new_in_idx = old_in_features + i
                # ç»§æ‰¿çˆ¶ç‰¹å¾çš„æƒé‡
                new_linear.weight[:, new_in_idx] = linear_layer.weight.data[:, split_idx]
            
            # å¤åˆ¶bias
            if linear_layer.bias is not None:
                new_linear.bias.data = linear_layer.bias.data
        
        return new_linear
    
    def _find_corresponding_batchnorm(self, model: nn.Module, conv_layer_name: str) -> Optional[str]:
        """æŸ¥æ‰¾Convå±‚å¯¹åº”çš„BatchNormå±‚ - å¢å¼ºç‰ˆæœ¬æ”¯æŒResNetæ¶æ„"""
        
        # é¦–å…ˆå°è¯•ç›´æ¥åŒ¹é…çš„æ¨¡å¼
        direct_patterns = [
            # æ ‡å‡†æ¨¡å¼: conv1 -> bn1
            conv_layer_name.replace('conv', 'bn'),
            # normå˜ä½“: conv1 -> norm1  
            conv_layer_name.replace('conv', 'norm'),
            # åç¼€æ¨¡å¼
            conv_layer_name + '.bn',
            conv_layer_name + '.norm',
        ]
        
        # æ”¶é›†æ‰€æœ‰BatchNormå±‚ç”¨äºè°ƒè¯•
        all_bn_layers = []
        for name, module in model.named_modules():
            if isinstance(module, (nn.BatchNorm2d, nn.BatchNorm1d)):
                all_bn_layers.append(name)
        
        logger.debug(f"Looking for BatchNorm for Conv layer: {conv_layer_name}")
        logger.debug(f"Available BatchNorm layers: {all_bn_layers}")
        
        # 1. æ£€æŸ¥ç›´æ¥æ¨¡å¼åŒ¹é…
        for pattern in direct_patterns:
            if pattern in all_bn_layers:
                logger.info(f"âœ… Found BatchNorm by direct pattern: {conv_layer_name} -> {pattern}")
                return pattern
        
        # 2. è§£æå±‚çº§ç»“æ„è¿›è¡Œæ™ºèƒ½åŒ¹é…
        conv_parts = conv_layer_name.split('.')
        
        for bn_name in all_bn_layers:
            bn_parts = bn_name.split('.')
            
            # ResNetæ¨¡å¼åŒ¹é…
            if self._is_resnet_bn_match(conv_parts, bn_parts):
                logger.info(f"âœ… Found BatchNorm by ResNet pattern: {conv_layer_name} -> {bn_name}")
                return bn_name
            
            # åºåˆ—æ¨¡å¼åŒ¹é… (ç”¨äºshortcutç­‰åºåˆ—)
            if self._is_sequential_bn_match(conv_parts, bn_parts):
                logger.info(f"âœ… Found BatchNorm by sequential pattern: {conv_layer_name} -> {bn_name}")
                return bn_name
        
        # 3. æŒ‰è·ç¦»æŸ¥æ‰¾æœ€è¿‘çš„BatchNorm
        nearest_bn = self._find_nearest_batchnorm(model, conv_layer_name)
        if nearest_bn:
            logger.info(f"âœ… Found BatchNorm by proximity: {conv_layer_name} -> {nearest_bn}")
            return nearest_bn
        
        logger.warning(f"âŒ No corresponding BatchNorm found for {conv_layer_name}")
        logger.warning(f"Available BatchNorm layers: {all_bn_layers}")
        return None
    
    def _is_resnet_bn_match(self, conv_parts: List[str], bn_parts: List[str]) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºResNeté£æ ¼çš„BatchNormåŒ¹é…"""
        if len(conv_parts) != len(bn_parts):
            return False
        
        # æ£€æŸ¥æ‰€æœ‰éƒ¨åˆ†exceptæœ€åä¸€ä¸ªæ˜¯å¦ç›¸åŒ
        if conv_parts[:-1] != bn_parts[:-1]:
            return False
        
        conv_final = conv_parts[-1]
        bn_final = bn_parts[-1]
        
        # æ ‡å‡†åŒ¹é…: conv1 -> bn1, conv2 -> bn2
        if conv_final.replace('conv', 'bn') == bn_final:
            return True
        
        return False
    
    def _is_sequential_bn_match(self, conv_parts: List[str], bn_parts: List[str]) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºSequentialåºåˆ—ä¸­çš„BatchNormåŒ¹é…"""
        # ç”¨äºå¤„ç†åºåˆ—: stem.0 (Conv) -> stem.1 (BN), layer1.0.shortcut.0 (Conv) -> layer1.0.shortcut.1 (BN)
        if len(conv_parts) != len(bn_parts):
            return False
        
        # æ£€æŸ¥å‰é¢çš„è·¯å¾„æ˜¯å¦ç›¸åŒ
        if conv_parts[:-1] != bn_parts[:-1]:
            return False
        
        try:
            conv_idx = int(conv_parts[-1])
            bn_idx = int(bn_parts[-1])
            # BatchNormé€šå¸¸ç´§è·Ÿåœ¨Convåé¢
            if bn_idx == conv_idx + 1:
                return True
        except ValueError:
            # å¤„ç†éæ•°å­—çš„æƒ…å†µï¼Œå¦‚æœæœ€åä¸€éƒ¨åˆ†ç›¸ä¼¼
            conv_final = conv_parts[-1].lower()
            bn_final = bn_parts[-1].lower()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯conv->bnçš„å˜ä½“
            if ('conv' in conv_final and 'bn' in bn_final) or ('conv' in conv_final and 'norm' in bn_final):
                return True
        
        return False
    
    def _find_nearest_batchnorm(self, model: nn.Module, conv_layer_name: str) -> Optional[str]:
        """æŒ‰æ¨¡å—éå†é¡ºåºæŸ¥æ‰¾æœ€è¿‘çš„BatchNormå±‚"""
        modules_list = list(model.named_modules())
        conv_index = None
        
        # æ‰¾åˆ°Convå±‚çš„ä½ç½®
        for i, (name, module) in enumerate(modules_list):
            if name == conv_layer_name:
                conv_index = i
                break
        
        if conv_index is None:
            return None
        
        # åœ¨Convå±‚åé¢æŸ¥æ‰¾æœ€è¿‘çš„BatchNorm
        for i in range(conv_index + 1, min(conv_index + 5, len(modules_list))):
            name, module = modules_list[i]
            if isinstance(module, (nn.BatchNorm2d, nn.BatchNorm1d)):
                return name
        
        return None

    def get_split_summary(self) -> Dict[str, Any]:
        """è·å–åˆ†è£‚æ“ä½œçš„æ€»ç»“"""
        return {
            'total_layers_split': len(self.split_statistics),
            'splits_per_layer': dict(self.split_statistics),
            'total_splits': sum(self.split_statistics.values()),
            'config': self.config
        }


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•å‡½æ•°
def test_neuron_division():
    """æµ‹è¯•ç¥ç»å…ƒåˆ†è£‚åŠŸèƒ½"""
    print("ğŸ§¬ Testing DNM Neuron Division")
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    class TestModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
            self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
            self.fc1 = nn.Linear(64 * 8 * 8, 128)
            self.fc2 = nn.Linear(128, 10)
            
        def forward(self, x):
            x = F.relu(self.conv1(x))
            x = F.max_pool2d(x, 2)
            x = F.relu(self.conv2(x))
            x = F.max_pool2d(x, 2)
            x = x.view(x.size(0), -1)
            x = F.relu(self.fc1(x))
            x = self.fc2(x)
            return x
    
    model = TestModel()
    dnm_division = DNMNeuronDivision()
    
    # æ³¨å†Œhooks
    dnm_division.register_model_hooks(model)
    
    # æ¨¡æ‹Ÿå‰å‘ä¼ æ’­
    dummy_input = torch.randn(16, 3, 32, 32)
    with torch.no_grad():
        _ = model(dummy_input)
    
    # æ‰§è¡Œåˆ†è£‚åˆ†æ
    result = dnm_division.analyze_and_split(model, epoch=15)
    
    print(f"Split result: {result}")
    print(f"Split summary: {dnm_division.get_split_summary()}")
    
    # æ¸…ç†
    dnm_division.remove_model_hooks(model)
    
    return model, result


if __name__ == "__main__":
    test_neuron_division()