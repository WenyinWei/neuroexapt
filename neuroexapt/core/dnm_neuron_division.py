#!/usr/bin/env python3
"""
Dynamic Neural Morphogenesis - ç¥ç»å…ƒåˆ†è£‚æ¨¡å—

åŸºäºä¿¡æ¯ç†µçš„ç¥ç»å…ƒåŠ¨æ€åˆ†è£‚æœºåˆ¶ï¼š
1. å®æ—¶ç›‘æ§æ¯ä¸ªç¥ç»å…ƒçš„ä¿¡æ¯æ‰¿è½½é‡
2. è¯†åˆ«ä¿¡æ¯è¿‡è½½çš„é«˜ç†µç¥ç»å…ƒ
3. æ‰§è¡Œæ™ºèƒ½åˆ†è£‚ï¼Œç»§æ‰¿æƒé‡å¹¶æ·»åŠ é€‚åº”æ€§å˜å¼‚
4. æ”¯æŒCNNå’Œå…¨è¿æ¥å±‚çš„åˆ†è£‚æ“ä½œ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
from typing import Dict, List, Tuple, Optional, Any
from collections import defaultdict
import logging

logger = logging.getLogger(__name__)


class NeuronInformationAnalyzer:
    """ç¥ç»å…ƒä¿¡æ¯åˆ†æå™¨ - è®¡ç®—ä¿¡æ¯ç†µå’Œè´Ÿè½½"""
    
    def __init__(self, bins=32, smoothing_factor=1e-8):
        self.bins = bins
        self.smoothing_factor = smoothing_factor
        self.activation_history = defaultdict(list)
        
    def analyze_activation_entropy(self, activations: torch.Tensor, layer_name: str) -> torch.Tensor:
        """
        åˆ†ææ¿€æ´»çš„ä¿¡æ¯ç†µ
        
        Args:
            activations: ç¥ç»å…ƒæ¿€æ´»å€¼ [batch_size, channels, ...] æˆ– [batch_size, neurons]
            layer_name: å±‚åç§°
            
        Returns:
            æ¯ä¸ªç¥ç»å…ƒ/é€šé“çš„ä¿¡æ¯ç†µ
        """
        if len(activations.shape) == 4:  # Conv2Då±‚
            return self._analyze_conv_entropy(activations)
        elif len(activations.shape) == 2:  # Linearå±‚
            return self._analyze_linear_entropy(activations)
        else:
            logger.warning(f"Unsupported activation shape: {activations.shape}")
            return torch.zeros(activations.shape[1])
    
    def _analyze_conv_entropy(self, activations: torch.Tensor) -> torch.Tensor:
        """åˆ†æå·ç§¯å±‚çš„é€šé“ç†µ"""
        B, C, H, W = activations.shape
        channel_entropies = []
        
        for c in range(C):
            # è·å–è¯¥é€šé“çš„æ‰€æœ‰æ¿€æ´»å€¼
            channel_data = activations[:, c, :, :].reshape(-1)
            
            # è®¡ç®—ä¿¡æ¯ç†µ
            entropy = self._calculate_entropy(channel_data)
            channel_entropies.append(entropy)
        
        return torch.tensor(channel_entropies, device=activations.device)
    
    def _analyze_linear_entropy(self, activations: torch.Tensor) -> torch.Tensor:
        """åˆ†æå…¨è¿æ¥å±‚çš„ç¥ç»å…ƒç†µ"""
        B, N = activations.shape
        neuron_entropies = []
        
        for n in range(N):
            # è·å–è¯¥ç¥ç»å…ƒçš„æ‰€æœ‰æ¿€æ´»å€¼
            neuron_data = activations[:, n]
            
            # è®¡ç®—ä¿¡æ¯ç†µ
            entropy = self._calculate_entropy(neuron_data)
            neuron_entropies.append(entropy)
        
        return torch.tensor(neuron_entropies, device=activations.device)
    
    def _calculate_entropy(self, data: torch.Tensor) -> float:
        """
        è®¡ç®—æ•°æ®çš„ä¿¡æ¯ç†µ
        
        ä¿¡æ¯ç†µå…¬å¼: H(X) = -Î£ p(x) * log2(p(x))
        é«˜ç†µè¡¨ç¤ºä¿¡æ¯åˆ†å¸ƒå‡åŒ€ï¼Œä½ç†µè¡¨ç¤ºä¿¡æ¯é›†ä¸­
        """
        if len(data) == 0:
            return 0.0
        
        # æ•°æ®é¢„å¤„ç†
        data = data.detach().cpu().float()
        
        # å¤„ç†å¸¸æ•°æ•°æ®
        if torch.std(data) < self.smoothing_factor:
            return 0.0
        
        # æ•°æ®å½’ä¸€åŒ–åˆ°[0,1]èŒƒå›´
        data_min, data_max = torch.min(data), torch.max(data)
        if data_max - data_min > self.smoothing_factor:
            normalized_data = (data - data_min) / (data_max - data_min)
        else:
            normalized_data = data
        
        # åˆ›å»ºç›´æ–¹å›¾
        hist = torch.histc(normalized_data, bins=self.bins, min=0.0, max=1.0)
        
        # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
        prob = hist / (hist.sum() + self.smoothing_factor)
        prob = prob[prob > 0]  # åªè€ƒè™‘éé›¶æ¦‚ç‡
        
        # è®¡ç®—ä¿¡æ¯ç†µ
        if len(prob) > 1:
            entropy = -torch.sum(prob * torch.log2(prob + self.smoothing_factor))
            return entropy.item()
        else:
            return 0.0
    
    def calculate_information_load(self, activations: torch.Tensor) -> Dict[str, float]:
        """
        è®¡ç®—ç¥ç»å…ƒçš„ä¿¡æ¯è´Ÿè½½æŒ‡æ ‡
        
        è¿”å›å¤šç»´åº¦çš„ä¿¡æ¯è´Ÿè½½åˆ†æï¼š
        - entropy: ä¿¡æ¯ç†µ
        - variance: æ–¹å·® (ä¿¡æ¯æ•£å¸ƒç¨‹åº¦)
        - sparsity: ç¨€ç–åº¦ (éé›¶æ¿€æ´»æ¯”ä¾‹)
        - dynamic_range: åŠ¨æ€èŒƒå›´ (æœ€å¤§å€¼-æœ€å°å€¼)
        """
        data = activations.detach().cpu().float()
        
        # åŸºç¡€ç»Ÿè®¡
        entropy = self._calculate_entropy(data)
        variance = torch.var(data).item()
        mean_abs = torch.mean(torch.abs(data)).item()
        
        # ç¨€ç–åº¦åˆ†æ
        threshold = mean_abs * 0.1  # 10%å¹³å‡å€¼ä½œä¸ºæ¿€æ´»é˜ˆå€¼
        active_ratio = torch.mean((torch.abs(data) > threshold).float()).item()
        
        # åŠ¨æ€èŒƒå›´
        dynamic_range = (torch.max(data) - torch.min(data)).item()
        
        # ä¿¡æ¯å¯†åº¦ (ç»“åˆç†µå’Œæ–¹å·®)
        information_density = entropy * math.sqrt(variance + self.smoothing_factor)
        
        return {
            'entropy': entropy,
            'variance': variance,
            'sparsity': 1.0 - active_ratio,  # ç¨€ç–åº¦ = 1 - æ¿€æ´»æ¯”ä¾‹
            'dynamic_range': dynamic_range,
            'information_density': information_density,
            'overload_score': self._calculate_overload_score(entropy, variance, active_ratio)
        }
    
    def _calculate_overload_score(self, entropy: float, variance: float, active_ratio: float) -> float:
        """
        è®¡ç®—ç¥ç»å…ƒè¿‡è½½è¯„åˆ†
        
        ç»¼åˆè€ƒè™‘ï¼š
        - é«˜ç†µ (ä¿¡æ¯å¤æ‚)
        - é«˜æ–¹å·® (æ¿€æ´»ä¸ç¨³å®š)  
        - é«˜æ¿€æ´»ç‡ (ç¥ç»å…ƒç¹å¿™)
        """
        # æ ‡å‡†åŒ–å„é¡¹æŒ‡æ ‡
        normalized_entropy = min(entropy / math.log2(self.bins), 1.0)
        normalized_variance = min(variance / 10.0, 1.0)  # å‡è®¾æ–¹å·®ä¸Šé™ä¸º10
        
        # åŠ æƒç»¼åˆè¯„åˆ†
        overload_score = (
            0.5 * normalized_entropy +      # ä¿¡æ¯å¤æ‚åº¦æƒé‡50%
            0.3 * normalized_variance +     # æ¿€æ´»ä¸ç¨³å®šæ€§æƒé‡30%
            0.2 * active_ratio             # ç¥ç»å…ƒç¹å¿™åº¦æƒé‡20%
        )
        
        return min(overload_score, 1.0)


class IntelligentNeuronSplitter:
    """æ™ºèƒ½ç¥ç»å…ƒåˆ†è£‚å™¨"""
    
    def __init__(self, 
                 entropy_threshold: float = 0.7,
                 overload_threshold: float = 0.6,
                 split_probability: float = 0.4,
                 max_splits_per_layer: int = 3,
                 inheritance_noise: float = 0.1):
        
        self.entropy_threshold = entropy_threshold
        self.overload_threshold = overload_threshold
        self.split_probability = split_probability
        self.max_splits_per_layer = max_splits_per_layer
        self.inheritance_noise = inheritance_noise
        
        self.analyzer = NeuronInformationAnalyzer()
        self.split_history = defaultdict(list)
        
    def decide_split_candidates(self, 
                              activations: torch.Tensor, 
                              layer_name: str) -> List[int]:
        """
        å†³å®šéœ€è¦åˆ†è£‚çš„ç¥ç»å…ƒå€™é€‰
        
        Args:
            activations: ç¥ç»å…ƒæ¿€æ´»å€¼
            layer_name: å±‚åç§°
            
        Returns:
            éœ€è¦åˆ†è£‚çš„ç¥ç»å…ƒ/é€šé“ç´¢å¼•åˆ—è¡¨
        """
        split_candidates = []
        
        if len(activations.shape) == 4:  # Convå±‚
            num_channels = activations.shape[1]
            
            for c in range(num_channels):
                channel_data = activations[:, c, :, :]
                info_load = self.analyzer.calculate_information_load(channel_data)
                
                # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†è£‚
                if self._should_split_neuron(info_load, c, layer_name):
                    split_candidates.append(c)
                    
        elif len(activations.shape) == 2:  # Linearå±‚
            num_neurons = activations.shape[1]
            
            for n in range(num_neurons):
                neuron_data = activations[:, n]
                info_load = self.analyzer.calculate_information_load(neuron_data)
                
                # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†è£‚
                if self._should_split_neuron(info_load, n, layer_name):
                    split_candidates.append(n)
        
        # é™åˆ¶åˆ†è£‚æ•°é‡
        if len(split_candidates) > self.max_splits_per_layer:
            # æŒ‰ç…§è¿‡è½½è¯„åˆ†æ’åºï¼Œé€‰æ‹©æœ€éœ€è¦åˆ†è£‚çš„
            candidate_scores = []
            for idx in split_candidates:
                if len(activations.shape) == 4:
                    data = activations[:, idx, :, :]
                else:
                    data = activations[:, idx]
                
                info_load = self.analyzer.calculate_information_load(data)
                candidate_scores.append((idx, info_load['overload_score']))
            
            # æ’åºå¹¶é€‰æ‹©å‰Nä¸ª
            candidate_scores.sort(key=lambda x: x[1], reverse=True)
            split_candidates = [idx for idx, _ in candidate_scores[:self.max_splits_per_layer]]
        
        # è®°å½•åˆ†è£‚å†å²
        if split_candidates:
            self.split_history[layer_name].extend(split_candidates)
            logger.info(f"Layer {layer_name}: Selected {len(split_candidates)} neurons for splitting")
        
        return split_candidates
    
    def _should_split_neuron(self, info_load: Dict[str, float], neuron_idx: int, layer_name: str) -> bool:
        """åˆ¤æ–­ç¥ç»å…ƒæ˜¯å¦åº”è¯¥åˆ†è£‚"""
        
        # æ¡ä»¶1: ä¿¡æ¯ç†µè¶…è¿‡é˜ˆå€¼
        entropy_condition = info_load['entropy'] > self.entropy_threshold
        
        # æ¡ä»¶2: è¿‡è½½è¯„åˆ†è¶…è¿‡é˜ˆå€¼
        overload_condition = info_load['overload_score'] > self.overload_threshold
        
        # æ¡ä»¶3: éšæœºæ¦‚ç‡
        random_condition = torch.rand(1).item() < self.split_probability
        
        # æ¡ä»¶4: é¿å…é‡å¤åˆ†è£‚åŒä¸€ç¥ç»å…ƒ
        recent_splits = self.split_history.get(layer_name, [])
        not_recently_split = neuron_idx not in recent_splits[-10:]  # æœ€è¿‘10æ¬¡åˆ†è£‚ä¸­ä¸åŒ…å«
        
        # ç»¼åˆåˆ¤æ–­
        should_split = (entropy_condition and overload_condition and 
                       random_condition and not_recently_split)
        
        if should_split:
            logger.debug(f"Neuron {neuron_idx} in {layer_name} marked for splitting: "
                        f"entropy={info_load['entropy']:.3f}, "
                        f"overload={info_load['overload_score']:.3f}")
        
        return should_split
    
    def execute_conv_split(self, conv_layer: nn.Conv2d, split_indices: List[int]) -> nn.Conv2d:
        """æ‰§è¡Œå·ç§¯å±‚é€šé“åˆ†è£‚"""
        if not split_indices:
            return conv_layer
        
        # åˆ›å»ºæ–°çš„å·ç§¯å±‚
        new_out_channels = conv_layer.out_channels + len(split_indices)
        new_conv = nn.Conv2d(
            in_channels=conv_layer.in_channels,
            out_channels=new_out_channels,
            kernel_size=conv_layer.kernel_size,
            stride=conv_layer.stride,
            padding=conv_layer.padding,
            dilation=conv_layer.dilation,
            groups=conv_layer.groups,
            bias=conv_layer.bias is not None,
            padding_mode=conv_layer.padding_mode
        ).to(conv_layer.weight.device)
        
        # æƒé‡ç»§æ‰¿å’Œåˆ†è£‚
        with torch.no_grad():
            # å¤åˆ¶åŸå§‹æƒé‡
            new_conv.weight[:conv_layer.out_channels] = conv_layer.weight.data
            if conv_layer.bias is not None:
                new_conv.bias[:conv_layer.out_channels] = conv_layer.bias.data
            
            # ä¸ºåˆ†è£‚çš„é€šé“åˆå§‹åŒ–æƒé‡
            for i, split_idx in enumerate(split_indices):
                new_idx = conv_layer.out_channels + i
                
                # ç»§æ‰¿çˆ¶é€šé“æƒé‡ + è‡ªé€‚åº”å™ªå£°
                parent_weight = conv_layer.weight.data[split_idx]
                noise_scale = self.inheritance_noise * torch.std(parent_weight)
                noise = torch.randn_like(parent_weight) * noise_scale
                
                new_conv.weight[new_idx] = parent_weight + noise
                
                if conv_layer.bias is not None:
                    parent_bias = conv_layer.bias.data[split_idx]
                    bias_noise = torch.randn(1, device=parent_bias.device) * noise_scale
                    new_conv.bias[new_idx] = parent_bias + bias_noise
        
        logger.info(f"Conv layer split: {conv_layer.out_channels} -> {new_out_channels} channels")
        return new_conv
    
    def execute_linear_split(self, linear_layer: nn.Linear, split_indices: List[int]) -> nn.Linear:
        """æ‰§è¡Œå…¨è¿æ¥å±‚ç¥ç»å…ƒåˆ†è£‚"""
        if not split_indices:
            return linear_layer
        
        # åˆ›å»ºæ–°çš„å…¨è¿æ¥å±‚
        new_out_features = linear_layer.out_features + len(split_indices)
        new_linear = nn.Linear(
            in_features=linear_layer.in_features,
            out_features=new_out_features,
            bias=linear_layer.bias is not None
        ).to(linear_layer.weight.device)
        
        # æƒé‡ç»§æ‰¿å’Œåˆ†è£‚
        with torch.no_grad():
            # å¤åˆ¶åŸå§‹æƒé‡
            new_linear.weight[:linear_layer.out_features] = linear_layer.weight.data
            if linear_layer.bias is not None:
                new_linear.bias[:linear_layer.out_features] = linear_layer.bias.data
            
            # ä¸ºåˆ†è£‚çš„ç¥ç»å…ƒåˆå§‹åŒ–æƒé‡
            for i, split_idx in enumerate(split_indices):
                new_idx = linear_layer.out_features + i
                
                # ç»§æ‰¿çˆ¶ç¥ç»å…ƒæƒé‡ + è‡ªé€‚åº”å™ªå£°
                parent_weight = linear_layer.weight.data[split_idx]
                noise_scale = self.inheritance_noise * torch.std(parent_weight)
                noise = torch.randn_like(parent_weight) * noise_scale
                
                new_linear.weight[new_idx] = parent_weight + noise
                
                if linear_layer.bias is not None:
                    parent_bias = linear_layer.bias.data[split_idx]
                    bias_noise = torch.randn(1, device=parent_bias.device) * noise_scale
                    new_linear.bias[new_idx] = parent_bias + bias_noise
        
        logger.info(f"Linear layer split: {linear_layer.out_features} -> {new_out_features} neurons")
        return new_linear


class DNMNeuronDivision:
    """DNMç¥ç»å…ƒåˆ†è£‚ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or self._default_config()
        self.splitter = IntelligentNeuronSplitter(**self.config['splitter'])
        self.activation_hooks = {}
        self.split_statistics = defaultdict(int)
        
    def _default_config(self) -> Dict:
        """é»˜è®¤é…ç½®"""
        return {
            'splitter': {
                'entropy_threshold': 0.7,
                'overload_threshold': 0.6,
                'split_probability': 0.4,
                'max_splits_per_layer': 3,
                'inheritance_noise': 0.1
            },
            'monitoring': {
                'target_layers': ['conv', 'linear'],  # ç›‘æ§çš„å±‚ç±»å‹
                'analysis_frequency': 5,  # æ¯5ä¸ªepochåˆ†æä¸€æ¬¡
                'min_epoch_before_split': 10  # æœ€å°è®­ç»ƒepochåæ‰å¼€å§‹åˆ†è£‚
            }
        }
    
    def register_model_hooks(self, model: nn.Module) -> None:
        """ä¸ºæ¨¡å‹æ³¨å†Œæ¿€æ´»ç›‘æ§hooks"""
        self.activation_cache = {}
        hooks = []
        
        def create_hook(name):
            def hook_fn(module, input, output):
                if isinstance(output, torch.Tensor):
                    self.activation_cache[name] = output.detach().clone()
            return hook_fn
        
        for name, module in model.named_modules():
            if self._should_monitor_layer(module, name):
                hook = module.register_forward_hook(create_hook(name))
                hooks.append(hook)
                logger.debug(f"Registered hook for layer: {name}")
        
        self.activation_hooks[id(model)] = hooks
    
    def remove_model_hooks(self, model: nn.Module) -> None:
        """ç§»é™¤æ¨¡å‹çš„hooks"""
        model_id = id(model)
        if model_id in self.activation_hooks:
            for hook in self.activation_hooks[model_id]:
                hook.remove()
            del self.activation_hooks[model_id]
            logger.debug("Removed all activation hooks")
    
    def _should_monitor_layer(self, module: nn.Module, name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç›‘æ§è¯¥å±‚"""
        target_types = {
            'conv': nn.Conv2d,
            'linear': nn.Linear
        }
        
        for layer_type in self.config['monitoring']['target_layers']:
            if isinstance(module, target_types.get(layer_type)):
                return True
        return False
    
    def analyze_and_split(self, model: nn.Module, epoch: int) -> Dict[str, Any]:
        """åˆ†ææ¨¡å‹å¹¶æ‰§è¡Œç¥ç»å…ƒåˆ†è£‚"""
        
        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³åˆ†è£‚æ¡ä»¶
        if epoch < self.config['monitoring']['min_epoch_before_split']:
            return {'splits_executed': 0, 'message': 'Too early for splitting'}
        
        if epoch % self.config['monitoring']['analysis_frequency'] != 0:
            return {'splits_executed': 0, 'message': 'Not analysis epoch'}
        
        split_decisions = {}
        
        # åˆ†æå„å±‚çš„æ¿€æ´»å¹¶å†³å®šåˆ†è£‚ç­–ç•¥
        for layer_name, activations in self.activation_cache.items():
            try:
                split_candidates = self.splitter.decide_split_candidates(
                    activations, layer_name
                )
                
                if split_candidates:
                    split_decisions[layer_name] = split_candidates
                    logger.info(f"Layer {layer_name}: {len(split_candidates)} neurons marked for splitting")
                    
            except Exception as e:
                logger.warning(f"Failed to analyze layer {layer_name}: {e}")
                continue
        
        # æ‰§è¡Œåˆ†è£‚æ“ä½œ
        splits_executed = self._execute_splits(model, split_decisions)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        for layer_name, candidates in split_decisions.items():
            self.split_statistics[layer_name] += len(candidates)
        
        # æ¸…ç†æ¿€æ´»ç¼“å­˜
        self.activation_cache.clear()
        
        result = {
            'splits_executed': splits_executed,
            'split_decisions': split_decisions,
            'total_splits_per_layer': dict(self.split_statistics),
            'message': f'Successfully executed {splits_executed} splits'
        }
        
        logger.info(f"DNM Neuron Division completed: {splits_executed} splits executed")
        return result
    
    def _execute_splits(self, model: nn.Module, split_decisions: Dict[str, List[int]]) -> int:
        """æ‰§è¡Œå…·ä½“çš„åˆ†è£‚æ“ä½œ"""
        total_splits = 0
        
        for layer_name, split_indices in split_decisions.items():
            try:
                # è·å–ç›®æ ‡å±‚
                target_module = self._get_module_by_name(model, layer_name)
                if target_module is None:
                    logger.warning(f"Could not find module: {layer_name}")
                    continue
                
                # æ ¹æ®å±‚ç±»å‹æ‰§è¡Œåˆ†è£‚
                if isinstance(target_module, nn.Conv2d):
                    new_module = self.splitter.execute_conv_split(target_module, split_indices)
                elif isinstance(target_module, nn.Linear):
                    new_module = self.splitter.execute_linear_split(target_module, split_indices)
                else:
                    logger.warning(f"Unsupported layer type for splitting: {type(target_module)}")
                    continue
                
                # æ›¿æ¢æ¨¡å‹ä¸­çš„å±‚
                self._replace_module_in_model(model, layer_name, new_module)
                total_splits += len(split_indices)
                
                logger.info(f"Successfully split layer {layer_name}: {len(split_indices)} new neurons/channels")
                
            except Exception as e:
                logger.error(f"Failed to split layer {layer_name}: {e}")
                continue
        
        return total_splits
    
    def _get_module_by_name(self, model: nn.Module, module_name: str) -> Optional[nn.Module]:
        """æ ¹æ®åç§°è·å–æ¨¡å—"""
        try:
            return model.get_submodule(module_name)
        except AttributeError:
            # å…¼å®¹è€ç‰ˆæœ¬PyTorch
            parts = module_name.split('.')
            module = model
            for part in parts:
                module = getattr(module, part)
            return module
        except Exception:
            return None
    
    def _replace_module_in_model(self, model: nn.Module, module_name: str, new_module: nn.Module) -> None:
        """åœ¨æ¨¡å‹ä¸­æ›¿æ¢æ¨¡å—"""
        parts = module_name.split('.')
        parent = model
        
        # æ‰¾åˆ°çˆ¶æ¨¡å—
        for part in parts[:-1]:
            parent = getattr(parent, part)
        
        # æ›¿æ¢ç›®æ ‡æ¨¡å—
        setattr(parent, parts[-1], new_module)
    
    def get_split_summary(self) -> Dict[str, Any]:
        """è·å–åˆ†è£‚æ“ä½œçš„æ€»ç»“"""
        return {
            'total_layers_split': len(self.split_statistics),
            'splits_per_layer': dict(self.split_statistics),
            'total_splits': sum(self.split_statistics.values()),
            'config': self.config
        }


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•å‡½æ•°
def test_neuron_division():
    """æµ‹è¯•ç¥ç»å…ƒåˆ†è£‚åŠŸèƒ½"""
    print("ğŸ§¬ Testing DNM Neuron Division")
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    class TestModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
            self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
            self.fc1 = nn.Linear(64 * 8 * 8, 128)
            self.fc2 = nn.Linear(128, 10)
            
        def forward(self, x):
            x = F.relu(self.conv1(x))
            x = F.max_pool2d(x, 2)
            x = F.relu(self.conv2(x))
            x = F.max_pool2d(x, 2)
            x = x.view(x.size(0), -1)
            x = F.relu(self.fc1(x))
            x = self.fc2(x)
            return x
    
    model = TestModel()
    dnm_division = DNMNeuronDivision()
    
    # æ³¨å†Œhooks
    dnm_division.register_model_hooks(model)
    
    # æ¨¡æ‹Ÿå‰å‘ä¼ æ’­
    dummy_input = torch.randn(16, 3, 32, 32)
    with torch.no_grad():
        _ = model(dummy_input)
    
    # æ‰§è¡Œåˆ†è£‚åˆ†æ
    result = dnm_division.analyze_and_split(model, epoch=15)
    
    print(f"Split result: {result}")
    print(f"Split summary: {dnm_division.get_split_summary()}")
    
    # æ¸…ç†
    dnm_division.remove_model_hooks(model)
    
    return model, result


if __name__ == "__main__":
    test_neuron_division()