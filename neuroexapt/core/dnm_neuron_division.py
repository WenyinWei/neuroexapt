#!/usr/bin/env python3
"""
"""
\defgroup group_dnm_neuron_division Dnm Neuron Division
\ingroup core
Dnm Neuron Division module for NeuroExapt framework.
"""


DNM Neuron Division Module - ç¥ç»å…ƒåˆ†è£‚ä¸“ç”¨æ¨¡å—

ğŸ§¬ æ ¸å¿ƒåŠŸèƒ½ï¼š
1. æ™ºèƒ½è¯†åˆ«åˆ†è£‚æ—¶æœº
2. æ‰§è¡Œä¸åŒç±»å‹çš„ç¥ç»å…ƒåˆ†è£‚
3. ä¿æŒç½‘ç»œåŠŸèƒ½æ€§
4. ä¼˜åŒ–å‚æ•°åˆå§‹åŒ–

ğŸ¯ ç›®æ ‡ï¼šå®ç°çœŸæ­£æœ‰æ•ˆçš„ç¥ç»å…ƒå¢é•¿å’Œç½‘ç»œæ‰©å±•
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional, Any
import copy
import math
from collections import defaultdict

logger = logging.getLogger(__name__)

class NeuronDivisionStrategies:
    """ç¥ç»å…ƒåˆ†è£‚ç­–ç•¥é›†åˆ"""
    
    @staticmethod
    def symmetric_division(original_weights: torch.Tensor, division_ratio: float = 0.5) -> Tuple[torch.Tensor, torch.Tensor]:
        """å¯¹ç§°åˆ†è£‚ï¼šå°†ä¸€ä¸ªç¥ç»å…ƒåˆ†è£‚ä¸ºä¸¤ä¸ªç›¸ä¼¼çš„ç¥ç»å…ƒ"""
        device = original_weights.device
        dtype = original_weights.dtype
        noise_scale = torch.std(original_weights) * 0.1
        
        # ç¬¬ä¸€ä¸ªç¥ç»å…ƒï¼šä¿æŒå¤§éƒ¨åˆ†åŸå§‹æƒé‡
        neuron1 = original_weights + torch.normal(0, noise_scale, size=original_weights.shape, device=device, dtype=dtype)
        
        # ç¬¬äºŒä¸ªç¥ç»å…ƒï¼šç¨å¾®ä¸åŒçš„æƒé‡
        neuron2 = original_weights * division_ratio + torch.normal(0, noise_scale, size=original_weights.shape, device=device, dtype=dtype)
        
        return neuron1, neuron2
    
    @staticmethod
    def asymmetric_division(original_weights: torch.Tensor, specialization_factor: float = 0.3) -> Tuple[torch.Tensor, torch.Tensor]:
        """éå¯¹ç§°åˆ†è£‚ï¼šåˆ›å»ºä¸“é—¨åŒ–çš„ç¥ç»å…ƒ"""
        std_dev = torch.std(original_weights)
        
        # ä¸»ç¥ç»å…ƒï¼šä¿æŒå¤§éƒ¨åˆ†åŠŸèƒ½
        main_neuron = original_weights * (1.0 + specialization_factor)
        
        # ä¸“é—¨åŒ–ç¥ç»å…ƒï¼šå…³æ³¨ç‰¹å®šæ¨¡å¼
        specialized_weights = torch.zeros_like(original_weights)
        # åªä¿ç•™æœ€é‡è¦çš„è¿æ¥
        threshold = torch.quantile(torch.abs(original_weights), 0.7)
        mask = torch.abs(original_weights) > threshold
        specialized_weights[mask] = original_weights[mask] * (1.0 + specialization_factor)
        
        return main_neuron, specialized_weights
    
    @staticmethod
    def functional_division(original_weights: torch.Tensor, activation_pattern: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """åŠŸèƒ½åˆ†è£‚ï¼šåŸºäºæ¿€æ´»æ¨¡å¼è¿›è¡Œåˆ†è£‚"""
        if activation_pattern is not None:
            # åŸºäºæ¿€æ´»æ¨¡å¼åˆ†å‰²æƒé‡
            high_activation_mask = activation_pattern > torch.median(activation_pattern)
            
            # é«˜æ¿€æ´»ç¥ç»å…ƒ
            high_act_neuron = original_weights.clone()
            high_act_neuron[~high_activation_mask] *= 0.3
            
            # ä½æ¿€æ´»ç¥ç»å…ƒ  
            low_act_neuron = original_weights.clone()
            low_act_neuron[high_activation_mask] *= 0.3
            
            return high_act_neuron, low_act_neuron
        else:
            # éšæœºåŠŸèƒ½åˆ†å‰²
            mask = torch.rand_like(original_weights) > 0.5
            neuron1 = original_weights.clone()
            neuron2 = original_weights.clone()
            
            neuron1[~mask] *= 0.2
            neuron2[mask] *= 0.2
            
            return neuron1, neuron2

class AdaptiveNeuronDivision:
    """è‡ªé€‚åº”ç¥ç»å…ƒåˆ†è£‚å™¨"""
    
    def __init__(self):
        self.division_history = defaultdict(list)
        self.performance_tracker = {}
        
    def execute_division(self, model: nn.Module, layer_name: str, 
                        division_strategy: str = 'adaptive',
                        target_expansion: float = 0.2) -> Tuple[nn.Module, int]:
        """æ‰§è¡Œç¥ç»å…ƒåˆ†è£‚"""
        
        # è·å–åŸå§‹è®¾å¤‡
        original_device = next(model.parameters()).device
        
        # æ·±æ‹·è´æ¨¡å‹å¹¶ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        new_model = copy.deepcopy(model).to(original_device)
        
        # æ‰¾åˆ°ç›®æ ‡å±‚
        target_layer = self._find_layer(new_model, layer_name)
        if target_layer is None:
            logger.warning(f"æœªæ‰¾åˆ°å±‚: {layer_name}")
            return model, 0
            
        # æ ¹æ®å±‚ç±»å‹æ‰§è¡Œåˆ†è£‚
        if isinstance(target_layer, nn.Linear):
            return self._divide_linear_layer(new_model, layer_name, target_layer, 
                                           division_strategy, target_expansion)
        elif isinstance(target_layer, nn.Conv2d):
            return self._divide_conv_layer(new_model, layer_name, target_layer,
                                         division_strategy, target_expansion)
        else:
            logger.warning(f"ä¸æ”¯æŒçš„å±‚ç±»å‹: {type(target_layer)}")
            return model, 0
    
    def _find_layer(self, model: nn.Module, layer_name: str) -> Optional[nn.Module]:
        """æŸ¥æ‰¾æŒ‡å®šå±‚"""
        for name, module in model.named_modules():
            if name == layer_name:
                return module
        return None
    
    def _divide_linear_layer(self, model: nn.Module, layer_name: str, layer: nn.Linear,
                           division_strategy: str, target_expansion: float) -> Tuple[nn.Module, int]:
        """åˆ†è£‚å…¨è¿æ¥å±‚"""
        
        original_out_features = layer.out_features
        expansion_size = max(1, int(original_out_features * target_expansion))
        new_out_features = original_out_features + expansion_size
        
        # è·å–åŸå§‹è®¾å¤‡å’Œæ•°æ®ç±»å‹
        device = layer.weight.device
        dtype = layer.weight.dtype
        
        # åˆ›å»ºæ–°çš„æƒé‡å’Œåç½®å¼ é‡ï¼ˆç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼‰
        new_weight = torch.zeros(new_out_features, layer.in_features, dtype=dtype, device=device)
        new_bias = torch.zeros(new_out_features, dtype=dtype, device=device) if layer.bias is not None else None
        
        # å¤åˆ¶åŸå§‹æƒé‡
        new_weight[:original_out_features] = layer.weight.data
        if new_bias is not None:
            new_bias[:original_out_features] = layer.bias.data
            
        # é€‰æ‹©åˆ†è£‚ç­–ç•¥
        strategy_func = self._get_division_strategy(division_strategy)
        
        # æ‰§è¡Œç¥ç»å…ƒåˆ†è£‚
        neurons_to_divide = self._select_neurons_for_division(layer, expansion_size)
        
        for i, neuron_idx in enumerate(neurons_to_divide):
            if i >= expansion_size:
                break
                
            original_weights = layer.weight.data[neuron_idx]
            original_bias = layer.bias.data[neuron_idx] if layer.bias is not None else torch.tensor(0.0, device=device)
            
            # æ‰§è¡Œåˆ†è£‚
            if division_strategy == 'symmetric':
                new_weights, _ = strategy_func(original_weights)
                new_weight[original_out_features + i] = new_weights
            elif division_strategy == 'asymmetric':
                _, specialized_weights = strategy_func(original_weights)
                new_weight[original_out_features + i] = specialized_weights
            else:  # adaptive
                new_weights, _ = self._adaptive_division(original_weights, layer_name, neuron_idx)
                new_weight[original_out_features + i] = new_weights
                
            # è®¾ç½®åç½®
            if new_bias is not None:
                new_bias[original_out_features + i] = original_bias * 0.9
        
        # æ›´æ–°å±‚å‚æ•°ï¼ˆç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Šï¼‰
        layer.out_features = new_out_features
        # ç¡®ä¿å‚æ•°åœ¨æ­£ç¡®è®¾å¤‡ä¸Šå¹¶ä¸”requires_grad=True
        new_weight_param = nn.Parameter(new_weight.to(device).detach().requires_grad_(True))
        layer.weight = new_weight_param
        if layer.bias is not None:
            new_bias_param = nn.Parameter(new_bias.to(device).detach().requires_grad_(True))
            layer.bias = new_bias_param
            
        # æ›´æ–°ä¸‹ä¸€å±‚çš„è¾“å…¥ç»´åº¦ï¼ˆå¦‚æœå­˜åœ¨ä¸”ä¸æ˜¯æœ€åä¸€å±‚ï¼‰
        if not self._is_final_layer(model, layer_name):
            self._update_next_layer_input(model, layer_name, expansion_size)
        
        # è®°å½•åˆ†è£‚å†å²
        self.division_history[layer_name].append({
            'expansion_size': expansion_size,
            'strategy': division_strategy,
            'neurons_divided': neurons_to_divide
        })
        
        logger.info(f"Linearå±‚åˆ†è£‚å®Œæˆ: {layer_name}, æ–°å¢ç¥ç»å…ƒ: {expansion_size}")
        return model, expansion_size * (layer.in_features + 1)
    
    def _divide_conv_layer(self, model: nn.Module, layer_name: str, layer: nn.Conv2d,
                          division_strategy: str, target_expansion: float) -> Tuple[nn.Module, int]:
        """åˆ†è£‚å·ç§¯å±‚"""
        
        original_out_channels = layer.out_channels
        expansion_size = max(1, int(original_out_channels * target_expansion))
        new_out_channels = original_out_channels + expansion_size
        
        # è·å–åŸå§‹è®¾å¤‡
        device = layer.weight.device
        
        # åˆ›å»ºæ–°çš„å·ç§¯å±‚
        new_conv = nn.Conv2d(
            layer.in_channels,
            new_out_channels,
            layer.kernel_size,
            layer.stride,
            layer.padding,
            layer.dilation,
            layer.groups,
            layer.bias is not None,
            layer.padding_mode
        ).to(device)  # ç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        
        # å¤åˆ¶åŸå§‹æƒé‡
        with torch.no_grad():
            new_conv.weight.data[:original_out_channels] = layer.weight.data
            if layer.bias is not None:
                new_conv.bias.data[:original_out_channels] = layer.bias.data
        
        # æ‰§è¡Œé€šé“åˆ†è£‚
        channels_to_divide = self._select_channels_for_division(layer, expansion_size)
        strategy_func = self._get_division_strategy(division_strategy)
        
        for i, channel_idx in enumerate(channels_to_divide):
            if i >= expansion_size:
                break
                
            original_kernel = layer.weight.data[channel_idx]
            original_bias = layer.bias.data[channel_idx] if layer.bias is not None else torch.tensor(0.0, device=device)
            
            # åˆ†è£‚å·ç§¯æ ¸
            if division_strategy == 'symmetric':
                new_kernel, _ = self._divide_conv_kernel(original_kernel, 'symmetric')
            elif division_strategy == 'asymmetric':
                _, new_kernel = self._divide_conv_kernel(original_kernel, 'asymmetric')
            else:  # adaptive
                new_kernel, _ = self._adaptive_conv_division(original_kernel, layer_name, channel_idx)
                
            new_conv.weight.data[original_out_channels + i] = new_kernel
            if new_conv.bias is not None:
                new_conv.bias.data[original_out_channels + i] = original_bias * 0.9
        
        # æ›¿æ¢å±‚
        self._replace_layer(model, layer_name, new_conv)
        
        # æ›´æ–°ä¸‹ä¸€å±‚çš„è¾“å…¥é€šé“æ•°ï¼ˆå¦‚æœä¸æ˜¯æœ€åä¸€å±‚ï¼‰
        if not self._is_final_layer(model, layer_name):
            self._update_next_conv_layer_input(model, layer_name, expansion_size)
        
        # è®°å½•åˆ†è£‚å†å²
        self.division_history[layer_name].append({
            'expansion_size': expansion_size,
            'strategy': division_strategy,
            'channels_divided': channels_to_divide
        })
        
        param_increase = expansion_size * layer.in_channels * layer.kernel_size[0] * layer.kernel_size[1]
        logger.info(f"Convå±‚åˆ†è£‚å®Œæˆ: {layer_name}, æ–°å¢é€šé“: {expansion_size}")
        return model, param_increase
    
    def _get_division_strategy(self, strategy_name: str):
        """è·å–åˆ†è£‚ç­–ç•¥å‡½æ•°"""
        strategies = {
            'symmetric': NeuronDivisionStrategies.symmetric_division,
            'asymmetric': NeuronDivisionStrategies.asymmetric_division,
            'functional': NeuronDivisionStrategies.functional_division
        }
        return strategies.get(strategy_name, NeuronDivisionStrategies.symmetric_division)
    
    def _select_neurons_for_division(self, layer: nn.Linear, num_divisions: int) -> List[int]:
        """é€‰æ‹©è¦åˆ†è£‚çš„ç¥ç»å…ƒ"""
        weights = layer.weight.data
        
        # è®¡ç®—æ¯ä¸ªç¥ç»å…ƒçš„é‡è¦æ€§åˆ†æ•°
        importance_scores = []
        for i in range(weights.size(0)):
            neuron_weights = weights[i]
            
            # ç»¼åˆå¤šä¸ªæŒ‡æ ‡
            weight_variance = torch.var(neuron_weights).item()
            weight_norm = torch.norm(neuron_weights).item()
            weight_sparsity = (torch.abs(neuron_weights) < 0.01).float().mean().item()
            
            # é«˜æ–¹å·®ã€é€‚ä¸­èŒƒæ•°ã€ä½ç¨€ç–æ€§çš„ç¥ç»å…ƒé€‚åˆåˆ†è£‚
            score = weight_variance * (1.0 - weight_sparsity) * min(weight_norm, 1.0)
            importance_scores.append((i, score))
        
        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„ç¥ç»å…ƒ
        importance_scores.sort(key=lambda x: x[1], reverse=True)
        return [idx for idx, _ in importance_scores[:num_divisions]]
    
    def _select_channels_for_division(self, layer: nn.Conv2d, num_divisions: int) -> List[int]:
        """é€‰æ‹©è¦åˆ†è£‚çš„å·ç§¯é€šé“"""
        weights = layer.weight.data
        
        importance_scores = []
        for i in range(weights.size(0)):
            channel_weights = weights[i]
            
            # è®¡ç®—é€šé“é‡è¦æ€§
            weight_energy = torch.sum(channel_weights ** 2).item()
            weight_diversity = torch.std(channel_weights).item()
            
            score = weight_energy * weight_diversity
            importance_scores.append((i, score))
        
        importance_scores.sort(key=lambda x: x[1], reverse=True)
        return [idx for idx, _ in importance_scores[:num_divisions]]
    
    def _adaptive_division(self, original_weights: torch.Tensor, layer_name: str, neuron_idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """è‡ªé€‚åº”åˆ†è£‚ç­–ç•¥"""
        # æ ¹æ®å†å²è¡¨ç°é€‰æ‹©æœ€ä½³ç­–ç•¥
        history = self.division_history.get(layer_name, [])
        
        if len(history) < 3:
            # åˆæœŸä½¿ç”¨å¯¹ç§°åˆ†è£‚
            return NeuronDivisionStrategies.symmetric_division(original_weights)
        else:
            # åŸºäºå†å²è¡¨ç°é€‰æ‹©ç­–ç•¥
            # è¿™é‡Œç®€åŒ–ä¸ºéšæœºé€‰æ‹©ï¼Œå®é™…åº”è¯¥åŸºäºæ€§èƒ½åé¦ˆ
            strategy = np.random.choice(['symmetric', 'asymmetric', 'functional'])
            func = self._get_division_strategy(strategy)
            return func(original_weights)
    
    def _adaptive_conv_division(self, original_kernel: torch.Tensor, layer_name: str, channel_idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """è‡ªé€‚åº”å·ç§¯åˆ†è£‚"""
        return self._divide_conv_kernel(original_kernel, 'symmetric')
    
    def _divide_conv_kernel(self, kernel: torch.Tensor, strategy: str) -> Tuple[torch.Tensor, torch.Tensor]:
        """åˆ†è£‚å·ç§¯æ ¸"""
        device = kernel.device
        dtype = kernel.dtype
        
        if strategy == 'symmetric':
            noise = torch.normal(0, torch.std(kernel) * 0.1, size=kernel.shape, device=device, dtype=dtype)
            kernel1 = kernel + noise
            kernel2 = kernel - noise
            return kernel1, kernel2
        elif strategy == 'asymmetric':
            # åˆ›å»ºä¸“é—¨åŒ–çš„æ ¸
            kernel1 = kernel * 1.1
            kernel2 = kernel * 0.5
            # åœ¨kernel2ä¸­å¢å¼ºè¾¹ç¼˜æ£€æµ‹
            if kernel.size(-1) >= 3 and kernel.size(-2) >= 3:
                edge_kernel = torch.tensor([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]], 
                                         dtype=dtype, device=device)
                kernel2[:, :, :3, :3] += edge_kernel.unsqueeze(0).unsqueeze(0) * 0.1
            return kernel1, kernel2
        else:
            flattened = kernel.view(-1)
            result1, result2 = NeuronDivisionStrategies.symmetric_division(flattened)
            return result1.view(kernel.shape), result2.view(kernel.shape)
    
    def _update_next_layer_input(self, model: nn.Module, current_layer_name: str, expansion_size: int):
        """æ›´æ–°ä¸‹ä¸€å±‚çš„è¾“å…¥ç»´åº¦"""
        layer_names = [name for name, _ in model.named_modules()]
        
        try:
            current_idx = layer_names.index(current_layer_name)
            if current_idx + 1 < len(layer_names):
                next_layer_name = layer_names[current_idx + 1]
                next_layer = self._find_layer(model, next_layer_name)
                
                if isinstance(next_layer, nn.Linear):
                    old_in_features = next_layer.in_features
                    new_in_features = old_in_features + expansion_size
                    
                    # è·å–è®¾å¤‡ä¿¡æ¯
                    device = next_layer.weight.device
                    dtype = next_layer.weight.dtype
                    
                    # åˆ›å»ºæ–°çš„æƒé‡çŸ©é˜µ
                    new_weight = torch.zeros(next_layer.out_features, new_in_features, dtype=dtype, device=device)
                    new_weight[:, :old_in_features] = next_layer.weight.data
                    
                    # åˆå§‹åŒ–æ–°çš„è¿æ¥æƒé‡
                    with torch.no_grad():
                        nn.init.normal_(new_weight[:, old_in_features:], mean=0, std=0.01)
                    
                    next_layer.in_features = new_in_features
                    next_layer.weight = nn.Parameter(new_weight)
                    
                    logger.info(f"æ›´æ–°ä¸‹ä¸€å±‚è¾“å…¥ç»´åº¦: {next_layer_name}, {old_in_features} -> {new_in_features}")
                    
        except (ValueError, IndexError):
            logger.warning(f"æ— æ³•æ‰¾åˆ°å±‚ {current_layer_name} çš„ä¸‹ä¸€å±‚")
    
    def _update_next_conv_layer_input(self, model: nn.Module, current_layer_name: str, expansion_size: int):
        """æ›´æ–°ä¸‹ä¸€ä¸ªå·ç§¯å±‚çš„è¾“å…¥é€šé“æ•°"""
        # å¯»æ‰¾ä¸‹ä¸€ä¸ªçº¿æ€§å±‚æˆ–å·ç§¯å±‚
        found_current = False
        
        for name, module in model.named_modules():
            if found_current and isinstance(module, (nn.Linear, nn.Conv2d)):
                if isinstance(module, nn.Conv2d):
                    # æ›´æ–°å·ç§¯å±‚è¾“å…¥é€šé“
                    old_in_channels = module.in_channels
                    new_in_channels = old_in_channels + expansion_size
                    
                    # è·å–è®¾å¤‡ä¿¡æ¯
                    device = module.weight.device
                    
                    new_conv = nn.Conv2d(
                        new_in_channels,
                        module.out_channels,
                        module.kernel_size,
                        module.stride,
                        module.padding,
                        module.dilation,
                        module.groups,
                        module.bias is not None,
                        module.padding_mode
                    ).to(device)  # ç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                    
                    # å¤åˆ¶æƒé‡å¹¶æ‰©å±•
                    with torch.no_grad():
                        new_conv.weight.data[:, :old_in_channels] = module.weight.data
                        if module.bias is not None:
                            new_conv.bias.data = module.bias.data
                            
                        # åˆå§‹åŒ–æ–°çš„è¾“å…¥é€šé“
                        nn.init.kaiming_normal_(new_conv.weight.data[:, old_in_channels:])
                    
                    self._replace_layer(model, name, new_conv)
                    logger.info(f"æ›´æ–°Convå±‚è¾“å…¥é€šé“: {name}, {old_in_channels} -> {new_in_channels}")
                break
                
            if name == current_layer_name:
                found_current = True
    
    def _is_final_layer(self, model: nn.Module, layer_name: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ€åä¸€å±‚"""
        layer_names = [name for name, module in model.named_modules() 
                      if isinstance(module, (nn.Linear, nn.Conv2d)) and name != '']
        
        if not layer_names:
            return True
            
        # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ˜¯åˆ†ç±»å™¨çš„è¾“å‡ºå±‚ï¼Œåˆ™è®¤ä¸ºæ˜¯æœ€åä¸€å±‚
        if 'classifier' in layer_name:
            # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ†ç±»å™¨ä¸­çš„æœ€åä¸€ä¸ªLinearå±‚
            parts = layer_name.split('.')
            if len(parts) >= 2:
                try:
                    layer_idx = int(parts[-1])
                    # å¯¹äºæˆ‘ä»¬çš„åˆ†ç±»å™¨ç»“æ„ï¼Œç¬¬6å±‚ï¼ˆç´¢å¼•6ï¼‰æ˜¯æœ€åçš„Linearå±‚
                    if layer_idx == 6:
                        return True
                except ValueError:
                    pass
            
        # æ‰¾åˆ°å½“å‰å±‚åœ¨åˆ—è¡¨ä¸­çš„ä½ç½®
        try:
            current_idx = layer_names.index(layer_name)
            return current_idx == len(layer_names) - 1
        except ValueError:
            return True  # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå‡è®¾æ˜¯æœ€åä¸€å±‚
    
    def _replace_layer(self, model: nn.Module, layer_name: str, new_layer: nn.Module):
        """æ›¿æ¢æ¨¡å‹ä¸­çš„å±‚"""
        parts = layer_name.split('.')
        
        if len(parts) == 1:
            setattr(model, layer_name, new_layer)
        else:
            parent = model
            for part in parts[:-1]:
                parent = getattr(parent, part)
            setattr(parent, parts[-1], new_layer)
    
    def get_division_statistics(self) -> Dict[str, Any]:
        """è·å–åˆ†è£‚ç»Ÿè®¡ä¿¡æ¯"""
        total_divisions = sum(len(history) for history in self.division_history.values())
        
        strategy_counts = defaultdict(int)
        total_expansions = 0
        
        for layer_name, history in self.division_history.items():
            for event in history:
                strategy_counts[event['strategy']] += 1
                total_expansions += event['expansion_size']
        
        return {
            'total_division_events': total_divisions,
            'total_neurons_added': total_expansions,
            'strategy_usage': dict(strategy_counts),
            'layers_modified': list(self.division_history.keys())
        }

