"""
Monte Carloä¸ç¡®å®šæ€§ä¼°è®¡å™¨
åŸºäºMonte Carlo Dropoutçš„ç®€å•æœ‰æ•ˆä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•

æ ¸å¿ƒæ€è·¯ï¼š
1. å¯ç”¨Dropoutåœ¨æ¨ç†æ—¶ä¿æŒæ¿€æ´»
2. å¤šæ¬¡å‰å‘ä¼ æ’­è·å¾—é¢„æµ‹åˆ†å¸ƒ
3. è®¡ç®—é¢„æµ‹æ–¹å·®ä½œä¸ºä¸ç¡®å®šæ€§åº¦é‡
4. æ¯”å˜åˆ†è´å¶æ–¯æ–¹æ³•æ›´ç›´æ¥ã€æ›´ç¨³å®š
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional
import logging
from collections import defaultdict
from dataclasses import dataclass
import weakref

logger = logging.getLogger(__name__)


@dataclass
class MCUncertaintyConfig:
    """Monte Carloä¸ç¡®å®šæ€§ä¼°è®¡é…ç½®"""
    n_samples: int = 50                    # Monte Carloé‡‡æ ·æ¬¡æ•°
    dropout_rate: float = 0.1              # Dropoutæ¦‚ç‡
    max_batches: int = 5                   # æ¯æ¬¡ä¼°è®¡ä½¿ç”¨çš„æœ€å¤§batchæ•°
    uncertainty_threshold: float = 1e-6    # æœ€å°ä¸ç¡®å®šæ€§é˜ˆå€¼
    use_wrapper: bool = True               # æ˜¯å¦ä½¿ç”¨åŒ…è£…å™¨è€ŒéåŠ¨æ€å±æ€§


class DropoutWrapper(nn.Module):
    """
    DropoutåŒ…è£…å™¨ - å®‰å…¨çš„æ›¿ä»£åŠ¨æ€å±æ€§æ·»åŠ æ–¹æ¡ˆ
    
    é¿å…ç›´æ¥ä¿®æ”¹åŸå§‹æ¨¡å—ç»“æ„ï¼Œæä¾›æ›´å¥½çš„éš”ç¦»æ€§
    """
    
    def __init__(self, module: nn.Module, dropout_rate: float = 0.1):
        super().__init__()
        self.module = module
        self.dropout = nn.Dropout(dropout_rate)
        
    def forward(self, x):
        return self.dropout(self.module(x))


class MonteCarloUncertaintyEstimator:
    """
    åŸºäºMonte Carlo Dropoutçš„ä¸ç¡®å®šæ€§ä¼°è®¡å™¨
    
    ä¼˜åŠ¿ï¼š
    - ç®€å•ç›´æ¥ï¼Œä¸éœ€è¦ä¿®æ”¹ç½‘ç»œæ¶æ„
    - è®¡ç®—ç¨³å®šï¼Œä¸ä¼šå‡ºç°NaNæˆ–0å€¼
    - ç†è®ºåŸºç¡€æ‰å®ï¼ˆGal & Ghahramani, 2016ï¼‰
    - ä½¿ç”¨åŒ…è£…å™¨é¿å…åŠ¨æ€å±æ€§ä¿®æ”¹
    """
    
    def __init__(self, config: MCUncertaintyConfig = None):
        """
        åˆå§‹åŒ–Monte Carloä¸ç¡®å®šæ€§ä¼°è®¡å™¨
        
        Args:
            config: é…ç½®å¯¹è±¡ï¼Œå¦‚æœNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or MCUncertaintyConfig()
        
        # ç”¨äºè·Ÿè¸ªå·²æ³¨å†Œçš„hookå’ŒåŒ…è£…å™¨
        self._active_hooks = weakref.WeakSet()
        self._module_wrappers = weakref.WeakKeyDictionary()
        
    def _prepare_model_for_mc_dropout(self, model: nn.Module) -> nn.Module:
        """
        ä¸ºMonte Carlo Dropoutå‡†å¤‡æ¨¡å‹
        
        Args:
            model: åŸå§‹æ¨¡å‹
            
        Returns:
            å‡†å¤‡å¥½çš„æ¨¡å‹ï¼ˆå¯èƒ½åŒ…å«åŒ…è£…å™¨ï¼‰
        """
        if self.config.use_wrapper:
            # ä½¿ç”¨åŒ…è£…å™¨æ–¹æ¡ˆ - æ›´å®‰å…¨
            return self._apply_dropout_wrappers(model)
        else:
            # ä½¿ç”¨åŸå§‹æ–¹æ¡ˆ - ä¿æŒå‘åå…¼å®¹
            self._enable_mc_dropout_legacy(model)
            return model
            
    def _apply_dropout_wrappers(self, model: nn.Module) -> nn.Module:
        """åº”ç”¨DropoutåŒ…è£…å™¨"""
        def wrap_modules(module):
            for name, child in module.named_children():
                if isinstance(child, nn.Dropout):
                    # å·²æœ‰Dropoutå±‚ï¼Œè®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
                    child.p = self.config.dropout_rate
                    child.train()
                elif isinstance(child, (nn.Conv2d, nn.Linear)):
                    # ä¸ºå·ç§¯å’Œçº¿æ€§å±‚æ·»åŠ åŒ…è£…å™¨
                    if child not in self._module_wrappers:
                        wrapper = DropoutWrapper(child, self.config.dropout_rate)
                        self._module_wrappers[child] = wrapper
                        setattr(module, name, wrapper)
                else:
                    # é€’å½’å¤„ç†å­æ¨¡å—
                    wrap_modules(child)
                    
        # åˆ›å»ºæ¨¡å‹å‰¯æœ¬ä»¥é¿å…ä¿®æ”¹åŸå§‹æ¨¡å‹
        model_copy = type(model).__new__(type(model))
        model_copy.__dict__.update(model.__dict__)
        wrap_modules(model_copy)
        return model_copy
        
    def _enable_mc_dropout_legacy(self, model: nn.Module):
        """
        ä¸ºæ¨¡å‹å¯ç”¨Monte Carlo Dropoutï¼ˆé—ç•™æ–¹æ³•ï¼‰
        
        Args:
            model: ç›®æ ‡æ¨¡å‹
        """
        def apply_mc_dropout(module):
            if isinstance(module, nn.Dropout):
                module.p = self.config.dropout_rate
                module.train()  # ä¿æŒè®­ç»ƒæ¨¡å¼ä»¥å¯ç”¨Dropout
            # ç§»é™¤åŠ¨æ€å±æ€§æ·»åŠ ä»¥é¿å…å®‰å…¨é—®é¢˜
                
        model.apply(apply_mc_dropout)
        
    def estimate_layer_uncertainty(self,
                                 model: nn.Module,
                                 layer_name: str,
                                 data_loader,
                                 device: torch.device) -> float:
        """
        ä¼°è®¡æŒ‡å®šå±‚çš„ä¸ç¡®å®šæ€§
        
        Args:
            model: ç½‘ç»œæ¨¡å‹
            layer_name: å±‚åç§°
            data_loader: æ•°æ®åŠ è½½å™¨
            device: è®¡ç®—è®¾å¤‡
            
        Returns:
            ä¸ç¡®å®šæ€§å€¼ï¼ˆé¢„æµ‹æ–¹å·®ï¼‰
        """
        # å‡†å¤‡æ¨¡å‹
        mc_model = self._prepare_model_for_mc_dropout(model)
        mc_model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œä½†Dropoutä»ç„¶æ¿€æ´»
        
        # æ³¨å†Œhookæ¥æ•è·æŒ‡å®šå±‚çš„è¾“å‡º
        layer_outputs = []
        handle = None  # ğŸ”§ ä¿®å¤ï¼šåˆå§‹åŒ–handleä¸ºNone
        
        def hook_fn(module, input, output):
            layer_outputs.append(output.detach().clone())
        
        # æ‰¾åˆ°ç›®æ ‡å±‚å¹¶æ³¨å†Œhook
        target_layer = None
        for name, module in mc_model.named_modules():
            if name == layer_name:
                target_layer = module
                handle = module.register_forward_hook(hook_fn)
                self._active_hooks.add(handle)  # è·Ÿè¸ªactive hook
                break
                
        if target_layer is None:
            logger.warning(f"Layer {layer_name} not found")
            return self.config.uncertainty_threshold
            
        try:
            all_predictions = []
            
            # Monte Carloé‡‡æ ·
            for sample_idx in range(self.config.n_samples):
                layer_outputs.clear()
                
                with torch.no_grad():
                    for batch_idx, (data, targets) in enumerate(data_loader):
                        if batch_idx >= self.config.max_batches:  # é…ç½®åŒ–æ‰¹æ¬¡é™åˆ¶
                            break
                            
                        data = data.to(device)
                        
                        # å‰å‘ä¼ æ’­
                        _ = mc_model(data)
                        
                        if layer_outputs:
                            # è®¡ç®—å±‚è¾“å‡ºçš„ç»Ÿè®¡ä¿¡æ¯
                            layer_output = layer_outputs[-1]
                            # ä½¿ç”¨å¹³å‡æ± åŒ–å‡å°‘ç»´åº¦
                            if len(layer_output.shape) > 2:
                                pooled = F.adaptive_avg_pool2d(layer_output, (1, 1))
                                features = pooled.view(pooled.size(0), -1)
                            else:
                                features = layer_output
                                
                            # è®¡ç®—ç‰¹å¾çš„L2èŒƒæ•°ä½œä¸ºä»£è¡¨æ€§ç»Ÿè®¡é‡
                            feature_norms = torch.norm(features, dim=1)
                            all_predictions.append(feature_norms.cpu().numpy())
            
            # è®¡ç®—ä¸ç¡®å®šæ€§ï¼ˆé¢„æµ‹æ–¹å·®ï¼‰
            if all_predictions:
                predictions_array = np.concatenate(all_predictions)
                uncertainty = np.var(predictions_array)
                
                # å½’ä¸€åŒ–ä¸ç¡®å®šæ€§å€¼
                uncertainty = float(uncertainty) / (np.mean(predictions_array) + 1e-8)
                
                logger.info(f"Layer {layer_name}: MC Uncertainty = {uncertainty:.6f}")
                return max(uncertainty, self.config.uncertainty_threshold)
            else:
                logger.warning(f"No predictions collected for layer {layer_name}")
                return self.config.uncertainty_threshold * 100  # è¿”å›è¾ƒå¤§çš„é»˜è®¤ä¸ç¡®å®šæ€§
                
        except Exception as e:
            logger.error(f"Error estimating uncertainty for layer {layer_name}: {e}")
            return self.config.uncertainty_threshold * 100
        finally:
            # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿hookè¢«æ­£ç¡®æ¸…ç†
            if handle is not None:
                handle.remove()
                if handle in self._active_hooks:
                    self._active_hooks.discard(handle)
                
    def estimate_model_uncertainty(self,
                                 model: nn.Module,
                                 data_loader,
                                 device: torch.device,
                                 target_layers: List[str] = None) -> Dict[str, float]:
        """
        ä¼°è®¡æ¨¡å‹å¤šå±‚çš„ä¸ç¡®å®šæ€§
        
        Args:
            model: ç½‘ç»œæ¨¡å‹
            data_loader: æ•°æ®åŠ è½½å™¨  
            device: è®¡ç®—è®¾å¤‡
            target_layers: ç›®æ ‡å±‚åˆ—è¡¨ï¼Œå¦‚æœNoneåˆ™ä¼°è®¡æ‰€æœ‰å·ç§¯å’Œçº¿æ€§å±‚
            
        Returns:
            å­—å…¸ï¼Œé”®ä¸ºå±‚åï¼Œå€¼ä¸ºä¸ç¡®å®šæ€§ä¼°è®¡
        """
        if target_layers is None:
            target_layers = []
            for name, module in model.named_modules():
                if isinstance(module, (nn.Conv2d, nn.Linear)):
                    target_layers.append(name)
                    
        uncertainties = {}
        
        for layer_name in target_layers:
            uncertainty = self.estimate_layer_uncertainty(
                model, layer_name, data_loader, device
            )
            uncertainties[layer_name] = uncertainty
            
        return uncertainties
        
    def estimate_predictive_uncertainty(self,
                                      model: nn.Module,
                                      data_loader,
                                      device: torch.device) -> Tuple[float, float]:
        """
        ä¼°è®¡æ¨¡å‹çš„é¢„æµ‹ä¸ç¡®å®šæ€§
        
        Args:
            model: ç½‘ç»œæ¨¡å‹
            data_loader: æ•°æ®åŠ è½½å™¨
            device: è®¡ç®—è®¾å¤‡
            
        Returns:
            (è®¤çŸ¥ä¸ç¡®å®šæ€§, éšæœºä¸ç¡®å®šæ€§)
        """
        # å‡†å¤‡æ¨¡å‹
        mc_model = self._prepare_model_for_mc_dropout(model)
        mc_model.eval()
        
        all_predictions = []
        all_entropies = []
        
        # Monte Carloé‡‡æ ·
        for sample_idx in range(self.config.n_samples):
            batch_predictions = []
            batch_entropies = []
            
            with torch.no_grad():
                for batch_idx, (data, targets) in enumerate(data_loader):
                    if batch_idx >= self.config.max_batches * 2:  # é¢„æµ‹ä¸ç¡®å®šæ€§ä½¿ç”¨æ›´å¤šæ•°æ®
                        break
                        
                    data = data.to(device)
                    outputs = mc_model(data)
                    
                    # è®¡ç®—é¢„æµ‹æ¦‚ç‡
                    probs = F.softmax(outputs, dim=1)
                    batch_predictions.append(probs.cpu().numpy())
                    
                    # è®¡ç®—é¢„æµ‹ç†µ
                    entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)
                    batch_entropies.append(entropy.cpu().numpy())
                    
            if batch_predictions:
                all_predictions.append(np.concatenate(batch_predictions))
                all_entropies.append(np.concatenate(batch_entropies))
                
        if not all_predictions:
            return 0.0, 0.0
            
        # è®¡ç®—è®¤çŸ¥ä¸ç¡®å®šæ€§ï¼ˆé¢„æµ‹åˆ†å¸ƒçš„æ–¹å·®ï¼‰
        predictions_array = np.array(all_predictions)  # [n_samples, n_data, n_classes]
        mean_predictions = np.mean(predictions_array, axis=0)
        
        # è®¤çŸ¥ä¸ç¡®å®šæ€§ï¼šé¢„æµ‹å‡å€¼çš„ç†µ - é¢„æµ‹ç†µçš„å‡å€¼
        mean_entropy = np.mean(all_entropies)
        predictive_entropy = -np.sum(mean_predictions * np.log(mean_predictions + 1e-8), axis=1)
        epistemic_uncertainty = np.mean(predictive_entropy) - mean_entropy
        
        # éšæœºä¸ç¡®å®šæ€§ï¼šé¢„æµ‹ç†µçš„å‡å€¼
        aleatoric_uncertainty = mean_entropy
        
        logger.info(f"Epistemic uncertainty: {epistemic_uncertainty:.6f}")
        logger.info(f"Aleatoric uncertainty: {aleatoric_uncertainty:.6f}")
        
        return float(max(epistemic_uncertainty, 0.0)), float(max(aleatoric_uncertainty, 0.0))
        
    def cleanup(self):
        """æ¸…ç†æ‰€æœ‰æ³¨å†Œçš„hookå’ŒåŒ…è£…å™¨"""
        # æ¸…ç†æ´»è·ƒçš„hook
        for handle in list(self._active_hooks):
            try:
                handle.remove()
            except:
                pass
        self._active_hooks.clear()
        
        # æ¸…ç†åŒ…è£…å™¨å¼•ç”¨
        self._module_wrappers.clear()
        
    def __del__(self):
        """ææ„å‡½æ•°ä¸­ç¡®ä¿æ¸…ç†"""
        try:
            self.cleanup()
        except:
            pass