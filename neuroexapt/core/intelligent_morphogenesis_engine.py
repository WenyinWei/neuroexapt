"""
æ™ºèƒ½å½¢æ€å‘ç”Ÿå¼•æ“

çœŸæ­£ç»¼åˆçš„æ¶æ„å˜å¼‚å†³ç­–ç³»ç»Ÿï¼Œè§£å†³ç»„ä»¶é—´é…åˆç”Ÿç¡¬çš„é—®é¢˜
æ ¸å¿ƒç†å¿µï¼šç²¾å‡†å®šä½å˜å¼‚ç‚¹ï¼Œæ™ºèƒ½é€‰æ‹©å˜å¼‚ç­–ç•¥
"""

from typing import Dict, Any, List, Tuple, Optional
import torch
import torch.nn as nn
import numpy as np
import logging
from collections import defaultdict
import json

logger = logging.getLogger(__name__)


class IntelligentMorphogenesisEngine:
    """
    æ™ºèƒ½å½¢æ€å‘ç”Ÿå¼•æ“
    
    è§£å†³ç°æœ‰é—®é¢˜ï¼š
    1. å„ç»„ä»¶é…åˆç”Ÿç¡¬ -> ç»Ÿä¸€çš„åˆ†æå†³ç­–æµæ°´çº¿
    2. æ£€æµ‹ç»“æœå…¨æ˜¯0 -> åŠ¨æ€é˜ˆå€¼å’Œåˆ†å±‚æ£€æµ‹
    3. å˜å¼‚ç‚¹ä¸æ˜ç¡® -> ç²¾å‡†å®šä½å’Œåˆ†çº§æ¨è
    4. ç­–ç•¥é€‰æ‹©ç®€é™‹ -> å¤šç»´åº¦ç»¼åˆå†³ç­–
    """
    
    def __init__(self):
        # æ ¸å¿ƒåˆ†æç»„ä»¶
        self._layer_analyzer = None
        self._performance_tracker = None
        self._mutation_executor = None
        
        # åŠ¨æ€é˜ˆå€¼ç®¡ç†
        self.adaptive_thresholds = {
            'bottleneck_severity': 0.3,        # åŠ¨æ€è°ƒæ•´
            'improvement_potential': 0.1,      # åŸºäºå†å²è°ƒæ•´
            'mutation_confidence': 0.6,        # è‡ªé€‚åº”
            'performance_plateau_ratio': 0.05  # ç›¸å¯¹åœæ»æ¯”ä¾‹
        }
        
        # åˆ†æå†å²è®°å½•
        self.analysis_history = []
        self.mutation_success_rate = {}
        
        # ç»¼åˆå†³ç­–æƒé‡
        self.decision_weights = {
            'performance_analysis': 0.3,
            'structural_analysis': 0.25,
            'information_flow': 0.2,
            'gradient_analysis': 0.15,
            'historical_success': 0.1
        }
    
    @property
    def layer_analyzer(self):
        """å»¶è¿ŸåŠ è½½å±‚åˆ†æå™¨"""
        if self._layer_analyzer is None:
            from .net2net_subnetwork_analyzer import Net2NetSubnetworkAnalyzer
            self._layer_analyzer = Net2NetSubnetworkAnalyzer()
        return self._layer_analyzer
    
    @property
    def performance_tracker(self):
        """å»¶è¿ŸåŠ è½½æ€§èƒ½è·Ÿè¸ªå™¨"""
        if self._performance_tracker is None:
            from .performance_tracker import PerformanceTracker
            self._performance_tracker = PerformanceTracker()
        return self._performance_tracker
    
    @property
    def mutation_executor(self):
        """å»¶è¿ŸåŠ è½½å˜å¼‚æ‰§è¡Œå™¨"""
        if self._mutation_executor is None:
            from .mutation_executor import MutationExecutor
            self._mutation_executor = MutationExecutor()
        return self._mutation_executor
    
    def comprehensive_morphogenesis_analysis(self, 
                                           model: nn.Module,
                                           context: Dict[str, Any]) -> Dict[str, Any]:
        """
        ç»¼åˆå½¢æ€å‘ç”Ÿåˆ†æ
        
        è®¾è®¡ç†å¿µï¼š
        1. å¤šå±‚æ¬¡åˆ†æï¼šä»ç²—ç²’åº¦åˆ°ç»†ç²’åº¦
        2. åŠ¨æ€é˜ˆå€¼ï¼šæ ¹æ®æ¨¡å‹çŠ¶æ€è°ƒæ•´æ•æ„Ÿåº¦
        3. ç»¼åˆè¯„åˆ†ï¼šå¤šç»´åº¦ä¿¡æ¯èåˆ
        4. ç²¾å‡†å®šä½ï¼šæ˜ç¡®æŒ‡å‡ºå˜å¼‚çš„å…·ä½“ä½ç½®å’Œæ–¹å¼
        """
        
        logger.info("ğŸ§  å¯åŠ¨æ™ºèƒ½å½¢æ€å‘ç”Ÿåˆ†æ")
        
        try:
            # 1. æ€§èƒ½æ€åŠ¿åˆ†æ
            performance_situation = self._analyze_performance_situation(context)
            
            # 2. æ¶æ„ç“¶é¢ˆæ·±åº¦æŒ–æ˜
            structural_bottlenecks = self._deep_structural_analysis(model, context)
            
            # 3. ä¿¡æ¯æµæ•ˆç‡åˆ†æ
            information_efficiency = self._analyze_information_efficiency(model, context)
            
            # 4. æ¢¯åº¦ä¼ æ’­è´¨é‡åˆ†æ
            gradient_quality = self._analyze_gradient_propagation(context)
            
            # 5. åŠ¨æ€è°ƒæ•´æ£€æµ‹é˜ˆå€¼
            self._adapt_detection_thresholds(performance_situation, structural_bottlenecks)
            
            # 6. ç»¼åˆå€™é€‰å˜å¼‚ç‚¹è¯†åˆ«
            mutation_candidates = self._identify_mutation_candidates(
                model, structural_bottlenecks, information_efficiency, gradient_quality
            )
            
            # 7. æ™ºèƒ½å˜å¼‚ç­–ç•¥ç”Ÿæˆ
            mutation_strategies = self._generate_intelligent_strategies(
                mutation_candidates, performance_situation, context
            )
            
            # 8. å¤šç»´åº¦å†³ç­–èåˆ
            final_decisions = self._multi_dimensional_decision_fusion(
                performance_situation, mutation_strategies, context
            )
            
            # 9. æ‰§è¡Œå»ºè®®ç”Ÿæˆ
            execution_plan = self._generate_execution_plan(final_decisions, model, context)
            
            # è®°å½•åˆ†æå†å²
            analysis_record = {
                'timestamp': context.get('current_epoch', 0),
                'performance_situation': performance_situation,
                'mutation_candidates_count': len(mutation_candidates),
                'final_decisions_count': len(final_decisions),
                'thresholds_used': self.adaptive_thresholds.copy()
            }
            self.analysis_history.append(analysis_record)
            
            comprehensive_result = {
                'analysis_summary': {
                    'performance_situation': performance_situation,
                    'structural_analysis': {
                        'total_layers_analyzed': len(list(model.named_modules())),
                        'bottlenecks_found': len(structural_bottlenecks),
                        'severity_distribution': self._categorize_bottlenecks(structural_bottlenecks)
                    },
                    'information_efficiency': information_efficiency,
                    'gradient_quality': gradient_quality
                },
                'mutation_candidates': mutation_candidates,
                'mutation_strategies': mutation_strategies,
                'final_decisions': final_decisions,
                'execution_plan': execution_plan,
                'adaptive_thresholds': self.adaptive_thresholds.copy(),
                'analysis_metadata': {
                    'engine_version': '2.0_intelligent',
                    'total_analysis_history': len(self.analysis_history),
                    'dynamic_threshold_adjustment': True
                }
            }
            
            logger.info(f"ğŸ¯ æ™ºèƒ½åˆ†æå®Œæˆ: å‘ç°{len(mutation_candidates)}ä¸ªå€™é€‰ç‚¹, {len(final_decisions)}ä¸ªæœ€ç»ˆå†³ç­–")
            return comprehensive_result
            
        except Exception as e:
            logger.error(f"âŒ æ™ºèƒ½å½¢æ€å‘ç”Ÿåˆ†æå¤±è´¥: {e}")
            return self._fallback_analysis()
    
    def _analyze_performance_situation(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½æ€åŠ¿"""
        
        performance_history = context.get('performance_history', [])
        current_accuracy = context.get('current_accuracy', 0.0)
        
        if len(performance_history) < 5:
            return {
                'situation_type': 'insufficient_data',
                'plateau_detected': False,
                'improvement_trend': 'unknown',
                'urgency_level': 'low'
            }
        
        # æ€§èƒ½è¶‹åŠ¿åˆ†æ
        recent_accuracies = performance_history[-10:]  # æœ€è¿‘10ä¸ªepoch
        trend_slope = np.polyfit(range(len(recent_accuracies)), recent_accuracies, 1)[0]
        
        # åœæ»æ£€æµ‹ï¼ˆæ›´æ•æ„Ÿçš„ç®—æ³•ï¼‰
        plateau_threshold = self.adaptive_thresholds['performance_plateau_ratio']
        recent_improvement = max(recent_accuracies) - min(recent_accuracies)
        is_plateau = recent_improvement < plateau_threshold
        
        # æ³¢åŠ¨æ€§åˆ†æ
        volatility = np.std(recent_accuracies)
        
        # é¥±å’Œåº¦è¯„ä¼°
        theoretical_max = 0.98  # å‡è®¾ç†è®ºæœ€å¤§å€¼
        saturation_ratio = current_accuracy / theoretical_max
        
        # ç»¼åˆæ€åŠ¿åˆ¤æ–­
        if saturation_ratio > 0.95:
            situation_type = 'high_saturation'
            urgency_level = 'medium'  # é«˜é¥±å’Œæ—¶éœ€è¦ç²¾ç»†åŒ–å˜å¼‚
        elif is_plateau and trend_slope < 0.001:
            situation_type = 'performance_plateau'
            urgency_level = 'high'
        elif trend_slope < -0.005:
            situation_type = 'performance_decline'
            urgency_level = 'high'
        elif volatility > 0.02:
            situation_type = 'unstable_training'
            urgency_level = 'medium'
        else:
            situation_type = 'normal_training'
            urgency_level = 'low'
        
        return {
            'situation_type': situation_type,
            'plateau_detected': is_plateau,
            'improvement_trend': 'positive' if trend_slope > 0.001 else 'negative' if trend_slope < -0.001 else 'flat',
            'urgency_level': urgency_level,
            'saturation_ratio': saturation_ratio,
            'volatility': volatility,
            'trend_slope': trend_slope,
            'recent_improvement': recent_improvement
        }
    
    def _deep_structural_analysis(self, model: nn.Module, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ·±åº¦æ¶æ„ç“¶é¢ˆåˆ†æ"""
        
        bottlenecks = []
        activations = context.get('activations', {})
        gradients = context.get('gradients', {})
        
        # é€å±‚æ·±åº¦åˆ†æ
        for name, module in model.named_modules():
            if not isinstance(module, (nn.Conv2d, nn.Linear)):
                continue
            
            bottleneck_info = {
                'layer_name': name,
                'layer_type': type(module).__name__,
                'bottleneck_scores': {},
                'bottleneck_types': [],
                'improvement_potential': 0.0
            }
            
            # 1. å‚æ•°å®¹é‡åˆ†æ
            param_capacity_score = self._analyze_parameter_capacity(module)
            bottleneck_info['bottleneck_scores']['parameter_capacity'] = param_capacity_score
            
            # 2. ä¿¡æ¯æµåˆ†æï¼ˆå¦‚æœæœ‰æ¿€æ´»å€¼ï¼‰
            if name in activations:
                info_flow_score = self._analyze_layer_information_flow(activations[name])
                bottleneck_info['bottleneck_scores']['information_flow'] = info_flow_score
            
            # 3. æ¢¯åº¦è´¨é‡åˆ†æï¼ˆå¦‚æœæœ‰æ¢¯åº¦ï¼‰
            if name in gradients:
                gradient_score = self._analyze_layer_gradient_quality(gradients[name])
                bottleneck_info['bottleneck_scores']['gradient_quality'] = gradient_score
            
            # 4. æ¶æ„æ•ˆç‡åˆ†æ
            arch_efficiency_score = self._analyze_architectural_efficiency(module, context)
            bottleneck_info['bottleneck_scores']['architectural_efficiency'] = arch_efficiency_score
            
            # ç»¼åˆè¯„åˆ†å’Œç“¶é¢ˆç±»å‹åˆ¤æ–­
            scores = bottleneck_info['bottleneck_scores']
            avg_score = np.mean(list(scores.values()))
            
            # åŠ¨æ€é˜ˆå€¼åˆ¤æ–­
            threshold = self.adaptive_thresholds['bottleneck_severity']
            if avg_score > threshold:
                # ç¡®å®šç“¶é¢ˆç±»å‹
                if scores.get('parameter_capacity', 0) > threshold:
                    bottleneck_info['bottleneck_types'].append('parameter_constraint')
                if scores.get('information_flow', 0) > threshold:
                    bottleneck_info['bottleneck_types'].append('information_bottleneck')
                if scores.get('gradient_quality', 0) > threshold:
                    bottleneck_info['bottleneck_types'].append('gradient_bottleneck')
                if scores.get('architectural_efficiency', 0) > threshold:
                    bottleneck_info['bottleneck_types'].append('architectural_inefficiency')
                
                # è®¡ç®—æ”¹è¿›æ½œåŠ›
                bottleneck_info['improvement_potential'] = min(1.0, avg_score * 1.2)
                bottlenecks.append(bottleneck_info)
        
        # æŒ‰æ”¹è¿›æ½œåŠ›æ’åº
        bottlenecks.sort(key=lambda x: x['improvement_potential'], reverse=True)
        
        logger.info(f"ğŸ” æ·±åº¦ç»“æ„åˆ†æ: å‘ç°{len(bottlenecks)}ä¸ªç“¶é¢ˆå±‚")
        return bottlenecks
    
    def _analyze_parameter_capacity(self, module: nn.Module) -> float:
        """åˆ†æå‚æ•°å®¹é‡çº¦æŸ"""
        
        if isinstance(module, nn.Conv2d):
            # å¯¹äºå·ç§¯å±‚ï¼Œåˆ†æé€šé“æ•°ç›¸å¯¹äºç‰¹å¾å¤æ‚åº¦çš„å……åˆ†æ€§
            channel_ratio = module.out_channels / max(64, module.in_channels)  # åŸºå‡†æ¯”ä¾‹
            kernel_efficiency = (module.kernel_size[0] * module.kernel_size[1]) / 9  # 3x3ä¸ºåŸºå‡†
            
            # é€šé“æ•°ä¸è¶³æˆ–æ ¸å¤ªå°éƒ½å¯èƒ½é€ æˆå®¹é‡çº¦æŸ
            capacity_constraint = max(0, 1 - channel_ratio) + max(0, 1 - kernel_efficiency)
            return min(1.0, capacity_constraint / 2)
            
        elif isinstance(module, nn.Linear):
            # å¯¹äºçº¿æ€§å±‚ï¼Œåˆ†æç‰¹å¾æ•°ç›¸å¯¹äºè¾“å…¥å¤æ‚åº¦çš„å……åˆ†æ€§
            feature_ratio = module.out_features / max(128, module.in_features)
            capacity_constraint = max(0, 1 - feature_ratio)
            return min(1.0, capacity_constraint)
        
        return 0.0
    
    def _analyze_layer_information_flow(self, activation: torch.Tensor) -> float:
        """åˆ†æå±‚çº§ä¿¡æ¯æµæ•ˆç‡"""
        
        try:
            # ä¿¡æ¯ç†µè®¡ç®—
            flat_activation = activation.flatten()
            
            # æœ‰æ•ˆä¿¡æ¯æ¯”ä¾‹
            non_zero_ratio = torch.count_nonzero(flat_activation).float() / flat_activation.numel()
            
            # æ¿€æ´»åˆ†å¸ƒçš„å‡åŒ€æ€§
            hist = torch.histc(flat_activation, bins=50)
            hist_normalized = hist / hist.sum()
            entropy = -torch.sum(hist_normalized * torch.log(hist_normalized + 1e-10))
            max_entropy = np.log(50)  # 50ä¸ªbinçš„æœ€å¤§ç†µ
            entropy_ratio = entropy / max_entropy
            
            # ä¿¡æ¯æµæ•ˆç‡ = 1 - æœ‰æ•ˆæ€§å’Œå‡åŒ€æ€§çš„ç»¼åˆ
            efficiency_loss = (1 - non_zero_ratio) * 0.6 + (1 - entropy_ratio) * 0.4
            return float(efficiency_loss)
            
        except Exception:
            return 0.5
    
    def _analyze_layer_gradient_quality(self, gradient: torch.Tensor) -> float:
        """åˆ†æå±‚çº§æ¢¯åº¦è´¨é‡"""
        
        try:
            # æ¢¯åº¦èŒƒæ•°
            grad_norm = torch.norm(gradient)
            
            # æ¢¯åº¦åˆ†å¸ƒ
            grad_std = torch.std(gradient)
            grad_mean = torch.abs(torch.mean(gradient))
            
            # æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸æ£€æµ‹
            if grad_norm < 1e-7:
                return 0.9  # ä¸¥é‡æ¢¯åº¦æ¶ˆå¤±
            elif grad_norm > 100:
                return 0.8  # æ¢¯åº¦çˆ†ç‚¸
            
            # æ¢¯åº¦åˆ†å¸ƒè´¨é‡
            signal_noise_ratio = grad_mean / (grad_std + 1e-10)
            quality_score = 1.0 / (1.0 + signal_noise_ratio)
            
            return float(quality_score)
            
        except Exception:
            return 0.5
    
    def _analyze_architectural_efficiency(self, module: nn.Module, context: Dict[str, Any]) -> float:
        """åˆ†ææ¶æ„æ•ˆç‡"""
        
        # å‚æ•°åˆ©ç”¨æ•ˆç‡
        param_count = sum(p.numel() for p in module.parameters())
        
        if isinstance(module, nn.Conv2d):
            # FLOPsä¼°ç®—å’Œæ•ˆç‡åˆ†æ
            theoretical_flops = module.out_channels * module.in_channels * np.prod(module.kernel_size)
            efficiency_score = min(1.0, param_count / (theoretical_flops + 1))
        elif isinstance(module, nn.Linear):
            # çº¿æ€§å±‚çš„å‚æ•°å¯†åº¦
            efficiency_score = min(1.0, param_count / (module.in_features * module.out_features + 1))
        else:
            efficiency_score = 0.5
        
        # è¿”å›æ•ˆç‡ä¸è¶³ç¨‹åº¦
        return 1.0 - efficiency_score
    
    def _analyze_information_efficiency(self, model: nn.Module, context: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå…¨å±€ä¿¡æ¯æ•ˆç‡"""
        
        activations = context.get('activations', {})
        
        if not activations:
            return {'overall_efficiency': 0.5, 'bottleneck_layers': []}
        
        layer_efficiencies = {}
        
        for layer_name, activation in activations.items():
            efficiency = 1.0 - self._analyze_layer_information_flow(activation)
            layer_efficiencies[layer_name] = efficiency
        
        overall_efficiency = np.mean(list(layer_efficiencies.values()))
        
        # æ‰¾å‡ºæ•ˆç‡æœ€ä½çš„å±‚
        sorted_layers = sorted(layer_efficiencies.items(), key=lambda x: x[1])
        bottleneck_layers = [layer for layer, eff in sorted_layers[:3] if eff < 0.6]
        
        return {
            'overall_efficiency': overall_efficiency,
            'layer_efficiencies': layer_efficiencies,
            'bottleneck_layers': bottleneck_layers,
            'efficiency_variance': np.var(list(layer_efficiencies.values()))
        }
    
    def _analyze_gradient_propagation(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ¢¯åº¦ä¼ æ’­è´¨é‡"""
        
        gradients = context.get('gradients', {})
        
        if not gradients:
            return {'overall_quality': 0.5, 'problematic_layers': []}
        
        layer_qualities = {}
        
        for layer_name, gradient in gradients.items():
            quality = 1.0 - self._analyze_layer_gradient_quality(gradient)
            layer_qualities[layer_name] = quality
        
        overall_quality = np.mean(list(layer_qualities.values()))
        
        # æ‰¾å‡ºæ¢¯åº¦è´¨é‡æœ€å·®çš„å±‚
        sorted_layers = sorted(layer_qualities.items(), key=lambda x: x[1])
        problematic_layers = [layer for layer, qual in sorted_layers[:3] if qual < 0.5]
        
        return {
            'overall_quality': overall_quality,
            'layer_qualities': layer_qualities,
            'problematic_layers': problematic_layers,
            'quality_variance': np.var(list(layer_qualities.values()))
        }
    
    def _adapt_detection_thresholds(self, performance_situation: Dict[str, Any], 
                                  structural_bottlenecks: List[Dict[str, Any]]):
        """åŠ¨æ€è°ƒæ•´æ£€æµ‹é˜ˆå€¼"""
        
        # æ ¹æ®æ€§èƒ½æ€åŠ¿è°ƒæ•´æ•æ„Ÿåº¦
        if performance_situation['situation_type'] == 'high_saturation':
            # é«˜é¥±å’ŒçŠ¶æ€ï¼Œæé«˜æ•æ„Ÿåº¦
            self.adaptive_thresholds['bottleneck_severity'] *= 0.8
            self.adaptive_thresholds['improvement_potential'] *= 0.7
        elif performance_situation['situation_type'] == 'performance_plateau':
            # åœæ»çŠ¶æ€ï¼Œä¸­ç­‰æ•æ„Ÿåº¦
            self.adaptive_thresholds['bottleneck_severity'] *= 0.9
        elif performance_situation['urgency_level'] == 'low':
            # æ­£å¸¸çŠ¶æ€ï¼Œé™ä½æ•æ„Ÿåº¦é¿å…è¿‡åº¦å˜å¼‚
            self.adaptive_thresholds['bottleneck_severity'] *= 1.1
        
        # æ ¹æ®å†å²æˆåŠŸç‡è°ƒæ•´
        if self.analysis_history:
            recent_analyses = self.analysis_history[-5:]
            avg_candidates = np.mean([a['mutation_candidates_count'] for a in recent_analyses])
            avg_decisions = np.mean([a['final_decisions_count'] for a in recent_analyses])
            
            # å¦‚æœå€™é€‰å¤ªå°‘ï¼Œé™ä½é˜ˆå€¼
            if avg_candidates < 1:
                self.adaptive_thresholds['bottleneck_severity'] *= 0.8
            # å¦‚æœå€™é€‰å¤ªå¤šï¼Œæé«˜é˜ˆå€¼
            elif avg_candidates > 5:
                self.adaptive_thresholds['bottleneck_severity'] *= 1.2
        
        # ç¡®ä¿é˜ˆå€¼åœ¨åˆç†èŒƒå›´å†…
        self.adaptive_thresholds['bottleneck_severity'] = np.clip(
            self.adaptive_thresholds['bottleneck_severity'], 0.1, 0.8
        )
        
        logger.info(f"ğŸ“Š åŠ¨æ€é˜ˆå€¼è°ƒæ•´: ç“¶é¢ˆæ£€æµ‹é˜ˆå€¼={self.adaptive_thresholds['bottleneck_severity']:.3f}")
    
    def _identify_mutation_candidates(self, model: nn.Module,
                                    structural_bottlenecks: List[Dict[str, Any]],
                                    information_efficiency: Dict[str, Any],
                                    gradient_quality: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è¯†åˆ«å˜å¼‚å€™é€‰ç‚¹"""
        
        candidates = []
        
        # ä»ç»“æ„ç“¶é¢ˆä¸­é€‰æ‹©å€™é€‰
        for bottleneck in structural_bottlenecks:
            candidate = {
                'layer_name': bottleneck['layer_name'],
                'layer_type': bottleneck['layer_type'],
                'selection_reasons': ['structural_bottleneck'],
                'bottleneck_types': bottleneck['bottleneck_types'],
                'improvement_potential': bottleneck['improvement_potential'],
                'priority_score': bottleneck['improvement_potential'],
                'recommended_mutations': []
            }
            
            # æ ¹æ®ç“¶é¢ˆç±»å‹æ¨èå˜å¼‚ç­–ç•¥
            if 'parameter_constraint' in bottleneck['bottleneck_types']:
                candidate['recommended_mutations'].append('width_expansion')
            if 'information_bottleneck' in bottleneck['bottleneck_types']:
                candidate['recommended_mutations'].extend(['depth_expansion', 'attention_enhancement'])
            if 'gradient_bottleneck' in bottleneck['bottleneck_types']:
                candidate['recommended_mutations'].extend(['residual_connection', 'batch_norm_insertion'])
            
            candidates.append(candidate)
        
        # ä»ä¿¡æ¯æ•ˆç‡åˆ†æä¸­è¡¥å……å€™é€‰
        for layer_name in information_efficiency.get('bottleneck_layers', []):
            # é¿å…é‡å¤
            if not any(c['layer_name'] == layer_name for c in candidates):
                candidate = {
                    'layer_name': layer_name,
                    'layer_type': 'unknown',
                    'selection_reasons': ['information_inefficiency'],
                    'bottleneck_types': ['information_flow'],
                    'improvement_potential': 0.7,
                    'priority_score': 0.7,
                    'recommended_mutations': ['information_enhancement', 'channel_attention']
                }
                candidates.append(candidate)
        
        # ä»æ¢¯åº¦è´¨é‡åˆ†æä¸­è¡¥å……å€™é€‰
        for layer_name in gradient_quality.get('problematic_layers', []):
            # é¿å…é‡å¤
            if not any(c['layer_name'] == layer_name for c in candidates):
                candidate = {
                    'layer_name': layer_name,
                    'layer_type': 'unknown',
                    'selection_reasons': ['gradient_quality_issue'],
                    'bottleneck_types': ['gradient_propagation'],
                    'improvement_potential': 0.6,
                    'priority_score': 0.6,
                    'recommended_mutations': ['residual_connection', 'layer_norm']
                }
                candidates.append(candidate)
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        candidates.sort(key=lambda x: x['priority_score'], reverse=True)
        
        logger.info(f"ğŸ¯ è¯†åˆ«å˜å¼‚å€™é€‰: {len(candidates)}ä¸ªå€™é€‰ç‚¹")
        return candidates
    
    def _generate_intelligent_strategies(self, candidates: List[Dict[str, Any]],
                                       performance_situation: Dict[str, Any],
                                       context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ™ºèƒ½å˜å¼‚ç­–ç•¥"""
        
        strategies = []
        
        for candidate in candidates:
            for mutation_type in candidate['recommended_mutations']:
                strategy = {
                    'target_layer': candidate['layer_name'],
                    'mutation_type': mutation_type,
                    'rationale': {
                        'bottleneck_types': candidate['bottleneck_types'],
                        'selection_reasons': candidate['selection_reasons'],
                        'improvement_potential': candidate['improvement_potential']
                    },
                    'expected_outcome': self._predict_mutation_outcome(
                        mutation_type, candidate, performance_situation
                    ),
                    'risk_assessment': self._assess_mutation_risk(
                        mutation_type, candidate, context
                    ),
                    'implementation_plan': self._create_implementation_plan(
                        mutation_type, candidate
                    )
                }
                strategies.append(strategy)
        
        return strategies
    
    def _predict_mutation_outcome(self, mutation_type: str, 
                                candidate: Dict[str, Any],
                                performance_situation: Dict[str, Any]) -> Dict[str, Any]:
        """é¢„æµ‹å˜å¼‚ç»“æœ"""
        
        base_improvement = candidate['improvement_potential'] * 0.05  # æœ€å¤§5%æ”¹è¿›
        
        # æ ¹æ®å˜å¼‚ç±»å‹è°ƒæ•´
        type_multipliers = {
            'width_expansion': 1.0,
            'depth_expansion': 0.8,
            'attention_enhancement': 1.2,
            'residual_connection': 0.9,
            'batch_norm_insertion': 0.7,
            'information_enhancement': 1.1,
            'channel_attention': 1.0,
            'layer_norm': 0.6
        }
        
        adjusted_improvement = base_improvement * type_multipliers.get(mutation_type, 1.0)
        
        # æ ¹æ®æ€§èƒ½æ€åŠ¿è°ƒæ•´
        if performance_situation['situation_type'] == 'high_saturation':
            adjusted_improvement *= 0.5  # é«˜é¥±å’Œæ—¶æ”¹è¿›è¾ƒå°
        elif performance_situation['situation_type'] == 'performance_plateau':
            adjusted_improvement *= 1.2  # åœæ»æ—¶æ”¹è¿›æ½œåŠ›è¾ƒå¤§
        
        return {
            'expected_accuracy_improvement': adjusted_improvement,
            'confidence_level': min(0.9, candidate['improvement_potential']),
            'parameter_increase': self._estimate_parameter_increase(mutation_type),
            'computational_overhead': self._estimate_computational_overhead(mutation_type)
        }
    
    def _assess_mutation_risk(self, mutation_type: str, 
                            candidate: Dict[str, Any], 
                            context: Dict[str, Any]) -> Dict[str, Any]:
        """è¯„ä¼°å˜å¼‚é£é™©"""
        
        # åŸºç¡€é£é™©è¯„åˆ†
        base_risks = {
            'width_expansion': 0.2,
            'depth_expansion': 0.4,
            'attention_enhancement': 0.3,
            'residual_connection': 0.1,
            'batch_norm_insertion': 0.1,
            'information_enhancement': 0.3,
            'channel_attention': 0.2,
            'layer_norm': 0.1
        }
        
        base_risk = base_risks.get(mutation_type, 0.5)
        
        # æ ¹æ®å†å²æˆåŠŸç‡è°ƒæ•´
        historical_success = self.mutation_success_rate.get(mutation_type, 0.5)
        risk_adjustment = 1.0 - historical_success
        
        final_risk = min(1.0, base_risk * (1 + risk_adjustment))
        
        return {
            'overall_risk': final_risk,
            'risk_factors': self._identify_risk_factors(mutation_type, candidate),
            'mitigation_strategies': self._suggest_risk_mitigation(mutation_type),
            'rollback_plan': self._create_rollback_plan(mutation_type)
        }
    
    def _create_implementation_plan(self, mutation_type: str, 
                                  candidate: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºå®æ–½è®¡åˆ’"""
        
        return {
            'preparation_steps': self._get_preparation_steps(mutation_type),
            'execution_steps': self._get_execution_steps(mutation_type, candidate),
            'validation_steps': self._get_validation_steps(mutation_type),
            'estimated_time': self._estimate_implementation_time(mutation_type)
        }
    
    def _multi_dimensional_decision_fusion(self, performance_situation: Dict[str, Any],
                                         mutation_strategies: List[Dict[str, Any]],
                                         context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """å¤šç»´åº¦å†³ç­–èåˆ"""
        
        final_decisions = []
        
        for strategy in mutation_strategies:
            # ç»¼åˆè¯„åˆ†
            decision_score = 0.0
            
            # æ€§èƒ½åˆ†ææƒé‡
            perf_score = strategy['expected_outcome']['expected_accuracy_improvement'] * 10
            decision_score += perf_score * self.decision_weights['performance_analysis']
            
            # ç»“æ„åˆ†ææƒé‡
            struct_score = strategy['rationale']['improvement_potential']
            decision_score += struct_score * self.decision_weights['structural_analysis']
            
            # é£é™©è°ƒæ•´
            risk_penalty = strategy['risk_assessment']['overall_risk']
            decision_score *= (1 - risk_penalty * 0.5)
            
            # å†å²æˆåŠŸç‡æƒé‡
            historical_success = self.mutation_success_rate.get(strategy['mutation_type'], 0.5)
            decision_score += historical_success * self.decision_weights['historical_success']
            
            # åªä¿ç•™é«˜åˆ†ç­–ç•¥
            confidence_threshold = self.adaptive_thresholds['mutation_confidence']
            if decision_score > confidence_threshold:
                decision = strategy.copy()
                decision['final_score'] = decision_score
                decision['selection_rationale'] = self._generate_selection_rationale(
                    strategy, decision_score, performance_situation
                )
                final_decisions.append(decision)
        
        # æŒ‰åˆ†æ•°æ’åºï¼Œé€‰æ‹©æœ€ä½³ç­–ç•¥
        final_decisions.sort(key=lambda x: x['final_score'], reverse=True)
        
        # æ ¹æ®æƒ…å†µé™åˆ¶æ•°é‡
        max_decisions = 3 if performance_situation['urgency_level'] == 'high' else 1
        final_decisions = final_decisions[:max_decisions]
        
        logger.info(f"ğŸ¯ å¤šç»´å†³ç­–èåˆ: {len(final_decisions)}ä¸ªæœ€ç»ˆå†³ç­–")
        return final_decisions
    
    def _generate_execution_plan(self, final_decisions: List[Dict[str, Any]], 
                               model: nn.Module, 
                               context: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ‰§è¡Œè®¡åˆ’"""
        
        if not final_decisions:
            return {
                'execute': False,
                'reason': 'no_viable_mutations',
                'recommendations': ['continue_training', 'adjust_hyperparameters']
            }
        
        primary_decision = final_decisions[0]
        
        return {
            'execute': True,
            'primary_mutation': {
                'target_layer': primary_decision['target_layer'],
                'mutation_type': primary_decision['mutation_type'],
                'expected_improvement': primary_decision['expected_outcome']['expected_accuracy_improvement'],
                'confidence': primary_decision['expected_outcome']['confidence_level']
            },
            'alternative_mutations': [
                {
                    'target_layer': d['target_layer'],
                    'mutation_type': d['mutation_type'],
                    'score': d['final_score']
                } for d in final_decisions[1:3]
            ],
            'execution_order': 'sequential',
            'monitoring_plan': {
                'metrics_to_track': ['accuracy', 'loss', 'gradient_norm'],
                'evaluation_frequency': 'every_epoch',
                'success_criteria': f"accuracy_improvement > {primary_decision['expected_outcome']['expected_accuracy_improvement'] * 0.5}"
            },
            'contingency_plan': primary_decision['risk_assessment']['rollback_plan']
        }
    
    # è¾…åŠ©æ–¹æ³•
    def _categorize_bottlenecks(self, bottlenecks: List[Dict[str, Any]]) -> Dict[str, int]:
        """åˆ†ç±»ç“¶é¢ˆ"""
        categories = defaultdict(int)
        for bottleneck in bottlenecks:
            for btype in bottleneck['bottleneck_types']:
                categories[btype] += 1
        return dict(categories)
    
    def _estimate_parameter_increase(self, mutation_type: str) -> int:
        """ä¼°è®¡å‚æ•°å¢åŠ é‡"""
        estimates = {
            'width_expansion': 50000,
            'depth_expansion': 100000,
            'attention_enhancement': 30000,
            'residual_connection': 0,
            'batch_norm_insertion': 100,
            'information_enhancement': 20000,
            'channel_attention': 5000,
            'layer_norm': 200
        }
        return estimates.get(mutation_type, 10000)
    
    def _estimate_computational_overhead(self, mutation_type: str) -> float:
        """ä¼°è®¡è®¡ç®—å¼€é”€"""
        overheads = {
            'width_expansion': 0.2,
            'depth_expansion': 0.3,
            'attention_enhancement': 0.4,
            'residual_connection': 0.05,
            'batch_norm_insertion': 0.02,
            'information_enhancement': 0.15,
            'channel_attention': 0.1,
            'layer_norm': 0.02
        }
        return overheads.get(mutation_type, 0.1)
    
    def _identify_risk_factors(self, mutation_type: str, candidate: Dict[str, Any]) -> List[str]:
        """è¯†åˆ«é£é™©å› ç´ """
        risk_factors = []
        
        if mutation_type in ['depth_expansion', 'attention_enhancement']:
            risk_factors.append('increased_overfitting_risk')
        if mutation_type in ['width_expansion', 'information_enhancement']:
            risk_factors.append('computational_overhead')
        if candidate['improvement_potential'] < 0.5:
            risk_factors.append('uncertain_benefit')
        
        return risk_factors
    
    def _suggest_risk_mitigation(self, mutation_type: str) -> List[str]:
        """å»ºè®®é£é™©ç¼“è§£æªæ–½"""
        mitigations = {
            'width_expansion': ['use_dropout', 'reduce_learning_rate'],
            'depth_expansion': ['use_residual_connections', 'careful_initialization'],
            'attention_enhancement': ['use_attention_dropout', 'layer_norm']
        }
        return mitigations.get(mutation_type, ['monitor_carefully'])
    
    def _create_rollback_plan(self, mutation_type: str) -> Dict[str, Any]:
        """åˆ›å»ºå›æ»šè®¡åˆ’"""
        return {
            'trigger_conditions': ['accuracy_drop > 2%', 'loss_divergence'],
            'rollback_steps': ['restore_checkpoint', 'adjust_learning_rate'],
            'recovery_strategy': 'conservative_training'
        }
    
    def _get_preparation_steps(self, mutation_type: str) -> List[str]:
        """è·å–å‡†å¤‡æ­¥éª¤"""
        return [
            'create_model_checkpoint',
            'backup_optimizer_state',
            'prepare_mutation_parameters'
        ]
    
    def _get_execution_steps(self, mutation_type: str, candidate: Dict[str, Any]) -> List[str]:
        """è·å–æ‰§è¡Œæ­¥éª¤"""
        base_steps = [
            f'locate_target_layer: {candidate["layer_name"]}',
            f'apply_mutation: {mutation_type}',
            'update_model_structure',
            'reinitialize_optimizer',
            'validate_mutation'
        ]
        return base_steps
    
    def _get_validation_steps(self, mutation_type: str) -> List[str]:
        """è·å–éªŒè¯æ­¥éª¤"""
        return [
            'check_model_integrity',
            'validate_forward_pass',
            'test_gradient_flow',
            'measure_performance_impact'
        ]
    
    def _estimate_implementation_time(self, mutation_type: str) -> str:
        """ä¼°è®¡å®æ–½æ—¶é—´"""
        return "1-2 epochs"
    
    def _generate_selection_rationale(self, strategy: Dict[str, Any], 
                                    score: float, 
                                    performance_situation: Dict[str, Any]) -> str:
        """ç”Ÿæˆé€‰æ‹©ç†ç”±"""
        
        rationale_parts = []
        
        if score > 0.8:
            rationale_parts.append("é«˜ç½®ä¿¡åº¦æ”¹è¿›é¢„æœŸ")
        
        if strategy['rationale']['improvement_potential'] > 0.7:
            rationale_parts.append("æ˜¾è‘—ç»“æ„æ”¹è¿›æ½œåŠ›")
        
        if performance_situation['situation_type'] == 'performance_plateau':
            rationale_parts.append("çªç ´æ€§èƒ½ç“¶é¢ˆéœ€è¦")
        
        if strategy['risk_assessment']['overall_risk'] < 0.3:
            rationale_parts.append("ä½é£é™©å®æ–½")
        
        return "; ".join(rationale_parts) if rationale_parts else "ç»¼åˆè¯„ä¼°æ¨è"
    
    def _fallback_analysis(self) -> Dict[str, Any]:
        """fallbackåˆ†æ"""
        return {
            'analysis_summary': {
                'status': 'fallback_mode',
                'structural_analysis': {'bottlenecks_found': 0}
            },
            'mutation_candidates': [],
            'mutation_strategies': [],
            'final_decisions': [],
            'execution_plan': {
                'execute': False,
                'reason': 'analysis_failed'
            }
        }
    
    def update_success_rate(self, mutation_type: str, success: bool):
        """æ›´æ–°å˜å¼‚æˆåŠŸç‡"""
        if mutation_type not in self.mutation_success_rate:
            self.mutation_success_rate[mutation_type] = 0.5
        
        # æŒ‡æ•°ç§»åŠ¨å¹³å‡æ›´æ–°
        alpha = 0.1
        current_rate = self.mutation_success_rate[mutation_type]
        new_rate = alpha * (1.0 if success else 0.0) + (1 - alpha) * current_rate
        self.mutation_success_rate[mutation_type] = new_rate
        
        logger.info(f"ğŸ“Š æ›´æ–°æˆåŠŸç‡: {mutation_type} = {new_rate:.3f}")