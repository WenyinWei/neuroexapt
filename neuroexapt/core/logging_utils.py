#!/usr/bin/env python3
"""
Logging Utilities for NeuroExapt - ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ

ğŸ”§ æä¾›ç»Ÿä¸€çš„æ—¥å¿—æ¥å£ï¼Œé¿å…å¾ªç¯å¯¼å…¥é—®é¢˜
"""

import logging
import os
import time
from typing import List


class ConfigurableLogger:
    """å¯é…ç½®çš„é«˜æ€§èƒ½æ—¥å¿—ç³»ç»Ÿï¼Œæ›¿ä»£ANSIå½©è‰²æ‰“å°"""
    
    def __init__(self, name: str = "neuroexapt", level: str = "INFO", enable_console: bool = True):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(getattr(logging, level.upper()))
        
        # é¿å…é‡å¤æ·»åŠ å¤„ç†å™¨
        if not self.logger.handlers:
            # æ§åˆ¶å°å¤„ç†å™¨
            if enable_console:
                console_handler = logging.StreamHandler()
                console_formatter = logging.Formatter(
                    '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                    datefmt='%H:%M:%S'
                )
                console_handler.setFormatter(console_formatter)
                self.logger.addHandler(console_handler)
            
            # æ–‡ä»¶å¤„ç†å™¨ï¼ˆå¯é€‰ï¼‰
            log_file = os.environ.get('NEUROEXAPT_LOG_FILE')
            if log_file:
                file_handler = logging.FileHandler(log_file)
                file_formatter = logging.Formatter(
                    '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'
                )
                file_handler.setFormatter(file_formatter)
                self.logger.addHandler(file_handler)
        
        self.section_stack = []
        
    def debug(self, message: str, *args, **kwargs):
        """è®°å½•è°ƒè¯•ä¿¡æ¯"""
        if self.logger.isEnabledFor(logging.DEBUG):
            indent = "  " * len(self.section_stack)
            self.logger.debug(f"{indent}{message}", *args, **kwargs)
    
    def info(self, message: str, *args, **kwargs):
        """è®°å½•ä¿¡æ¯"""
        if self.logger.isEnabledFor(logging.INFO):
            indent = "  " * len(self.section_stack)
            self.logger.info(f"{indent}{message}", *args, **kwargs)
    
    def warning(self, message: str, *args, **kwargs):
        """è®°å½•è­¦å‘Š"""
        if self.logger.isEnabledFor(logging.WARNING):
            indent = "  " * len(self.section_stack)
            self.logger.warning(f"{indent}{message}", *args, **kwargs)
    
    def error(self, message: str, *args, **kwargs):
        """è®°å½•é”™è¯¯"""
        if self.logger.isEnabledFor(logging.ERROR):
            indent = "  " * len(self.section_stack)
            self.logger.error(f"{indent}{message}", *args, **kwargs)
    
    def success(self, message: str, *args, **kwargs):
        """è®°å½•æˆåŠŸä¿¡æ¯ï¼ˆä½¿ç”¨INFOçº§åˆ«ï¼‰"""
        if self.logger.isEnabledFor(logging.INFO):
            indent = "  " * len(self.section_stack)
            self.logger.info(f"{indent}âœ… {message}", *args, **kwargs)
    
    def enter_section(self, section_name: str):
        """è¿›å…¥æ–°çš„æ—¥å¿—åŒºåŸŸ"""
        indent = "  " * len(self.section_stack)
        self.logger.info(f"{indent}ğŸ” è¿›å…¥ {section_name}")
        self.section_stack.append(section_name)
    
    def exit_section(self, section_name: str):
        """é€€å‡ºæ—¥å¿—åŒºåŸŸ"""
        if self.section_stack and self.section_stack[-1] == section_name:
            self.section_stack.pop()
        indent = "  " * len(self.section_stack)
        self.logger.info(f"{indent}âœ… å®Œæˆ {section_name}")
    
    def log_tensor_info(self, tensor, name: str):
        """è®°å½•å¼ é‡ä¿¡æ¯"""
        if not self.logger.isEnabledFor(logging.DEBUG):
            return
            
        if tensor is None:
            self.warning(f"âŒ {name}: None")
            return
        
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯torch tensor
            if hasattr(tensor, 'device') and hasattr(tensor, 'shape') and hasattr(tensor, 'dtype'):
                device_info = f"({tensor.device})" if hasattr(tensor, 'device') else ""
                self.debug(f"ğŸ“Š {name}: shape={list(tensor.shape)}, dtype={tensor.dtype}, device={device_info}")
            else:
                # å¤„ç†å…¶ä»–ç±»å‹çš„tensor-likeå¯¹è±¡
                if hasattr(tensor, 'shape'):
                    self.debug(f"ğŸ“Š {name}: shape={getattr(tensor, 'shape', 'unknown')}")
                else:
                    self.debug(f"ğŸ“Š {name}: {type(tensor).__name__}")
        except Exception as e:
            self.warning(f"âš ï¸ æ— æ³•è®°å½•å¼ é‡ä¿¡æ¯ {name}: {e}")
    
    def log_model_info(self, model, name: str = "Model"):
        """è®°å½•æ¨¡å‹ä¿¡æ¯"""
        if not self.logger.isEnabledFor(logging.INFO):
            return
            
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯PyTorchæ¨¡å‹
            if hasattr(model, 'parameters'):
                total_params = sum(p.numel() for p in model.parameters())
                trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
                
                # å°è¯•è·å–è®¾å¤‡ä¿¡æ¯
                try:
                    device = next(model.parameters()).device if list(model.parameters()) else "Unknown"
                except StopIteration:
                    device = "Unknown"
                
                self.info(f"ğŸ§  {name}: æ€»å‚æ•°={total_params:,}, å¯è®­ç»ƒ={trainable_params:,}, è®¾å¤‡={device}")
            else:
                # å¤„ç†å…¶ä»–ç±»å‹çš„æ¨¡å‹å¯¹è±¡
                self.info(f"ğŸ§  {name}: {type(model).__name__}")
        except Exception as e:
            self.warning(f"âš ï¸ æ— æ³•è®°å½•æ¨¡å‹ä¿¡æ¯ {name}: {e}")


# å…¨å±€æ—¥å¿—é…ç½®
_log_level = os.environ.get('NEUROEXAPT_LOG_LEVEL', 'INFO')
_enable_console = os.environ.get('NEUROEXAPT_ENABLE_CONSOLE', 'true').lower() in ('true', '1', 'yes')

# åˆ›å»ºå…¨å±€loggerå®ä¾‹
logger = ConfigurableLogger("neuroexapt.dnm", _log_level, _enable_console)


class DebugPrinter:
    """è°ƒè¯•è¾“å‡ºç®¡ç†å™¨ - å…¼å®¹æ—§ç‰ˆæœ¬æ¥å£"""
    
    def __init__(self, enabled: bool = True):
        self.enabled = enabled
        self.indent_level = 0
        
    def print_debug(self, message: str, level: str = "INFO"):
        """æ‰“å°è°ƒè¯•ä¿¡æ¯"""
        if not self.enabled:
            return
            
        indent = "  " * self.indent_level
        timestamp = time.strftime("%H:%M:%S", time.localtime())
        
        # ä½¿ç”¨ç»Ÿä¸€çš„loggerç³»ç»Ÿ
        if level == "DEBUG":
            logger.debug(f"{message}")
        elif level == "INFO":
            logger.info(f"{message}")
        elif level == "WARNING":
            logger.warning(f"{message}")
        elif level == "ERROR":
            logger.error(f"{message}")
        elif level == "SUCCESS":
            logger.success(f"{message}")
        else:
            logger.info(f"{message}")
        
    def enter_section(self, section_name: str):
        """è¿›å…¥æ–°çš„è°ƒè¯•åŒºåŸŸ"""
        logger.enter_section(section_name)
        self.indent_level += 1
        
    def exit_section(self, section_name: str):
        """é€€å‡ºè°ƒè¯•åŒºåŸŸ"""
        self.indent_level = max(0, self.indent_level - 1)
        logger.exit_section(section_name)
    
    def log_tensor_info(self, tensor, name: str):
        """è®°å½•å¼ é‡ä¿¡æ¯ï¼ˆå…¼å®¹æ¥å£ï¼‰"""
        if self.enabled:
            logger.log_tensor_info(tensor, name)
    
    def log_model_info(self, model, name: str = "Model"):
        """è®°å½•æ¨¡å‹ä¿¡æ¯ï¼ˆå…¼å®¹æ¥å£ï¼‰"""
        if self.enabled:
            logger.log_model_info(model, name)
    
    def print_tensor_info(self, tensor, name: str):
        """æ‰“å°å¼ é‡ä¿¡æ¯ï¼ˆå…¼å®¹æ¥å£ï¼‰"""
        if self.enabled:
            logger.log_tensor_info(tensor, name)
    
    def print_model_info(self, model, name: str = "Model"):
        """æ‰“å°æ¨¡å‹ä¿¡æ¯ï¼ˆå…¼å®¹æ¥å£ï¼‰"""
        if self.enabled:
            logger.log_model_info(model, name)


# Loggerå®ä¾‹ç¼“å­˜ï¼Œé¿å…é‡å¤åˆ›å»º
_logger_cache = {}

def get_logger(name: str = None) -> ConfigurableLogger:
    """è·å–loggerå®ä¾‹ï¼Œæ”¯æŒç¼“å­˜ä»¥ä¿æŒçŠ¶æ€ä¸€è‡´æ€§"""
    if name is None:
        return logger
    
    # ä½¿ç”¨ç¼“å­˜é¿å…é‡å¤åˆ›å»ºloggerå®ä¾‹
    if name not in _logger_cache:
        _logger_cache[name] = ConfigurableLogger(name, _log_level, _enable_console)
    
    return _logger_cache[name]