#!/usr/bin/env python3
"""
@defgroup group_net2net_subnetwork_analyzer Net2Net Subnetwork Analyzer
@ingroup core
Net2Net Subnetwork Analyzer module for NeuroExapt framework.

Net2Netå­ç½‘ç»œåˆ†æå™¨ - Net2Net Subnetwork Analyzer

ğŸ¯ æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä»æŒ‡å®šå±‚åˆ°è¾“å‡ºå±‚æå–å­ç½‘ç»œ
2. è¯„ä¼°å­ç½‘ç»œçš„å˜å¼‚æ½œåŠ›
3. é¢„æµ‹å˜å¼‚åçš„å‡†ç¡®ç‡æå‡ç©ºé—´
4. åˆ†æå¯è¡Œå‚æ•°ç©ºé—´åœ¨æ€»å‚æ•°ç©ºé—´ä¸­çš„å æ¯”
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Any, Set
from collections import OrderedDict, defaultdict
import copy
import logging

from .logging_utils import logger

class SubnetworkExtractor:
    """å­ç½‘ç»œæå–å™¨"""
    
    def __init__(self):
        self.extracted_subnetworks = {}
        self.layer_dependencies = {}
        
    def extract_subnetwork_from_layer(self, 
                                    model: nn.Module, 
                                    start_layer_name: str,
                                    include_start_layer: bool = True) -> Tuple[nn.Module, Dict[str, Any]]:
        """
        ä»æŒ‡å®šå±‚å¼€å§‹æå–åˆ°è¾“å‡ºå±‚çš„å­ç½‘ç»œ
        
        Args:
            model: åŸå§‹æ¨¡å‹
            start_layer_name: èµ·å§‹å±‚åç§°
            include_start_layer: æ˜¯å¦åŒ…å«èµ·å§‹å±‚
            
        Returns:
            å­ç½‘ç»œæ¨¡å‹å’Œæå–ä¿¡æ¯
        """
        logger.enter_section(f"æå–å­ç½‘ç»œ: {start_layer_name}")
        
        # 1. åˆ†ææ¨¡å‹ç»“æ„ï¼Œæ‰¾åˆ°æ‰€æœ‰å±‚çš„ä¾èµ–å…³ç³»
        layer_graph = self._build_layer_dependency_graph(model)
        
        # 2. ä»èµ·å§‹å±‚å¼€å§‹ï¼Œæ‰¾åˆ°æ‰€æœ‰åç»­å±‚
        target_layers = self._find_downstream_layers(layer_graph, start_layer_name, include_start_layer)
        
        logger.info(f"è¯†åˆ«å‡º{len(target_layers)}ä¸ªä¸‹æ¸¸å±‚")
        
        # 3. æ„å»ºå­ç½‘ç»œ
        subnetwork = self._build_subnetwork(model, target_layers, start_layer_name)
        
        # 4. åˆ†æå­ç½‘ç»œä¿¡æ¯
        subnetwork_info = self._analyze_subnetwork(subnetwork, target_layers, start_layer_name)
        
        logger.info(f"å­ç½‘ç»œæ„å»ºå®Œæˆ: {subnetwork_info['total_params']:,}å‚æ•°")
        logger.exit_section(f"æå–å­ç½‘ç»œ: {start_layer_name}")
        
        return subnetwork, subnetwork_info
    
    def _build_layer_dependency_graph(self, model: nn.Module) -> Dict[str, List[str]]:
        """æ„å»ºå±‚ä¾èµ–å…³ç³»å›¾"""
        layer_graph = defaultdict(list)
        named_modules = dict(model.named_modules())
        
        # åˆ†æResNeté£æ ¼çš„å‰å‘ä¼ æ’­ä¾èµ–
        for name, module in named_modules.items():
            if name == '':  # è·³è¿‡æ ¹æ¨¡å—
                continue
                
            # è§£æå±‚çš„é€»è¾‘ä½ç½®
            parts = name.split('.')
            
            if len(parts) >= 2:
                # å¯¹äºå±‚çº§ç»“æ„ï¼Œæ·»åŠ ä¾èµ–å…³ç³»
                parent_parts = parts[:-1]
                parent_name = '.'.join(parent_parts)
                
                if parent_name in named_modules:
                    layer_graph[parent_name].append(name)
        
        # æ·»åŠ ç‰¹æ®Šçš„ä¾èµ–å…³ç³»ï¼ˆåŸºäºResNetæ¶æ„ï¼‰
        self._add_resnet_dependencies(layer_graph, named_modules)
        
        return dict(layer_graph)
    
    def _add_resnet_dependencies(self, layer_graph: Dict[str, List[str]], named_modules: Dict[str, nn.Module]):
        """æ·»åŠ ResNetç‰¹å®šçš„ä¾èµ–å…³ç³»"""
        
        # ä¸»å¹²ä¾èµ–ï¼šconv1 -> feature_blocks -> classifier
        main_sequence = []
        
        # æŸ¥æ‰¾ä¸»è¦ç»„ä»¶
        if 'conv1' in named_modules:
            main_sequence.append('conv1')
        
        # æŸ¥æ‰¾feature blocks
        feature_blocks = []
        for name in named_modules.keys():
            if name.startswith('feature_block'):
                if '.' not in name[len('feature_block'):]:  # åªè¦é¡¶çº§feature_block
                    feature_blocks.append(name)
        
        feature_blocks.sort()  # æŒ‰åç§°æ’åº
        main_sequence.extend(feature_blocks)
        
        # æŸ¥æ‰¾åˆ†ç±»å™¨
        if 'classifier' in named_modules:
            main_sequence.append('classifier')
        
        # å»ºç«‹ä¸»å¹²ä¾èµ–
        for i in range(len(main_sequence) - 1):
            current = main_sequence[i]
            next_layer = main_sequence[i + 1]
            if current not in layer_graph:
                layer_graph[current] = []
            layer_graph[current].append(next_layer)
    
    def _find_downstream_layers(self, 
                               layer_graph: Dict[str, List[str]], 
                               start_layer: str, 
                               include_start: bool = True) -> Set[str]:
        """æ‰¾åˆ°æŒ‡å®šå±‚ä¹‹åçš„æ‰€æœ‰ä¸‹æ¸¸å±‚"""
        
        downstream_layers = set()
        
        if include_start:
            downstream_layers.add(start_layer)
        
        # BFSéå†æ‰¾åˆ°æ‰€æœ‰ä¸‹æ¸¸å±‚
        queue = [start_layer]
        visited = set([start_layer])
        
        while queue:
            current_layer = queue.pop(0)
            
            # æ·»åŠ å½“å‰å±‚çš„æ‰€æœ‰ä¸‹æ¸¸å±‚
            if current_layer in layer_graph:
                for next_layer in layer_graph[current_layer]:
                    if next_layer not in visited:
                        downstream_layers.add(next_layer)
                        queue.append(next_layer)
                        visited.add(next_layer)
        
        return downstream_layers
    
    def _build_subnetwork(self, 
                         model: nn.Module, 
                         target_layers: Set[str], 
                         start_layer_name: str) -> nn.Module:
        """æ„å»ºåŒ…å«ç›®æ ‡å±‚çš„å­ç½‘ç»œ"""
        
        # è·å–æ¨¡å‹çš„æ‰€æœ‰å‘½åæ¨¡å—
        named_modules = dict(model.named_modules())
        
        # åˆ›å»ºå­ç½‘ç»œæ¨¡å—å­—å…¸
        subnetwork_modules = OrderedDict()
        
        # æ·»åŠ ç›®æ ‡å±‚åˆ°å­ç½‘ç»œ
        for layer_name in sorted(target_layers):
            if layer_name in named_modules:
                subnetwork_modules[layer_name] = copy.deepcopy(named_modules[layer_name])
        
        # åˆ›å»ºåŠ¨æ€å­ç½‘ç»œç±»
        class DynamicSubnetwork(nn.Module):
            def __init__(self, modules_dict, start_layer):
                super().__init__()
                self.start_layer = start_layer
                self.modules_dict = nn.ModuleDict(modules_dict)
                
                # åˆ†æè¾“å…¥è¾“å‡ºç»´åº¦
                self._analyze_io_dims()
            
            def _analyze_io_dims(self):
                """åˆ†æè¾“å…¥è¾“å‡ºç»´åº¦"""
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ ¹æ®æ¨¡å‹ç»“æ„åŠ¨æ€åˆ†æ
                self.input_dim = None
                self.output_dim = None
                
                # æŸ¥æ‰¾è¾“å…¥å’Œè¾“å‡ºå±‚
                layer_names = list(self.modules_dict.keys())
                if layer_names:
                    first_layer = self.modules_dict[layer_names[0]]
                    last_layer = self.modules_dict[layer_names[-1]]
                    
                    # å°è¯•è·å–è¾“å…¥ç»´åº¦
                    if hasattr(first_layer, 'in_features'):
                        self.input_dim = first_layer.in_features
                    elif hasattr(first_layer, 'in_channels'):
                        self.input_dim = first_layer.in_channels
                    
                    # å°è¯•è·å–è¾“å‡ºç»´åº¦
                    if hasattr(last_layer, 'out_features'):
                        self.output_dim = last_layer.out_features
                    elif hasattr(last_layer, 'out_channels'):
                        self.output_dim = last_layer.out_channels
            
            def forward(self, x):
                """ç®€åŒ–çš„å‰å‘ä¼ æ’­"""
                # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…éœ€è¦æ ¹æ®å…·ä½“æ¶æ„å®ç°
                current = x
                
                for name, module in self.modules_dict.items():
                    try:
                        if isinstance(module, (nn.Linear, nn.Conv2d)):
                            current = module(current)
                        elif isinstance(module, nn.Sequential):
                            current = module(current)
                        else:
                            # å¯¹äºå…¶ä»–ç±»å‹çš„æ¨¡å—ï¼Œå°è¯•ç›´æ¥è°ƒç”¨
                            current = module(current)
                    except Exception as e:
                        logger.warning(f"å­ç½‘ç»œå‰å‘ä¼ æ’­åœ¨å±‚{name}å¤±è´¥: {e}")
                        break
                
                return current
        
        subnetwork = DynamicSubnetwork(subnetwork_modules, start_layer_name)
        return subnetwork
    
    def _analyze_subnetwork(self, 
                           subnetwork: nn.Module, 
                           target_layers: Set[str], 
                           start_layer_name: str) -> Dict[str, Any]:
        """åˆ†æå­ç½‘ç»œçš„ç‰¹æ€§"""
        
        total_params = sum(p.numel() for p in subnetwork.parameters())
        trainable_params = sum(p.numel() for p in subnetwork.parameters() if p.requires_grad)
        
        # åˆ†æå±‚ç±»å‹åˆ†å¸ƒ
        layer_types = defaultdict(int)
        for name, module in subnetwork.named_modules():
            if name:  # è·³è¿‡æ ¹æ¨¡å—
                layer_types[type(module).__name__] += 1
        
        return {
            'start_layer': start_layer_name,
            'layer_count': len(target_layers),
            'total_params': total_params,
            'trainable_params': trainable_params,
            'layer_types': dict(layer_types),
            'input_dim': getattr(subnetwork, 'input_dim', None),
            'output_dim': getattr(subnetwork, 'output_dim', None)
        }

class ParameterSpaceAnalyzer:
    """å‚æ•°ç©ºé—´åˆ†æå™¨"""
    
    def __init__(self):
        self.analysis_cache = {}
    
    def analyze_parameter_space_efficiency(self, 
                                         subnetwork: nn.Module,
                                         activations: torch.Tensor,
                                         gradients: torch.Tensor,
                                         targets: torch.Tensor) -> Dict[str, float]:
        """
        åˆ†æå‚æ•°ç©ºé—´æ•ˆç‡
        
        æ ¸å¿ƒæ€æƒ³ï¼šè¯„ä¼°å½“å‰å‚æ•°åœ¨è§£å†³åˆ†ç±»ä»»åŠ¡æ—¶çš„æ•ˆç‡
        """
        logger.enter_section("å‚æ•°ç©ºé—´æ•ˆç‡åˆ†æ")
        
        try:
            # 1. è®¡ç®—å‚æ•°åˆ©ç”¨ç‡
            param_utilization = self._compute_parameter_utilization(subnetwork, gradients)
            
            # 2. åˆ†æè¡¨ç¤ºèƒ½åŠ›
            representation_capacity = self._compute_representation_capacity(activations, targets)
            
            # 3. è¯„ä¼°å†—ä½™åº¦
            redundancy_ratio = self._compute_parameter_redundancy(subnetwork, activations)
            
            # 4. è®¡ç®—å¯è¡Œå‚æ•°ç©ºé—´å æ¯”
            feasible_space_ratio = self._estimate_feasible_parameter_space(
                subnetwork, activations, targets
            )
            
            # 5. ç»¼åˆæ•ˆç‡è¯„åˆ†
            overall_efficiency = (
                0.3 * param_utilization +
                0.3 * representation_capacity +
                0.2 * (1.0 - redundancy_ratio) +  # å†—ä½™åº¦è¶Šä½è¶Šå¥½
                0.2 * feasible_space_ratio
            )
            
            analysis_result = {
                'parameter_utilization': param_utilization,
                'representation_capacity': representation_capacity,
                'redundancy_ratio': redundancy_ratio,
                'feasible_space_ratio': feasible_space_ratio,
                'overall_efficiency': overall_efficiency
            }
            
            logger.info(f"å‚æ•°ç©ºé—´åˆ†æå®Œæˆ: æ•´ä½“æ•ˆç‡={overall_efficiency:.3f}")
            logger.exit_section("å‚æ•°ç©ºé—´æ•ˆç‡åˆ†æ")
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"å‚æ•°ç©ºé—´åˆ†æå¤±è´¥: {e}")
            logger.exit_section("å‚æ•°ç©ºé—´æ•ˆç‡åˆ†æ")
            return {
                'parameter_utilization': 0.0,
                'representation_capacity': 0.0,
                'redundancy_ratio': 1.0,
                'feasible_space_ratio': 0.0,
                'overall_efficiency': 0.0
            }
    
    def _compute_parameter_utilization(self, subnetwork: nn.Module, gradients: torch.Tensor) -> float:
        """è®¡ç®—å‚æ•°åˆ©ç”¨ç‡ - æœ‰å¤šå°‘å‚æ•°åœ¨ç§¯æå‚ä¸å­¦ä¹ """
        
        total_params = sum(p.numel() for p in subnetwork.parameters())
        if total_params == 0:
            return 0.0
        
        # è®¡ç®—æœ‰æ•ˆæ¢¯åº¦çš„å‚æ•°æ•°é‡
        active_params = 0
        for param in subnetwork.parameters():
            if param.grad is not None:
                # æ¢¯åº¦æ˜¾è‘—éé›¶çš„å‚æ•°è¢«è®¤ä¸ºæ˜¯æ´»è·ƒçš„
                significant_grads = torch.abs(param.grad) > 1e-6
                active_params += significant_grads.sum().item()
        
        utilization = active_params / total_params
        return min(utilization, 1.0)
    
    def _compute_representation_capacity(self, activations: torch.Tensor, targets: torch.Tensor) -> float:
        """è®¡ç®—è¡¨ç¤ºèƒ½åŠ› - ç½‘ç»œèƒ½å¤šå¥½åœ°åŒºåˆ†ä¸åŒç±»åˆ«"""
        
        try:
            # è®¡ç®—ç±»é—´åˆ†ç¦»åº¦
            if len(activations.shape) > 2:
                # å¯¹äºå·ç§¯å±‚è¾“å‡ºï¼Œå–å¹³å‡æ± åŒ–
                activations_flat = F.adaptive_avg_pool2d(activations, (1, 1)).flatten(1)
            else:
                activations_flat = activations
            
            # è®¡ç®—ä¸åŒç±»åˆ«çš„æ¿€æ´»åˆ†å¸ƒ
            unique_targets = torch.unique(targets)
            if len(unique_targets) < 2:
                return 0.0
            
            class_centers = []
            for target_class in unique_targets:
                mask = targets == target_class
                if mask.sum() > 0:
                    class_center = activations_flat[mask].mean(dim=0)
                    class_centers.append(class_center)
            
            if len(class_centers) < 2:
                return 0.0
            
            # è®¡ç®—ç±»é—´è·ç¦»
            class_centers = torch.stack(class_centers)
            distances = torch.pdist(class_centers)
            avg_inter_class_distance = distances.mean().item()
            
            # è®¡ç®—ç±»å†…æ–¹å·®
            intra_class_variance = 0.0
            for target_class in unique_targets:
                mask = targets == target_class
                if mask.sum() > 1:
                    class_activations = activations_flat[mask]
                    class_center = class_activations.mean(dim=0)
                    variance = ((class_activations - class_center) ** 2).mean().item()
                    intra_class_variance += variance
            
            intra_class_variance /= len(unique_targets)
            
            # è¡¨ç¤ºèƒ½åŠ› = ç±»é—´è·ç¦» / ç±»å†…æ–¹å·®
            if intra_class_variance > 0:
                representation_capacity = avg_inter_class_distance / (intra_class_variance + 1e-8)
            else:
                representation_capacity = avg_inter_class_distance
            
            # å½’ä¸€åŒ–åˆ°[0, 1]
            return min(representation_capacity / 10.0, 1.0)
            
        except Exception as e:
            logger.warning(f"è¡¨ç¤ºèƒ½åŠ›è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _compute_parameter_redundancy(self, subnetwork: nn.Module, activations: torch.Tensor) -> float:
        """è®¡ç®—å‚æ•°å†—ä½™åº¦"""
        
        try:
            redundancy_scores = []
            
            for name, module in subnetwork.named_modules():
                if isinstance(module, (nn.Linear, nn.Conv2d)):
                    weight = module.weight.data
                    
                    # è®¡ç®—æƒé‡çŸ©é˜µçš„æœ‰æ•ˆç§©
                    if len(weight.shape) == 2:  # Linear layer
                        rank = torch.matrix_rank(weight).item()
                        max_rank = min(weight.shape[0], weight.shape[1])
                    else:  # Conv2d layer
                        # å°†å·ç§¯æƒé‡é‡å¡‘ä¸º2DçŸ©é˜µ
                        weight_2d = weight.view(weight.shape[0], -1)
                        rank = torch.matrix_rank(weight_2d).item()
                        max_rank = min(weight_2d.shape[0], weight_2d.shape[1])
                    
                    if max_rank > 0:
                        rank_ratio = rank / max_rank
                        redundancy = 1.0 - rank_ratio  # ç§©è¶Šä½ï¼Œå†—ä½™åº¦è¶Šé«˜
                        redundancy_scores.append(redundancy)
            
            if redundancy_scores:
                return np.mean(redundancy_scores)
            else:
                return 0.0
                
        except Exception as e:
            logger.warning(f"å‚æ•°å†—ä½™åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _estimate_feasible_parameter_space(self, 
                                         subnetwork: nn.Module, 
                                         activations: torch.Tensor, 
                                         targets: torch.Tensor) -> float:
        """
        ä¼°è®¡å¯è¡Œå‚æ•°ç©ºé—´å æ¯”
        
        æ ¸å¿ƒæ€æƒ³ï¼šåœ¨å½“å‰å‚æ•°é™„è¿‘ï¼Œæœ‰å¤šå¤§æ¯”ä¾‹çš„å‚æ•°å˜åŒ–èƒ½å¤Ÿç»´æŒæˆ–æ”¹å–„æ€§èƒ½
        """
        
        try:
            # ä½¿ç”¨é‡‡æ ·æ–¹æ³•ä¼°è®¡å¯è¡Œå‚æ•°ç©ºé—´
            original_params = [p.data.clone() for p in subnetwork.parameters()]
            
            # è®¡ç®—åŸå§‹æ€§èƒ½
            with torch.no_grad():
                original_output = subnetwork(activations)
                if len(original_output.shape) > 1 and original_output.shape[1] > 1:
                    original_loss = F.cross_entropy(original_output, targets, reduction='mean')
                else:
                    original_loss = F.mse_loss(original_output.squeeze(), targets.float())
            
            # é‡‡æ ·æµ‹è¯•
            feasible_count = 0
            total_samples = 50  # å‡å°‘é‡‡æ ·æ•°é‡ä»¥æé«˜é€Ÿåº¦
            noise_scale = 0.01  # å°æ‰°åŠ¨
            
            for _ in range(total_samples):
                # æ·»åŠ éšæœºæ‰°åŠ¨
                for param in subnetwork.parameters():
                    noise = torch.randn_like(param.data) * noise_scale
                    param.data.add_(noise)
                
                # æµ‹è¯•æ‰°åŠ¨åçš„æ€§èƒ½
                try:
                    with torch.no_grad():
                        perturbed_output = subnetwork(activations)
                        if len(perturbed_output.shape) > 1 and perturbed_output.shape[1] > 1:
                            perturbed_loss = F.cross_entropy(perturbed_output, targets, reduction='mean')
                        else:
                            perturbed_loss = F.mse_loss(perturbed_output.squeeze(), targets.float())
                    
                    # å¦‚æœæŸå¤±æ²¡æœ‰æ˜¾è‘—å¢åŠ ï¼Œè®¤ä¸ºæ˜¯å¯è¡Œçš„
                    if perturbed_loss <= original_loss * 1.1:  # å…è®¸10%çš„æ€§èƒ½ä¸‹é™
                        feasible_count += 1
                        
                except Exception:
                    pass  # æ‰°åŠ¨å¯¼è‡´çš„é”™è¯¯è§†ä¸ºä¸å¯è¡Œ
                
                # æ¢å¤åŸå§‹å‚æ•°
                for param, original_param in zip(subnetwork.parameters(), original_params):
                    param.data.copy_(original_param)
            
            feasible_ratio = feasible_count / total_samples
            return feasible_ratio
            
        except Exception as e:
            logger.warning(f"å¯è¡Œå‚æ•°ç©ºé—´ä¼°è®¡å¤±è´¥: {e}")
            return 0.0

class MutationPotentialPredictor:
    """å˜å¼‚æ½œåŠ›é¢„æµ‹å™¨"""
    
    def __init__(self):
        self.predictor_cache = {}
    
    def predict_mutation_potential(self, 
                                 subnetwork: nn.Module,
                                 subnetwork_info: Dict[str, Any],
                                 parameter_space_analysis: Dict[str, float],
                                 current_accuracy: float) -> Dict[str, Any]:
        """
        é¢„æµ‹å˜å¼‚æ½œåŠ›å’Œå¯èƒ½çš„å‡†ç¡®ç‡æå‡
        
        Args:
            subnetwork: æå–çš„å­ç½‘ç»œ
            subnetwork_info: å­ç½‘ç»œåˆ†æä¿¡æ¯
            parameter_space_analysis: å‚æ•°ç©ºé—´åˆ†æç»“æœ
            current_accuracy: å½“å‰å‡†ç¡®ç‡
            
        Returns:
            å˜å¼‚æ½œåŠ›é¢„æµ‹ç»“æœ
        """
        logger.enter_section("å˜å¼‚æ½œåŠ›é¢„æµ‹")
        
        try:
            # 1. åŸºäºå‚æ•°ç©ºé—´æ•ˆç‡é¢„æµ‹æå‡ç©ºé—´
            efficiency = parameter_space_analysis['overall_efficiency']
            improvement_potential = self._compute_improvement_potential(efficiency, current_accuracy)
            
            # 2. é¢„æµ‹ä¸åŒå˜å¼‚ç­–ç•¥çš„æ•ˆæœ
            strategy_predictions = self._predict_strategy_effects(
                subnetwork_info, parameter_space_analysis, current_accuracy
            )
            
            # 3. ä¼°è®¡æœ€ä¼˜å˜å¼‚å¼ºåº¦
            optimal_mutation_strength = self._estimate_optimal_mutation_strength(
                parameter_space_analysis, improvement_potential
            )
            
            # 4. é£é™©è¯„ä¼°
            risk_assessment = self._assess_mutation_risks(
                subnetwork_info, parameter_space_analysis
            )
            
            prediction_result = {
                'improvement_potential': improvement_potential,
                'strategy_predictions': strategy_predictions,
                'optimal_mutation_strength': optimal_mutation_strength,
                'risk_assessment': risk_assessment,
                'confidence': self._compute_prediction_confidence(parameter_space_analysis)
            }
            
            logger.info(f"å˜å¼‚æ½œåŠ›é¢„æµ‹å®Œæˆ: æ”¹è¿›æ½œåŠ›={improvement_potential:.3f}")
            logger.exit_section("å˜å¼‚æ½œåŠ›é¢„æµ‹")
            
            return prediction_result
            
        except Exception as e:
            logger.error(f"å˜å¼‚æ½œåŠ›é¢„æµ‹å¤±è´¥: {e}")
            logger.exit_section("å˜å¼‚æ½œåŠ›é¢„æµ‹")
            return {
                'improvement_potential': 0.0,
                'strategy_predictions': {},
                'optimal_mutation_strength': 0.1,
                'risk_assessment': {'overall_risk': 1.0},
                'confidence': 0.0
            }
    
    def _compute_improvement_potential(self, efficiency: float, current_accuracy: float) -> float:
        """åŸºäºæ•ˆç‡å’Œå½“å‰å‡†ç¡®ç‡è®¡ç®—æ”¹è¿›æ½œåŠ›"""
        
        # æ•ˆç‡è¶Šä½ï¼Œæ”¹è¿›ç©ºé—´è¶Šå¤§
        efficiency_factor = 1.0 - efficiency
        
        # å‡†ç¡®ç‡è¶Šé«˜ï¼Œæ”¹è¿›è¶Šå›°éš¾
        if current_accuracy > 0.95:
            accuracy_factor = 0.1
        elif current_accuracy > 0.90:
            accuracy_factor = 0.3
        elif current_accuracy > 0.80:
            accuracy_factor = 0.6
        else:
            accuracy_factor = 1.0
        
        # ç»¼åˆæ”¹è¿›æ½œåŠ›
        improvement_potential = efficiency_factor * accuracy_factor
        
        return min(improvement_potential, 1.0)
    
    def _predict_strategy_effects(self, 
                                subnetwork_info: Dict[str, Any],
                                parameter_space_analysis: Dict[str, float],
                                current_accuracy: float) -> Dict[str, Dict[str, float]]:
        """é¢„æµ‹ä¸åŒå˜å¼‚ç­–ç•¥çš„æ•ˆæœ"""
        
        strategies = {
            'width_expansion': self._predict_width_expansion_effect(
                subnetwork_info, parameter_space_analysis
            ),
            'depth_increase': self._predict_depth_increase_effect(
                subnetwork_info, parameter_space_analysis
            ),
            'parallel_division': self._predict_parallel_division_effect(
                subnetwork_info, parameter_space_analysis
            ),
            'hybrid_mutation': self._predict_hybrid_mutation_effect(
                subnetwork_info, parameter_space_analysis
            )
        }
        
        return strategies
    
    def _predict_width_expansion_effect(self, 
                                      subnetwork_info: Dict[str, Any],
                                      parameter_space_analysis: Dict[str, float]) -> Dict[str, float]:
        """é¢„æµ‹å®½åº¦æ‰©å±•æ•ˆæœ"""
        
        redundancy = parameter_space_analysis['redundancy_ratio']
        utilization = parameter_space_analysis['parameter_utilization']
        
        # å†—ä½™åº¦ä½ä¸”åˆ©ç”¨ç‡é«˜çš„å±‚é€‚åˆå®½åº¦æ‰©å±•
        expansion_benefit = (1.0 - redundancy) * utilization
        
        return {
            'expected_accuracy_gain': expansion_benefit * 0.02,  # æœ€å¤š2%çš„æå‡
            'parameter_cost': 0.3,  # ç›¸å¯¹å‚æ•°å¢é•¿
            'implementation_difficulty': 0.2,  # å®ç°éš¾åº¦
            'stability_risk': 0.1  # ç¨³å®šæ€§é£é™©
        }
    
    def _predict_depth_increase_effect(self, 
                                     subnetwork_info: Dict[str, Any],
                                     parameter_space_analysis: Dict[str, float]) -> Dict[str, float]:
        """é¢„æµ‹æ·±åº¦å¢åŠ æ•ˆæœ"""
        
        representation_capacity = parameter_space_analysis['representation_capacity']
        
        # è¡¨ç¤ºèƒ½åŠ›ä¸è¶³çš„å­ç½‘ç»œé€‚åˆå¢åŠ æ·±åº¦
        depth_benefit = 1.0 - representation_capacity
        
        return {
            'expected_accuracy_gain': depth_benefit * 0.015,  # æœ€å¤š1.5%çš„æå‡
            'parameter_cost': 0.5,  # è¾ƒé«˜çš„å‚æ•°å¢é•¿
            'implementation_difficulty': 0.4,  # ä¸­ç­‰å®ç°éš¾åº¦
            'stability_risk': 0.3  # ä¸­ç­‰ç¨³å®šæ€§é£é™©
        }
    
    def _predict_parallel_division_effect(self, 
                                        subnetwork_info: Dict[str, Any],
                                        parameter_space_analysis: Dict[str, float]) -> Dict[str, float]:
        """é¢„æµ‹å¹¶è¡Œåˆ†è£‚æ•ˆæœ"""
        
        feasible_space = parameter_space_analysis['feasible_space_ratio']
        
        # å¯è¡Œç©ºé—´å¤§çš„å­ç½‘ç»œé€‚åˆå¹¶è¡Œåˆ†è£‚
        parallel_benefit = feasible_space
        
        return {
            'expected_accuracy_gain': parallel_benefit * 0.025,  # æœ€å¤š2.5%çš„æå‡
            'parameter_cost': 0.4,  # ä¸­ç­‰å‚æ•°å¢é•¿
            'implementation_difficulty': 0.3,  # ä¸­ç­‰å®ç°éš¾åº¦
            'stability_risk': 0.2  # è¾ƒä½ç¨³å®šæ€§é£é™©
        }
    
    def _predict_hybrid_mutation_effect(self, 
                                      subnetwork_info: Dict[str, Any],
                                      parameter_space_analysis: Dict[str, float]) -> Dict[str, float]:
        """é¢„æµ‹æ··åˆå˜å¼‚æ•ˆæœ"""
        
        overall_efficiency = parameter_space_analysis['overall_efficiency']
        
        # æ•ˆç‡ä½çš„å­ç½‘ç»œé€‚åˆæ··åˆå˜å¼‚
        hybrid_benefit = (1.0 - overall_efficiency) * 1.2  # æ··åˆç­–ç•¥çš„åŠ æˆ
        
        return {
            'expected_accuracy_gain': min(hybrid_benefit * 0.03, 0.04),  # æœ€å¤š4%çš„æå‡
            'parameter_cost': 0.6,  # è¾ƒé«˜å‚æ•°å¢é•¿
            'implementation_difficulty': 0.6,  # è¾ƒé«˜å®ç°éš¾åº¦
            'stability_risk': 0.4  # ä¸­ç­‰ç¨³å®šæ€§é£é™©
        }
    
    def _estimate_optimal_mutation_strength(self, 
                                          parameter_space_analysis: Dict[str, float],
                                          improvement_potential: float) -> float:
        """ä¼°è®¡æœ€ä¼˜å˜å¼‚å¼ºåº¦"""
        
        efficiency = parameter_space_analysis['overall_efficiency']
        feasible_space = parameter_space_analysis['feasible_space_ratio']
        
        # åŸºäºæ•ˆç‡å’Œå¯è¡Œç©ºé—´ç¡®å®šå˜å¼‚å¼ºåº¦
        base_strength = 0.1  # åŸºç¡€å˜å¼‚å¼ºåº¦
        
        # æ•ˆç‡ä½çš„å­ç½‘ç»œå¯ä»¥æ‰¿å—æ›´å¼ºçš„å˜å¼‚
        efficiency_factor = (1.0 - efficiency) * 2.0
        
        # å¯è¡Œç©ºé—´å¤§çš„å­ç½‘ç»œå¯ä»¥æ‰¿å—æ›´å¼ºçš„å˜å¼‚
        feasible_factor = feasible_space * 1.5
        
        optimal_strength = base_strength + min(efficiency_factor + feasible_factor, 0.8)
        
        return min(optimal_strength, 1.0)
    
    def _assess_mutation_risks(self, 
                             subnetwork_info: Dict[str, Any],
                             parameter_space_analysis: Dict[str, float]) -> Dict[str, float]:
        """è¯„ä¼°å˜å¼‚é£é™©"""
        
        # å‚æ•°æ•°é‡é£é™©
        param_count = subnetwork_info['total_params']
        param_risk = min(param_count / 1e6, 1.0)  # å‚æ•°è¶Šå¤šé£é™©è¶Šé«˜
        
        # æ•ˆç‡é£é™©
        efficiency = parameter_space_analysis['overall_efficiency']
        efficiency_risk = efficiency  # æ•ˆç‡é«˜çš„ç³»ç»Ÿå˜å¼‚é£é™©é«˜
        
        # å¯è¡Œç©ºé—´é£é™©
        feasible_space = parameter_space_analysis['feasible_space_ratio']
        space_risk = 1.0 - feasible_space  # å¯è¡Œç©ºé—´å°é£é™©é«˜
        
        # ç»¼åˆé£é™©
        overall_risk = (param_risk + efficiency_risk + space_risk) / 3.0
        
        return {
            'parameter_risk': param_risk,
            'efficiency_risk': efficiency_risk,
            'space_risk': space_risk,
            'overall_risk': overall_risk
        }
    
    def _compute_prediction_confidence(self, parameter_space_analysis: Dict[str, float]) -> float:
        """è®¡ç®—é¢„æµ‹ç½®ä¿¡åº¦"""
        
        # åŸºäºåˆ†æç»“æœçš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§è®¡ç®—ç½®ä¿¡åº¦
        analysis_completeness = len([v for v in parameter_space_analysis.values() if v > 0]) / len(parameter_space_analysis)
        
        # ç»“æœçš„ä¸€è‡´æ€§æ£€æŸ¥
        efficiency = parameter_space_analysis['overall_efficiency']
        redundancy = parameter_space_analysis['redundancy_ratio']
        
        # æ•ˆç‡å’Œå†—ä½™åº¦åº”è¯¥è´Ÿç›¸å…³
        consistency = 1.0 - abs(efficiency + redundancy - 1.0)
        
        confidence = (analysis_completeness + consistency) / 2.0
        
        return min(confidence, 1.0)

class Net2NetSubnetworkAnalyzer:
    """Net2Netå­ç½‘ç»œåˆ†æå™¨ä¸»ç±»"""
    
    def __init__(self):
        self.extractor = SubnetworkExtractor()
        self.param_analyzer = ParameterSpaceAnalyzer()
        self.predictor = MutationPotentialPredictor()
    
    def analyze_layer_mutation_potential(self, 
                                       model: nn.Module,
                                       layer_name: str,
                                       activations: Dict[str, torch.Tensor],
                                       gradients: Dict[str, torch.Tensor],
                                       targets: torch.Tensor,
                                       current_accuracy: float) -> Dict[str, Any]:
        """
        åˆ†ææŒ‡å®šå±‚çš„å˜å¼‚æ½œåŠ›
        
        Args:
            model: å®Œæ•´æ¨¡å‹
            layer_name: ç›®æ ‡å±‚åç§°
            activations: æ¿€æ´»å€¼å­—å…¸
            gradients: æ¢¯åº¦å­—å…¸
            targets: çœŸå®æ ‡ç­¾
            current_accuracy: å½“å‰å‡†ç¡®ç‡
            
        Returns:
            å®Œæ•´çš„å˜å¼‚æ½œåŠ›åˆ†æç»“æœ
        """
        logger.enter_section(f"Net2Netåˆ†æ: {layer_name}")
        
        try:
            # 1. æå–å­ç½‘ç»œ
            subnetwork, subnetwork_info = self.extractor.extract_subnetwork_from_layer(
                model, layer_name
            )
            
            # 2. è·å–è¯¥å±‚çš„æ¿€æ´»å’Œæ¢¯åº¦
            if layer_name in activations and layer_name in gradients:
                layer_activation = activations[layer_name]
                layer_gradient = gradients[layer_name]
            else:
                logger.warning(f"å±‚{layer_name}ç¼ºå°‘æ¿€æ´»å€¼æˆ–æ¢¯åº¦ä¿¡æ¯")
                layer_activation = torch.randn(32, 64)  # é»˜è®¤å€¼
                layer_gradient = torch.randn(32, 64)
            
            # 3. åˆ†æå‚æ•°ç©ºé—´æ•ˆç‡
            param_space_analysis = self.param_analyzer.analyze_parameter_space_efficiency(
                subnetwork, layer_activation, layer_gradient, targets
            )
            
            # 4. é¢„æµ‹å˜å¼‚æ½œåŠ›
            mutation_prediction = self.predictor.predict_mutation_potential(
                subnetwork, subnetwork_info, param_space_analysis, current_accuracy
            )
            
            # 5. ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
            analysis_result = {
                'layer_name': layer_name,
                'subnetwork_info': subnetwork_info,
                'parameter_space_analysis': param_space_analysis,
                'mutation_prediction': mutation_prediction,
                'recommendation': self._generate_recommendation(
                    layer_name, param_space_analysis, mutation_prediction
                )
            }
            
            logger.success(f"Net2Netåˆ†æå®Œæˆ: {layer_name}")
            logger.exit_section(f"Net2Netåˆ†æ: {layer_name}")
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"Net2Netåˆ†æå¤±è´¥: {layer_name} - {e}")
            logger.exit_section(f"Net2Netåˆ†æ: {layer_name}")
            return {
                'layer_name': layer_name,
                'error': str(e),
                'recommendation': {'action': 'skip', 'reason': f'åˆ†æå¤±è´¥: {e}'}
            }
    
    def _generate_recommendation(self, 
                               layer_name: str,
                               param_space_analysis: Dict[str, float],
                               mutation_prediction: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆå˜å¼‚å»ºè®®"""
        
        improvement_potential = mutation_prediction['improvement_potential']
        risk_assessment = mutation_prediction['risk_assessment']
        strategy_predictions = mutation_prediction['strategy_predictions']
        
        # é€‰æ‹©æœ€ä½³ç­–ç•¥
        best_strategy = None
        best_score = -1.0
        
        for strategy_name, strategy_info in strategy_predictions.items():
            # ç»¼åˆè¯„åˆ†ï¼šæœŸæœ›æ”¶ç›Š - é£é™© - æˆæœ¬
            score = (
                strategy_info['expected_accuracy_gain'] * 2.0 -
                strategy_info['stability_risk'] -
                strategy_info['parameter_cost'] * 0.5
            )
            
            if score > best_score:
                best_score = score
                best_strategy = strategy_name
        
        # ç”Ÿæˆå»ºè®®
        if improvement_potential > 0.3 and risk_assessment['overall_risk'] < 0.6:
            action = 'mutate'
            priority = 'high' if improvement_potential > 0.6 else 'medium'
        elif improvement_potential > 0.1 and risk_assessment['overall_risk'] < 0.8:
            action = 'consider'
            priority = 'low'
        else:
            action = 'skip'
            priority = 'none'
        
        return {
            'action': action,
            'priority': priority,
            'recommended_strategy': best_strategy,
            'expected_gain': strategy_predictions.get(best_strategy, {}).get('expected_accuracy_gain', 0.0),
            'risk_level': risk_assessment['overall_risk'],
            'reason': f"æ”¹è¿›æ½œåŠ›={improvement_potential:.3f}, é£é™©={risk_assessment['overall_risk']:.3f}"
        }