#!/usr/bin/env python3
"""
Net2Netå­ç½‘ç»œåˆ†æå™¨ - è§£å†³å¾ªç¯ä¾èµ–é—®é¢˜

æ ¸å¿ƒæ€æƒ³ï¼šåˆ†æç½‘ç»œä¸­çš„ä¿¡æ¯æµæ¼ç‚¹å’Œç“¶é¢ˆï¼ŒæŒ‡å¯¼åŠ¨æ€æ¶æ„å˜å¼‚

ä¸»è¦åŠŸèƒ½ï¼š
1. å­ç½‘ç»œç‰¹å¾æå–å’Œåˆ†æ
2. ä¿¡æ¯æµç“¶é¢ˆæ£€æµ‹å’Œé‡åŒ–  
3. å‚æ•°ç©ºé—´å¯†åº¦åˆ†æ
4. å˜å¼‚æ½œåŠ›é¢„æµ‹å’Œæ”¶ç›Šä¼°è®¡
5. å˜å¼‚ç­–ç•¥æ¨èå’Œæ‰§è¡ŒæŒ‡å¯¼
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import logging
from typing import Dict, List, Any, Tuple, Optional, Union
from collections import OrderedDict, defaultdict
import copy

from .logging_utils import logger
# å»¶è¿Ÿå¯¼å…¥ä»¥é¿å…å¾ªç¯ä¾èµ–
# from .bayesian_prediction import BayesianMutationBenefitPredictor
from .layer_analysis import InformationFlowAnalyzer, InformationLeakDetector


class SubnetworkExtractor:
    """å­ç½‘ç»œæå–å™¨ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰"""
    
    def __init__(self):
        self.extracted_subnets = {}
        
    def extract_key_subnetworks(self, model: nn.Module, context: Dict[str, Any]) -> Dict[str, Any]:
        """æå–å…³é”®å­ç½‘ç»œ"""
        subnets = {}
        
        # åŸºäºå±‚ç±»å‹åˆ†ç»„
        layer_groups = self._group_layers_by_type(model)
        
        # åŸºäºä¿¡æ¯æµåˆ†ç»„
        flow_groups = self._group_by_information_flow(model, context)
        
        # åŸºäºå˜å¼‚æ½œåŠ›åˆ†ç»„
        mutation_groups = self._group_by_mutation_potential(model, context)
        
        subnets['layer_type_groups'] = layer_groups
        subnets['information_flow_groups'] = flow_groups
        subnets['mutation_potential_groups'] = mutation_groups
        
        return subnets
    
    def _group_layers_by_type(self, model: nn.Module) -> Dict[str, List[str]]:
        """æŒ‰å±‚ç±»å‹åˆ†ç»„"""
        groups = defaultdict(list)
        for name, module in model.named_modules():
            if name:  # è·³è¿‡root module
                module_type = type(module).__name__
                groups[module_type].append(name)
        return dict(groups)
    
    def _group_by_information_flow(self, model: nn.Module, context: Dict[str, Any]) -> Dict[str, List[str]]:
        """åŸºäºä¿¡æ¯æµç‰¹å¾åˆ†ç»„"""
        # ç®€åŒ–å®ç°
        groups = {'high_flow': [], 'medium_flow': [], 'low_flow': []}
        
        for name, module in model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)):
                # ç®€å•å¯å‘å¼åˆ†ç»„
                if hasattr(module, 'out_channels'):
                    if module.out_channels > 256:
                        groups['high_flow'].append(name)
                    elif module.out_channels > 64:
                        groups['medium_flow'].append(name)
                    else:
                        groups['low_flow'].append(name)
        
        return groups
    
    def _group_by_mutation_potential(self, model: nn.Module, context: Dict[str, Any]) -> Dict[str, List[str]]:
        """åŸºäºå˜å¼‚æ½œåŠ›åˆ†ç»„"""
        groups = {'high_potential': [], 'medium_potential': [], 'low_potential': []}
        
        # ç®€åŒ–çš„å¯å‘å¼åˆ†ç»„
        for name, module in model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)):
                param_count = sum(p.numel() for p in module.parameters())
                if param_count > 10000:
                    groups['high_potential'].append(name)
                elif param_count > 1000:
                    groups['medium_potential'].append(name)
                else:
                    groups['low_potential'].append(name)
        
        return groups


class ParameterSpaceAnalyzer:
    """å‚æ•°ç©ºé—´åˆ†æå™¨ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰"""
    
    def __init__(self):
        self.density_cache = {}
    
    def analyze_parameter_space_density(self, 
                                      layer_name: str,
                                      layer_module: nn.Module,
                                      context: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå‚æ•°ç©ºé—´å¯†åº¦"""
        
        # è·å–å‚æ•°ç»Ÿè®¡
        param_stats = self._get_parameter_statistics(layer_module)
        
        # ä¼°è®¡å¯†åº¦åˆ†å¸ƒ
        density_info = self._estimate_density_distribution(param_stats, context)
        
        # åˆ†æé«˜å‡†ç¡®ç‡åŒºåŸŸ
        high_acc_regions = self._analyze_high_accuracy_regions(density_info, context)
        
        return {
            'layer_name': layer_name,
            'parameter_count': param_stats['total_params'],
            'parameter_distribution': param_stats['distribution'],
            'density_estimation': density_info,
            'high_accuracy_regions': high_acc_regions,
            'mutation_readiness': self._calculate_mutation_readiness(high_acc_regions)
        }
    
    def _get_parameter_statistics(self, module: nn.Module) -> Dict[str, Any]:
        """è·å–å‚æ•°ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_params': sum(p.numel() for p in module.parameters()),
            'trainable_params': sum(p.numel() for p in module.parameters() if p.requires_grad),
            'distribution': {}
        }
        
        for name, param in module.named_parameters():
            if param.requires_grad:
                stats['distribution'][name] = {
                    'shape': list(param.shape),
                    'mean': float(param.data.mean()),
                    'std': float(param.data.std()),
                    'min': float(param.data.min()),
                    'max': float(param.data.max())
                }
        
        return stats
    
    def _estimate_density_distribution(self, param_stats: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """ä¼°è®¡å¯†åº¦åˆ†å¸ƒ"""
        return {
            'estimated_density': 0.7,  # ç®€åŒ–å®ç°
            'confidence': 0.8,
            'distribution_type': 'gaussian_mixture'
        }
    
    def _analyze_high_accuracy_regions(self, density_info: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æé«˜å‡†ç¡®ç‡åŒºåŸŸ"""
        return {
            'region_proportion': 0.3,  # ç®€åŒ–å®ç°
            'peak_density': 0.9,
            'connectivity': 0.6
        }
    
    def _calculate_mutation_readiness(self, high_acc_regions: Dict[str, Any]) -> float:
        """è®¡ç®—å˜å¼‚å‡†å¤‡åº¦"""
        return 1.0 - high_acc_regions['region_proportion']


class MutationPotentialPredictor:
    """å˜å¼‚æ½œåŠ›é¢„æµ‹å™¨ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰"""
    
    def __init__(self):
        self.prediction_cache = {}
    
    def predict_mutation_potential(self, 
                                 layer_analysis: Dict[str, Any],
                                 context: Dict[str, Any]) -> Dict[str, Any]:
        """é¢„æµ‹å˜å¼‚æ½œåŠ›"""
        
        # åŸºäºå‚æ•°ç©ºé—´åˆ†æé¢„æµ‹
        param_potential = self._predict_from_parameter_space(layer_analysis, context)
        
        # åŸºäºä¿¡æ¯æµåˆ†æé¢„æµ‹
        flow_potential = self._predict_from_information_flow(layer_analysis, context)
        
        # ç»¼åˆé¢„æµ‹
        combined_potential = self._combine_predictions(param_potential, flow_potential)
        
        return {
            'parameter_space_potential': param_potential,
            'information_flow_potential': flow_potential,
            'combined_potential': combined_potential,
            'confidence': self._calculate_prediction_confidence(combined_potential),
            'recommended_mutations': self._recommend_mutations(combined_potential)
        }
    
    def _predict_from_parameter_space(self, analysis: Dict[str, Any], context: Dict[str, Any]) -> float:
        """åŸºäºå‚æ•°ç©ºé—´é¢„æµ‹æ½œåŠ›"""
        if 'mutation_readiness' in analysis:
            return analysis['mutation_readiness']
        return 0.5  # é»˜è®¤å€¼
    
    def _predict_from_information_flow(self, analysis: Dict[str, Any], context: Dict[str, Any]) -> float:
        """åŸºäºä¿¡æ¯æµé¢„æµ‹æ½œåŠ›"""
        # ç®€åŒ–å®ç°
        return 0.6
    
    def _combine_predictions(self, param_potential: float, flow_potential: float) -> float:
        """ç»„åˆé¢„æµ‹ç»“æœ"""
        return 0.6 * param_potential + 0.4 * flow_potential
    
    def _calculate_prediction_confidence(self, potential: float) -> float:
        """è®¡ç®—é¢„æµ‹ç½®ä¿¡åº¦"""
        return min(0.9, max(0.1, abs(potential - 0.5) * 2))
    
    def _recommend_mutations(self, potential: float) -> List[str]:
        """æ¨èå˜å¼‚ç­–ç•¥"""
        recommendations = []
        if potential > 0.7:
            recommendations.append('aggressive_widening')
        elif potential > 0.5:
            recommendations.append('moderate_widening')
        else:
            recommendations.append('conservative_mutation')
        return recommendations


class Net2NetSubnetworkAnalyzer:
    """Net2Netå­ç½‘ç»œåˆ†æå™¨ä¸»ç±» - ä½¿ç”¨å»¶è¿ŸåŠ è½½é¿å…å¾ªç¯ä¾èµ–"""
    
    def __init__(self):
        # åŸæœ‰ç»„ä»¶
        self.extractor = SubnetworkExtractor()
        self.param_analyzer = ParameterSpaceAnalyzer()
        self.predictor = MutationPotentialPredictor()
        
        # æ–°çš„æ¨¡å—åŒ–ç»„ä»¶
        self.info_flow_analyzer = InformationFlowAnalyzer()
        self.leak_detector = InformationLeakDetector()
        
        # å»¶è¿ŸåŠ è½½è´å¶æ–¯é¢„æµ‹å™¨
        self._bayesian_predictor = None
    
    @property
    def bayesian_predictor(self):
        """å»¶è¿ŸåŠ è½½è´å¶æ–¯é¢„æµ‹å™¨"""
        if self._bayesian_predictor is None:
            try:
                from .bayesian_prediction import BayesianMutationBenefitPredictor
                self._bayesian_predictor = BayesianMutationBenefitPredictor()
            except ImportError as e:
                logger.warning(f"Could not import BayesianMutationBenefitPredictor: {e}")
                # ä½¿ç”¨ç®€åŒ–çš„é¢„æµ‹å™¨ä½œä¸ºå›é€€
                self._bayesian_predictor = self._create_simple_predictor()
        return self._bayesian_predictor
    
    def _create_simple_predictor(self):
        """åˆ›å»ºç®€åŒ–çš„é¢„æµ‹å™¨ä½œä¸ºå›é€€"""
        class SimpleBayesianPredictor:
            def predict_mutation_benefit(self, layer_analysis, mutation_strategy, current_accuracy, model_complexity):
                return {
                    'expected_accuracy_gain': 0.01,
                    'confidence_interval': {'95%': (0.0, 0.02)},
                    'success_probability': 0.6,
                    'risk_adjusted_benefit': {'risk_adjusted_gain': 0.005},
                    'uncertainty_metrics': {'prediction_confidence': 0.4},
                    'recommendation_strength': "weak_recommend"
                }
        return SimpleBayesianPredictor()
    
    def analyze_all_layers(self, model: nn.Module, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ†ææ‰€æœ‰å±‚çš„å˜å¼‚æ½œåŠ›å’Œä¿¡æ¯æµç“¶é¢ˆ
        
        è¿™æ˜¯å®ç°ç¥ç»ç½‘ç»œæœ€ä¼˜å˜å¼‚ç†è®ºçš„æ ¸å¿ƒæ–¹æ³•ï¼š
        1. æ£€æµ‹ä¿¡æ¯æµæ¼ç‚¹ - æŸå±‚æˆä¸ºä¿¡æ¯æå–ç“¶é¢ˆï¼Œå¯¼è‡´åç»­å±‚æ— æ³•æå‡å‡†ç¡®ç‡
        2. åˆ†æå‚æ•°ç©ºé—´å¯†åº¦ - æ¼ç‚¹å±‚çš„å‚æ•°ç©ºé—´ä¸­é«˜å‡†ç¡®ç‡åŒºåŸŸå æ¯”è¾ƒå°
        3. é¢„æµ‹å˜å¼‚æ”¶ç›Š - å˜å¼‚åå‚æ•°ç©ºé—´ä¸­é«˜å‡†ç¡®ç‡åŒºåŸŸå æ¯”æå‡
        4. æŒ‡å¯¼æ¶æ„å˜å¼‚ - è®©æ¼ç‚¹å±‚å˜å¾—æ›´å¤æ‚ï¼Œæå–æ›´å¤šä¿¡æ¯
        """
        
        logger.info("ğŸ” å¼€å§‹å…¨å±‚åˆ†æ...")
        
        try:
            # 1. æå–å…³é”®å­ç½‘ç»œ
            subnetworks = self.extractor.extract_key_subnetworks(model, context)
            
            # 2. ä¿¡æ¯æµåˆ†æ
            info_flow_analysis = self._analyze_information_flow_comprehensive(model, context)
            
            # 3. é€å±‚è¯¦ç»†åˆ†æ
            layer_analyses = self._analyze_layers_detailed(model, context)
            
            # 4. ç“¶é¢ˆæ£€æµ‹å’Œé‡åŒ–
            bottleneck_analysis = self._detect_and_quantify_bottlenecks(layer_analyses, info_flow_analysis)
            
            # 5. å˜å¼‚æ”¶ç›Šé¢„æµ‹
            mutation_predictions = self._predict_mutation_benefits(layer_analyses, context)
            
            # 6. ç”Ÿæˆå˜å¼‚å»ºè®®
            mutation_recommendations = self._generate_mutation_recommendations(
                bottleneck_analysis, mutation_predictions, context
            )
            
            comprehensive_analysis = {
                'subnetworks': subnetworks,
                'information_flow': info_flow_analysis,
                'layer_analyses': layer_analyses,
                'bottleneck_analysis': bottleneck_analysis,
                'mutation_predictions': mutation_predictions,
                'recommendations': mutation_recommendations,
                'analysis_metadata': {
                    'timestamp': context.get('timestamp', 'unknown'),
                    'model_size': sum(p.numel() for p in model.parameters()),
                    'analysis_depth': 'comprehensive'
                }
            }
            
            logger.info(f"âœ… å…¨å±‚åˆ†æå®Œæˆï¼Œå‘ç° {len(bottleneck_analysis.get('detected_bottlenecks', []))} ä¸ªç“¶é¢ˆ")
            return comprehensive_analysis
            
        except Exception as e:
            logger.error(f"âŒ å…¨å±‚åˆ†æå¤±è´¥: {e}")
            return self._fallback_analysis(model, context)
    
    def _analyze_information_flow_comprehensive(self, model: nn.Module, context: Dict[str, Any]) -> Dict[str, Any]:
        """ç»¼åˆä¿¡æ¯æµåˆ†æ"""
        try:
            # ä½¿ç”¨ä¿¡æ¯æµåˆ†æå™¨
            flow_analysis = self.info_flow_analyzer.analyze_information_flow(model, context)
            
            # ä½¿ç”¨æ³„æ¼æ£€æµ‹å™¨
            leak_analysis = self.leak_detector.detect_information_leaks(model, context)
            
            return {
                'flow_patterns': flow_analysis,
                'leak_detection': leak_analysis,
                'flow_efficiency': self._calculate_flow_efficiency(flow_analysis, leak_analysis)
            }
        except Exception as e:
            logger.warning(f"ä¿¡æ¯æµåˆ†æå¤±è´¥: {e}")
            return {'flow_patterns': {}, 'leak_detection': {}, 'flow_efficiency': 0.5}
    
    def _analyze_layers_detailed(self, model: nn.Module, context: Dict[str, Any]) -> Dict[str, Any]:
        """è¯¦ç»†çš„é€å±‚åˆ†æ"""
        layer_analyses = {}
        
        for name, module in model.named_modules():
            if self._is_analyzable_layer(module):
                try:
                    # å‚æ•°ç©ºé—´åˆ†æ
                    param_analysis = self.param_analyzer.analyze_parameter_space_density(name, module, context)
                    
                    # å˜å¼‚æ½œåŠ›é¢„æµ‹
                    mutation_potential = self.predictor.predict_mutation_potential(param_analysis, context)
                    
                    layer_analyses[name] = {
                        'parameter_analysis': param_analysis,
                        'mutation_potential': mutation_potential,
                        'layer_type': type(module).__name__,
                        'layer_size': sum(p.numel() for p in module.parameters())
                    }
                except Exception as e:
                    logger.warning(f"å±‚ {name} åˆ†æå¤±è´¥: {e}")
                    continue
        
        return layer_analyses
    
    def _is_analyzable_layer(self, module: nn.Module) -> bool:
        """åˆ¤æ–­å±‚æ˜¯å¦å¯åˆ†æ"""
        return isinstance(module, (nn.Conv2d, nn.Linear, nn.BatchNorm2d))
    
    def _detect_and_quantify_bottlenecks(self, layer_analyses: Dict[str, Any], info_flow: Dict[str, Any]) -> Dict[str, Any]:
        """æ£€æµ‹å’Œé‡åŒ–ç“¶é¢ˆ"""
        bottlenecks = []
        
        for layer_name, analysis in layer_analyses.items():
            # åŸºäºå˜å¼‚æ½œåŠ›åˆ¤æ–­ç“¶é¢ˆ
            potential = analysis.get('mutation_potential', {}).get('combined_potential', 0.5)
            readiness = analysis.get('parameter_analysis', {}).get('mutation_readiness', 0.5)
            
            if potential > 0.7 and readiness > 0.6:
                bottleneck_score = (potential + readiness) / 2
                bottlenecks.append({
                    'layer_name': layer_name,
                    'bottleneck_score': bottleneck_score,
                    'bottleneck_type': 'parameter_space_constraint',
                    'severity': 'high' if bottleneck_score > 0.8 else 'medium'
                })
        
        return {
            'detected_bottlenecks': bottlenecks,
            'bottleneck_count': len(bottlenecks),
            'average_severity': np.mean([b['bottleneck_score'] for b in bottlenecks]) if bottlenecks else 0.0
        }
    
    def _predict_mutation_benefits(self, layer_analyses: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """é¢„æµ‹å˜å¼‚æ”¶ç›Š"""
        predictions = {}
        
        current_accuracy = context.get('current_accuracy', 0.8)
        model_complexity = context.get('model_complexity', {'parameters': 1000000})
        
        for layer_name, analysis in layer_analyses.items():
            try:
                # ä½¿ç”¨è´å¶æ–¯é¢„æµ‹å™¨ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
                layer_prediction = self.bayesian_predictor.predict_mutation_benefit(
                    layer_analysis=analysis,
                    mutation_strategy='moderate_widening',
                    current_accuracy=current_accuracy,
                    model_complexity=model_complexity
                )
                predictions[layer_name] = layer_prediction
            except Exception as e:
                logger.warning(f"å±‚ {layer_name} å˜å¼‚æ”¶ç›Šé¢„æµ‹å¤±è´¥: {e}")
                # ä½¿ç”¨ç®€åŒ–é¢„æµ‹
                predictions[layer_name] = {
                    'expected_accuracy_gain': 0.005,
                    'confidence_interval': {'95%': (0.0, 0.01)},
                    'recommendation_strength': 'weak_recommend'
                }
        
        return predictions
    
    def _generate_mutation_recommendations(self, bottleneck_analysis: Dict[str, Any], 
                                         mutation_predictions: Dict[str, Any], 
                                         context: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆå˜å¼‚å»ºè®®"""
        recommendations = []
        
        # åŸºäºç“¶é¢ˆåˆ†æç”Ÿæˆå»ºè®®
        for bottleneck in bottleneck_analysis.get('detected_bottlenecks', []):
            layer_name = bottleneck['layer_name']
            if layer_name in mutation_predictions:
                pred = mutation_predictions[layer_name]
                if pred.get('recommendation_strength') in ['recommend', 'strong_recommend']:
                    recommendations.append({
                        'layer_name': layer_name,
                        'mutation_type': 'widening',
                        'priority': bottleneck['bottleneck_score'],
                        'expected_gain': pred.get('expected_accuracy_gain', 0.01),
                        'confidence': pred.get('uncertainty_metrics', {}).get('prediction_confidence', 0.5)
                    })
        
        # æ’åºå»ºè®®
        recommendations.sort(key=lambda x: x['priority'], reverse=True)
        
        return {
            'mutations': recommendations[:5],  # æœ€å¤šæ¨è5ä¸ªå˜å¼‚
            'total_candidates': len(recommendations),
            'average_expected_gain': np.mean([r['expected_gain'] for r in recommendations]) if recommendations else 0.0
        }
    
    def _calculate_flow_efficiency(self, flow_analysis: Dict[str, Any], leak_analysis: Dict[str, Any]) -> float:
        """è®¡ç®—ä¿¡æ¯æµæ•ˆç‡"""
        # ç®€åŒ–å®ç°
        return 0.7
    
    def _fallback_analysis(self, model: nn.Module, context: Dict[str, Any]) -> Dict[str, Any]:
        """å›é€€åˆ†æ"""
        logger.info("ä½¿ç”¨å›é€€åˆ†ææ¨¡å¼")
        return {
            'subnetworks': {},
            'information_flow': {},
            'layer_analyses': {},
            'bottleneck_analysis': {'detected_bottlenecks': []},
            'mutation_predictions': {},
            'recommendations': {'mutations': []},
            'analysis_metadata': {'analysis_depth': 'fallback'}
        }