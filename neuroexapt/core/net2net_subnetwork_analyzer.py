#!/usr/bin/env python3
"""
@defgroup group_net2net_subnetwork_analyzer Net2Net Subnetwork Analyzer
@ingroup core
Net2Net Subnetwork Analyzer module for NeuroExapt framework.

Net2Netå­ç½‘ç»œåˆ†æå™¨ - Net2Net Subnetwork Analyzer

ğŸ¯ æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä»æŒ‡å®šå±‚åˆ°è¾“å‡ºå±‚æå–å­ç½‘ç»œ
2. è¯„ä¼°å­ç½‘ç»œçš„å˜å¼‚æ½œåŠ›
3. é¢„æµ‹å˜å¼‚åçš„å‡†ç¡®ç‡æå‡ç©ºé—´
4. åˆ†æå¯è¡Œå‚æ•°ç©ºé—´åœ¨æ€»å‚æ•°ç©ºé—´ä¸­çš„å æ¯”
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Any, Set
from collections import OrderedDict, defaultdict
import copy
import logging
import time

from .logging_utils import logger

class SubnetworkExtractor:
    """å­ç½‘ç»œæå–å™¨"""
    
    def __init__(self):
        self.extracted_subnetworks = {}
        self.layer_dependencies = {}
        
    def extract_subnetwork_from_layer(self, 
                                    model: nn.Module, 
                                    start_layer_name: str,
                                    include_start_layer: bool = True) -> Tuple[nn.Module, Dict[str, Any]]:
        """
        ä»æŒ‡å®šå±‚å¼€å§‹æå–åˆ°è¾“å‡ºå±‚çš„å­ç½‘ç»œ
        
        Args:
            model: åŸå§‹æ¨¡å‹
            start_layer_name: èµ·å§‹å±‚åç§°
            include_start_layer: æ˜¯å¦åŒ…å«èµ·å§‹å±‚
            
        Returns:
            å­ç½‘ç»œæ¨¡å‹å’Œæå–ä¿¡æ¯
        """
        logger.enter_section(f"æå–å­ç½‘ç»œ: {start_layer_name}")
        
        # 1. åˆ†ææ¨¡å‹ç»“æ„ï¼Œæ‰¾åˆ°æ‰€æœ‰å±‚çš„ä¾èµ–å…³ç³»
        layer_graph = self._build_layer_dependency_graph(model)
        
        # 2. ä»èµ·å§‹å±‚å¼€å§‹ï¼Œæ‰¾åˆ°æ‰€æœ‰åç»­å±‚
        target_layers = self._find_downstream_layers(layer_graph, start_layer_name, include_start_layer)
        
        logger.info(f"è¯†åˆ«å‡º{len(target_layers)}ä¸ªä¸‹æ¸¸å±‚")
        
        # 3. æ„å»ºå­ç½‘ç»œ
        subnetwork = self._build_subnetwork(model, target_layers, start_layer_name)
        
        # 4. åˆ†æå­ç½‘ç»œä¿¡æ¯
        subnetwork_info = self._analyze_subnetwork(subnetwork, target_layers, start_layer_name)
        
        logger.info(f"å­ç½‘ç»œæ„å»ºå®Œæˆ: {subnetwork_info['total_params']:,}å‚æ•°")
        logger.exit_section(f"æå–å­ç½‘ç»œ: {start_layer_name}")
        
        return subnetwork, subnetwork_info
    
    def _build_layer_dependency_graph(self, model: nn.Module) -> Dict[str, List[str]]:
        """æ„å»ºå±‚ä¾èµ–å…³ç³»å›¾"""
        layer_graph = defaultdict(list)
        named_modules = dict(model.named_modules())
        
        # åˆ†æResNeté£æ ¼çš„å‰å‘ä¼ æ’­ä¾èµ–
        for name, module in named_modules.items():
            if name == '':  # è·³è¿‡æ ¹æ¨¡å—
                continue
                
            # è§£æå±‚çš„é€»è¾‘ä½ç½®
            parts = name.split('.')
            
            if len(parts) >= 2:
                # å¯¹äºå±‚çº§ç»“æ„ï¼Œæ·»åŠ ä¾èµ–å…³ç³»
                parent_parts = parts[:-1]
                parent_name = '.'.join(parent_parts)
                
                if parent_name in named_modules:
                    layer_graph[parent_name].append(name)
        
        # æ·»åŠ ç‰¹æ®Šçš„ä¾èµ–å…³ç³»ï¼ˆåŸºäºResNetæ¶æ„ï¼‰
        self._add_resnet_dependencies(layer_graph, named_modules)
        
        return dict(layer_graph)
    
    def _add_resnet_dependencies(self, layer_graph: Dict[str, List[str]], named_modules: Dict[str, nn.Module]):
        """æ·»åŠ ResNetç‰¹å®šçš„ä¾èµ–å…³ç³»"""
        
        # ä¸»å¹²ä¾èµ–ï¼šconv1 -> feature_blocks -> classifier
        main_sequence = []
        
        # æŸ¥æ‰¾ä¸»è¦ç»„ä»¶
        if 'conv1' in named_modules:
            main_sequence.append('conv1')
        
        # æŸ¥æ‰¾feature blocks
        feature_blocks = []
        for name in named_modules.keys():
            if name.startswith('feature_block'):
                if '.' not in name[len('feature_block'):]:  # åªè¦é¡¶çº§feature_block
                    feature_blocks.append(name)
        
        feature_blocks.sort()  # æŒ‰åç§°æ’åº
        main_sequence.extend(feature_blocks)
        
        # æŸ¥æ‰¾åˆ†ç±»å™¨
        if 'classifier' in named_modules:
            main_sequence.append('classifier')
        
        # å»ºç«‹ä¸»å¹²ä¾èµ–
        for i in range(len(main_sequence) - 1):
            current = main_sequence[i]
            next_layer = main_sequence[i + 1]
            if current not in layer_graph:
                layer_graph[current] = []
            layer_graph[current].append(next_layer)
    
    def _find_downstream_layers(self, 
                               layer_graph: Dict[str, List[str]], 
                               start_layer: str, 
                               include_start: bool = True) -> Set[str]:
        """æ‰¾åˆ°æŒ‡å®šå±‚ä¹‹åçš„æ‰€æœ‰ä¸‹æ¸¸å±‚"""
        
        downstream_layers = set()
        
        if include_start:
            downstream_layers.add(start_layer)
        
        # BFSéå†æ‰¾åˆ°æ‰€æœ‰ä¸‹æ¸¸å±‚
        queue = [start_layer]
        visited = set([start_layer])
        
        while queue:
            current_layer = queue.pop(0)
            
            # æ·»åŠ å½“å‰å±‚çš„æ‰€æœ‰ä¸‹æ¸¸å±‚
            if current_layer in layer_graph:
                for next_layer in layer_graph[current_layer]:
                    if next_layer not in visited:
                        downstream_layers.add(next_layer)
                        queue.append(next_layer)
                        visited.add(next_layer)
        
        return downstream_layers
    
    def _build_subnetwork(self, 
                         model: nn.Module, 
                         target_layers: Set[str], 
                         start_layer_name: str) -> nn.Module:
        """æ„å»ºåŒ…å«ç›®æ ‡å±‚çš„å­ç½‘ç»œ"""
        
        # è·å–æ¨¡å‹çš„æ‰€æœ‰å‘½åæ¨¡å—
        named_modules = dict(model.named_modules())
        
        # åˆ›å»ºå­ç½‘ç»œæ¨¡å—å­—å…¸
        subnetwork_modules = OrderedDict()
        
        # æ·»åŠ ç›®æ ‡å±‚åˆ°å­ç½‘ç»œ
        for layer_name in sorted(target_layers):
            if layer_name in named_modules:
                subnetwork_modules[layer_name] = copy.deepcopy(named_modules[layer_name])
        
        # åˆ›å»ºåŠ¨æ€å­ç½‘ç»œç±»
        class DynamicSubnetwork(nn.Module):
            def __init__(self, modules_dict, start_layer):
                super().__init__()
                self.start_layer = start_layer
                self.modules_dict = nn.ModuleDict(modules_dict)
                
                # åˆ†æè¾“å…¥è¾“å‡ºç»´åº¦
                self._analyze_io_dims()
            
            def _analyze_io_dims(self):
                """åˆ†æè¾“å…¥è¾“å‡ºç»´åº¦"""
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ ¹æ®æ¨¡å‹ç»“æ„åŠ¨æ€åˆ†æ
                self.input_dim = None
                self.output_dim = None
                
                # æŸ¥æ‰¾è¾“å…¥å’Œè¾“å‡ºå±‚
                layer_names = list(self.modules_dict.keys())
                if layer_names:
                    first_layer = self.modules_dict[layer_names[0]]
                    last_layer = self.modules_dict[layer_names[-1]]
                    
                    # å°è¯•è·å–è¾“å…¥ç»´åº¦
                    if hasattr(first_layer, 'in_features'):
                        self.input_dim = first_layer.in_features
                    elif hasattr(first_layer, 'in_channels'):
                        self.input_dim = first_layer.in_channels
                    
                    # å°è¯•è·å–è¾“å‡ºç»´åº¦
                    if hasattr(last_layer, 'out_features'):
                        self.output_dim = last_layer.out_features
                    elif hasattr(last_layer, 'out_channels'):
                        self.output_dim = last_layer.out_channels
            
            def forward(self, x):
                """ç®€åŒ–çš„å‰å‘ä¼ æ’­"""
                # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…éœ€è¦æ ¹æ®å…·ä½“æ¶æ„å®ç°
                current = x
                
                for name, module in self.modules_dict.items():
                    try:
                        if isinstance(module, (nn.Linear, nn.Conv2d)):
                            current = module(current)
                        elif isinstance(module, nn.Sequential):
                            current = module(current)
                        else:
                            # å¯¹äºå…¶ä»–ç±»å‹çš„æ¨¡å—ï¼Œå°è¯•ç›´æ¥è°ƒç”¨
                            current = module(current)
                    except Exception as e:
                        logger.warning(f"å­ç½‘ç»œå‰å‘ä¼ æ’­åœ¨å±‚{name}å¤±è´¥: {e}")
                        break
                
                return current
        
        subnetwork = DynamicSubnetwork(subnetwork_modules, start_layer_name)
        return subnetwork
    
    def _analyze_subnetwork(self, 
                           subnetwork: nn.Module, 
                           target_layers: Set[str], 
                           start_layer_name: str) -> Dict[str, Any]:
        """åˆ†æå­ç½‘ç»œçš„ç‰¹æ€§"""
        
        total_params = sum(p.numel() for p in subnetwork.parameters())
        trainable_params = sum(p.numel() for p in subnetwork.parameters() if p.requires_grad)
        
        # åˆ†æå±‚ç±»å‹åˆ†å¸ƒ
        layer_types = defaultdict(int)
        for name, module in subnetwork.named_modules():
            if name:  # è·³è¿‡æ ¹æ¨¡å—
                layer_types[type(module).__name__] += 1
        
        return {
            'start_layer': start_layer_name,
            'layer_count': len(target_layers),
            'total_params': total_params,
            'trainable_params': trainable_params,
            'layer_types': dict(layer_types),
            'input_dim': getattr(subnetwork, 'input_dim', None),
            'output_dim': getattr(subnetwork, 'output_dim', None)
        }

class ParameterSpaceAnalyzer:
    """å‚æ•°ç©ºé—´åˆ†æå™¨"""
    
    def __init__(self):
        self.analysis_cache = {}
    
    def analyze_parameter_space_efficiency(self, 
                                         subnetwork: nn.Module,
                                         activations: torch.Tensor,
                                         gradients: torch.Tensor,
                                         targets: torch.Tensor) -> Dict[str, float]:
        """
        åˆ†æå‚æ•°ç©ºé—´æ•ˆç‡
        
        æ ¸å¿ƒæ€æƒ³ï¼šè¯„ä¼°å½“å‰å‚æ•°åœ¨è§£å†³åˆ†ç±»ä»»åŠ¡æ—¶çš„æ•ˆç‡
        """
        logger.enter_section("å‚æ•°ç©ºé—´æ•ˆç‡åˆ†æ")
        
        try:
            # 1. è®¡ç®—å‚æ•°åˆ©ç”¨ç‡
            param_utilization = self._compute_parameter_utilization(subnetwork, gradients)
            
            # 2. åˆ†æè¡¨ç¤ºèƒ½åŠ›
            representation_capacity = self._compute_representation_capacity(activations, targets)
            
            # 3. è¯„ä¼°å†—ä½™åº¦
            redundancy_ratio = self._compute_parameter_redundancy(subnetwork, activations)
            
            # 4. è®¡ç®—å¯è¡Œå‚æ•°ç©ºé—´å æ¯”
            feasible_space_ratio = self._estimate_feasible_parameter_space(
                subnetwork, activations, targets
            )
            
            # 5. ç»¼åˆæ•ˆç‡è¯„åˆ†
            overall_efficiency = (
                0.3 * param_utilization +
                0.3 * representation_capacity +
                0.2 * (1.0 - redundancy_ratio) +  # å†—ä½™åº¦è¶Šä½è¶Šå¥½
                0.2 * feasible_space_ratio
            )
            
            analysis_result = {
                'parameter_utilization': param_utilization,
                'representation_capacity': representation_capacity,
                'redundancy_ratio': redundancy_ratio,
                'feasible_space_ratio': feasible_space_ratio,
                'overall_efficiency': overall_efficiency
            }
            
            logger.info(f"å‚æ•°ç©ºé—´åˆ†æå®Œæˆ: æ•´ä½“æ•ˆç‡={overall_efficiency:.3f}")
            logger.exit_section("å‚æ•°ç©ºé—´æ•ˆç‡åˆ†æ")
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"å‚æ•°ç©ºé—´åˆ†æå¤±è´¥: {e}")
            logger.exit_section("å‚æ•°ç©ºé—´æ•ˆç‡åˆ†æ")
            return {
                'parameter_utilization': 0.0,
                'representation_capacity': 0.0,
                'redundancy_ratio': 1.0,
                'feasible_space_ratio': 0.0,
                'overall_efficiency': 0.0
            }
    
    def _compute_parameter_utilization(self, subnetwork: nn.Module, gradients: torch.Tensor) -> float:
        """è®¡ç®—å‚æ•°åˆ©ç”¨ç‡ - æœ‰å¤šå°‘å‚æ•°åœ¨ç§¯æå‚ä¸å­¦ä¹ """
        
        total_params = sum(p.numel() for p in subnetwork.parameters())
        if total_params == 0:
            return 0.0
        
        # è®¡ç®—æœ‰æ•ˆæ¢¯åº¦çš„å‚æ•°æ•°é‡
        active_params = 0
        for param in subnetwork.parameters():
            if param.grad is not None:
                # æ¢¯åº¦æ˜¾è‘—éé›¶çš„å‚æ•°è¢«è®¤ä¸ºæ˜¯æ´»è·ƒçš„
                significant_grads = torch.abs(param.grad) > 1e-6
                active_params += significant_grads.sum().item()
        
        utilization = active_params / total_params
        return min(utilization, 1.0)
    
    def _compute_representation_capacity(self, activations: torch.Tensor, targets: torch.Tensor) -> float:
        """è®¡ç®—è¡¨ç¤ºèƒ½åŠ› - ç½‘ç»œèƒ½å¤šå¥½åœ°åŒºåˆ†ä¸åŒç±»åˆ«"""
        
        try:
            # è®¡ç®—ç±»é—´åˆ†ç¦»åº¦
            if len(activations.shape) > 2:
                # å¯¹äºå·ç§¯å±‚è¾“å‡ºï¼Œå–å¹³å‡æ± åŒ–
                activations_flat = F.adaptive_avg_pool2d(activations, (1, 1)).flatten(1)
            else:
                activations_flat = activations
            
            # è®¡ç®—ä¸åŒç±»åˆ«çš„æ¿€æ´»åˆ†å¸ƒ
            unique_targets = torch.unique(targets)
            if len(unique_targets) < 2:
                return 0.0
            
            class_centers = []
            for target_class in unique_targets:
                mask = targets == target_class
                if mask.sum() > 0:
                    class_center = activations_flat[mask].mean(dim=0)
                    class_centers.append(class_center)
            
            if len(class_centers) < 2:
                return 0.0
            
            # è®¡ç®—ç±»é—´è·ç¦»
            class_centers = torch.stack(class_centers)
            distances = torch.pdist(class_centers)
            avg_inter_class_distance = distances.mean().item()
            
            # è®¡ç®—ç±»å†…æ–¹å·®
            intra_class_variance = 0.0
            for target_class in unique_targets:
                mask = targets == target_class
                if mask.sum() > 1:
                    class_activations = activations_flat[mask]
                    class_center = class_activations.mean(dim=0)
                    variance = ((class_activations - class_center) ** 2).mean().item()
                    intra_class_variance += variance
            
            intra_class_variance /= len(unique_targets)
            
            # è¡¨ç¤ºèƒ½åŠ› = ç±»é—´è·ç¦» / ç±»å†…æ–¹å·®
            if intra_class_variance > 0:
                representation_capacity = avg_inter_class_distance / (intra_class_variance + 1e-8)
            else:
                representation_capacity = avg_inter_class_distance
            
            # å½’ä¸€åŒ–åˆ°[0, 1]
            return min(representation_capacity / 10.0, 1.0)
            
        except Exception as e:
            logger.warning(f"è¡¨ç¤ºèƒ½åŠ›è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _compute_parameter_redundancy(self, subnetwork: nn.Module, activations: torch.Tensor) -> float:
        """è®¡ç®—å‚æ•°å†—ä½™åº¦"""
        
        try:
            redundancy_scores = []
            
            for name, module in subnetwork.named_modules():
                if isinstance(module, (nn.Linear, nn.Conv2d)):
                    weight = module.weight.data
                    
                    # è®¡ç®—æƒé‡çŸ©é˜µçš„æœ‰æ•ˆç§©
                    if len(weight.shape) == 2:  # Linear layer
                        rank = torch.matrix_rank(weight).item()
                        max_rank = min(weight.shape[0], weight.shape[1])
                    else:  # Conv2d layer
                        # å°†å·ç§¯æƒé‡é‡å¡‘ä¸º2DçŸ©é˜µ
                        weight_2d = weight.view(weight.shape[0], -1)
                        rank = torch.matrix_rank(weight_2d).item()
                        max_rank = min(weight_2d.shape[0], weight_2d.shape[1])
                    
                    if max_rank > 0:
                        rank_ratio = rank / max_rank
                        redundancy = 1.0 - rank_ratio  # ç§©è¶Šä½ï¼Œå†—ä½™åº¦è¶Šé«˜
                        redundancy_scores.append(redundancy)
            
            if redundancy_scores:
                return np.mean(redundancy_scores)
            else:
                return 0.0
                
        except Exception as e:
            logger.warning(f"å‚æ•°å†—ä½™åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _estimate_feasible_parameter_space(self, 
                                         subnetwork: nn.Module, 
                                         activations: torch.Tensor, 
                                         targets: torch.Tensor) -> float:
        """
        ä¼°è®¡å¯è¡Œå‚æ•°ç©ºé—´å æ¯”
        
        æ ¸å¿ƒæ€æƒ³ï¼šåœ¨å½“å‰å‚æ•°é™„è¿‘ï¼Œæœ‰å¤šå¤§æ¯”ä¾‹çš„å‚æ•°å˜åŒ–èƒ½å¤Ÿç»´æŒæˆ–æ”¹å–„æ€§èƒ½
        """
        
        try:
            # ä½¿ç”¨é‡‡æ ·æ–¹æ³•ä¼°è®¡å¯è¡Œå‚æ•°ç©ºé—´
            original_params = [p.data.clone() for p in subnetwork.parameters()]
            
            # è®¡ç®—åŸå§‹æ€§èƒ½
            with torch.no_grad():
                original_output = subnetwork(activations)
                if len(original_output.shape) > 1 and original_output.shape[1] > 1:
                    original_loss = F.cross_entropy(original_output, targets, reduction='mean')
                else:
                    original_loss = F.mse_loss(original_output.squeeze(), targets.float())
            
            # é‡‡æ ·æµ‹è¯•
            feasible_count = 0
            total_samples = 50  # å‡å°‘é‡‡æ ·æ•°é‡ä»¥æé«˜é€Ÿåº¦
            noise_scale = 0.01  # å°æ‰°åŠ¨
            
            for _ in range(total_samples):
                # æ·»åŠ éšæœºæ‰°åŠ¨
                for param in subnetwork.parameters():
                    noise = torch.randn_like(param.data) * noise_scale
                    param.data.add_(noise)
                
                # æµ‹è¯•æ‰°åŠ¨åçš„æ€§èƒ½
                try:
                    with torch.no_grad():
                        perturbed_output = subnetwork(activations)
                        if len(perturbed_output.shape) > 1 and perturbed_output.shape[1] > 1:
                            perturbed_loss = F.cross_entropy(perturbed_output, targets, reduction='mean')
                        else:
                            perturbed_loss = F.mse_loss(perturbed_output.squeeze(), targets.float())
                    
                    # å¦‚æœæŸå¤±æ²¡æœ‰æ˜¾è‘—å¢åŠ ï¼Œè®¤ä¸ºæ˜¯å¯è¡Œçš„
                    if perturbed_loss <= original_loss * 1.1:  # å…è®¸10%çš„æ€§èƒ½ä¸‹é™
                        feasible_count += 1
                        
                except Exception:
                    pass  # æ‰°åŠ¨å¯¼è‡´çš„é”™è¯¯è§†ä¸ºä¸å¯è¡Œ
                
                # æ¢å¤åŸå§‹å‚æ•°
                for param, original_param in zip(subnetwork.parameters(), original_params):
                    param.data.copy_(original_param)
            
            feasible_ratio = feasible_count / total_samples
            return feasible_ratio
            
        except Exception as e:
            logger.warning(f"å¯è¡Œå‚æ•°ç©ºé—´ä¼°è®¡å¤±è´¥: {e}")
            return 0.0

class MutationPotentialPredictor:
    """å˜å¼‚æ½œåŠ›é¢„æµ‹å™¨"""
    
    def __init__(self):
        self.predictor_cache = {}
    
    def predict_mutation_potential(self, 
                                 subnetwork: nn.Module,
                                 subnetwork_info: Dict[str, Any],
                                 parameter_space_analysis: Dict[str, float],
                                 current_accuracy: float) -> Dict[str, Any]:
        """
        é¢„æµ‹å˜å¼‚æ½œåŠ›å’Œå¯èƒ½çš„å‡†ç¡®ç‡æå‡
        
        Args:
            subnetwork: æå–çš„å­ç½‘ç»œ
            subnetwork_info: å­ç½‘ç»œåˆ†æä¿¡æ¯
            parameter_space_analysis: å‚æ•°ç©ºé—´åˆ†æç»“æœ
            current_accuracy: å½“å‰å‡†ç¡®ç‡
            
        Returns:
            å˜å¼‚æ½œåŠ›é¢„æµ‹ç»“æœ
        """
        logger.enter_section("å˜å¼‚æ½œåŠ›é¢„æµ‹")
        
        try:
            # 1. åŸºäºå‚æ•°ç©ºé—´æ•ˆç‡é¢„æµ‹æå‡ç©ºé—´
            efficiency = parameter_space_analysis['overall_efficiency']
            improvement_potential = self._compute_improvement_potential(efficiency, current_accuracy)
            
            # 2. é¢„æµ‹ä¸åŒå˜å¼‚ç­–ç•¥çš„æ•ˆæœ
            strategy_predictions = self._predict_strategy_effects(
                subnetwork_info, parameter_space_analysis, current_accuracy
            )
            
            # 3. ä¼°è®¡æœ€ä¼˜å˜å¼‚å¼ºåº¦
            optimal_mutation_strength = self._estimate_optimal_mutation_strength(
                parameter_space_analysis, improvement_potential
            )
            
            # 4. é£é™©è¯„ä¼°
            risk_assessment = self._assess_mutation_risks(
                subnetwork_info, parameter_space_analysis
            )
            
            prediction_result = {
                'improvement_potential': improvement_potential,
                'strategy_predictions': strategy_predictions,
                'optimal_mutation_strength': optimal_mutation_strength,
                'risk_assessment': risk_assessment,
                'confidence': self._compute_prediction_confidence(parameter_space_analysis)
            }
            
            logger.info(f"å˜å¼‚æ½œåŠ›é¢„æµ‹å®Œæˆ: æ”¹è¿›æ½œåŠ›={improvement_potential:.3f}")
            logger.exit_section("å˜å¼‚æ½œåŠ›é¢„æµ‹")
            
            return prediction_result
            
        except Exception as e:
            logger.error(f"å˜å¼‚æ½œåŠ›é¢„æµ‹å¤±è´¥: {e}")
            logger.exit_section("å˜å¼‚æ½œåŠ›é¢„æµ‹")
            return {
                'improvement_potential': 0.0,
                'strategy_predictions': {},
                'optimal_mutation_strength': 0.1,
                'risk_assessment': {'overall_risk': 1.0},
                'confidence': 0.0
            }
    
    def _compute_improvement_potential(self, efficiency: float, current_accuracy: float) -> float:
        """åŸºäºæ•ˆç‡å’Œå½“å‰å‡†ç¡®ç‡è®¡ç®—æ”¹è¿›æ½œåŠ›"""
        
        # æ•ˆç‡è¶Šä½ï¼Œæ”¹è¿›ç©ºé—´è¶Šå¤§
        efficiency_factor = 1.0 - efficiency
        
        # å‡†ç¡®ç‡è¶Šé«˜ï¼Œæ”¹è¿›è¶Šå›°éš¾
        if current_accuracy > 0.95:
            accuracy_factor = 0.1
        elif current_accuracy > 0.90:
            accuracy_factor = 0.3
        elif current_accuracy > 0.80:
            accuracy_factor = 0.6
        else:
            accuracy_factor = 1.0
        
        # ç»¼åˆæ”¹è¿›æ½œåŠ›
        improvement_potential = efficiency_factor * accuracy_factor
        
        return min(improvement_potential, 1.0)
    
    def _predict_strategy_effects(self, 
                                subnetwork_info: Dict[str, Any],
                                parameter_space_analysis: Dict[str, float],
                                current_accuracy: float) -> Dict[str, Dict[str, float]]:
        """é¢„æµ‹ä¸åŒå˜å¼‚ç­–ç•¥çš„æ•ˆæœ"""
        
        strategies = {
            'width_expansion': self._predict_width_expansion_effect(
                subnetwork_info, parameter_space_analysis
            ),
            'depth_increase': self._predict_depth_increase_effect(
                subnetwork_info, parameter_space_analysis
            ),
            'parallel_division': self._predict_parallel_division_effect(
                subnetwork_info, parameter_space_analysis
            ),
            'hybrid_mutation': self._predict_hybrid_mutation_effect(
                subnetwork_info, parameter_space_analysis
            )
        }
        
        return strategies
    
    def _predict_width_expansion_effect(self, 
                                      subnetwork_info: Dict[str, Any],
                                      parameter_space_analysis: Dict[str, float]) -> Dict[str, float]:
        """é¢„æµ‹å®½åº¦æ‰©å±•æ•ˆæœ"""
        
        redundancy = parameter_space_analysis['redundancy_ratio']
        utilization = parameter_space_analysis['parameter_utilization']
        
        # å†—ä½™åº¦ä½ä¸”åˆ©ç”¨ç‡é«˜çš„å±‚é€‚åˆå®½åº¦æ‰©å±•
        expansion_benefit = (1.0 - redundancy) * utilization
        
        return {
            'expected_accuracy_gain': expansion_benefit * 0.02,  # æœ€å¤š2%çš„æå‡
            'parameter_cost': 0.3,  # ç›¸å¯¹å‚æ•°å¢é•¿
            'implementation_difficulty': 0.2,  # å®ç°éš¾åº¦
            'stability_risk': 0.1  # ç¨³å®šæ€§é£é™©
        }
    
    def _predict_depth_increase_effect(self, 
                                     subnetwork_info: Dict[str, Any],
                                     parameter_space_analysis: Dict[str, float]) -> Dict[str, float]:
        """é¢„æµ‹æ·±åº¦å¢åŠ æ•ˆæœ"""
        
        representation_capacity = parameter_space_analysis['representation_capacity']
        
        # è¡¨ç¤ºèƒ½åŠ›ä¸è¶³çš„å­ç½‘ç»œé€‚åˆå¢åŠ æ·±åº¦
        depth_benefit = 1.0 - representation_capacity
        
        return {
            'expected_accuracy_gain': depth_benefit * 0.015,  # æœ€å¤š1.5%çš„æå‡
            'parameter_cost': 0.5,  # è¾ƒé«˜çš„å‚æ•°å¢é•¿
            'implementation_difficulty': 0.4,  # ä¸­ç­‰å®ç°éš¾åº¦
            'stability_risk': 0.3  # ä¸­ç­‰ç¨³å®šæ€§é£é™©
        }
    
    def _predict_parallel_division_effect(self, 
                                        subnetwork_info: Dict[str, Any],
                                        parameter_space_analysis: Dict[str, float]) -> Dict[str, float]:
        """é¢„æµ‹å¹¶è¡Œåˆ†è£‚æ•ˆæœ"""
        
        feasible_space = parameter_space_analysis['feasible_space_ratio']
        
        # å¯è¡Œç©ºé—´å¤§çš„å­ç½‘ç»œé€‚åˆå¹¶è¡Œåˆ†è£‚
        parallel_benefit = feasible_space
        
        return {
            'expected_accuracy_gain': parallel_benefit * 0.025,  # æœ€å¤š2.5%çš„æå‡
            'parameter_cost': 0.4,  # ä¸­ç­‰å‚æ•°å¢é•¿
            'implementation_difficulty': 0.3,  # ä¸­ç­‰å®ç°éš¾åº¦
            'stability_risk': 0.2  # è¾ƒä½ç¨³å®šæ€§é£é™©
        }
    
    def _predict_hybrid_mutation_effect(self, 
                                      subnetwork_info: Dict[str, Any],
                                      parameter_space_analysis: Dict[str, float]) -> Dict[str, float]:
        """é¢„æµ‹æ··åˆå˜å¼‚æ•ˆæœ"""
        
        overall_efficiency = parameter_space_analysis['overall_efficiency']
        
        # æ•ˆç‡ä½çš„å­ç½‘ç»œé€‚åˆæ··åˆå˜å¼‚
        hybrid_benefit = (1.0 - overall_efficiency) * 1.2  # æ··åˆç­–ç•¥çš„åŠ æˆ
        
        return {
            'expected_accuracy_gain': min(hybrid_benefit * 0.03, 0.04),  # æœ€å¤š4%çš„æå‡
            'parameter_cost': 0.6,  # è¾ƒé«˜å‚æ•°å¢é•¿
            'implementation_difficulty': 0.6,  # è¾ƒé«˜å®ç°éš¾åº¦
            'stability_risk': 0.4  # ä¸­ç­‰ç¨³å®šæ€§é£é™©
        }
    
    def _estimate_optimal_mutation_strength(self, 
                                          parameter_space_analysis: Dict[str, float],
                                          improvement_potential: float) -> float:
        """ä¼°è®¡æœ€ä¼˜å˜å¼‚å¼ºåº¦"""
        
        efficiency = parameter_space_analysis['overall_efficiency']
        feasible_space = parameter_space_analysis['feasible_space_ratio']
        
        # åŸºäºæ•ˆç‡å’Œå¯è¡Œç©ºé—´ç¡®å®šå˜å¼‚å¼ºåº¦
        base_strength = 0.1  # åŸºç¡€å˜å¼‚å¼ºåº¦
        
        # æ•ˆç‡ä½çš„å­ç½‘ç»œå¯ä»¥æ‰¿å—æ›´å¼ºçš„å˜å¼‚
        efficiency_factor = (1.0 - efficiency) * 2.0
        
        # å¯è¡Œç©ºé—´å¤§çš„å­ç½‘ç»œå¯ä»¥æ‰¿å—æ›´å¼ºçš„å˜å¼‚
        feasible_factor = feasible_space * 1.5
        
        optimal_strength = base_strength + min(efficiency_factor + feasible_factor, 0.8)
        
        return min(optimal_strength, 1.0)
    
    def _assess_mutation_risks(self, 
                             subnetwork_info: Dict[str, Any],
                             parameter_space_analysis: Dict[str, float]) -> Dict[str, float]:
        """è¯„ä¼°å˜å¼‚é£é™©"""
        
        # å‚æ•°æ•°é‡é£é™©
        param_count = subnetwork_info['total_params']
        param_risk = min(param_count / 1e6, 1.0)  # å‚æ•°è¶Šå¤šé£é™©è¶Šé«˜
        
        # æ•ˆç‡é£é™©
        efficiency = parameter_space_analysis['overall_efficiency']
        efficiency_risk = efficiency  # æ•ˆç‡é«˜çš„ç³»ç»Ÿå˜å¼‚é£é™©é«˜
        
        # å¯è¡Œç©ºé—´é£é™©
        feasible_space = parameter_space_analysis['feasible_space_ratio']
        space_risk = 1.0 - feasible_space  # å¯è¡Œç©ºé—´å°é£é™©é«˜
        
        # ç»¼åˆé£é™©
        overall_risk = (param_risk + efficiency_risk + space_risk) / 3.0
        
        return {
            'parameter_risk': param_risk,
            'efficiency_risk': efficiency_risk,
            'space_risk': space_risk,
            'overall_risk': overall_risk
        }
    
    def _compute_prediction_confidence(self, parameter_space_analysis: Dict[str, float]) -> float:
        """è®¡ç®—é¢„æµ‹ç½®ä¿¡åº¦"""
        
        # åŸºäºåˆ†æç»“æœçš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§è®¡ç®—ç½®ä¿¡åº¦
        analysis_completeness = len([v for v in parameter_space_analysis.values() if v > 0]) / len(parameter_space_analysis)
        
        # ç»“æœçš„ä¸€è‡´æ€§æ£€æŸ¥
        efficiency = parameter_space_analysis['overall_efficiency']
        redundancy = parameter_space_analysis['redundancy_ratio']
        
        # æ•ˆç‡å’Œå†—ä½™åº¦åº”è¯¥è´Ÿç›¸å…³
        consistency = 1.0 - abs(efficiency + redundancy - 1.0)
        
        confidence = (analysis_completeness + consistency) / 2.0
        
        return min(confidence, 1.0)

class Net2NetSubnetworkAnalyzer:
    """Net2Netå­ç½‘ç»œåˆ†æå™¨ä¸»ç±»"""
    
    def __init__(self):
        self.extractor = SubnetworkExtractor()
        self.param_analyzer = ParameterSpaceAnalyzer()
        self.predictor = MutationPotentialPredictor()
        
        # æ–°å¢ï¼šä¿¡æ¯æµåˆ†æå™¨
        self.info_flow_analyzer = InformationFlowAnalyzer()
        self.leak_detector = InformationLeakDetector()
    
    def analyze_all_layers(self, model: nn.Module, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ†ææ‰€æœ‰å±‚çš„å˜å¼‚æ½œåŠ›å’Œä¿¡æ¯æµç“¶é¢ˆ
        
        è¿™æ˜¯å®ç°ç¥ç»ç½‘ç»œæœ€ä¼˜å˜å¼‚ç†è®ºçš„æ ¸å¿ƒæ–¹æ³•ï¼š
        1. æ£€æµ‹ä¿¡æ¯æµæ¼ç‚¹ - æŸå±‚æˆä¸ºä¿¡æ¯æå–ç“¶é¢ˆï¼Œå¯¼è‡´åç»­å±‚æ— æ³•æå‡å‡†ç¡®ç‡
        2. åˆ†æå‚æ•°ç©ºé—´å¯†åº¦ - æ¼ç‚¹å±‚çš„å‚æ•°ç©ºé—´ä¸­é«˜å‡†ç¡®ç‡åŒºåŸŸå æ¯”è¾ƒå°
        3. é¢„æµ‹å˜å¼‚æ”¶ç›Š - å˜å¼‚åå‚æ•°ç©ºé—´ä¸­é«˜å‡†ç¡®ç‡åŒºåŸŸå æ¯”æå‡
        4. æŒ‡å¯¼æ¶æ„å˜å¼‚ - è®©æ¼ç‚¹å±‚å˜å¾—æ›´å¤æ‚ï¼Œæå–æ›´å¤šä¿¡æ¯
        
        Args:
            model: ç¥ç»ç½‘ç»œæ¨¡å‹
            context: åˆ†æä¸Šä¸‹æ–‡ï¼ŒåŒ…å«æ¿€æ´»å€¼ã€æ¢¯åº¦ã€ç›®æ ‡ç­‰
            
        Returns:
            åŒ…å«æ‰€æœ‰å±‚åˆ†æç»“æœå’Œå˜å¼‚å»ºè®®çš„å­—å…¸
        """
        logger.enter_section("Net2Netå…¨å±‚åˆ†æ")
        
        try:
            activations = context.get('activations', {})
            gradients = context.get('gradients', {})
            targets = context.get('targets')
            current_accuracy = context.get('current_accuracy', 0.0)
            
            # 1. ä¿¡æ¯æµå…¨å±€åˆ†æ
            logger.info("ğŸ” æ‰§è¡Œä¿¡æ¯æµå…¨å±€åˆ†æ...")
            flow_analysis = self._analyze_global_information_flow(
                model, activations, gradients, targets
            )
            
            # 2. æ£€æµ‹ä¿¡æ¯æ³„éœ²æ¼ç‚¹
            logger.info("ğŸ•³ï¸ æ£€æµ‹ä¿¡æ¯æ³„éœ²æ¼ç‚¹...")
            leak_points = self._detect_information_leak_points(
                model, activations, gradients, targets, current_accuracy
            )
            
            # 3. åˆ†ææ¯å±‚çš„å˜å¼‚æ½œåŠ›
            logger.info("ğŸ“Š åˆ†æå„å±‚å˜å¼‚æ½œåŠ›...")
            layer_analyses = {}
            
            for layer_name in activations.keys():
                if self._is_analyzable_layer(model, layer_name):
                    layer_analysis = self.analyze_layer_mutation_potential(
                        model, layer_name, activations, gradients, 
                        targets, current_accuracy
                    )
                    
                    # å¢å¼ºåˆ†æï¼šæ·»åŠ ä¿¡æ¯æµæ¼ç‚¹è¯„ä¼°
                    layer_analysis['leak_assessment'] = self._assess_layer_leak_potential(
                        layer_name, activations, gradients, leak_points
                    )
                    
                    layer_analyses[layer_name] = layer_analysis
            
            # 4. ç”Ÿæˆå…¨å±€å˜å¼‚ç­–ç•¥
            logger.info("ğŸ¯ ç”Ÿæˆå…¨å±€å˜å¼‚ç­–ç•¥...")
            global_strategy = self._generate_global_mutation_strategy(
                layer_analyses, leak_points, flow_analysis, current_accuracy
            )
            
            # 5. ç»„è£…å®Œæ•´åˆ†æç»“æœ
            complete_analysis = {
                'global_flow_analysis': flow_analysis,
                'detected_leak_points': leak_points,
                'layer_analyses': layer_analyses,
                'global_mutation_strategy': global_strategy,
                'analysis_metadata': {
                    'total_layers_analyzed': len(layer_analyses),
                    'critical_leak_points': len([lp for lp in leak_points if lp['severity'] > 0.7]),
                    'high_potential_layers': len([la for la in layer_analyses.values() 
                                                 if la.get('mutation_prediction', {}).get('improvement_potential', 0) > 0.5]),
                    'analysis_timestamp': time.time()
                }
            }
            
            logger.success(f"Net2Netå…¨å±‚åˆ†æå®Œæˆï¼Œå‘ç°{len(leak_points)}ä¸ªæ½œåœ¨æ¼ç‚¹")
            logger.exit_section("Net2Netå…¨å±‚åˆ†æ")
            
            return complete_analysis
            
        except Exception as e:
            logger.error(f"Net2Netå…¨å±‚åˆ†æå¤±è´¥: {e}")
            logger.exit_section("Net2Netå…¨å±‚åˆ†æ")
            return {
                'error': str(e),
                'global_mutation_strategy': {'action': 'skip', 'reason': f'åˆ†æå¤±è´¥: {e}'}
            }
    
    def _analyze_global_information_flow(self, model: nn.Module, 
                                       activations: Dict[str, torch.Tensor],
                                       gradients: Dict[str, torch.Tensor],
                                       targets: torch.Tensor) -> Dict[str, Any]:
        """åˆ†æå…¨å±€ä¿¡æ¯æµæ¨¡å¼"""
        
        flow_metrics = {}
        layer_names = list(activations.keys())
        
        for i, layer_name in enumerate(layer_names):
            if layer_name not in gradients:
                continue
                
            activation = activations[layer_name]
            gradient = gradients[layer_name]
            
            # è®¡ç®—ä¿¡æ¯å¯†åº¦æŒ‡æ ‡
            info_density = self._calculate_information_density(activation, gradient)
            
            # è®¡ç®—ä¿¡æ¯ä¼ é€’æ•ˆç‡ï¼ˆä¸ä¸‹ä¸€å±‚çš„ç›¸å…³æ€§ï¼‰
            transfer_efficiency = 0.0
            if i < len(layer_names) - 1:
                next_layer = layer_names[i + 1]
                if next_layer in activations:
                    transfer_efficiency = self._calculate_transfer_efficiency(
                        activation, activations[next_layer]
                    )
            
            # è®¡ç®—ä¿¡æ¯ä¿ç•™ç‡ï¼ˆä¸ç›®æ ‡çš„ç›¸å…³æ€§ï¼‰
            target_correlation = self._calculate_target_correlation(activation, targets)
            
            flow_metrics[layer_name] = {
                'information_density': info_density,
                'transfer_efficiency': transfer_efficiency,
                'target_correlation': target_correlation,
                'flow_bottleneck_score': self._calculate_bottleneck_score(
                    info_density, transfer_efficiency, target_correlation
                )
            }
        
        return {
            'layer_flow_metrics': flow_metrics,
            'global_bottleneck_score': np.mean([m['flow_bottleneck_score'] 
                                              for m in flow_metrics.values()]),
            'critical_bottlenecks': [name for name, metrics in flow_metrics.items() 
                                   if metrics['flow_bottleneck_score'] > 0.7]
        }
    
    def _detect_information_leak_points(self, model: nn.Module,
                                      activations: Dict[str, torch.Tensor],
                                      gradients: Dict[str, torch.Tensor],
                                      targets: torch.Tensor,
                                      current_accuracy: float) -> List[Dict[str, Any]]:
        """
        æ£€æµ‹ä¿¡æ¯æ³„éœ²æ¼ç‚¹
        
        æ¼ç‚¹çš„ç‰¹å¾ï¼š
        1. è¯¥å±‚çš„ä¿¡æ¯å¯†åº¦æ˜¾è‘—ä½äºå‰å±‚
        2. è¯¥å±‚çš„æ¢¯åº¦æ–¹å·®å¾ˆå°ï¼ˆå­¦ä¹ å›°éš¾ï¼‰
        3. åç»­å­ç½‘ç»œçš„å‚æ•°ç©ºé—´ä¸­é«˜å‡†ç¡®ç‡åŒºåŸŸå æ¯”å°
        4. å˜å¼‚è¯¥å±‚åï¼Œåç»­å­ç½‘ç»œæ€§èƒ½æå‡æ˜æ˜¾
        """
        
        leak_points = []
        layer_names = list(activations.keys())
        
        for i, layer_name in enumerate(layer_names[1:], 1):  # è·³è¿‡ç¬¬ä¸€å±‚
            if layer_name not in gradients:
                continue
                
            # è·å–å½“å‰å±‚å’Œå‰ä¸€å±‚çš„æ•°æ®
            current_activation = activations[layer_name]
            current_gradient = gradients[layer_name]
            prev_layer = layer_names[i-1]
            
            if prev_layer not in activations:
                continue
                
            prev_activation = activations[prev_layer]
            
            # 1. ä¿¡æ¯å¯†åº¦ä¸‹é™æ£€æµ‹
            current_info_density = self._calculate_information_density(
                current_activation, current_gradient
            )
            prev_info_density = self._calculate_information_density(
                prev_activation, gradients.get(prev_layer, torch.zeros_like(prev_activation))
            )
            
            info_drop = prev_info_density - current_info_density
            
            # 2. æ¢¯åº¦å­¦ä¹ å›°éš¾æ£€æµ‹
            gradient_variance = torch.var(current_gradient).item()
            learning_difficulty = 1.0 / (1.0 + gradient_variance)  # æ–¹å·®è¶Šå°ï¼Œå­¦ä¹ è¶Šå›°éš¾
            
            # 3. åç»­å­ç½‘ç»œæ•ˆç‡è¯„ä¼°
            posterior_efficiency = self._evaluate_posterior_subnetwork_efficiency(
                model, layer_name, activations, targets
            )
            
            # 4. å˜å¼‚æ½œåŠ›è¯„ä¼°
            mutation_potential = self._estimate_mutation_improvement_potential(
                current_activation, current_gradient, targets, current_accuracy
            )
            
            # ç»¼åˆè¯„ä¼°æ¼ç‚¹ä¸¥é‡ç¨‹åº¦
            leak_severity = (
                info_drop * 0.3 +
                learning_difficulty * 0.2 +
                (1.0 - posterior_efficiency) * 0.3 +
                mutation_potential * 0.2
            )
            
            if leak_severity > 0.5:  # é˜ˆå€¼å¯è°ƒ
                leak_points.append({
                    'layer_name': layer_name,
                    'severity': leak_severity,
                    'info_density_drop': info_drop,
                    'learning_difficulty': learning_difficulty,
                    'posterior_efficiency': posterior_efficiency,
                    'mutation_potential': mutation_potential,
                    'leak_type': self._classify_leak_type(
                        info_drop, learning_difficulty, posterior_efficiency
                    )
                })
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
        leak_points.sort(key=lambda x: x['severity'], reverse=True)
        
        return leak_points
    
    def _assess_layer_leak_potential(self, layer_name: str,
                                   activations: Dict[str, torch.Tensor],
                                   gradients: Dict[str, torch.Tensor],
                                   leak_points: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è¯„ä¼°ç‰¹å®šå±‚çš„æ¼ç‚¹æ½œåŠ›"""
        
        # æ£€æŸ¥è¯¥å±‚æ˜¯å¦è¢«è¯†åˆ«ä¸ºæ¼ç‚¹
        is_leak_point = any(lp['layer_name'] == layer_name for lp in leak_points)
        
        if is_leak_point:
            leak_info = next(lp for lp in leak_points if lp['layer_name'] == layer_name)
            
            return {
                'is_leak_point': True,
                'leak_severity': leak_info['severity'],
                'leak_type': leak_info['leak_type'],
                'recommended_mutation_priority': 'high' if leak_info['severity'] > 0.7 else 'medium',
                'expected_improvement': leak_info['mutation_potential']
            }
        else:
            return {
                'is_leak_point': False,
                'leak_severity': 0.0,
                'recommended_mutation_priority': 'low',
                'expected_improvement': 0.0
            }
    
    def _generate_global_mutation_strategy(self, layer_analyses: Dict[str, Any],
                                         leak_points: List[Dict[str, Any]],
                                         flow_analysis: Dict[str, Any],
                                         current_accuracy: float) -> Dict[str, Any]:
        """ç”Ÿæˆå…¨å±€å˜å¼‚ç­–ç•¥"""
        
        # 1. ä¼˜å…ˆå¤„ç†ä¸¥é‡æ¼ç‚¹
        priority_targets = []
        for leak_point in leak_points:
            if leak_point['severity'] > 0.7:
                priority_targets.append({
                    'layer_name': leak_point['layer_name'],
                    'priority': 'critical',
                    'expected_improvement': leak_point['mutation_potential'],
                    'strategy': self._select_optimal_mutation_strategy(leak_point)
                })
        
        # 2. è€ƒè™‘é«˜æ½œåŠ›éæ¼ç‚¹å±‚
        for layer_name, analysis in layer_analyses.items():
            mutation_potential = analysis.get('mutation_prediction', {}).get('improvement_potential', 0)
            if mutation_potential > 0.6 and not any(t['layer_name'] == layer_name for t in priority_targets):
                priority_targets.append({
                    'layer_name': layer_name,
                    'priority': 'high',
                    'expected_improvement': mutation_potential,
                    'strategy': analysis.get('recommendation', {}).get('strategy', 'widening')
                })
        
        # 3. ç”Ÿæˆæ‰§è¡Œè®¡åˆ’
        execution_plan = self._create_mutation_execution_plan(
            priority_targets, current_accuracy, flow_analysis
        )
        
        return {
            'priority_targets': priority_targets,
            'execution_plan': execution_plan,
            'global_improvement_estimate': sum(t['expected_improvement'] for t in priority_targets),
            'recommended_sequence': [t['layer_name'] for t in 
                                   sorted(priority_targets, key=lambda x: x['expected_improvement'], reverse=True)]
        }
    
    def _calculate_information_density(self, activation: torch.Tensor, gradient: torch.Tensor) -> float:
        """è®¡ç®—ä¿¡æ¯å¯†åº¦"""
        # ä½¿ç”¨æ¿€æ´»å€¼çš„ç†µå’Œæ¢¯åº¦çš„æ–¹å·®ä½œä¸ºä¿¡æ¯å¯†åº¦æŒ‡æ ‡
        activation_entropy = self._calculate_entropy(activation)
        gradient_variance = torch.var(gradient).item()
        
        # å½’ä¸€åŒ–å¹¶ç»„åˆ
        info_density = (activation_entropy + np.log(1 + gradient_variance)) / 2
        return float(info_density)
    
    def _calculate_entropy(self, tensor: torch.Tensor) -> float:
        """è®¡ç®—å¼ é‡çš„è¿‘ä¼¼ç†µ"""
        # å°†å¼ é‡å±•å¹³å¹¶è®¡ç®—ç›´æ–¹å›¾
        flat_tensor = tensor.flatten()
        hist, _ = np.histogram(flat_tensor.cpu().numpy(), bins=50, density=True)
        
        # é¿å…log(0)
        hist = hist + 1e-10
        entropy = -np.sum(hist * np.log(hist))
        
        return float(entropy)
    
    def _calculate_transfer_efficiency(self, current_activation: torch.Tensor, 
                                     next_activation: torch.Tensor) -> float:
        """è®¡ç®—ä¿¡æ¯ä¼ é€’æ•ˆç‡"""
        # è®¡ç®—æ¿€æ´»å€¼ä¹‹é—´çš„ç›¸å…³æ€§
        curr_flat = current_activation.flatten()
        next_flat = next_activation.flatten()
        
        # è°ƒæ•´å°ºå¯¸ä»¥åŒ¹é…
        min_size = min(len(curr_flat), len(next_flat))
        curr_flat = curr_flat[:min_size]
        next_flat = next_flat[:min_size]
        
        correlation = torch.corrcoef(torch.stack([curr_flat, next_flat]))[0, 1]
        
        # å¤„ç†NaNæƒ…å†µ
        if torch.isnan(correlation):
            return 0.0
            
        return float(torch.abs(correlation))
    
    def _calculate_target_correlation(self, activation: torch.Tensor, targets: torch.Tensor) -> float:
        """è®¡ç®—ä¸ç›®æ ‡çš„ç›¸å…³æ€§"""
        # ç®€åŒ–çš„ç›¸å…³æ€§è®¡ç®—
        activation_mean = torch.mean(activation, dim=tuple(range(1, activation.dim())))
        
        if len(activation_mean) != len(targets):
            return 0.0
            
        # è®¡ç®—ä¸ç›®æ ‡çš„ç›¸å…³æ€§
        try:
            correlation = torch.corrcoef(torch.stack([
                activation_mean.float(),
                targets.float()
            ]))[0, 1]
            
            if torch.isnan(correlation):
                return 0.0
                
            return float(torch.abs(correlation))
        except:
            return 0.0
    
    def _calculate_bottleneck_score(self, info_density: float, transfer_efficiency: float, 
                                  target_correlation: float) -> float:
        """è®¡ç®—ç“¶é¢ˆåˆ†æ•°"""
        # ç“¶é¢ˆåˆ†æ•° = ä¿¡æ¯å¯†åº¦ä½ + ä¼ é€’æ•ˆç‡ä½ + ç›®æ ‡ç›¸å…³æ€§ä½
        bottleneck_score = (
            (1.0 - min(info_density / 10.0, 1.0)) * 0.4 +
            (1.0 - transfer_efficiency) * 0.3 +
            (1.0 - target_correlation) * 0.3
        )
        
        return float(bottleneck_score)
    
    def _evaluate_posterior_subnetwork_efficiency(self, model: nn.Module, layer_name: str,
                                                activations: Dict[str, torch.Tensor],
                                                targets: torch.Tensor) -> float:
        """è¯„ä¼°åç»­å­ç½‘ç»œæ•ˆç‡"""
        # è·å–è¯¥å±‚ä¹‹åçš„æ‰€æœ‰å±‚
        layer_names = list(activations.keys())
        try:
            layer_idx = layer_names.index(layer_name)
            posterior_layers = layer_names[layer_idx + 1:]
        except ValueError:
            return 0.5  # é»˜è®¤ä¸­ç­‰æ•ˆç‡
        
        if not posterior_layers:
            return 1.0  # æœ€åä¸€å±‚ï¼Œæ•ˆç‡ä¸º1
        
        # è®¡ç®—åç»­å±‚çš„å¹³å‡ä¿¡æ¯å¤„ç†æ•ˆç‡
        efficiency_scores = []
        
        for post_layer in posterior_layers:
            if post_layer in activations:
                post_activation = activations[post_layer]
                target_corr = self._calculate_target_correlation(post_activation, targets)
                efficiency_scores.append(target_corr)
        
        if not efficiency_scores:
            return 0.5
            
        return float(np.mean(efficiency_scores))
    
    def _estimate_mutation_improvement_potential(self, activation: torch.Tensor,
                                               gradient: torch.Tensor,
                                               targets: torch.Tensor,
                                               current_accuracy: float) -> float:
        """ä¼°ç®—å˜å¼‚æ”¹è¿›æ½œåŠ›"""
        # åŸºäºæ¢¯åº¦å’Œæ¿€æ´»æ¨¡å¼ä¼°ç®—å˜å¼‚åçš„æ”¹è¿›æ½œåŠ›
        
        # 1. æ¢¯åº¦å¤šæ ·æ€§ï¼ˆé«˜å¤šæ ·æ€§ = é«˜æ”¹è¿›æ½œåŠ›ï¼‰
        gradient_diversity = torch.std(gradient).item()
        
        # 2. æ¿€æ´»é¥±å’Œåº¦ï¼ˆä½é¥±å’Œåº¦ = é«˜æ”¹è¿›æ½œåŠ›ï¼‰
        activation_saturation = torch.mean(torch.sigmoid(activation)).item()
        saturation_score = 1.0 - abs(activation_saturation - 0.5) * 2  # 0.5ä¸ºæœ€ä½³
        
        # 3. å½“å‰å‡†ç¡®ç‡è·ç¦»ä¸Šé™çš„ç©ºé—´
        accuracy_headroom = (0.95 - current_accuracy) / 0.95
        
        # ç»¼åˆè¯„ä¼°
        improvement_potential = (
            gradient_diversity * 0.3 +
            saturation_score * 0.3 +
            accuracy_headroom * 0.4
        )
        
        return float(np.clip(improvement_potential, 0.0, 1.0))
    
    def _classify_leak_type(self, info_drop: float, learning_difficulty: float, 
                          posterior_efficiency: float) -> str:
        """åˆ†ç±»æ¼ç‚¹ç±»å‹"""
        if info_drop > 0.5:
            return "information_compression_bottleneck"
        elif learning_difficulty > 0.7:
            return "gradient_learning_bottleneck"
        elif posterior_efficiency < 0.3:
            return "representational_bottleneck"
        else:
            return "general_bottleneck"
    
    def _select_optimal_mutation_strategy(self, leak_point: Dict[str, Any]) -> str:
        """ä¸ºæ¼ç‚¹é€‰æ‹©æœ€ä¼˜å˜å¼‚ç­–ç•¥"""
        leak_type = leak_point['leak_type']
        severity = leak_point['severity']
        
        if leak_type == "information_compression_bottleneck":
            return "widening"  # å¢åŠ é€šé“æ•°
        elif leak_type == "gradient_learning_bottleneck":
            return "deepening"  # å¢åŠ å±‚æ•°
        elif leak_type == "representational_bottleneck":
            return "hybrid_expansion"  # æ··åˆæ‰©å±•
        else:
            # æ ¹æ®ä¸¥é‡ç¨‹åº¦é€‰æ‹©
            if severity > 0.8:
                return "aggressive_widening"
            else:
                return "conservative_widening"
    
    def _create_mutation_execution_plan(self, priority_targets: List[Dict[str, Any]],
                                      current_accuracy: float,
                                      flow_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºå˜å¼‚æ‰§è¡Œè®¡åˆ’"""
        
        # æ ¹æ®å½“å‰å‡†ç¡®ç‡å’Œå…¨å±€æµåˆ†æç¡®å®šæ‰§è¡Œç­–ç•¥
        if current_accuracy < 0.85:
            execution_mode = "conservative"
            max_concurrent = 1
        elif current_accuracy < 0.92:
            execution_mode = "moderate"
            max_concurrent = 2
        else:
            execution_mode = "aggressive"
            max_concurrent = 3
        
        return {
            'execution_mode': execution_mode,
            'max_concurrent_mutations': max_concurrent,
            'total_expected_improvement': sum(t['expected_improvement'] for t in priority_targets),
            'estimated_parameter_cost': len(priority_targets) * 5000,  # ä¼°ç®—
            'execution_phases': self._plan_execution_phases(priority_targets, max_concurrent)
        }
    
    def _plan_execution_phases(self, targets: List[Dict[str, Any]], max_concurrent: int) -> List[List[str]]:
        """è§„åˆ’æ‰§è¡Œé˜¶æ®µ"""
        phases = []
        
        # æŒ‰ä¼˜å…ˆçº§åˆ†ç»„
        critical = [t for t in targets if t['priority'] == 'critical']
        high = [t for t in targets if t['priority'] == 'high']
        
        # ç¬¬ä¸€é˜¶æ®µï¼šå…³é”®æ¼ç‚¹
        if critical:
            phases.append([t['layer_name'] for t in critical[:max_concurrent]])
        
        # ç¬¬äºŒé˜¶æ®µï¼šé«˜æ½œåŠ›å±‚
        if high:
            phases.append([t['layer_name'] for t in high[:max_concurrent]])
        
        return phases
    
    def _is_analyzable_layer(self, model: nn.Module, layer_name: str) -> bool:
        """åˆ¤æ–­å±‚æ˜¯å¦å¯åˆ†æ"""
        try:
            module = dict(model.named_modules())[layer_name]
            return isinstance(module, (nn.Linear, nn.Conv2d, nn.Conv1d))
        except:
            return False

    def analyze_layer_mutation_potential(self, 
                                       model: nn.Module,
                                       layer_name: str,
                                       activations: Dict[str, torch.Tensor],
                                       gradients: Dict[str, torch.Tensor],
                                       targets: torch.Tensor,
                                       current_accuracy: float) -> Dict[str, Any]:
        """
        åˆ†ææŒ‡å®šå±‚çš„å˜å¼‚æ½œåŠ›
        
        Args:
            model: å®Œæ•´æ¨¡å‹
            layer_name: ç›®æ ‡å±‚åç§°
            activations: æ¿€æ´»å€¼å­—å…¸
            gradients: æ¢¯åº¦å­—å…¸
            targets: çœŸå®æ ‡ç­¾
            current_accuracy: å½“å‰å‡†ç¡®ç‡
            
        Returns:
            å®Œæ•´çš„å˜å¼‚æ½œåŠ›åˆ†æç»“æœ
        """
        logger.enter_section(f"Net2Netåˆ†æ: {layer_name}")
        
        try:
            # 1. æå–å­ç½‘ç»œ
            subnetwork, subnetwork_info = self.extractor.extract_subnetwork_from_layer(
                model, layer_name
            )
            
            # 2. è·å–è¯¥å±‚çš„æ¿€æ´»å’Œæ¢¯åº¦
            if layer_name in activations and layer_name in gradients:
                layer_activation = activations[layer_name]
                layer_gradient = gradients[layer_name]
            else:
                logger.warning(f"å±‚{layer_name}ç¼ºå°‘æ¿€æ´»å€¼æˆ–æ¢¯åº¦ä¿¡æ¯")
                layer_activation = torch.randn(32, 64)  # é»˜è®¤å€¼
                layer_gradient = torch.randn(32, 64)
            
            # 3. åˆ†æå‚æ•°ç©ºé—´æ•ˆç‡
            param_space_analysis = self.param_analyzer.analyze_parameter_space_efficiency(
                subnetwork, layer_activation, layer_gradient, targets
            )
            
            # 4. é¢„æµ‹å˜å¼‚æ½œåŠ›
            mutation_prediction = self.predictor.predict_mutation_potential(
                subnetwork, subnetwork_info, param_space_analysis, current_accuracy
            )
            
            # 5. ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
            analysis_result = {
                'layer_name': layer_name,
                'subnetwork_info': subnetwork_info,
                'parameter_space_analysis': param_space_analysis,
                'mutation_prediction': mutation_prediction,
                'recommendation': self._generate_recommendation(
                    layer_name, param_space_analysis, mutation_prediction
                )
            }
            
            logger.success(f"Net2Netåˆ†æå®Œæˆ: {layer_name}")
            logger.exit_section(f"Net2Netåˆ†æ: {layer_name}")
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"Net2Netåˆ†æå¤±è´¥: {layer_name} - {e}")
            logger.exit_section(f"Net2Netåˆ†æ: {layer_name}")
            return {
                'layer_name': layer_name,
                'error': str(e),
                'recommendation': {'action': 'skip', 'reason': f'åˆ†æå¤±è´¥: {e}'}
            }

    def _generate_recommendation(self, 
                               layer_name: str,
                               param_space_analysis: Dict[str, float],
                               mutation_prediction: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆå˜å¼‚å»ºè®®"""
        
        improvement_potential = mutation_prediction['improvement_potential']
        risk_assessment = mutation_prediction['risk_assessment']
        strategy_predictions = mutation_prediction['strategy_predictions']
        
        # é€‰æ‹©æœ€ä½³ç­–ç•¥
        best_strategy = None
        best_score = -1.0
        
        for strategy_name, strategy_info in strategy_predictions.items():
            # ç»¼åˆè¯„åˆ†ï¼šæœŸæœ›æ”¶ç›Š - é£é™© - æˆæœ¬
            score = (
                strategy_info['expected_accuracy_gain'] * 2.0 -
                strategy_info['stability_risk'] -
                strategy_info['parameter_cost'] * 0.5
            )
            
            if score > best_score:
                best_score = score
                best_strategy = strategy_name
        
        # ç”Ÿæˆå»ºè®®
        if improvement_potential > 0.3 and risk_assessment['overall_risk'] < 0.6:
            action = 'mutate'
            priority = 'high' if improvement_potential > 0.6 else 'medium'
        elif improvement_potential > 0.1 and risk_assessment['overall_risk'] < 0.8:
            action = 'consider'
            priority = 'low'
        else:
            action = 'skip'
            priority = 'none'
        
        return {
            'action': action,
            'priority': priority,
            'recommended_strategy': best_strategy,
            'expected_gain': strategy_predictions.get(best_strategy, {}).get('expected_accuracy_gain', 0.0),
            'risk_level': risk_assessment['overall_risk'],
            'reason': f"æ”¹è¿›æ½œåŠ›={improvement_potential:.3f}, é£é™©={risk_assessment['overall_risk']:.3f}"
        }

# æ–°å¢ï¼šä¿¡æ¯æµåˆ†æå™¨ç±»
class InformationFlowAnalyzer:
    """ä¿¡æ¯æµåˆ†æå™¨"""
    
    def __init__(self):
        self.flow_patterns = {}
        
    def analyze_flow_patterns(self, activations: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """åˆ†æä¿¡æ¯æµæ¨¡å¼"""
        # å®ç°ä¿¡æ¯æµåˆ†æé€»è¾‘
        return {}

class InformationLeakDetector:
    """ä¿¡æ¯æ³„éœ²æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.leak_thresholds = {
            'entropy_drop': 0.5,
            'gradient_variance': 0.1,
            'correlation_loss': 0.3
        }
    
    def detect_leaks(self, layer_data: Dict[str, torch.Tensor]) -> List[Dict[str, Any]]:
        """æ£€æµ‹ä¿¡æ¯æ³„éœ²ç‚¹"""
        # å®ç°æ³„éœ²æ£€æµ‹é€»è¾‘
        return []