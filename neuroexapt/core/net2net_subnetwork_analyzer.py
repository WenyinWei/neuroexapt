#!/usr/bin/env python3
"""
@defgroup group_net2net_subnetwork_analyzer Net2Net Subnetwork Analyzer
@ingroup core
Net2Net Subnetwork Analyzer module for NeuroExapt framework.

Net2Netå­ç½‘ç»œåˆ†æå™¨ - Net2Net Subnetwork Analyzer

ğŸ¯ æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä»æŒ‡å®šå±‚åˆ°è¾“å‡ºå±‚æå–å­ç½‘ç»œ
2. è¯„ä¼°å­ç½‘ç»œçš„å˜å¼‚æ½œåŠ›
3. é¢„æµ‹å˜å¼‚åçš„å‡†ç¡®ç‡æå‡ç©ºé—´
4. åˆ†æå¯è¡Œå‚æ•°ç©ºé—´åœ¨æ€»å‚æ•°ç©ºé—´ä¸­çš„å æ¯”
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Any, Set
from collections import OrderedDict, defaultdict
import copy
import logging
import time

from .logging_utils import logger

class SubnetworkExtractor:
    """å­ç½‘ç»œæå–å™¨"""
    
    def __init__(self):
        self.extracted_subnetworks = {}
        self.layer_dependencies = {}
        
    def extract_subnetwork_from_layer(self, 
                                    model: nn.Module, 
                                    start_layer_name: str,
                                    include_start_layer: bool = True) -> Tuple[nn.Module, Dict[str, Any]]:
        """
        ä»æŒ‡å®šå±‚å¼€å§‹æå–åˆ°è¾“å‡ºå±‚çš„å­ç½‘ç»œ
        
        Args:
            model: åŸå§‹æ¨¡å‹
            start_layer_name: èµ·å§‹å±‚åç§°
            include_start_layer: æ˜¯å¦åŒ…å«èµ·å§‹å±‚
            
        Returns:
            å­ç½‘ç»œæ¨¡å‹å’Œæå–ä¿¡æ¯
        """
        logger.enter_section(f"æå–å­ç½‘ç»œ: {start_layer_name}")
        
        # 1. åˆ†ææ¨¡å‹ç»“æ„ï¼Œæ‰¾åˆ°æ‰€æœ‰å±‚çš„ä¾èµ–å…³ç³»
        layer_graph = self._build_layer_dependency_graph(model)
        
        # 2. ä»èµ·å§‹å±‚å¼€å§‹ï¼Œæ‰¾åˆ°æ‰€æœ‰åç»­å±‚
        target_layers = self._find_downstream_layers(layer_graph, start_layer_name, include_start_layer)
        
        logger.info(f"è¯†åˆ«å‡º{len(target_layers)}ä¸ªä¸‹æ¸¸å±‚")
        
        # 3. æ„å»ºå­ç½‘ç»œ
        subnetwork = self._build_subnetwork(model, target_layers, start_layer_name)
        
        # 4. åˆ†æå­ç½‘ç»œä¿¡æ¯
        subnetwork_info = self._analyze_subnetwork(subnetwork, target_layers, start_layer_name)
        
        logger.info(f"å­ç½‘ç»œæ„å»ºå®Œæˆ: {subnetwork_info['total_params']:,}å‚æ•°")
        logger.exit_section(f"æå–å­ç½‘ç»œ: {start_layer_name}")
        
        return subnetwork, subnetwork_info
    
    def _build_layer_dependency_graph(self, model: nn.Module) -> Dict[str, List[str]]:
        """æ„å»ºå±‚ä¾èµ–å…³ç³»å›¾"""
        layer_graph = defaultdict(list)
        named_modules = dict(model.named_modules())
        
        # åˆ†æResNeté£æ ¼çš„å‰å‘ä¼ æ’­ä¾èµ–
        for name, module in named_modules.items():
            if name == '':  # è·³è¿‡æ ¹æ¨¡å—
                continue
                
            # è§£æå±‚çš„é€»è¾‘ä½ç½®
            parts = name.split('.')
            
            if len(parts) >= 2:
                # å¯¹äºå±‚çº§ç»“æ„ï¼Œæ·»åŠ ä¾èµ–å…³ç³»
                parent_parts = parts[:-1]
                parent_name = '.'.join(parent_parts)
                
                if parent_name in named_modules:
                    layer_graph[parent_name].append(name)
        
        # æ·»åŠ ç‰¹æ®Šçš„ä¾èµ–å…³ç³»ï¼ˆåŸºäºResNetæ¶æ„ï¼‰
        self._add_resnet_dependencies(layer_graph, named_modules)
        
        return dict(layer_graph)
    
    def _add_resnet_dependencies(self, layer_graph: Dict[str, List[str]], named_modules: Dict[str, nn.Module]):
        """æ·»åŠ ResNetç‰¹å®šçš„ä¾èµ–å…³ç³»"""
        
        # ä¸»å¹²ä¾èµ–ï¼šconv1 -> feature_blocks -> classifier
        main_sequence = []
        
        # æŸ¥æ‰¾ä¸»è¦ç»„ä»¶
        if 'conv1' in named_modules:
            main_sequence.append('conv1')
        
        # æŸ¥æ‰¾feature blocks
        feature_blocks = []
        for name in named_modules.keys():
            if name.startswith('feature_block'):
                if '.' not in name[len('feature_block'):]:  # åªè¦é¡¶çº§feature_block
                    feature_blocks.append(name)
        
        feature_blocks.sort()  # æŒ‰åç§°æ’åº
        main_sequence.extend(feature_blocks)
        
        # æŸ¥æ‰¾åˆ†ç±»å™¨
        if 'classifier' in named_modules:
            main_sequence.append('classifier')
        
        # å»ºç«‹ä¸»å¹²ä¾èµ–
        for i in range(len(main_sequence) - 1):
            current = main_sequence[i]
            next_layer = main_sequence[i + 1]
            if current not in layer_graph:
                layer_graph[current] = []
            layer_graph[current].append(next_layer)
    
    def _find_downstream_layers(self, 
                               layer_graph: Dict[str, List[str]], 
                               start_layer: str, 
                               include_start: bool = True) -> Set[str]:
        """æ‰¾åˆ°æŒ‡å®šå±‚ä¹‹åçš„æ‰€æœ‰ä¸‹æ¸¸å±‚"""
        
        downstream_layers = set()
        
        if include_start:
            downstream_layers.add(start_layer)
        
        # BFSéå†æ‰¾åˆ°æ‰€æœ‰ä¸‹æ¸¸å±‚
        queue = [start_layer]
        visited = set([start_layer])
        
        while queue:
            current_layer = queue.pop(0)
            
            # æ·»åŠ å½“å‰å±‚çš„æ‰€æœ‰ä¸‹æ¸¸å±‚
            if current_layer in layer_graph:
                for next_layer in layer_graph[current_layer]:
                    if next_layer not in visited:
                        downstream_layers.add(next_layer)
                        queue.append(next_layer)
                        visited.add(next_layer)
        
        return downstream_layers
    
    def _build_subnetwork(self, 
                         model: nn.Module, 
                         target_layers: Set[str], 
                         start_layer_name: str) -> nn.Module:
        """æ„å»ºåŒ…å«ç›®æ ‡å±‚çš„å­ç½‘ç»œ"""
        
        # è·å–æ¨¡å‹çš„æ‰€æœ‰å‘½åæ¨¡å—
        named_modules = dict(model.named_modules())
        
        # åˆ›å»ºå­ç½‘ç»œæ¨¡å—å­—å…¸
        subnetwork_modules = OrderedDict()
        
        # æ·»åŠ ç›®æ ‡å±‚åˆ°å­ç½‘ç»œ
        for layer_name in sorted(target_layers):
            if layer_name in named_modules:
                subnetwork_modules[layer_name] = copy.deepcopy(named_modules[layer_name])
        
        # åˆ›å»ºåŠ¨æ€å­ç½‘ç»œç±»
        class DynamicSubnetwork(nn.Module):
            def __init__(self, modules_dict, start_layer):
                super().__init__()
                self.start_layer = start_layer
                self.modules_dict = nn.ModuleDict(modules_dict)
                
                # åˆ†æè¾“å…¥è¾“å‡ºç»´åº¦
                self._analyze_io_dims()
            
            def _analyze_io_dims(self):
                """åˆ†æè¾“å…¥è¾“å‡ºç»´åº¦"""
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ ¹æ®æ¨¡å‹ç»“æ„åŠ¨æ€åˆ†æ
                self.input_dim = None
                self.output_dim = None
                
                # æŸ¥æ‰¾è¾“å…¥å’Œè¾“å‡ºå±‚
                layer_names = list(self.modules_dict.keys())
                if layer_names:
                    first_layer = self.modules_dict[layer_names[0]]
                    last_layer = self.modules_dict[layer_names[-1]]
                    
                    # å°è¯•è·å–è¾“å…¥ç»´åº¦
                    if hasattr(first_layer, 'in_features'):
                        self.input_dim = first_layer.in_features
                    elif hasattr(first_layer, 'in_channels'):
                        self.input_dim = first_layer.in_channels
                    
                    # å°è¯•è·å–è¾“å‡ºç»´åº¦
                    if hasattr(last_layer, 'out_features'):
                        self.output_dim = last_layer.out_features
                    elif hasattr(last_layer, 'out_channels'):
                        self.output_dim = last_layer.out_channels
            
            def forward(self, x):
                """ç®€åŒ–çš„å‰å‘ä¼ æ’­"""
                # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…éœ€è¦æ ¹æ®å…·ä½“æ¶æ„å®ç°
                current = x
                
                for name, module in self.modules_dict.items():
                    try:
                        if isinstance(module, (nn.Linear, nn.Conv2d)):
                            current = module(current)
                        elif isinstance(module, nn.Sequential):
                            current = module(current)
                        else:
                            # å¯¹äºå…¶ä»–ç±»å‹çš„æ¨¡å—ï¼Œå°è¯•ç›´æ¥è°ƒç”¨
                            current = module(current)
                    except Exception as e:
                        logger.warning(f"å­ç½‘ç»œå‰å‘ä¼ æ’­åœ¨å±‚{name}å¤±è´¥: {e}")
                        break
                
                return current
        
        subnetwork = DynamicSubnetwork(subnetwork_modules, start_layer_name)
        return subnetwork
    
    def _analyze_subnetwork(self, 
                           subnetwork: nn.Module, 
                           target_layers: Set[str], 
                           start_layer_name: str) -> Dict[str, Any]:
        """åˆ†æå­ç½‘ç»œçš„ç‰¹æ€§"""
        
        total_params = sum(p.numel() for p in subnetwork.parameters())
        trainable_params = sum(p.numel() for p in subnetwork.parameters() if p.requires_grad)
        
        # åˆ†æå±‚ç±»å‹åˆ†å¸ƒ
        layer_types = defaultdict(int)
        for name, module in subnetwork.named_modules():
            if name:  # è·³è¿‡æ ¹æ¨¡å—
                layer_types[type(module).__name__] += 1
        
        return {
            'start_layer': start_layer_name,
            'layer_count': len(target_layers),
            'total_params': total_params,
            'trainable_params': trainable_params,
            'layer_types': dict(layer_types),
            'input_dim': getattr(subnetwork, 'input_dim', None),
            'output_dim': getattr(subnetwork, 'output_dim', None)
        }

class ParameterSpaceAnalyzer:
    """å‚æ•°ç©ºé—´åˆ†æå™¨"""
    
    def __init__(self):
        self.analysis_cache = {}
    
    def analyze_parameter_space_efficiency(self, 
                                         subnetwork: nn.Module,
                                         activations: torch.Tensor,
                                         gradients: torch.Tensor,
                                         targets: torch.Tensor) -> Dict[str, float]:
        """
        åˆ†æå‚æ•°ç©ºé—´æ•ˆç‡
        
        æ ¸å¿ƒæ€æƒ³ï¼šè¯„ä¼°å½“å‰å‚æ•°åœ¨è§£å†³åˆ†ç±»ä»»åŠ¡æ—¶çš„æ•ˆç‡
        """
        logger.enter_section("å‚æ•°ç©ºé—´æ•ˆç‡åˆ†æ")
        
        try:
            # 1. è®¡ç®—å‚æ•°åˆ©ç”¨ç‡
            param_utilization = self._compute_parameter_utilization(subnetwork, gradients)
            
            # 2. åˆ†æè¡¨ç¤ºèƒ½åŠ›
            representation_capacity = self._compute_representation_capacity(activations, targets)
            
            # 3. è¯„ä¼°å†—ä½™åº¦
            redundancy_ratio = self._compute_parameter_redundancy(subnetwork, activations)
            
            # 4. è®¡ç®—å¯è¡Œå‚æ•°ç©ºé—´å æ¯”
            feasible_space_ratio = self._estimate_feasible_parameter_space(
                subnetwork, activations, targets
            )
            
            # 5. ç»¼åˆæ•ˆç‡è¯„åˆ†
            overall_efficiency = (
                0.3 * param_utilization +
                0.3 * representation_capacity +
                0.2 * (1.0 - redundancy_ratio) +  # å†—ä½™åº¦è¶Šä½è¶Šå¥½
                0.2 * feasible_space_ratio
            )
            
            analysis_result = {
                'parameter_utilization': param_utilization,
                'representation_capacity': representation_capacity,
                'redundancy_ratio': redundancy_ratio,
                'feasible_space_ratio': feasible_space_ratio,
                'overall_efficiency': overall_efficiency
            }
            
            logger.info(f"å‚æ•°ç©ºé—´åˆ†æå®Œæˆ: æ•´ä½“æ•ˆç‡={overall_efficiency:.3f}")
            logger.exit_section("å‚æ•°ç©ºé—´æ•ˆç‡åˆ†æ")
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"å‚æ•°ç©ºé—´åˆ†æå¤±è´¥: {e}")
            logger.exit_section("å‚æ•°ç©ºé—´æ•ˆç‡åˆ†æ")
            return {
                'parameter_utilization': 0.0,
                'representation_capacity': 0.0,
                'redundancy_ratio': 1.0,
                'feasible_space_ratio': 0.0,
                'overall_efficiency': 0.0
            }
    
    def _compute_parameter_utilization(self, subnetwork: nn.Module, gradients: torch.Tensor) -> float:
        """è®¡ç®—å‚æ•°åˆ©ç”¨ç‡ - æœ‰å¤šå°‘å‚æ•°åœ¨ç§¯æå‚ä¸å­¦ä¹ """
        
        total_params = sum(p.numel() for p in subnetwork.parameters())
        if total_params == 0:
            return 0.0
        
        # è®¡ç®—æœ‰æ•ˆæ¢¯åº¦çš„å‚æ•°æ•°é‡
        active_params = 0
        for param in subnetwork.parameters():
            if param.grad is not None:
                # æ¢¯åº¦æ˜¾è‘—éé›¶çš„å‚æ•°è¢«è®¤ä¸ºæ˜¯æ´»è·ƒçš„
                significant_grads = torch.abs(param.grad) > 1e-6
                active_params += significant_grads.sum().item()
        
        utilization = active_params / total_params
        return min(utilization, 1.0)
    
    def _compute_representation_capacity(self, activations: torch.Tensor, targets: torch.Tensor) -> float:
        """è®¡ç®—è¡¨ç¤ºèƒ½åŠ› - ç½‘ç»œèƒ½å¤šå¥½åœ°åŒºåˆ†ä¸åŒç±»åˆ«"""
        
        try:
            # è®¡ç®—ç±»é—´åˆ†ç¦»åº¦
            if len(activations.shape) > 2:
                # å¯¹äºå·ç§¯å±‚è¾“å‡ºï¼Œå–å¹³å‡æ± åŒ–
                activations_flat = F.adaptive_avg_pool2d(activations, (1, 1)).flatten(1)
            else:
                activations_flat = activations
            
            # è®¡ç®—ä¸åŒç±»åˆ«çš„æ¿€æ´»åˆ†å¸ƒ
            unique_targets = torch.unique(targets)
            if len(unique_targets) < 2:
                return 0.0
            
            class_centers = []
            for target_class in unique_targets:
                mask = targets == target_class
                if mask.sum() > 0:
                    class_center = activations_flat[mask].mean(dim=0)
                    class_centers.append(class_center)
            
            if len(class_centers) < 2:
                return 0.0
            
            # è®¡ç®—ç±»é—´è·ç¦»
            class_centers = torch.stack(class_centers)
            distances = torch.pdist(class_centers)
            avg_inter_class_distance = distances.mean().item()
            
            # è®¡ç®—ç±»å†…æ–¹å·®
            intra_class_variance = 0.0
            for target_class in unique_targets:
                mask = targets == target_class
                if mask.sum() > 1:
                    class_activations = activations_flat[mask]
                    class_center = class_activations.mean(dim=0)
                    variance = ((class_activations - class_center) ** 2).mean().item()
                    intra_class_variance += variance
            
            intra_class_variance /= len(unique_targets)
            
            # è¡¨ç¤ºèƒ½åŠ› = ç±»é—´è·ç¦» / ç±»å†…æ–¹å·®
            if intra_class_variance > 0:
                representation_capacity = avg_inter_class_distance / (intra_class_variance + 1e-8)
            else:
                representation_capacity = avg_inter_class_distance
            
            # å½’ä¸€åŒ–åˆ°[0, 1]
            return min(representation_capacity / 10.0, 1.0)
            
        except Exception as e:
            logger.warning(f"è¡¨ç¤ºèƒ½åŠ›è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _compute_parameter_redundancy(self, subnetwork: nn.Module, activations: torch.Tensor) -> float:
        """è®¡ç®—å‚æ•°å†—ä½™åº¦"""
        
        try:
            redundancy_scores = []
            
            for name, module in subnetwork.named_modules():
                if isinstance(module, (nn.Linear, nn.Conv2d)):
                    weight = module.weight.data
                    
                    # è®¡ç®—æƒé‡çŸ©é˜µçš„æœ‰æ•ˆç§©
                    if len(weight.shape) == 2:  # Linear layer
                        rank = torch.matrix_rank(weight).item()
                        max_rank = min(weight.shape[0], weight.shape[1])
                    else:  # Conv2d layer
                        # å°†å·ç§¯æƒé‡é‡å¡‘ä¸º2DçŸ©é˜µ
                        weight_2d = weight.view(weight.shape[0], -1)
                        rank = torch.matrix_rank(weight_2d).item()
                        max_rank = min(weight_2d.shape[0], weight_2d.shape[1])
                    
                    if max_rank > 0:
                        rank_ratio = rank / max_rank
                        redundancy = 1.0 - rank_ratio  # ç§©è¶Šä½ï¼Œå†—ä½™åº¦è¶Šé«˜
                        redundancy_scores.append(redundancy)
            
            if redundancy_scores:
                return np.mean(redundancy_scores)
            else:
                return 0.0
                
        except Exception as e:
            logger.warning(f"å‚æ•°å†—ä½™åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _estimate_feasible_parameter_space(self, 
                                         subnetwork: nn.Module, 
                                         activations: torch.Tensor, 
                                         targets: torch.Tensor) -> float:
        """
        ä¼°è®¡å¯è¡Œå‚æ•°ç©ºé—´å æ¯”
        
        æ ¸å¿ƒæ€æƒ³ï¼šåœ¨å½“å‰å‚æ•°é™„è¿‘ï¼Œæœ‰å¤šå¤§æ¯”ä¾‹çš„å‚æ•°å˜åŒ–èƒ½å¤Ÿç»´æŒæˆ–æ”¹å–„æ€§èƒ½
        """
        
        try:
            # ä½¿ç”¨é‡‡æ ·æ–¹æ³•ä¼°è®¡å¯è¡Œå‚æ•°ç©ºé—´
            original_params = [p.data.clone() for p in subnetwork.parameters()]
            
            # è®¡ç®—åŸå§‹æ€§èƒ½
            with torch.no_grad():
                original_output = subnetwork(activations)
                if len(original_output.shape) > 1 and original_output.shape[1] > 1:
                    original_loss = F.cross_entropy(original_output, targets, reduction='mean')
                else:
                    original_loss = F.mse_loss(original_output.squeeze(), targets.float())
            
            # é‡‡æ ·æµ‹è¯•
            feasible_count = 0
            total_samples = 50  # å‡å°‘é‡‡æ ·æ•°é‡ä»¥æé«˜é€Ÿåº¦
            noise_scale = 0.01  # å°æ‰°åŠ¨
            
            for _ in range(total_samples):
                # æ·»åŠ éšæœºæ‰°åŠ¨
                for param in subnetwork.parameters():
                    noise = torch.randn_like(param.data) * noise_scale
                    param.data.add_(noise)
                
                # æµ‹è¯•æ‰°åŠ¨åçš„æ€§èƒ½
                try:
                    with torch.no_grad():
                        perturbed_output = subnetwork(activations)
                        if len(perturbed_output.shape) > 1 and perturbed_output.shape[1] > 1:
                            perturbed_loss = F.cross_entropy(perturbed_output, targets, reduction='mean')
                        else:
                            perturbed_loss = F.mse_loss(perturbed_output.squeeze(), targets.float())
                    
                    # å¦‚æœæŸå¤±æ²¡æœ‰æ˜¾è‘—å¢åŠ ï¼Œè®¤ä¸ºæ˜¯å¯è¡Œçš„
                    if perturbed_loss <= original_loss * 1.1:  # å…è®¸10%çš„æ€§èƒ½ä¸‹é™
                        feasible_count += 1
                        
                except Exception:
                    pass  # æ‰°åŠ¨å¯¼è‡´çš„é”™è¯¯è§†ä¸ºä¸å¯è¡Œ
                
                # æ¢å¤åŸå§‹å‚æ•°
                for param, original_param in zip(subnetwork.parameters(), original_params):
                    param.data.copy_(original_param)
            
            feasible_ratio = feasible_count / total_samples
            return feasible_ratio
            
        except Exception as e:
            logger.warning(f"å¯è¡Œå‚æ•°ç©ºé—´ä¼°è®¡å¤±è´¥: {e}")
            return 0.0

class MutationPotentialPredictor:
    """å˜å¼‚æ½œåŠ›é¢„æµ‹å™¨"""
    
    def __init__(self):
        self.predictor_cache = {}
    
    def predict_mutation_potential(self, 
                                 subnetwork: nn.Module,
                                 subnetwork_info: Dict[str, Any],
                                 parameter_space_analysis: Dict[str, float],
                                 current_accuracy: float) -> Dict[str, Any]:
        """
        é¢„æµ‹å˜å¼‚æ½œåŠ›å’Œå¯èƒ½çš„å‡†ç¡®ç‡æå‡
        
        Args:
            subnetwork: æå–çš„å­ç½‘ç»œ
            subnetwork_info: å­ç½‘ç»œåˆ†æä¿¡æ¯
            parameter_space_analysis: å‚æ•°ç©ºé—´åˆ†æç»“æœ
            current_accuracy: å½“å‰å‡†ç¡®ç‡
            
        Returns:
            å˜å¼‚æ½œåŠ›é¢„æµ‹ç»“æœ
        """
        logger.enter_section("å˜å¼‚æ½œåŠ›é¢„æµ‹")
        
        try:
            # 1. åŸºäºå‚æ•°ç©ºé—´æ•ˆç‡é¢„æµ‹æå‡ç©ºé—´
            efficiency = parameter_space_analysis['overall_efficiency']
            improvement_potential = self._compute_improvement_potential(efficiency, current_accuracy)
            
            # 2. é¢„æµ‹ä¸åŒå˜å¼‚ç­–ç•¥çš„æ•ˆæœ
            strategy_predictions = self._predict_strategy_effects(
                subnetwork_info, parameter_space_analysis, current_accuracy
            )
            
            # 3. ä¼°è®¡æœ€ä¼˜å˜å¼‚å¼ºåº¦
            optimal_mutation_strength = self._estimate_optimal_mutation_strength(
                parameter_space_analysis, improvement_potential
            )
            
            # 4. é£é™©è¯„ä¼°
            risk_assessment = self._assess_mutation_risks(
                subnetwork_info, parameter_space_analysis
            )
            
            prediction_result = {
                'improvement_potential': improvement_potential,
                'strategy_predictions': strategy_predictions,
                'optimal_mutation_strength': optimal_mutation_strength,
                'risk_assessment': risk_assessment,
                'confidence': self._compute_prediction_confidence(parameter_space_analysis)
            }
            
            logger.info(f"å˜å¼‚æ½œåŠ›é¢„æµ‹å®Œæˆ: æ”¹è¿›æ½œåŠ›={improvement_potential:.3f}")
            logger.exit_section("å˜å¼‚æ½œåŠ›é¢„æµ‹")
            
            return prediction_result
            
        except Exception as e:
            logger.error(f"å˜å¼‚æ½œåŠ›é¢„æµ‹å¤±è´¥: {e}")
            logger.exit_section("å˜å¼‚æ½œåŠ›é¢„æµ‹")
            return {
                'improvement_potential': 0.0,
                'strategy_predictions': {},
                'optimal_mutation_strength': 0.1,
                'risk_assessment': {'overall_risk': 1.0},
                'confidence': 0.0
            }
    
    def _compute_improvement_potential(self, efficiency: float, current_accuracy: float) -> float:
        """åŸºäºæ•ˆç‡å’Œå½“å‰å‡†ç¡®ç‡è®¡ç®—æ”¹è¿›æ½œåŠ›"""
        
        # æ•ˆç‡è¶Šä½ï¼Œæ”¹è¿›ç©ºé—´è¶Šå¤§
        efficiency_factor = 1.0 - efficiency
        
        # å‡†ç¡®ç‡è¶Šé«˜ï¼Œæ”¹è¿›è¶Šå›°éš¾
        if current_accuracy > 0.95:
            accuracy_factor = 0.1
        elif current_accuracy > 0.90:
            accuracy_factor = 0.3
        elif current_accuracy > 0.80:
            accuracy_factor = 0.6
        else:
            accuracy_factor = 1.0
        
        # ç»¼åˆæ”¹è¿›æ½œåŠ›
        improvement_potential = efficiency_factor * accuracy_factor
        
        return min(improvement_potential, 1.0)
    
    def _predict_strategy_effects(self, 
                                subnetwork_info: Dict[str, Any],
                                parameter_space_analysis: Dict[str, float],
                                current_accuracy: float) -> Dict[str, Dict[str, float]]:
        """é¢„æµ‹ä¸åŒå˜å¼‚ç­–ç•¥çš„æ•ˆæœ"""
        
        strategies = {
            'width_expansion': self._predict_width_expansion_effect(
                subnetwork_info, parameter_space_analysis
            ),
            'depth_increase': self._predict_depth_increase_effect(
                subnetwork_info, parameter_space_analysis
            ),
            'parallel_division': self._predict_parallel_division_effect(
                subnetwork_info, parameter_space_analysis
            ),
            'hybrid_mutation': self._predict_hybrid_mutation_effect(
                subnetwork_info, parameter_space_analysis
            )
        }
        
        return strategies
    
    def _predict_width_expansion_effect(self, 
                                      subnetwork_info: Dict[str, Any],
                                      parameter_space_analysis: Dict[str, float]) -> Dict[str, float]:
        """é¢„æµ‹å®½åº¦æ‰©å±•æ•ˆæœ"""
        
        redundancy = parameter_space_analysis['redundancy_ratio']
        utilization = parameter_space_analysis['parameter_utilization']
        
        # å†—ä½™åº¦ä½ä¸”åˆ©ç”¨ç‡é«˜çš„å±‚é€‚åˆå®½åº¦æ‰©å±•
        expansion_benefit = (1.0 - redundancy) * utilization
        
        return {
            'expected_accuracy_gain': expansion_benefit * 0.02,  # æœ€å¤š2%çš„æå‡
            'parameter_cost': 0.3,  # ç›¸å¯¹å‚æ•°å¢é•¿
            'implementation_difficulty': 0.2,  # å®ç°éš¾åº¦
            'stability_risk': 0.1  # ç¨³å®šæ€§é£é™©
        }
    
    def _predict_depth_increase_effect(self, 
                                     subnetwork_info: Dict[str, Any],
                                     parameter_space_analysis: Dict[str, float]) -> Dict[str, float]:
        """é¢„æµ‹æ·±åº¦å¢åŠ æ•ˆæœ"""
        
        representation_capacity = parameter_space_analysis['representation_capacity']
        
        # è¡¨ç¤ºèƒ½åŠ›ä¸è¶³çš„å­ç½‘ç»œé€‚åˆå¢åŠ æ·±åº¦
        depth_benefit = 1.0 - representation_capacity
        
        return {
            'expected_accuracy_gain': depth_benefit * 0.015,  # æœ€å¤š1.5%çš„æå‡
            'parameter_cost': 0.5,  # è¾ƒé«˜çš„å‚æ•°å¢é•¿
            'implementation_difficulty': 0.4,  # ä¸­ç­‰å®ç°éš¾åº¦
            'stability_risk': 0.3  # ä¸­ç­‰ç¨³å®šæ€§é£é™©
        }
    
    def _predict_parallel_division_effect(self, 
                                        subnetwork_info: Dict[str, Any],
                                        parameter_space_analysis: Dict[str, float]) -> Dict[str, float]:
        """é¢„æµ‹å¹¶è¡Œåˆ†è£‚æ•ˆæœ"""
        
        feasible_space = parameter_space_analysis['feasible_space_ratio']
        
        # å¯è¡Œç©ºé—´å¤§çš„å­ç½‘ç»œé€‚åˆå¹¶è¡Œåˆ†è£‚
        parallel_benefit = feasible_space
        
        return {
            'expected_accuracy_gain': parallel_benefit * 0.025,  # æœ€å¤š2.5%çš„æå‡
            'parameter_cost': 0.4,  # ä¸­ç­‰å‚æ•°å¢é•¿
            'implementation_difficulty': 0.3,  # ä¸­ç­‰å®ç°éš¾åº¦
            'stability_risk': 0.2  # è¾ƒä½ç¨³å®šæ€§é£é™©
        }
    
    def _predict_hybrid_mutation_effect(self, 
                                      subnetwork_info: Dict[str, Any],
                                      parameter_space_analysis: Dict[str, float]) -> Dict[str, float]:
        """é¢„æµ‹æ··åˆå˜å¼‚æ•ˆæœ"""
        
        overall_efficiency = parameter_space_analysis['overall_efficiency']
        
        # æ•ˆç‡ä½çš„å­ç½‘ç»œé€‚åˆæ··åˆå˜å¼‚
        hybrid_benefit = (1.0 - overall_efficiency) * 1.2  # æ··åˆç­–ç•¥çš„åŠ æˆ
        
        return {
            'expected_accuracy_gain': min(hybrid_benefit * 0.03, 0.04),  # æœ€å¤š4%çš„æå‡
            'parameter_cost': 0.6,  # è¾ƒé«˜å‚æ•°å¢é•¿
            'implementation_difficulty': 0.6,  # è¾ƒé«˜å®ç°éš¾åº¦
            'stability_risk': 0.4  # ä¸­ç­‰ç¨³å®šæ€§é£é™©
        }
    
    def _estimate_optimal_mutation_strength(self, 
                                          parameter_space_analysis: Dict[str, float],
                                          improvement_potential: float) -> float:
        """ä¼°è®¡æœ€ä¼˜å˜å¼‚å¼ºåº¦"""
        
        efficiency = parameter_space_analysis['overall_efficiency']
        feasible_space = parameter_space_analysis['feasible_space_ratio']
        
        # åŸºäºæ•ˆç‡å’Œå¯è¡Œç©ºé—´ç¡®å®šå˜å¼‚å¼ºåº¦
        base_strength = 0.1  # åŸºç¡€å˜å¼‚å¼ºåº¦
        
        # æ•ˆç‡ä½çš„å­ç½‘ç»œå¯ä»¥æ‰¿å—æ›´å¼ºçš„å˜å¼‚
        efficiency_factor = (1.0 - efficiency) * 2.0
        
        # å¯è¡Œç©ºé—´å¤§çš„å­ç½‘ç»œå¯ä»¥æ‰¿å—æ›´å¼ºçš„å˜å¼‚
        feasible_factor = feasible_space * 1.5
        
        optimal_strength = base_strength + min(efficiency_factor + feasible_factor, 0.8)
        
        return min(optimal_strength, 1.0)
    
    def _assess_mutation_risks(self, 
                             subnetwork_info: Dict[str, Any],
                             parameter_space_analysis: Dict[str, float]) -> Dict[str, float]:
        """è¯„ä¼°å˜å¼‚é£é™©"""
        
        # å‚æ•°æ•°é‡é£é™©
        param_count = subnetwork_info['total_params']
        param_risk = min(param_count / 1e6, 1.0)  # å‚æ•°è¶Šå¤šé£é™©è¶Šé«˜
        
        # æ•ˆç‡é£é™©
        efficiency = parameter_space_analysis['overall_efficiency']
        efficiency_risk = efficiency  # æ•ˆç‡é«˜çš„ç³»ç»Ÿå˜å¼‚é£é™©é«˜
        
        # å¯è¡Œç©ºé—´é£é™©
        feasible_space = parameter_space_analysis['feasible_space_ratio']
        space_risk = 1.0 - feasible_space  # å¯è¡Œç©ºé—´å°é£é™©é«˜
        
        # ç»¼åˆé£é™©
        overall_risk = (param_risk + efficiency_risk + space_risk) / 3.0
        
        return {
            'parameter_risk': param_risk,
            'efficiency_risk': efficiency_risk,
            'space_risk': space_risk,
            'overall_risk': overall_risk
        }
    
    def _compute_prediction_confidence(self, parameter_space_analysis: Dict[str, float]) -> float:
        """è®¡ç®—é¢„æµ‹ç½®ä¿¡åº¦"""
        
        # åŸºäºåˆ†æç»“æœçš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§è®¡ç®—ç½®ä¿¡åº¦
        analysis_completeness = len([v for v in parameter_space_analysis.values() if v > 0]) / len(parameter_space_analysis)
        
        # ç»“æœçš„ä¸€è‡´æ€§æ£€æŸ¥
        efficiency = parameter_space_analysis['overall_efficiency']
        redundancy = parameter_space_analysis['redundancy_ratio']
        
        # æ•ˆç‡å’Œå†—ä½™åº¦åº”è¯¥è´Ÿç›¸å…³
        consistency = 1.0 - abs(efficiency + redundancy - 1.0)
        
        confidence = (analysis_completeness + consistency) / 2.0
        
        return min(confidence, 1.0)

class Net2NetSubnetworkAnalyzer:
    """Net2Netå­ç½‘ç»œåˆ†æå™¨ä¸»ç±»"""
    
    def __init__(self):
        self.extractor = SubnetworkExtractor()
        self.param_analyzer = ParameterSpaceAnalyzer()
        self.predictor = MutationPotentialPredictor()
        
        # æ–°å¢ï¼šä¿¡æ¯æµåˆ†æå™¨
        self.info_flow_analyzer = InformationFlowAnalyzer()
        self.leak_detector = InformationLeakDetector()
        
        # æ–°å¢ï¼šè´å¶æ–¯å˜å¼‚æ”¶ç›Šé¢„æµ‹å™¨
        self.bayesian_predictor = BayesianMutationBenefitPredictor()
    
    def analyze_all_layers(self, model: nn.Module, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ†ææ‰€æœ‰å±‚çš„å˜å¼‚æ½œåŠ›å’Œä¿¡æ¯æµç“¶é¢ˆ
        
        è¿™æ˜¯å®ç°ç¥ç»ç½‘ç»œæœ€ä¼˜å˜å¼‚ç†è®ºçš„æ ¸å¿ƒæ–¹æ³•ï¼š
        1. æ£€æµ‹ä¿¡æ¯æµæ¼ç‚¹ - æŸå±‚æˆä¸ºä¿¡æ¯æå–ç“¶é¢ˆï¼Œå¯¼è‡´åç»­å±‚æ— æ³•æå‡å‡†ç¡®ç‡
        2. åˆ†æå‚æ•°ç©ºé—´å¯†åº¦ - æ¼ç‚¹å±‚çš„å‚æ•°ç©ºé—´ä¸­é«˜å‡†ç¡®ç‡åŒºåŸŸå æ¯”è¾ƒå°
        3. é¢„æµ‹å˜å¼‚æ”¶ç›Š - å˜å¼‚åå‚æ•°ç©ºé—´ä¸­é«˜å‡†ç¡®ç‡åŒºåŸŸå æ¯”æå‡
        4. æŒ‡å¯¼æ¶æ„å˜å¼‚ - è®©æ¼ç‚¹å±‚å˜å¾—æ›´å¤æ‚ï¼Œæå–æ›´å¤šä¿¡æ¯
        
        Args:
            model: ç¥ç»ç½‘ç»œæ¨¡å‹
            context: åˆ†æä¸Šä¸‹æ–‡ï¼ŒåŒ…å«æ¿€æ´»å€¼ã€æ¢¯åº¦ã€ç›®æ ‡ç­‰
            
        Returns:
            åŒ…å«æ‰€æœ‰å±‚åˆ†æç»“æœå’Œå˜å¼‚å»ºè®®çš„å­—å…¸
        """
        logger.enter_section("Net2Netå…¨å±‚åˆ†æ")
        
        try:
            activations = context.get('activations', {})
            gradients = context.get('gradients', {})
            targets = context.get('targets')
            current_accuracy = context.get('current_accuracy', 0.0)
            
            # 1. ä¿¡æ¯æµå…¨å±€åˆ†æ
            logger.info("ğŸ” æ‰§è¡Œä¿¡æ¯æµå…¨å±€åˆ†æ...")
            flow_analysis = self._analyze_global_information_flow(
                model, activations, gradients, targets
            )
            
            # 2. æ£€æµ‹ä¿¡æ¯æ³„éœ²æ¼ç‚¹
            logger.info("ğŸ•³ï¸ æ£€æµ‹ä¿¡æ¯æ³„éœ²æ¼ç‚¹...")
            leak_points = self._detect_information_leak_points(
                model, activations, gradients, targets, current_accuracy
            )
            
            # 3. åˆ†ææ¯å±‚çš„å˜å¼‚æ½œåŠ›
            logger.info("ğŸ“Š åˆ†æå„å±‚å˜å¼‚æ½œåŠ›...")
            layer_analyses = {}
            
            for layer_name in activations.keys():
                if self._is_analyzable_layer(model, layer_name):
                    layer_analysis = self.analyze_layer_mutation_potential(
                        model, layer_name, activations, gradients, 
                        targets, current_accuracy
                    )
                    
                    # å¢å¼ºåˆ†æï¼šæ·»åŠ ä¿¡æ¯æµæ¼ç‚¹è¯„ä¼°
                    layer_analysis['leak_assessment'] = self._assess_layer_leak_potential(
                        layer_name, activations, gradients, leak_points
                    )
                    
                    layer_analyses[layer_name] = layer_analysis
            
            # 4. è´å¶æ–¯æ”¶ç›Šé¢„æµ‹
            logger.info("ğŸ§  æ‰§è¡Œè´å¶æ–¯å˜å¼‚æ”¶ç›Šé¢„æµ‹...")
            bayesian_predictions = self.predict_mutation_benefits_with_bayesian(
                layer_analyses, current_accuracy, model
            )
            
            # 5. ç»¼åˆå˜å¼‚ç­–ç•¥é¢„æµ‹ï¼ˆSerial/Parallel + å±‚ç±»å‹ç»„åˆï¼‰
            logger.info("ğŸ­ é¢„æµ‹ç»¼åˆå˜å¼‚ç­–ç•¥...")
            comprehensive_strategies = self.predict_comprehensive_strategies_for_top_candidates(
                layer_analyses, current_accuracy, model, top_n=3
            )
            
            # 6. ç”Ÿæˆå…¨å±€å˜å¼‚ç­–ç•¥ï¼ˆç»“åˆè´å¶æ–¯é¢„æµ‹å’Œç»¼åˆç­–ç•¥ï¼‰
            logger.info("ğŸ¯ ç”Ÿæˆå…¨å±€å˜å¼‚ç­–ç•¥...")
            global_strategy = self._generate_global_mutation_strategy(
                layer_analyses, leak_points, flow_analysis, current_accuracy, 
                bayesian_predictions, comprehensive_strategies
            )
            
            # 7. ç»„è£…å®Œæ•´åˆ†æç»“æœ
            complete_analysis = {
                'global_flow_analysis': flow_analysis,
                'detected_leak_points': leak_points,
                'layer_analyses': layer_analyses,
                'bayesian_benefit_predictions': bayesian_predictions,
                'comprehensive_mutation_strategies': comprehensive_strategies,
                'global_mutation_strategy': global_strategy,
                'analysis_metadata': {
                    'total_layers_analyzed': len(layer_analyses),
                    'critical_leak_points': len([lp for lp in leak_points if lp['severity'] > 0.7]),
                    'high_potential_layers': len([la for la in layer_analyses.values() 
                                                 if la.get('mutation_prediction', {}).get('improvement_potential', 0) > 0.5]),
                    'high_confidence_predictions': len([bp for bp in bayesian_predictions.values() 
                                                       if bp.get('bayesian_prediction', {}).get('uncertainty_metrics', {}).get('prediction_confidence', 0) > 0.7]),
                    'strong_recommendations': len([bp for bp in bayesian_predictions.values() 
                                                  if bp.get('bayesian_prediction', {}).get('recommendation_strength', '') == 'strong_recommend']),
                    'comprehensive_strategies_count': len(comprehensive_strategies),
                    'analysis_timestamp': time.time()
                }
            }
            
            logger.success(f"Net2Netå…¨å±‚åˆ†æå®Œæˆï¼Œå‘ç°{len(leak_points)}ä¸ªæ½œåœ¨æ¼ç‚¹")
            logger.exit_section("Net2Netå…¨å±‚åˆ†æ")
            
            return complete_analysis
            
        except Exception as e:
            logger.error(f"Net2Netå…¨å±‚åˆ†æå¤±è´¥: {e}")
            logger.exit_section("Net2Netå…¨å±‚åˆ†æ")
            return {
                'error': str(e),
                'global_mutation_strategy': {'action': 'skip', 'reason': f'åˆ†æå¤±è´¥: {e}'}
            }
    
    def _analyze_global_information_flow(self, model: nn.Module, 
                                       activations: Dict[str, torch.Tensor],
                                       gradients: Dict[str, torch.Tensor],
                                       targets: torch.Tensor) -> Dict[str, Any]:
        """åˆ†æå…¨å±€ä¿¡æ¯æµæ¨¡å¼"""
        
        flow_metrics = {}
        layer_names = list(activations.keys())
        
        for i, layer_name in enumerate(layer_names):
            if layer_name not in gradients:
                continue
                
            activation = activations[layer_name]
            gradient = gradients[layer_name]
            
            # è®¡ç®—ä¿¡æ¯å¯†åº¦æŒ‡æ ‡
            info_density = self._calculate_information_density(activation, gradient)
            
            # è®¡ç®—ä¿¡æ¯ä¼ é€’æ•ˆç‡ï¼ˆä¸ä¸‹ä¸€å±‚çš„ç›¸å…³æ€§ï¼‰
            transfer_efficiency = 0.0
            if i < len(layer_names) - 1:
                next_layer = layer_names[i + 1]
                if next_layer in activations:
                    transfer_efficiency = self._calculate_transfer_efficiency(
                        activation, activations[next_layer]
                    )
            
            # è®¡ç®—ä¿¡æ¯ä¿ç•™ç‡ï¼ˆä¸ç›®æ ‡çš„ç›¸å…³æ€§ï¼‰
            target_correlation = self._calculate_target_correlation(activation, targets)
            
            flow_metrics[layer_name] = {
                'information_density': info_density,
                'transfer_efficiency': transfer_efficiency,
                'target_correlation': target_correlation,
                'flow_bottleneck_score': self._calculate_bottleneck_score(
                    info_density, transfer_efficiency, target_correlation
                )
            }
        
        return {
            'layer_flow_metrics': flow_metrics,
            'global_bottleneck_score': np.mean([m['flow_bottleneck_score'] 
                                              for m in flow_metrics.values()]),
            'critical_bottlenecks': [name for name, metrics in flow_metrics.items() 
                                   if metrics['flow_bottleneck_score'] > 0.7]
        }
    
    def _detect_information_leak_points(self, model: nn.Module,
                                      activations: Dict[str, torch.Tensor],
                                      gradients: Dict[str, torch.Tensor],
                                      targets: torch.Tensor,
                                      current_accuracy: float) -> List[Dict[str, Any]]:
        """
        æ£€æµ‹ä¿¡æ¯æ³„éœ²æ¼ç‚¹
        
        æ¼ç‚¹çš„ç‰¹å¾ï¼š
        1. è¯¥å±‚çš„ä¿¡æ¯å¯†åº¦æ˜¾è‘—ä½äºå‰å±‚
        2. è¯¥å±‚çš„æ¢¯åº¦æ–¹å·®å¾ˆå°ï¼ˆå­¦ä¹ å›°éš¾ï¼‰
        3. åç»­å­ç½‘ç»œçš„å‚æ•°ç©ºé—´ä¸­é«˜å‡†ç¡®ç‡åŒºåŸŸå æ¯”å°
        4. å˜å¼‚è¯¥å±‚åï¼Œåç»­å­ç½‘ç»œæ€§èƒ½æå‡æ˜æ˜¾
        """
        
        leak_points = []
        layer_names = list(activations.keys())
        
        for i, layer_name in enumerate(layer_names[1:], 1):  # è·³è¿‡ç¬¬ä¸€å±‚
            if layer_name not in gradients:
                continue
                
            # è·å–å½“å‰å±‚å’Œå‰ä¸€å±‚çš„æ•°æ®
            current_activation = activations[layer_name]
            current_gradient = gradients[layer_name]
            prev_layer = layer_names[i-1]
            
            if prev_layer not in activations:
                continue
                
            prev_activation = activations[prev_layer]
            
            # 1. ä¿¡æ¯å¯†åº¦ä¸‹é™æ£€æµ‹
            current_info_density = self._calculate_information_density(
                current_activation, current_gradient
            )
            prev_info_density = self._calculate_information_density(
                prev_activation, gradients.get(prev_layer, torch.zeros_like(prev_activation))
            )
            
            info_drop = prev_info_density - current_info_density
            
            # 2. æ¢¯åº¦å­¦ä¹ å›°éš¾æ£€æµ‹
            gradient_variance = torch.var(current_gradient).item()
            learning_difficulty = 1.0 / (1.0 + gradient_variance)  # æ–¹å·®è¶Šå°ï¼Œå­¦ä¹ è¶Šå›°éš¾
            
            # 3. åç»­å­ç½‘ç»œæ•ˆç‡è¯„ä¼°
            posterior_efficiency = self._evaluate_posterior_subnetwork_efficiency(
                model, layer_name, activations, targets
            )
            
            # 4. å˜å¼‚æ½œåŠ›è¯„ä¼°
            mutation_potential = self._estimate_mutation_improvement_potential(
                current_activation, current_gradient, targets, current_accuracy
            )
            
            # ç»¼åˆè¯„ä¼°æ¼ç‚¹ä¸¥é‡ç¨‹åº¦
            leak_severity = (
                info_drop * 0.3 +
                learning_difficulty * 0.2 +
                (1.0 - posterior_efficiency) * 0.3 +
                mutation_potential * 0.2
            )
            
            if leak_severity > 0.5:  # é˜ˆå€¼å¯è°ƒ
                leak_points.append({
                    'layer_name': layer_name,
                    'severity': leak_severity,
                    'info_density_drop': info_drop,
                    'learning_difficulty': learning_difficulty,
                    'posterior_efficiency': posterior_efficiency,
                    'mutation_potential': mutation_potential,
                    'leak_type': self._classify_leak_type(
                        info_drop, learning_difficulty, posterior_efficiency
                    )
                })
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
        leak_points.sort(key=lambda x: x['severity'], reverse=True)
        
        return leak_points
    
    def _assess_layer_leak_potential(self, layer_name: str,
                                   activations: Dict[str, torch.Tensor],
                                   gradients: Dict[str, torch.Tensor],
                                   leak_points: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è¯„ä¼°ç‰¹å®šå±‚çš„æ¼ç‚¹æ½œåŠ›"""
        
        # æ£€æŸ¥è¯¥å±‚æ˜¯å¦è¢«è¯†åˆ«ä¸ºæ¼ç‚¹
        is_leak_point = any(lp['layer_name'] == layer_name for lp in leak_points)
        
        if is_leak_point:
            leak_info = next(lp for lp in leak_points if lp['layer_name'] == layer_name)
            
            return {
                'is_leak_point': True,
                'leak_severity': leak_info['severity'],
                'leak_type': leak_info['leak_type'],
                'recommended_mutation_priority': 'high' if leak_info['severity'] > 0.7 else 'medium',
                'expected_improvement': leak_info['mutation_potential']
            }
        else:
            return {
                'is_leak_point': False,
                'leak_severity': 0.0,
                'recommended_mutation_priority': 'low',
                'expected_improvement': 0.0
            }
    
    def _generate_global_mutation_strategy(self, layer_analyses: Dict[str, Any],
                                         leak_points: List[Dict[str, Any]],
                                         flow_analysis: Dict[str, Any],
                                         current_accuracy: float,
                                         bayesian_predictions: Dict[str, Dict[str, Any]] = None,
                                         comprehensive_strategies: Dict[str, Dict[str, Any]] = None) -> Dict[str, Any]:
        """ç”Ÿæˆå…¨å±€å˜å¼‚ç­–ç•¥"""
        
        # 1. ä¼˜å…ˆå¤„ç†ä¸¥é‡æ¼ç‚¹
        priority_targets = []
        for leak_point in leak_points:
            if leak_point['severity'] > 0.7:
                priority_targets.append({
                    'layer_name': leak_point['layer_name'],
                    'priority': 'critical',
                    'expected_improvement': leak_point['mutation_potential'],
                    'strategy': self._select_optimal_mutation_strategy(leak_point)
                })
        
        # 2. è€ƒè™‘é«˜æ½œåŠ›éæ¼ç‚¹å±‚ï¼ˆç»“åˆè´å¶æ–¯é¢„æµ‹ï¼‰
        if bayesian_predictions:
            # ä½¿ç”¨è´å¶æ–¯é¢„æµ‹ç»“æœé‡æ–°æ’åº
            bayesian_sorted = sorted(
                bayesian_predictions.items(),
                key=lambda x: x[1].get('combined_score', 0),
                reverse=True
            )
            
            for layer_name, bayesian_result in bayesian_sorted:
                if layer_name in layer_analyses:
                    combined_score = bayesian_result.get('combined_score', 0)
                    bayesian_gain = bayesian_result.get('bayesian_prediction', {}).get('expected_accuracy_gain', 0)
                    confidence = bayesian_result.get('bayesian_prediction', {}).get('uncertainty_metrics', {}).get('prediction_confidence', 0)
                    
                    # è´å¶æ–¯é©±åŠ¨çš„é€‰æ‹©æ ‡å‡†
                    if (combined_score > 0.02 and confidence > 0.6 and bayesian_gain > 0.005 and 
                        not any(t['layer_name'] == layer_name for t in priority_targets)):
                        
                        priority_targets.append({
                            'layer_name': layer_name,
                            'priority': 'high' if combined_score > 0.05 else 'medium',
                            'expected_improvement': bayesian_gain,
                            'strategy': bayesian_result.get('mutation_strategy', 'widening'),
                            'bayesian_confidence': confidence,
                            'combined_score': combined_score,
                            'recommendation_strength': bayesian_result.get('bayesian_prediction', {}).get('recommendation_strength', 'neutral')
                        })
        else:
            # fallbackåˆ°åŸæ¥çš„é€»è¾‘
            for layer_name, analysis in layer_analyses.items():
                mutation_potential = analysis.get('mutation_prediction', {}).get('improvement_potential', 0)
                if mutation_potential > 0.6 and not any(t['layer_name'] == layer_name for t in priority_targets):
                    priority_targets.append({
                        'layer_name': layer_name,
                        'priority': 'high',
                        'expected_improvement': mutation_potential,
                        'strategy': analysis.get('recommendation', {}).get('strategy', 'widening')
                    })
        
        # 3. ç”Ÿæˆæ‰§è¡Œè®¡åˆ’
        execution_plan = self._create_mutation_execution_plan(
            priority_targets, current_accuracy, flow_analysis
        )
        
        # é›†æˆç»¼åˆç­–ç•¥ä¿¡æ¯
        enhanced_targets = []
        for target in priority_targets:
            layer_name = target['layer_name']
            enhanced_target = target.copy()
            
            # æ·»åŠ ç»¼åˆç­–ç•¥ä¿¡æ¯
            if comprehensive_strategies and layer_name in comprehensive_strategies:
                comp_strategy = comprehensive_strategies[layer_name]['comprehensive_strategy']
                enhanced_target.update({
                    'detailed_mutation_mode': comp_strategy.get('mutation_mode', 'unknown'),
                    'layer_combination_strategy': comp_strategy.get('layer_combination', {}),
                    'implementation_timeline': comp_strategy.get('implementation_details', {}).get('expected_timeline', 'unknown'),
                    'comprehensive_confidence': comp_strategy.get('confidence', 0.5),
                    'total_expected_gain': comp_strategy.get('expected_total_gain', 0.0)
                })
            
            enhanced_targets.append(enhanced_target)
        
        return {
            'priority_targets': enhanced_targets,
            'execution_plan': execution_plan,
            'comprehensive_strategies_summary': self._summarize_comprehensive_strategies(comprehensive_strategies),
            'global_improvement_estimate': sum(t.get('total_expected_gain', t.get('expected_improvement', 0)) for t in enhanced_targets),
            'recommended_sequence': [t['layer_name'] for t in 
                                   sorted(enhanced_targets, key=lambda x: x.get('total_expected_gain', x.get('expected_improvement', 0)), reverse=True)]
        }
    
    def _calculate_information_density(self, activation: torch.Tensor, gradient: torch.Tensor) -> float:
        """è®¡ç®—ä¿¡æ¯å¯†åº¦"""
        # ä½¿ç”¨æ¿€æ´»å€¼çš„ç†µå’Œæ¢¯åº¦çš„æ–¹å·®ä½œä¸ºä¿¡æ¯å¯†åº¦æŒ‡æ ‡
        activation_entropy = self._calculate_entropy(activation)
        gradient_variance = torch.var(gradient).item()
        
        # å½’ä¸€åŒ–å¹¶ç»„åˆ
        info_density = (activation_entropy + np.log(1 + gradient_variance)) / 2
        return float(info_density)
    
    def _calculate_entropy(self, tensor: torch.Tensor) -> float:
        """è®¡ç®—å¼ é‡çš„è¿‘ä¼¼ç†µ"""
        # å°†å¼ é‡å±•å¹³å¹¶è®¡ç®—ç›´æ–¹å›¾
        flat_tensor = tensor.flatten()
        hist, _ = np.histogram(flat_tensor.cpu().numpy(), bins=50, density=True)
        
        # é¿å…log(0)
        hist = hist + 1e-10
        entropy = -np.sum(hist * np.log(hist))
        
        return float(entropy)
    
    def _calculate_transfer_efficiency(self, current_activation: torch.Tensor, 
                                     next_activation: torch.Tensor) -> float:
        """è®¡ç®—ä¿¡æ¯ä¼ é€’æ•ˆç‡"""
        # è®¡ç®—æ¿€æ´»å€¼ä¹‹é—´çš„ç›¸å…³æ€§
        curr_flat = current_activation.flatten()
        next_flat = next_activation.flatten()
        
        # è°ƒæ•´å°ºå¯¸ä»¥åŒ¹é…
        min_size = min(len(curr_flat), len(next_flat))
        curr_flat = curr_flat[:min_size]
        next_flat = next_flat[:min_size]
        
        correlation = torch.corrcoef(torch.stack([curr_flat, next_flat]))[0, 1]
        
        # å¤„ç†NaNæƒ…å†µ
        if torch.isnan(correlation):
            return 0.0
            
        return float(torch.abs(correlation))
    
    def _calculate_target_correlation(self, activation: torch.Tensor, targets: torch.Tensor) -> float:
        """è®¡ç®—ä¸ç›®æ ‡çš„ç›¸å…³æ€§"""
        # ç®€åŒ–çš„ç›¸å…³æ€§è®¡ç®—
        activation_mean = torch.mean(activation, dim=tuple(range(1, activation.dim())))
        
        if len(activation_mean) != len(targets):
            return 0.0
            
        # è®¡ç®—ä¸ç›®æ ‡çš„ç›¸å…³æ€§
        try:
            correlation = torch.corrcoef(torch.stack([
                activation_mean.float(),
                targets.float()
            ]))[0, 1]
            
            if torch.isnan(correlation):
                return 0.0
                
            return float(torch.abs(correlation))
        except:
            return 0.0
    
    def _calculate_bottleneck_score(self, info_density: float, transfer_efficiency: float, 
                                  target_correlation: float) -> float:
        """è®¡ç®—ç“¶é¢ˆåˆ†æ•°"""
        # ç“¶é¢ˆåˆ†æ•° = ä¿¡æ¯å¯†åº¦ä½ + ä¼ é€’æ•ˆç‡ä½ + ç›®æ ‡ç›¸å…³æ€§ä½
        bottleneck_score = (
            (1.0 - min(info_density / 10.0, 1.0)) * 0.4 +
            (1.0 - transfer_efficiency) * 0.3 +
            (1.0 - target_correlation) * 0.3
        )
        
        return float(bottleneck_score)
    
    def _evaluate_posterior_subnetwork_efficiency(self, model: nn.Module, layer_name: str,
                                                activations: Dict[str, torch.Tensor],
                                                targets: torch.Tensor) -> float:
        """è¯„ä¼°åç»­å­ç½‘ç»œæ•ˆç‡"""
        # è·å–è¯¥å±‚ä¹‹åçš„æ‰€æœ‰å±‚
        layer_names = list(activations.keys())
        try:
            layer_idx = layer_names.index(layer_name)
            posterior_layers = layer_names[layer_idx + 1:]
        except ValueError:
            return 0.5  # é»˜è®¤ä¸­ç­‰æ•ˆç‡
        
        if not posterior_layers:
            return 1.0  # æœ€åä¸€å±‚ï¼Œæ•ˆç‡ä¸º1
        
        # è®¡ç®—åç»­å±‚çš„å¹³å‡ä¿¡æ¯å¤„ç†æ•ˆç‡
        efficiency_scores = []
        
        for post_layer in posterior_layers:
            if post_layer in activations:
                post_activation = activations[post_layer]
                target_corr = self._calculate_target_correlation(post_activation, targets)
                efficiency_scores.append(target_corr)
        
        if not efficiency_scores:
            return 0.5
            
        return float(np.mean(efficiency_scores))
    
    def _estimate_mutation_improvement_potential(self, activation: torch.Tensor,
                                               gradient: torch.Tensor,
                                               targets: torch.Tensor,
                                               current_accuracy: float) -> float:
        """ä¼°ç®—å˜å¼‚æ”¹è¿›æ½œåŠ›"""
        # åŸºäºæ¢¯åº¦å’Œæ¿€æ´»æ¨¡å¼ä¼°ç®—å˜å¼‚åçš„æ”¹è¿›æ½œåŠ›
        
        # 1. æ¢¯åº¦å¤šæ ·æ€§ï¼ˆé«˜å¤šæ ·æ€§ = é«˜æ”¹è¿›æ½œåŠ›ï¼‰
        gradient_diversity = torch.std(gradient).item()
        
        # 2. æ¿€æ´»é¥±å’Œåº¦ï¼ˆä½é¥±å’Œåº¦ = é«˜æ”¹è¿›æ½œåŠ›ï¼‰
        activation_saturation = torch.mean(torch.sigmoid(activation)).item()
        saturation_score = 1.0 - abs(activation_saturation - 0.5) * 2  # 0.5ä¸ºæœ€ä½³
        
        # 3. å½“å‰å‡†ç¡®ç‡è·ç¦»ä¸Šé™çš„ç©ºé—´
        accuracy_headroom = (0.95 - current_accuracy) / 0.95
        
        # ç»¼åˆè¯„ä¼°
        improvement_potential = (
            gradient_diversity * 0.3 +
            saturation_score * 0.3 +
            accuracy_headroom * 0.4
        )
        
        return float(np.clip(improvement_potential, 0.0, 1.0))
    
    def _classify_leak_type(self, info_drop: float, learning_difficulty: float, 
                          posterior_efficiency: float) -> str:
        """åˆ†ç±»æ¼ç‚¹ç±»å‹"""
        if info_drop > 0.5:
            return "information_compression_bottleneck"
        elif learning_difficulty > 0.7:
            return "gradient_learning_bottleneck"
        elif posterior_efficiency < 0.3:
            return "representational_bottleneck"
        else:
            return "general_bottleneck"
    
    def _select_optimal_mutation_strategy(self, leak_point: Dict[str, Any]) -> str:
        """ä¸ºæ¼ç‚¹é€‰æ‹©æœ€ä¼˜å˜å¼‚ç­–ç•¥"""
        leak_type = leak_point['leak_type']
        severity = leak_point['severity']
        
        if leak_type == "information_compression_bottleneck":
            return "widening"  # å¢åŠ é€šé“æ•°
        elif leak_type == "gradient_learning_bottleneck":
            return "deepening"  # å¢åŠ å±‚æ•°
        elif leak_type == "representational_bottleneck":
            return "hybrid_expansion"  # æ··åˆæ‰©å±•
        else:
            # æ ¹æ®ä¸¥é‡ç¨‹åº¦é€‰æ‹©
            if severity > 0.8:
                return "aggressive_widening"
            else:
                return "conservative_widening"
    
    def predict_mutation_benefits_with_bayesian(self, 
                                              layer_analyses: Dict[str, Any],
                                              current_accuracy: float,
                                              model: nn.Module) -> Dict[str, Dict[str, Any]]:
        """
        ä½¿ç”¨è´å¶æ–¯æ¨æ–­ä¸ºæ‰€æœ‰å€™é€‰å±‚é¢„æµ‹å˜å¼‚æ”¶ç›Š
        
        Args:
            layer_analyses: æ‰€æœ‰å±‚çš„åˆ†æç»“æœ
            current_accuracy: å½“å‰å‡†ç¡®ç‡
            model: ç¥ç»ç½‘ç»œæ¨¡å‹
            
        Returns:
            æ¯å±‚çš„è´å¶æ–¯æ”¶ç›Šé¢„æµ‹ç»“æœ
        """
        logger.enter_section("è´å¶æ–¯å˜å¼‚æ”¶ç›Šæ‰¹é‡é¢„æµ‹")
        
        bayesian_predictions = {}
        
        # è®¡ç®—æ¨¡å‹å¤æ‚åº¦æŒ‡æ ‡
        model_complexity = self._calculate_model_complexity(model)
        
        for layer_name, layer_analysis in layer_analyses.items():
            try:
                # è·å–æ¨èçš„å˜å¼‚ç­–ç•¥
                recommendation = layer_analysis.get('recommendation', {})
                mutation_strategy = recommendation.get('recommended_strategy', 'widening')
                
                # å¦‚æœæ˜¯æ¼ç‚¹ï¼Œä½¿ç”¨æ¼ç‚¹ç‰¹å®šçš„ç­–ç•¥
                leak_assessment = layer_analysis.get('leak_assessment', {})
                if leak_assessment.get('is_leak_point', False):
                    leak_type = leak_assessment.get('leak_type', 'general_bottleneck')
                    mutation_strategy = self._get_strategy_for_leak_type(leak_type)
                
                # è´å¶æ–¯æ”¶ç›Šé¢„æµ‹
                bayesian_result = self.bayesian_predictor.predict_mutation_benefit(
                    layer_analysis=layer_analysis,
                    mutation_strategy=mutation_strategy,
                    current_accuracy=current_accuracy,
                    model_complexity=model_complexity
                )
                
                # å¢å¼ºåˆ†æç»“æœ
                bayesian_predictions[layer_name] = {
                    'mutation_strategy': mutation_strategy,
                    'bayesian_prediction': bayesian_result,
                    'is_leak_point': leak_assessment.get('is_leak_point', False),
                    'leak_severity': leak_assessment.get('leak_severity', 0.0),
                    'combined_score': self._calculate_combined_benefit_score(
                        layer_analysis, bayesian_result
                    )
                }
                
                logger.info(f"ğŸ¯ {layer_name}: ç­–ç•¥={mutation_strategy}, "
                          f"æœŸæœ›æ”¶ç›Š={bayesian_result['expected_accuracy_gain']:.4f}, "
                          f"ç½®ä¿¡åº¦={bayesian_result['uncertainty_metrics']['prediction_confidence']:.3f}")
                
            except Exception as e:
                logger.error(f"âŒ è´å¶æ–¯é¢„æµ‹å¤±è´¥ {layer_name}: {e}")
                bayesian_predictions[layer_name] = {
                    'mutation_strategy': 'widening',
                    'bayesian_prediction': self.bayesian_predictor._fallback_prediction('widening', current_accuracy),
                    'error': str(e)
                }
        
        logger.success(f"å®Œæˆ{len(bayesian_predictions)}ä¸ªå±‚çš„è´å¶æ–¯æ”¶ç›Šé¢„æµ‹")
        logger.exit_section("è´å¶æ–¯å˜å¼‚æ”¶ç›Šæ‰¹é‡é¢„æµ‹")
        
        return bayesian_predictions
    
    def predict_comprehensive_strategies_for_top_candidates(self,
                                                          layer_analyses: Dict[str, Any],
                                                          current_accuracy: float,
                                                          model: nn.Module,
                                                          top_n: int = 3) -> Dict[str, Dict[str, Any]]:
        """
        ä¸ºå‰Nä¸ªå€™é€‰å±‚é¢„æµ‹ç»¼åˆå˜å¼‚ç­–ç•¥
        åŒ…æ‹¬å˜å¼‚æ¨¡å¼é€‰æ‹©å’Œå±‚ç±»å‹ç»„åˆé¢„æµ‹
        """
        logger.enter_section("ç»¼åˆç­–ç•¥é¢„æµ‹")
        
        try:
            comprehensive_strategies = {}
            
            # é€‰æ‹©top Nå€™é€‰å±‚
            candidates = []
            for layer_name, analysis in layer_analyses.items():
                improvement_potential = analysis.get('mutation_prediction', {}).get('improvement_potential', 0)
                leak_severity = analysis.get('leak_assessment', {}).get('leak_severity', 0)
                combined_score = improvement_potential + leak_severity * 0.5
                candidates.append((layer_name, combined_score, analysis))
            
            # æŒ‰è¯„åˆ†æ’åºå¹¶é€‰æ‹©å‰Nä¸ª
            candidates.sort(key=lambda x: x[1], reverse=True)
            top_candidates = candidates[:top_n]
            
            for layer_name, score, layer_analysis in top_candidates:
                logger.info(f"ğŸ¯ é¢„æµ‹ {layer_name} çš„ç»¼åˆå˜å¼‚ç­–ç•¥...")
                
                # é¢„æµ‹ç»¼åˆç­–ç•¥
                comprehensive_strategy = self.bayesian_predictor.predict_comprehensive_mutation_strategy(
                    layer_analysis=layer_analysis,
                    current_accuracy=current_accuracy,
                    model=model,
                    target_layer_name=layer_name
                )
                
                comprehensive_strategies[layer_name] = {
                    'layer_score': score,
                    'comprehensive_strategy': comprehensive_strategy,
                    'detailed_breakdown': {
                        'mode_analysis': self._extract_mode_analysis(comprehensive_strategy),
                        'combination_analysis': self._extract_combination_analysis(comprehensive_strategy),
                        'implementation_plan': comprehensive_strategy.get('implementation_details', {})
                    }
                }
                
                # è¯¦ç»†æ—¥å¿—è¾“å‡º
                mode = comprehensive_strategy['mutation_mode']
                combo = comprehensive_strategy['layer_combination']['combination']
                total_gain = comprehensive_strategy['expected_total_gain']
                confidence = comprehensive_strategy['confidence']
                
                logger.info(f"  ğŸ“‹ {layer_name}: {mode} + {combo}")
                logger.info(f"    ğŸ’¡ æ€»æœŸæœ›æ”¶ç›Š: {total_gain:.4f}")
                logger.info(f"    ğŸ¯ ç½®ä¿¡åº¦: {confidence:.3f}")
            
            logger.success(f"å®Œæˆ{len(comprehensive_strategies)}ä¸ªå±‚çš„ç»¼åˆç­–ç•¥é¢„æµ‹")
            logger.exit_section("ç»¼åˆç­–ç•¥é¢„æµ‹")
            
            return comprehensive_strategies
            
        except Exception as e:
            logger.error(f"ç»¼åˆç­–ç•¥é¢„æµ‹å¤±è´¥: {e}")
            logger.exit_section("ç»¼åˆç­–ç•¥é¢„æµ‹")
            return {}

    def _extract_mode_analysis(self, comprehensive_strategy: Dict[str, Any]) -> Dict[str, Any]:
        """æå–å˜å¼‚æ¨¡å¼åˆ†æ"""
        return {
            'recommended_mode': comprehensive_strategy.get('mutation_mode', 'unknown'),
            'mode_reasoning': "åŸºäºç“¶é¢ˆç±»å‹å’Œå‡†ç¡®ç‡é˜¶æ®µçš„æœ€ä¼˜é€‰æ‹©",
            'alternatives': ['serial_division', 'parallel_division', 'hybrid_division']
        }

    def _extract_combination_analysis(self, comprehensive_strategy: Dict[str, Any]) -> Dict[str, Any]:
        """æå–å±‚ç»„åˆåˆ†æ"""
        layer_combo = comprehensive_strategy.get('layer_combination', {})
        return {
            'recommended_combination': layer_combo.get('combination', 'unknown'),
            'combination_type': layer_combo.get('type', 'unknown'),
            'synergy_score': layer_combo.get('synergy', 0.5),
            'implementation_cost': layer_combo.get('implementation_cost', 1.0)
        }
    
    def _calculate_model_complexity(self, model: nn.Module) -> Dict[str, float]:
        """è®¡ç®—æ¨¡å‹å¤æ‚åº¦æŒ‡æ ‡"""
        
        total_params = sum(p.numel() for p in model.parameters())
        
        # è®¡ç®—å±‚æ·±åº¦å’Œå¹³å‡å®½åº¦
        layer_count = 0
        total_width = 0
        
        for name, module in model.named_modules():
            if isinstance(module, (nn.Linear, nn.Conv2d)):
                layer_count += 1
                
                if isinstance(module, nn.Linear):
                    total_width += module.out_features
                elif isinstance(module, nn.Conv2d):
                    total_width += module.out_channels
        
        avg_width = total_width / max(layer_count, 1)
        
        return {
            'total_parameters': float(total_params),
            'layer_depth': float(layer_count),
            'layer_width': float(avg_width)
        }
    
    def _get_strategy_for_leak_type(self, leak_type: str) -> str:
        """æ ¹æ®æ¼ç‚¹ç±»å‹è·å–æœ€ä¼˜ç­–ç•¥"""
        
        strategy_mapping = {
            'information_compression_bottleneck': 'widening',
            'gradient_learning_bottleneck': 'deepening',
            'representational_bottleneck': 'hybrid_expansion',
            'general_bottleneck': 'widening'
        }
        
        return strategy_mapping.get(leak_type, 'widening')
    
    def _calculate_combined_benefit_score(self, 
                                        layer_analysis: Dict[str, Any],
                                        bayesian_result: Dict[str, Any]) -> float:
        """è®¡ç®—ç»¼åˆæ”¶ç›Šè¯„åˆ†"""
        
        # åŸå§‹å˜å¼‚æ½œåŠ›
        original_potential = layer_analysis.get('mutation_prediction', {}).get('improvement_potential', 0.0)
        
        # è´å¶æ–¯æœŸæœ›æ”¶ç›Š
        bayesian_gain = bayesian_result.get('expected_accuracy_gain', 0.0)
        
        # è´å¶æ–¯ç½®ä¿¡åº¦
        confidence = bayesian_result.get('uncertainty_metrics', {}).get('prediction_confidence', 0.5)
        
        # é£é™©è°ƒæ•´æ”¶ç›Š
        risk_adjusted = bayesian_result.get('risk_adjusted_benefit', {}).get('risk_adjusted_gain', 0.0)
        
        # ç»¼åˆè¯„åˆ†
        combined_score = (
            original_potential * 0.3 +
            bayesian_gain * 0.4 +
            risk_adjusted * 0.3
        ) * confidence
        
        return float(combined_score)
    
    def _create_mutation_execution_plan(self, priority_targets: List[Dict[str, Any]],
                                      current_accuracy: float,
                                      flow_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºå˜å¼‚æ‰§è¡Œè®¡åˆ’"""
        
        # æ ¹æ®å½“å‰å‡†ç¡®ç‡å’Œå…¨å±€æµåˆ†æç¡®å®šæ‰§è¡Œç­–ç•¥
        if current_accuracy < 0.85:
            execution_mode = "conservative"
            max_concurrent = 1
        elif current_accuracy < 0.92:
            execution_mode = "moderate"
            max_concurrent = 2
        else:
            execution_mode = "aggressive"
            max_concurrent = 3
        
        return {
            'execution_mode': execution_mode,
            'max_concurrent_mutations': max_concurrent,
            'total_expected_improvement': sum(t['expected_improvement'] for t in priority_targets),
            'estimated_parameter_cost': len(priority_targets) * 5000,  # ä¼°ç®—
            'execution_phases': self._plan_execution_phases(priority_targets, max_concurrent)
        }
    
    def _plan_execution_phases(self, targets: List[Dict[str, Any]], max_concurrent: int) -> List[List[str]]:
        """è§„åˆ’æ‰§è¡Œé˜¶æ®µ"""
        phases = []
        
        # æŒ‰ä¼˜å…ˆçº§åˆ†ç»„
        critical = [t for t in targets if t['priority'] == 'critical']
        high = [t for t in targets if t['priority'] == 'high']
        
        # ç¬¬ä¸€é˜¶æ®µï¼šå…³é”®æ¼ç‚¹
        if critical:
            phases.append([t['layer_name'] for t in critical[:max_concurrent]])
        
        # ç¬¬äºŒé˜¶æ®µï¼šé«˜æ½œåŠ›å±‚
        if high:
            phases.append([t['layer_name'] for t in high[:max_concurrent]])
        
        return phases
    
    def _summarize_comprehensive_strategies(self, comprehensive_strategies: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """æ€»ç»“ç»¼åˆç­–ç•¥"""
        if not comprehensive_strategies:
            return {}
        
        # ç»Ÿè®¡å˜å¼‚æ¨¡å¼åå¥½
        mode_counts = {}
        combination_types = {}
        total_expected_gain = 0.0
        avg_confidence = 0.0
        
        for layer_name, strategy_data in comprehensive_strategies.items():
            comp_strategy = strategy_data['comprehensive_strategy']
            
            # ç»Ÿè®¡å˜å¼‚æ¨¡å¼
            mode = comp_strategy.get('mutation_mode', 'unknown')
            mode_counts[mode] = mode_counts.get(mode, 0) + 1
            
            # ç»Ÿè®¡å±‚ç»„åˆç±»å‹
            combo_type = comp_strategy.get('layer_combination', {}).get('type', 'unknown')
            combination_types[combo_type] = combination_types.get(combo_type, 0) + 1
            
            # ç´¯åŠ æŒ‡æ ‡
            total_expected_gain += comp_strategy.get('expected_total_gain', 0.0)
            avg_confidence += comp_strategy.get('confidence', 0.0)
        
        n_strategies = len(comprehensive_strategies)
        avg_confidence /= max(n_strategies, 1)
        
        # æ‰¾å‡ºæœ€å—æ¨èçš„æ¨¡å¼å’Œç»„åˆ
        preferred_mode = max(mode_counts.items(), key=lambda x: x[1])[0] if mode_counts else 'serial_division'
        preferred_combination = max(combination_types.items(), key=lambda x: x[1])[0] if combination_types else 'heterogeneous'
        
        return {
            'total_strategies_analyzed': n_strategies,
            'preferred_mutation_mode': preferred_mode,
            'preferred_combination_type': preferred_combination,
            'mode_distribution': mode_counts,
            'combination_distribution': combination_types,
            'total_expected_improvement': total_expected_gain,
            'average_confidence': avg_confidence,
            'strategy_recommendations': [
                f"ä¸»è¦æ¨è: {preferred_mode} å˜å¼‚æ¨¡å¼",
                f"é¦–é€‰ç»„åˆ: {preferred_combination} å±‚ç»„åˆ",
                f"æ€»æœŸæœ›æ”¶ç›Š: {total_expected_gain:.4f}",
                f"å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}"
            ]
        }
    
    def _is_analyzable_layer(self, model: nn.Module, layer_name: str) -> bool:
        """åˆ¤æ–­å±‚æ˜¯å¦å¯åˆ†æ"""
        try:
            module = dict(model.named_modules())[layer_name]
            return isinstance(module, (nn.Linear, nn.Conv2d, nn.Conv1d))
        except:
            return False

    def analyze_layer_mutation_potential(self, 
                                       model: nn.Module,
                                       layer_name: str,
                                       activations: Dict[str, torch.Tensor],
                                       gradients: Dict[str, torch.Tensor],
                                       targets: torch.Tensor,
                                       current_accuracy: float) -> Dict[str, Any]:
        """
        åˆ†ææŒ‡å®šå±‚çš„å˜å¼‚æ½œåŠ›
        
        Args:
            model: å®Œæ•´æ¨¡å‹
            layer_name: ç›®æ ‡å±‚åç§°
            activations: æ¿€æ´»å€¼å­—å…¸
            gradients: æ¢¯åº¦å­—å…¸
            targets: çœŸå®æ ‡ç­¾
            current_accuracy: å½“å‰å‡†ç¡®ç‡
            
        Returns:
            å®Œæ•´çš„å˜å¼‚æ½œåŠ›åˆ†æç»“æœ
        """
        logger.enter_section(f"Net2Netåˆ†æ: {layer_name}")
        
        try:
            # 1. æå–å­ç½‘ç»œ
            subnetwork, subnetwork_info = self.extractor.extract_subnetwork_from_layer(
                model, layer_name
            )
            
            # 2. è·å–è¯¥å±‚çš„æ¿€æ´»å’Œæ¢¯åº¦
            if layer_name in activations and layer_name in gradients:
                layer_activation = activations[layer_name]
                layer_gradient = gradients[layer_name]
            else:
                logger.warning(f"å±‚{layer_name}ç¼ºå°‘æ¿€æ´»å€¼æˆ–æ¢¯åº¦ä¿¡æ¯")
                layer_activation = torch.randn(32, 64)  # é»˜è®¤å€¼
                layer_gradient = torch.randn(32, 64)
            
            # 3. åˆ†æå‚æ•°ç©ºé—´æ•ˆç‡
            param_space_analysis = self.param_analyzer.analyze_parameter_space_efficiency(
                subnetwork, layer_activation, layer_gradient, targets
            )
            
            # 4. é¢„æµ‹å˜å¼‚æ½œåŠ›
            mutation_prediction = self.predictor.predict_mutation_potential(
                subnetwork, subnetwork_info, param_space_analysis, current_accuracy
            )
            
            # 5. ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
            analysis_result = {
                'layer_name': layer_name,
                'subnetwork_info': subnetwork_info,
                'parameter_space_analysis': param_space_analysis,
                'mutation_prediction': mutation_prediction,
                'recommendation': self._generate_recommendation(
                    layer_name, param_space_analysis, mutation_prediction
                )
            }
            
            logger.success(f"Net2Netåˆ†æå®Œæˆ: {layer_name}")
            logger.exit_section(f"Net2Netåˆ†æ: {layer_name}")
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"Net2Netåˆ†æå¤±è´¥: {layer_name} - {e}")
            logger.exit_section(f"Net2Netåˆ†æ: {layer_name}")
            return {
                'layer_name': layer_name,
                'error': str(e),
                'recommendation': {'action': 'skip', 'reason': f'åˆ†æå¤±è´¥: {e}'}
            }

    def _generate_recommendation(self, 
                               layer_name: str,
                               param_space_analysis: Dict[str, float],
                               mutation_prediction: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆå˜å¼‚å»ºè®®"""
        
        improvement_potential = mutation_prediction['improvement_potential']
        risk_assessment = mutation_prediction['risk_assessment']
        strategy_predictions = mutation_prediction['strategy_predictions']
        
        # é€‰æ‹©æœ€ä½³ç­–ç•¥
        best_strategy = None
        best_score = -1.0
        
        for strategy_name, strategy_info in strategy_predictions.items():
            # ç»¼åˆè¯„åˆ†ï¼šæœŸæœ›æ”¶ç›Š - é£é™© - æˆæœ¬
            score = (
                strategy_info['expected_accuracy_gain'] * 2.0 -
                strategy_info['stability_risk'] -
                strategy_info['parameter_cost'] * 0.5
            )
            
            if score > best_score:
                best_score = score
                best_strategy = strategy_name
        
        # ç”Ÿæˆå»ºè®®
        if improvement_potential > 0.3 and risk_assessment['overall_risk'] < 0.6:
            action = 'mutate'
            priority = 'high' if improvement_potential > 0.6 else 'medium'
        elif improvement_potential > 0.1 and risk_assessment['overall_risk'] < 0.8:
            action = 'consider'
            priority = 'low'
        else:
            action = 'skip'
            priority = 'none'
        
        return {
            'action': action,
            'priority': priority,
            'recommended_strategy': best_strategy,
            'expected_gain': strategy_predictions.get(best_strategy, {}).get('expected_accuracy_gain', 0.0),
            'risk_level': risk_assessment['overall_risk'],
            'reason': f"æ”¹è¿›æ½œåŠ›={improvement_potential:.3f}, é£é™©={risk_assessment['overall_risk']:.3f}"
        }

# æ–°å¢ï¼šä¿¡æ¯æµåˆ†æå™¨ç±»
class InformationFlowAnalyzer:
    """ä¿¡æ¯æµåˆ†æå™¨"""
    
    def __init__(self):
        self.flow_patterns = {}
        
    def analyze_flow_patterns(self, activations: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """åˆ†æä¿¡æ¯æµæ¨¡å¼"""
        # å®ç°ä¿¡æ¯æµåˆ†æé€»è¾‘
        return {}

class InformationLeakDetector:
    """ä¿¡æ¯æ³„éœ²æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.leak_thresholds = {
            'entropy_drop': 0.5,
            'gradient_variance': 0.1,
            'correlation_loss': 0.3
        }
    
    def detect_leaks(self, layer_data: Dict[str, torch.Tensor]) -> List[Dict[str, Any]]:
        """æ£€æµ‹ä¿¡æ¯æ³„éœ²ç‚¹"""
        # å®ç°æ³„éœ²æ£€æµ‹é€»è¾‘
        return []

# æ–°å¢ï¼šåŸºäºè´å¶æ–¯æ¨æ–­çš„å˜å¼‚æ”¶ç›Šé¢„æµ‹å™¨
class BayesianMutationBenefitPredictor:
    """
    åŸºäºè´å¶æ–¯æ¨æ–­çš„å˜å¼‚æ”¶ç›Šé¢„æµ‹å™¨
    
    ä½¿ç”¨è´å¶æ–¯ç»Ÿè®¡ã€é«˜æ–¯è¿‡ç¨‹å›å½’å’Œè’™ç‰¹å¡ç½—é‡‡æ ·æ¥é¢„æµ‹æ¶æ„å˜å¼‚çš„æœŸæœ›æ”¶ç›Š
    """
    
    def __init__(self):
        self.prior_knowledge = self._initialize_prior_knowledge()
        self.gp_hyperparams = {
            'length_scale': 1.0,
            'variance': 1.0,
            'noise_variance': 0.01
        }
        self.mc_samples = 1000  # è’™ç‰¹å¡ç½—é‡‡æ ·æ•°
        
        # å†å²å˜å¼‚æ•°æ®ï¼ˆç”¨äºæ›´æ–°å…ˆéªŒï¼‰
        self.mutation_history = []
        
    def _initialize_prior_knowledge(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–å…ˆéªŒçŸ¥è¯†"""
        return {
            # ä¸åŒå˜å¼‚ç±»å‹çš„å†å²æˆåŠŸç‡å…ˆéªŒ
            'mutation_success_priors': {
                'widening': {'alpha': 3, 'beta': 2},  # Betaåˆ†å¸ƒå‚æ•°ï¼Œå€¾å‘äºæˆåŠŸ
                'deepening': {'alpha': 2, 'beta': 3},  # ç›¸å¯¹ä¿å®ˆ
                'hybrid_expansion': {'alpha': 4, 'beta': 2},  # è¾ƒä¸ºæ¿€è¿›
                'aggressive_widening': {'alpha': 2, 'beta': 1}  # é«˜é£é™©é«˜æ”¶ç›Š
            },
            
            # Serial vs Parallel mutation å…ˆéªŒçŸ¥è¯†
            'mutation_mode_priors': {
                'serial_division': {
                    'success_rate': {'alpha': 5, 'beta': 3},  # ç›¸å¯¹ç¨³å®š
                    'best_for': ['gradient_learning_bottleneck', 'representational_bottleneck'],
                    'accuracy_preference': {'low': 0.7, 'medium': 0.8, 'high': 0.6}
                },
                'parallel_division': {
                    'success_rate': {'alpha': 4, 'beta': 4},  # ä¸­ç­‰é£é™©
                    'best_for': ['information_compression_bottleneck'],
                    'accuracy_preference': {'low': 0.6, 'medium': 0.7, 'high': 0.8}
                },
                'hybrid_division': {
                    'success_rate': {'alpha': 6, 'beta': 2},  # æ¿€è¿›ä½†é«˜æ”¶ç›Š
                    'best_for': ['general_bottleneck'],
                    'accuracy_preference': {'low': 0.8, 'medium': 0.9, 'high': 0.7}
                }
            },
            
            # å±‚ç±»å‹ç»„åˆç­–ç•¥å…ˆéªŒ (åŒç§ vs å¼‚ç§)
            'layer_combination_priors': {
                'homogeneous': {  # åŒç§å±‚
                    'conv2d_conv2d': {'effectiveness': 0.7, 'stability': 0.9},
                    'linear_linear': {'effectiveness': 0.6, 'stability': 0.8},
                    'batch_norm_batch_norm': {'effectiveness': 0.5, 'stability': 0.9}
                },
                'heterogeneous': {  # å¼‚ç§å±‚ç»„åˆ
                    'conv2d_depthwise_conv': {'effectiveness': 0.8, 'stability': 0.7},
                    'conv2d_batch_norm': {'effectiveness': 0.9, 'stability': 0.8},
                    'conv2d_dropout': {'effectiveness': 0.6, 'stability': 0.7},
                    'conv2d_attention': {'effectiveness': 0.85, 'stability': 0.6},
                    'linear_dropout': {'effectiveness': 0.7, 'stability': 0.8},
                    'linear_batch_norm': {'effectiveness': 0.8, 'stability': 0.9},
                    'conv2d_pool': {'effectiveness': 0.5, 'stability': 0.9},
                    'conv2d_residual_block': {'effectiveness': 0.9, 'stability': 0.8}
                }
            },
            
            # ä¸åŒç½‘ç»œå±‚æ“ä½œçš„é€‚ç”¨æ€§å…ˆéªŒ
            'layer_operation_priors': {
                'conv2d': {
                    'feature_extraction_boost': 0.9,
                    'spatial_processing': 0.95,
                    'parameter_efficiency': 0.7,
                    'computation_cost': 0.6
                },
                'depthwise_conv': {
                    'feature_extraction_boost': 0.7,
                    'spatial_processing': 0.8,
                    'parameter_efficiency': 0.9,
                    'computation_cost': 0.8
                },
                'batch_norm': {
                    'feature_extraction_boost': 0.4,
                    'spatial_processing': 0.3,
                    'parameter_efficiency': 0.9,
                    'computation_cost': 0.9,
                    'stability_boost': 0.9
                },
                'dropout': {
                    'feature_extraction_boost': 0.2,
                    'spatial_processing': 0.1,
                    'parameter_efficiency': 1.0,
                    'computation_cost': 0.95,
                    'overfitting_prevention': 0.8
                },
                'attention': {
                    'feature_extraction_boost': 0.85,
                    'spatial_processing': 0.7,
                    'parameter_efficiency': 0.5,
                    'computation_cost': 0.3,
                    'long_range_dependency': 0.95
                },
                'pool': {
                    'feature_extraction_boost': 0.3,
                    'spatial_processing': 0.6,
                    'parameter_efficiency': 1.0,
                    'computation_cost': 0.9,
                    'dimensionality_reduction': 0.9
                },
                'residual_connection': {
                    'feature_extraction_boost': 0.6,
                    'spatial_processing': 0.5,
                    'parameter_efficiency': 0.8,
                    'computation_cost': 0.7,
                    'gradient_flow': 0.95
                }
            },
            
            # ä¸åŒç“¶é¢ˆç±»å‹å¯¹å˜å¼‚çš„å“åº”æ€§å…ˆéªŒ
            'bottleneck_response_priors': {
                'information_compression_bottleneck': {
                    'widening_response': 0.8,
                    'deepening_response': 0.3,
                    'hybrid_response': 0.6,
                    'preferred_operations': ['conv2d', 'attention', 'residual_connection']
                },
                'gradient_learning_bottleneck': {
                    'widening_response': 0.4,
                    'deepening_response': 0.7,
                    'hybrid_response': 0.5,
                    'preferred_operations': ['batch_norm', 'residual_connection', 'dropout']
                },
                'representational_bottleneck': {
                    'widening_response': 0.6,
                    'deepening_response': 0.5,
                    'hybrid_response': 0.9,
                    'preferred_operations': ['attention', 'conv2d', 'depthwise_conv']
                }
            },
            
            # å‡†ç¡®ç‡é˜¶æ®µå¯¹å˜å¼‚æ”¶ç›Šçš„å½±å“
            'accuracy_stage_priors': {
                'low': (0.0, 0.85),    # ä½å‡†ç¡®ç‡é˜¶æ®µï¼Œå˜å¼‚æ”¶ç›Šè¾ƒå¤§
                'medium': (0.85, 0.92), # ä¸­ç­‰å‡†ç¡®ç‡ï¼Œæ”¶ç›Šé€’å‡
                'high': (0.92, 1.0)     # é«˜å‡†ç¡®ç‡ï¼Œæ”¶ç›Šå¾®å°ä½†å…³é”®
            }
        }
    
    def predict_mutation_benefit(self, 
                               layer_analysis: Dict[str, Any],
                               mutation_strategy: str,
                               current_accuracy: float,
                               model_complexity: Dict[str, float]) -> Dict[str, Any]:
        """
        ä½¿ç”¨è´å¶æ–¯æ¨æ–­é¢„æµ‹å˜å¼‚æ”¶ç›Š
        
        Args:
            layer_analysis: å±‚åˆ†æç»“æœ
            mutation_strategy: å˜å¼‚ç­–ç•¥ç±»å‹
            current_accuracy: å½“å‰å‡†ç¡®ç‡
            model_complexity: æ¨¡å‹å¤æ‚åº¦æŒ‡æ ‡
            
        Returns:
            åŒ…å«æœŸæœ›æ”¶ç›Šã€ç½®ä¿¡åŒºé—´ã€é£é™©è¯„ä¼°çš„é¢„æµ‹ç»“æœ
        """
        logger.enter_section(f"è´å¶æ–¯å˜å¼‚æ”¶ç›Šé¢„æµ‹: {mutation_strategy}")
        
        try:
            # 1. æ„å»ºç‰¹å¾å‘é‡
            feature_vector = self._extract_feature_vector(
                layer_analysis, current_accuracy, model_complexity
            )
            
            # 2. è´å¶æ–¯åéªŒæ¨æ–­
            posterior_params = self._bayesian_posterior_inference(
                feature_vector, mutation_strategy, layer_analysis
            )
            
            # 3. é«˜æ–¯è¿‡ç¨‹å›å½’é¢„æµ‹
            gp_prediction = self._gaussian_process_prediction(
                feature_vector, posterior_params
            )
            
            # 4. è’™ç‰¹å¡ç½—æœŸæœ›ä¼°è®¡
            mc_estimate = self._monte_carlo_benefit_estimation(
                gp_prediction, mutation_strategy, current_accuracy
            )
            
            # 5. ä¸ç¡®å®šæ€§é‡åŒ–
            uncertainty_metrics = self._quantify_prediction_uncertainty(
                gp_prediction, mc_estimate, feature_vector
            )
            
            # 6. é£é™©è°ƒæ•´æ”¶ç›Š
            risk_adjusted_benefit = self._calculate_risk_adjusted_benefit(
                mc_estimate, uncertainty_metrics, mutation_strategy
            )
            
            prediction_result = {
                'expected_accuracy_gain': mc_estimate['expected_gain'],
                'confidence_interval': mc_estimate['confidence_interval'],
                'success_probability': posterior_params['success_probability'],
                'risk_adjusted_benefit': risk_adjusted_benefit,
                'uncertainty_metrics': uncertainty_metrics,
                'bayesian_evidence': posterior_params['evidence'],
                'recommendation_strength': self._calculate_recommendation_strength(
                    risk_adjusted_benefit, uncertainty_metrics
                )
            }
            
            logger.success(f"è´å¶æ–¯é¢„æµ‹å®Œæˆ: æœŸæœ›æ”¶ç›Š={mc_estimate['expected_gain']:.4f}")
            logger.exit_section(f"è´å¶æ–¯å˜å¼‚æ”¶ç›Šé¢„æµ‹: {mutation_strategy}")
            
            return prediction_result
            
        except Exception as e:
            logger.error(f"è´å¶æ–¯é¢„æµ‹å¤±è´¥: {e}")
            logger.exit_section(f"è´å¶æ–¯å˜å¼‚æ”¶ç›Šé¢„æµ‹: {mutation_strategy}")
            return self._fallback_prediction(mutation_strategy, current_accuracy)
    
    def _extract_feature_vector(self, layer_analysis: Dict[str, Any], 
                              current_accuracy: float,
                              model_complexity: Dict[str, float]) -> np.ndarray:
        """æå–ç”¨äºé¢„æµ‹çš„ç‰¹å¾å‘é‡"""
        
        # ä»å±‚åˆ†æä¸­æå–å…³é”®ç‰¹å¾
        mutation_prediction = layer_analysis.get('mutation_prediction', {})
        param_analysis = layer_analysis.get('parameter_space_analysis', {})
        leak_assessment = layer_analysis.get('leak_assessment', {})
        
        features = [
            # åŸºç¡€å‡†ç¡®ç‡å’Œæ”¹è¿›ç©ºé—´
            current_accuracy,
            1.0 - current_accuracy,  # æ”¹è¿›ç©ºé—´
            
            # å±‚ç‰¹æ€§
            mutation_prediction.get('improvement_potential', 0.0),
            param_analysis.get('efficiency_score', 0.0),
            param_analysis.get('utilization_rate', 0.0),
            
            # æ¼ç‚¹ç‰¹å¾
            leak_assessment.get('leak_severity', 0.0),
            1.0 if leak_assessment.get('is_leak_point', False) else 0.0,
            
            # æ¨¡å‹å¤æ‚åº¦
            model_complexity.get('total_parameters', 0.0) / 1e6,  # ç™¾ä¸‡å‚æ•°ä¸ºå•ä½
            model_complexity.get('layer_depth', 0.0) / 50.0,      # å½’ä¸€åŒ–æ·±åº¦
            model_complexity.get('layer_width', 0.0) / 1000.0,    # å½’ä¸€åŒ–å®½åº¦
            
            # æ¢¯åº¦å’Œæ¿€æ´»ç»Ÿè®¡
            mutation_prediction.get('gradient_diversity', 0.0),
            mutation_prediction.get('activation_saturation', 0.5),
        ]
        
        return np.array(features, dtype=np.float32)
    
    def _bayesian_posterior_inference(self, feature_vector: np.ndarray,
                                    mutation_strategy: str,
                                    layer_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """è´å¶æ–¯åéªŒæ¨æ–­"""
        
        # è·å–å…ˆéªŒå‚æ•°
        mutation_prior = self.prior_knowledge['mutation_success_priors'].get(
            mutation_strategy, {'alpha': 2, 'beta': 2}
        )
        
        # è·å–ç“¶é¢ˆç±»å‹ç›¸å…³çš„å…ˆéªŒ
        leak_assessment = layer_analysis.get('leak_assessment', {})
        leak_type = leak_assessment.get('leak_type', 'general_bottleneck')
        
        bottleneck_prior = self.prior_knowledge['bottleneck_response_priors'].get(
            leak_type, {}
        )
        
        strategy_response = bottleneck_prior.get(f"{mutation_strategy}_response", 0.5)
        
        # è´å¶æ–¯æ›´æ–°ï¼šæ ¹æ®è§‚æµ‹ç‰¹å¾æ›´æ–°å…ˆéªŒ
        # ä½¿ç”¨å…±è½­å…ˆéªŒ-åéªŒæ›´æ–°
        observed_evidence = self._calculate_evidence_strength(feature_vector)
        
        # Betaåˆ†å¸ƒçš„å…±è½­æ›´æ–°
        alpha_posterior = mutation_prior['alpha'] + observed_evidence['positive_evidence']
        beta_posterior = mutation_prior['beta'] + observed_evidence['negative_evidence']
        
        # è®¡ç®—åéªŒæˆåŠŸæ¦‚ç‡
        success_probability = alpha_posterior / (alpha_posterior + beta_posterior)
        
        # è´å¶æ–¯è¯æ®ï¼ˆè¾¹é™…ä¼¼ç„¶ï¼‰
        evidence = self._calculate_marginal_likelihood(
            feature_vector, mutation_strategy, strategy_response
        )
        
        return {
            'success_probability': success_probability,
            'alpha_posterior': alpha_posterior,
            'beta_posterior': beta_posterior,
            'evidence': evidence,
            'strategy_response': strategy_response
        }
    
    def _calculate_evidence_strength(self, feature_vector: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—è§‚æµ‹è¯æ®å¼ºåº¦"""
        
        # åŸºäºç‰¹å¾å‘é‡è®¡ç®—æ”¯æŒå’Œåå¯¹å˜å¼‚çš„è¯æ®
        current_acc = feature_vector[0]
        improvement_space = feature_vector[1]
        improvement_potential = feature_vector[2]
        leak_severity = feature_vector[5]
        is_leak_point = feature_vector[6]
        
        # æ­£é¢è¯æ®ï¼šæ”¯æŒå˜å¼‚çš„å› ç´ 
        positive_evidence = (
            improvement_space * 2.0 +           # æ”¹è¿›ç©ºé—´å¤§
            improvement_potential * 1.5 +       # æ”¹è¿›æ½œåŠ›é«˜
            leak_severity * 2.0 +               # æ¼ç‚¹ä¸¥é‡
            is_leak_point * 1.0                 # ç¡®å®æ˜¯æ¼ç‚¹
        )
        
        # è´Ÿé¢è¯æ®ï¼šåå¯¹å˜å¼‚çš„å› ç´ 
        negative_evidence = (
            current_acc * 1.0 +                 # å½“å‰å‡†ç¡®ç‡å·²ç»å¾ˆé«˜
            (1.0 - improvement_potential) * 1.0 # æ”¹è¿›æ½œåŠ›ä½
        )
        
        return {
            'positive_evidence': max(0.1, positive_evidence),
            'negative_evidence': max(0.1, negative_evidence)
        }
    
    def _calculate_marginal_likelihood(self, feature_vector: np.ndarray,
                                     mutation_strategy: str,
                                     strategy_response: float) -> float:
        """è®¡ç®—è¾¹é™…ä¼¼ç„¶ï¼ˆè´å¶æ–¯è¯æ®ï¼‰"""
        
        # ä½¿ç”¨é«˜æ–¯ä¼¼ç„¶å‡½æ•°
        likelihood = 0.0
        
        # åŸºäºç‰¹å¾ç›¸ä¼¼æ€§è®¡ç®—ä¼¼ç„¶
        for historical_mutation in self.mutation_history:
            if historical_mutation['strategy'] == mutation_strategy:
                feature_similarity = self._calculate_feature_similarity(
                    feature_vector, historical_mutation['features']
                )
                
                success = historical_mutation['success']
                likelihood += feature_similarity * (success if success else (1 - success))
        
        # å¦‚æœæ²¡æœ‰å†å²æ•°æ®ï¼Œä½¿ç”¨å…ˆéªŒå“åº”æ€§
        if likelihood == 0.0:
            likelihood = strategy_response
        
        return float(np.clip(likelihood, 0.01, 0.99))
    
    def _gaussian_process_prediction(self, feature_vector: np.ndarray,
                                   posterior_params: Dict[str, Any]) -> Dict[str, Any]:
        """é«˜æ–¯è¿‡ç¨‹å›å½’é¢„æµ‹"""
        
        # æ„å»ºæ ¸å‡½æ•°ï¼ˆRBFæ ¸ï¼‰
        def rbf_kernel(x1, x2, length_scale=1.0, variance=1.0):
            return variance * np.exp(-0.5 * np.sum((x1 - x2)**2) / length_scale**2)
        
        # å¦‚æœæœ‰å†å²æ•°æ®ï¼Œä½¿ç”¨GPå›å½’
        if self.mutation_history:
            # æ„å»ºè®­ç»ƒæ•°æ®
            X_train = np.array([m['features'] for m in self.mutation_history])
            y_train = np.array([m['actual_gain'] for m in self.mutation_history])
            
            # è®¡ç®—æ ¸çŸ©é˜µ
            n_train = len(X_train)
            K = np.zeros((n_train, n_train))
            
            for i in range(n_train):
                for j in range(n_train):
                    K[i, j] = rbf_kernel(
                        X_train[i], X_train[j],
                        self.gp_hyperparams['length_scale'],
                        self.gp_hyperparams['variance']
                    )
            
            # æ·»åŠ å™ªå£°é¡¹
            K += np.eye(n_train) * self.gp_hyperparams['noise_variance']
            
            # è®¡ç®—é¢„æµ‹
            k_star = np.array([
                rbf_kernel(feature_vector, X_train[i],
                          self.gp_hyperparams['length_scale'],
                          self.gp_hyperparams['variance'])
                for i in range(n_train)
            ])
            
            try:
                K_inv = np.linalg.inv(K)
                mean_pred = k_star.T @ K_inv @ y_train
                
                k_star_star = rbf_kernel(
                    feature_vector, feature_vector,
                    self.gp_hyperparams['length_scale'],
                    self.gp_hyperparams['variance']
                )
                
                var_pred = k_star_star - k_star.T @ K_inv @ k_star
                
            except np.linalg.LinAlgError:
                # å¦‚æœçŸ©é˜µå¥‡å¼‚ï¼Œä½¿ç”¨ä¼ªé€†
                K_pinv = np.linalg.pinv(K)
                mean_pred = k_star.T @ K_pinv @ y_train
                var_pred = self.gp_hyperparams['variance']
        
        else:
            # æ²¡æœ‰å†å²æ•°æ®æ—¶ï¼Œä½¿ç”¨å…ˆéªŒå‡å€¼å’Œæ–¹å·®
            mean_pred = posterior_params['success_probability'] * 0.05  # å‡è®¾æœ€å¤§æ”¶ç›Š5%
            var_pred = self.gp_hyperparams['variance']
        
        return {
            'mean_prediction': float(mean_pred),
            'variance_prediction': float(var_pred),
            'std_prediction': float(np.sqrt(var_pred))
        }
    
    def _monte_carlo_benefit_estimation(self, gp_prediction: Dict[str, Any],
                                      mutation_strategy: str,
                                      current_accuracy: float) -> Dict[str, Any]:
        """è’™ç‰¹å¡ç½—æœŸæœ›æ”¶ç›Šä¼°è®¡"""
        
        mean = gp_prediction['mean_prediction']
        std = gp_prediction['std_prediction']
        
        # ä»é¢„æµ‹åˆ†å¸ƒä¸­é‡‡æ ·
        samples = np.random.normal(mean, std, self.mc_samples)
        
        # è€ƒè™‘å˜å¼‚ç­–ç•¥çš„é£é™©ç‰¹æ€§
        strategy_risk_factor = {
            'widening': 0.9,
            'deepening': 0.8,
            'hybrid_expansion': 0.7,
            'aggressive_widening': 0.6
        }.get(mutation_strategy, 0.8)
        
        # åº”ç”¨é£é™©è°ƒæ•´
        risk_adjusted_samples = samples * strategy_risk_factor
        
        # ç¡®ä¿æ”¶ç›Šä¸è¶…è¿‡ç†è®ºä¸Šé™
        max_possible_gain = min(0.95 - current_accuracy, 0.1)  # æœ€å¤§æ”¶ç›Šé™åˆ¶
        risk_adjusted_samples = np.clip(risk_adjusted_samples, -0.02, max_possible_gain)
        
        # è®¡ç®—ç»Ÿè®¡é‡
        expected_gain = np.mean(risk_adjusted_samples)
        gain_std = np.std(risk_adjusted_samples)
        
        # ç½®ä¿¡åŒºé—´
        confidence_interval = {
            '95%': (
                np.percentile(risk_adjusted_samples, 2.5),
                np.percentile(risk_adjusted_samples, 97.5)
            ),
            '90%': (
                np.percentile(risk_adjusted_samples, 5),
                np.percentile(risk_adjusted_samples, 95)
            ),
            '68%': (
                np.percentile(risk_adjusted_samples, 16),
                np.percentile(risk_adjusted_samples, 84)
            )
        }
        
        # æˆåŠŸæ¦‚ç‡ï¼ˆæ”¶ç›Šä¸ºæ­£çš„æ¦‚ç‡ï¼‰
        success_probability = np.mean(risk_adjusted_samples > 0)
        
        return {
            'expected_gain': float(expected_gain),
            'gain_std': float(gain_std),
            'confidence_interval': confidence_interval,
            'success_probability': float(success_probability),
            'samples': risk_adjusted_samples
        }
    
    def _quantify_prediction_uncertainty(self, gp_prediction: Dict[str, Any],
                                       mc_estimate: Dict[str, Any],
                                       feature_vector: np.ndarray) -> Dict[str, Any]:
        """é‡åŒ–é¢„æµ‹ä¸ç¡®å®šæ€§"""
        
        # è®¤çŸ¥ä¸ç¡®å®šæ€§ï¼ˆæ¨¡å‹ä¸ç¡®å®šæ€§ï¼‰
        epistemic_uncertainty = gp_prediction['variance_prediction']
        
        # å¶ç„¶ä¸ç¡®å®šæ€§ï¼ˆæ•°æ®å™ªå£°ï¼‰
        aleatoric_uncertainty = mc_estimate['gain_std']**2
        
        # æ€»ä¸ç¡®å®šæ€§
        total_uncertainty = epistemic_uncertainty + aleatoric_uncertainty
        
        # åŸºäºç‰¹å¾çš„ä¸ç¡®å®šæ€§è¯„ä¼°
        feature_uncertainty = self._assess_feature_uncertainty(feature_vector)
        
        # é¢„æµ‹ç½®ä¿¡åº¦
        prediction_confidence = 1.0 / (1.0 + total_uncertainty)
        
        return {
            'epistemic_uncertainty': float(epistemic_uncertainty),
            'aleatoric_uncertainty': float(aleatoric_uncertainty),
            'total_uncertainty': float(total_uncertainty),
            'feature_uncertainty': feature_uncertainty,
            'prediction_confidence': float(prediction_confidence)
        }
    
    def _calculate_risk_adjusted_benefit(self, mc_estimate: Dict[str, Any],
                                       uncertainty_metrics: Dict[str, Any],
                                       mutation_strategy: str) -> Dict[str, Any]:
        """è®¡ç®—é£é™©è°ƒæ•´åçš„æ”¶ç›Š"""
        
        expected_gain = mc_estimate['expected_gain']
        total_uncertainty = uncertainty_metrics['total_uncertainty']
        success_prob = mc_estimate['success_probability']
        
        # é£é™©è°ƒæ•´ç³»æ•°
        risk_aversion_factor = 0.5  # å¯è°ƒå‚æ•°
        uncertainty_penalty = risk_aversion_factor * total_uncertainty
        
        # é£é™©è°ƒæ•´æ”¶ç›Š = æœŸæœ›æ”¶ç›Š - ä¸ç¡®å®šæ€§æƒ©ç½š
        risk_adjusted_gain = expected_gain - uncertainty_penalty
        
        # å¤æ™®æ¯”ç‡ï¼ˆæ”¶ç›Šé£é™©æ¯”ï¼‰
        sharpe_ratio = expected_gain / (mc_estimate['gain_std'] + 1e-8)
        
        # ä»·å€¼é£é™©ï¼ˆVaRï¼‰
        var_95 = mc_estimate['confidence_interval']['95%'][0]  # 5%åˆ†ä½æ•°
        
        # æ¡ä»¶ä»·å€¼é£é™©ï¼ˆCVaRï¼‰
        samples = mc_estimate['samples']
        cvar_95 = np.mean(samples[samples <= var_95])
        
        return {
            'risk_adjusted_gain': float(risk_adjusted_gain),
            'sharpe_ratio': float(sharpe_ratio),
            'value_at_risk_95': float(var_95),
            'conditional_var_95': float(cvar_95),
            'risk_reward_score': float(expected_gain / (total_uncertainty + 1e-8))
        }
    
    def _calculate_recommendation_strength(self, risk_adjusted_benefit: Dict[str, Any],
                                         uncertainty_metrics: Dict[str, Any]) -> str:
        """è®¡ç®—æ¨èå¼ºåº¦"""
        
        gain = risk_adjusted_benefit['risk_adjusted_gain']
        confidence = uncertainty_metrics['prediction_confidence']
        sharpe_ratio = risk_adjusted_benefit['sharpe_ratio']
        
        # ç»¼åˆè¯„åˆ†
        score = gain * confidence * (1 + sharpe_ratio)
        
        if score > 0.02 and confidence > 0.7:
            return "strong_recommend"
        elif score > 0.01 and confidence > 0.5:
            return "recommend"
        elif score > 0.005:
            return "weak_recommend"
        elif score > -0.005:
            return "neutral"
        else:
            return "not_recommend"
    
    def _assess_feature_uncertainty(self, feature_vector: np.ndarray) -> Dict[str, float]:
        """è¯„ä¼°ç‰¹å¾ä¸ç¡®å®šæ€§"""
        
        return {
            'accuracy_uncertainty': 0.01,  # å‡†ç¡®ç‡æµ‹é‡è¯¯å·®
            'layer_analysis_uncertainty': 0.1,  # å±‚åˆ†æçš„ä¸ç¡®å®šæ€§
            'model_complexity_uncertainty': 0.05  # å¤æ‚åº¦ä¼°è®¡è¯¯å·®
        }
    
    def _calculate_feature_similarity(self, features1: np.ndarray, features2: np.ndarray) -> float:
        """è®¡ç®—ç‰¹å¾ç›¸ä¼¼æ€§"""
        
        # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼æ€§
        dot_product = np.dot(features1, features2)
        norm1 = np.linalg.norm(features1)
        norm2 = np.linalg.norm(features2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
            
        similarity = dot_product / (norm1 * norm2)
        return float(np.clip(similarity, 0.0, 1.0))
    
    def _fallback_prediction(self, mutation_strategy: str, current_accuracy: float) -> Dict[str, Any]:
        """fallbacké¢„æµ‹ï¼ˆå½“è´å¶æ–¯é¢„æµ‹å¤±è´¥æ—¶ï¼‰"""
        
        # ç®€å•çš„å¯å‘å¼é¢„æµ‹
        base_gain = max(0.01, (0.95 - current_accuracy) * 0.1)
        
        strategy_multipliers = {
            'widening': 0.8,
            'deepening': 0.6,
            'hybrid_expansion': 1.0,
            'aggressive_widening': 1.2
        }
        
        expected_gain = base_gain * strategy_multipliers.get(mutation_strategy, 0.8)
        
        return {
            'expected_accuracy_gain': expected_gain,
            'confidence_interval': {'95%': (0.0, expected_gain * 2)},
            'success_probability': 0.5,
            'risk_adjusted_benefit': {'risk_adjusted_gain': expected_gain * 0.5},
            'uncertainty_metrics': {'prediction_confidence': 0.3},
            'recommendation_strength': "weak_recommend" if expected_gain > 0.005 else "neutral"
        }
    
    def update_with_mutation_result(self, 
                                  feature_vector: np.ndarray,
                                  mutation_strategy: str,
                                  actual_gain: float,
                                  success: bool):
        """ç”¨å®é™…å˜å¼‚ç»“æœæ›´æ–°æ¨¡å‹"""
        
        self.mutation_history.append({
            'features': feature_vector,
            'strategy': mutation_strategy,
            'actual_gain': actual_gain,
            'success': success,
            'timestamp': time.time()
        })
        
        # ä¿æŒå†å²è®°å½•å¤§å°
        if len(self.mutation_history) > 100:
            self.mutation_history = self.mutation_history[-100:]
        
        logger.info(f"æ›´æ–°è´å¶æ–¯æ¨¡å‹: {mutation_strategy}, å®é™…æ”¶ç›Š={actual_gain:.4f}")

    def predict_optimal_mutation_mode(self, 
                                    layer_analysis: Dict[str, Any],
                                    current_accuracy: float,
                                    model_complexity: Dict[str, float]) -> Dict[str, Any]:
        """
        é¢„æµ‹æœ€ä¼˜å˜å¼‚æ¨¡å¼ (Serial vs Parallel vs Hybrid Division)
        
        Args:
            layer_analysis: å±‚åˆ†æç»“æœ
            current_accuracy: å½“å‰å‡†ç¡®ç‡
            model_complexity: æ¨¡å‹å¤æ‚åº¦
            
        Returns:
            å„ç§å˜å¼‚æ¨¡å¼çš„æ”¶ç›Šé¢„æµ‹å’Œæ¨è
        """
        logger.enter_section("å˜å¼‚æ¨¡å¼é¢„æµ‹åˆ†æ")
        
        try:
            leak_assessment = layer_analysis.get('leak_assessment', {})
            leak_type = leak_assessment.get('leak_type', 'general_bottleneck')
            leak_severity = leak_assessment.get('leak_severity', 0.0)
            
            # ç¡®å®šå‡†ç¡®ç‡é˜¶æ®µ
            accuracy_stage = self._get_accuracy_stage(current_accuracy)
            
            mode_predictions = {}
            
            # é¢„æµ‹æ¯ç§å˜å¼‚æ¨¡å¼çš„æ”¶ç›Š
            for mode_name, mode_config in self.prior_knowledge['mutation_mode_priors'].items():
                # è®¡ç®—è¯¥æ¨¡å¼å¯¹å½“å‰ç“¶é¢ˆç±»å‹çš„é€‚é…åº¦
                bottleneck_fit = 1.0 if leak_type in mode_config['best_for'] else 0.6
                
                # è®¡ç®—è¯¥æ¨¡å¼å¯¹å½“å‰å‡†ç¡®ç‡é˜¶æ®µçš„é€‚é…åº¦
                accuracy_fit = mode_config['accuracy_preference'][accuracy_stage]
                
                # è®¡ç®—å¤æ‚åº¦é€‚é…åº¦
                complexity_fit = self._calculate_complexity_fit(mode_name, model_complexity)
                
                # è´å¶æ–¯åéªŒæ¦‚ç‡
                alpha = mode_config['success_rate']['alpha']
                beta = mode_config['success_rate']['beta']
                
                # è§‚æµ‹è¯æ®è°ƒæ•´
                evidence_adjustment = leak_severity * bottleneck_fit * accuracy_fit
                alpha_posterior = alpha + evidence_adjustment
                beta_posterior = beta + (1.0 - evidence_adjustment)
                
                success_probability = alpha_posterior / (alpha_posterior + beta_posterior)
                
                # æœŸæœ›æ”¶ç›Šè®¡ç®—
                base_gain = self._calculate_base_mutation_gain(current_accuracy, leak_severity)
                mode_multiplier = self._get_mode_multiplier(mode_name, leak_type, accuracy_stage)
                expected_gain = base_gain * mode_multiplier * success_probability
                
                # é£é™©è¯„ä¼°
                risk_score = self._calculate_mode_risk(mode_name, model_complexity, current_accuracy)
                
                mode_predictions[mode_name] = {
                    'expected_accuracy_gain': float(expected_gain),
                    'success_probability': float(success_probability),
                    'bottleneck_fit': float(bottleneck_fit),
                    'accuracy_stage_fit': float(accuracy_fit),
                    'complexity_fit': float(complexity_fit),
                    'risk_score': float(risk_score),
                    'recommendation_score': float(expected_gain * success_probability / (risk_score + 0.1)),
                    'optimal_for': mode_config['best_for']
                }
            
            # é€‰æ‹©æœ€ä¼˜æ¨¡å¼
            best_mode = max(mode_predictions.items(), 
                          key=lambda x: x[1]['recommendation_score'])
            
            prediction_result = {
                'recommended_mode': best_mode[0],
                'mode_predictions': mode_predictions,
                'confidence': best_mode[1]['success_probability'],
                'expected_improvement': best_mode[1]['expected_accuracy_gain'],
                'reasoning': self._generate_mode_reasoning(best_mode, leak_type, accuracy_stage)
            }
            
            logger.success(f"æœ€ä¼˜å˜å¼‚æ¨¡å¼: {best_mode[0]} (æ”¶ç›Š={best_mode[1]['expected_accuracy_gain']:.4f})")
            logger.exit_section("å˜å¼‚æ¨¡å¼é¢„æµ‹åˆ†æ")
            
            return prediction_result
            
        except Exception as e:
            logger.error(f"å˜å¼‚æ¨¡å¼é¢„æµ‹å¤±è´¥: {e}")
            logger.exit_section("å˜å¼‚æ¨¡å¼é¢„æµ‹åˆ†æ")
            return self._fallback_mode_prediction(current_accuracy)

    def predict_optimal_layer_combinations(self, 
                                         layer_analysis: Dict[str, Any],
                                         target_layer_type: str,
                                         mutation_mode: str,
                                         current_accuracy: float) -> Dict[str, Any]:
        """
        é¢„æµ‹æœ€ä¼˜å±‚ç±»å‹ç»„åˆ (åŒç§ vs å¼‚ç§å±‚)
        
        Args:
            layer_analysis: å±‚åˆ†æç»“æœ
            target_layer_type: ç›®æ ‡å±‚ç±»å‹ (conv2d, linearç­‰)
            mutation_mode: å˜å¼‚æ¨¡å¼ (serial_division, parallel_divisionç­‰)
            current_accuracy: å½“å‰å‡†ç¡®ç‡
            
        Returns:
            å±‚ç±»å‹ç»„åˆçš„æ”¶ç›Šé¢„æµ‹å’Œæ¨è
        """
        logger.enter_section(f"å±‚ç»„åˆé¢„æµ‹: {target_layer_type}")
        
        try:
            leak_assessment = layer_analysis.get('leak_assessment', {})
            leak_type = leak_assessment.get('leak_type', 'general_bottleneck')
            
            # è·å–ç“¶é¢ˆç±»å‹çš„é¦–é€‰æ“ä½œ
            preferred_ops = self.prior_knowledge['bottleneck_response_priors'].get(
                leak_type, {}
            ).get('preferred_operations', ['conv2d', 'batch_norm'])
            
            combination_predictions = {}
            
            # 1. åŒç§å±‚ç»„åˆé¢„æµ‹
            homo_key = f"{target_layer_type}_{target_layer_type}"
            if homo_key in self.prior_knowledge['layer_combination_priors']['homogeneous']:
                homo_config = self.prior_knowledge['layer_combination_priors']['homogeneous'][homo_key]
                homo_prediction = self._predict_combination_benefit(
                    homo_config, target_layer_type, target_layer_type, 
                    leak_type, mutation_mode, current_accuracy, 'homogeneous'
                )
                combination_predictions['homogeneous'] = homo_prediction
            
            # 2. å¼‚ç§å±‚ç»„åˆé¢„æµ‹
            hetero_predictions = {}
            for operation in preferred_ops:
                if operation != target_layer_type:  # é¿å…é‡å¤
                    hetero_key = f"{target_layer_type}_{operation}"
                    reverse_key = f"{operation}_{target_layer_type}"
                    
                    # æŸ¥æ‰¾é…ç½®
                    hetero_config = None
                    final_key = None
                    if hetero_key in self.prior_knowledge['layer_combination_priors']['heterogeneous']:
                        hetero_config = self.prior_knowledge['layer_combination_priors']['heterogeneous'][hetero_key]
                        final_key = hetero_key
                    elif reverse_key in self.prior_knowledge['layer_combination_priors']['heterogeneous']:
                        hetero_config = self.prior_knowledge['layer_combination_priors']['heterogeneous'][reverse_key]
                        final_key = reverse_key
                    
                    if hetero_config:
                        hetero_prediction = self._predict_combination_benefit(
                            hetero_config, target_layer_type, operation,
                            leak_type, mutation_mode, current_accuracy, 'heterogeneous'
                        )
                        hetero_predictions[final_key] = hetero_prediction
            
            combination_predictions['heterogeneous'] = hetero_predictions
            
            # é€‰æ‹©æœ€ä¼˜ç»„åˆ
            best_combination = self._select_best_combination(combination_predictions)
            
            prediction_result = {
                'recommended_combination': best_combination,
                'combination_predictions': combination_predictions,
                'target_layer_type': target_layer_type,
                'mutation_mode': mutation_mode,
                'detailed_analysis': self._generate_combination_analysis(
                    best_combination, combination_predictions, leak_type
                )
            }
            
            logger.success(f"æœ€ä¼˜å±‚ç»„åˆ: {best_combination['type']} - {best_combination['combination']}")
            logger.exit_section(f"å±‚ç»„åˆé¢„æµ‹: {target_layer_type}")
            
            return prediction_result
            
        except Exception as e:
            logger.error(f"å±‚ç»„åˆé¢„æµ‹å¤±è´¥: {e}")
            logger.exit_section(f"å±‚ç»„åˆé¢„æµ‹: {target_layer_type}")
            return self._fallback_combination_prediction(target_layer_type)

    def predict_comprehensive_mutation_strategy(self,
                                               layer_analysis: Dict[str, Any],
                                               current_accuracy: float,
                                               model: nn.Module,
                                               target_layer_name: str) -> Dict[str, Any]:
        """
        ç»¼åˆé¢„æµ‹å®Œæ•´çš„å˜å¼‚ç­–ç•¥
        åŒ…æ‹¬: å˜å¼‚æ¨¡å¼ + å±‚ç±»å‹ç»„åˆ + å…·ä½“å‚æ•°
        """
        logger.enter_section(f"ç»¼åˆå˜å¼‚ç­–ç•¥é¢„æµ‹: {target_layer_name}")
        
        try:
            model_complexity = self._calculate_model_complexity(model)
            target_layer_type = self._get_layer_type(model, target_layer_name)
            
            # 1. é¢„æµ‹æœ€ä¼˜å˜å¼‚æ¨¡å¼
            mode_prediction = self.predict_optimal_mutation_mode(
                layer_analysis, current_accuracy, model_complexity
            )
            
            # 2. é¢„æµ‹æœ€ä¼˜å±‚ç»„åˆ
            combination_prediction = self.predict_optimal_layer_combinations(
                layer_analysis, target_layer_type, 
                mode_prediction['recommended_mode'], current_accuracy
            )
            
            # 3. é¢„æµ‹å…·ä½“å‚æ•°é…ç½®
            parameter_prediction = self._predict_optimal_parameters(
                layer_analysis, mode_prediction['recommended_mode'],
                combination_prediction['recommended_combination'], 
                current_accuracy, model_complexity
            )
            
            # 4. ç»¼åˆè¯„åˆ†å’Œæœ€ç»ˆæ¨è
            comprehensive_score = self._calculate_comprehensive_score(
                mode_prediction, combination_prediction, parameter_prediction
            )
            
            final_strategy = {
                'mutation_mode': mode_prediction['recommended_mode'],
                'layer_combination': combination_prediction['recommended_combination'],
                'parameters': parameter_prediction,
                'comprehensive_score': comprehensive_score,
                'expected_total_gain': (
                    mode_prediction['expected_improvement'] *
                    combination_prediction['recommended_combination']['expected_gain']
                ),
                'confidence': min(
                    mode_prediction['confidence'],
                    combination_prediction['recommended_combination']['confidence']
                ),
                'implementation_details': self._generate_implementation_details(
                    mode_prediction, combination_prediction, parameter_prediction
                )
            }
            
            logger.success(f"ç»¼åˆç­–ç•¥: {final_strategy['mutation_mode']} + "
                         f"{final_strategy['layer_combination']['combination']} "
                         f"(æ€»æ”¶ç›Š={final_strategy['expected_total_gain']:.4f})")
            logger.exit_section(f"ç»¼åˆå˜å¼‚ç­–ç•¥é¢„æµ‹: {target_layer_name}")
            
            return final_strategy
            
        except Exception as e:
            logger.error(f"ç»¼åˆé¢„æµ‹å¤±è´¥: {e}")
            logger.exit_section(f"ç»¼åˆå˜å¼‚ç­–ç•¥é¢„æµ‹: {target_layer_name}")
            return self._fallback_comprehensive_prediction(target_layer_name)

    def _get_accuracy_stage(self, accuracy: float) -> str:
        """ç¡®å®šå‡†ç¡®ç‡é˜¶æ®µ"""
        for stage, (low, high) in self.prior_knowledge['accuracy_stage_priors'].items():
            if low <= accuracy < high:
                return stage
        return 'high'

    def _calculate_complexity_fit(self, mode_name: str, model_complexity: Dict[str, float]) -> float:
        """è®¡ç®—å¤æ‚åº¦é€‚é…åº¦"""
        total_params = model_complexity.get('total_parameters', 0)
        layer_depth = model_complexity.get('layer_depth', 0)
        
        # ä¸åŒæ¨¡å¼å¯¹å¤æ‚åº¦çš„é€‚é…æ€§
        if mode_name == 'serial_division':
            # Serialé€‚åˆæ·±åº¦å¢åŠ 
            return min(1.0, (50 - layer_depth) / 50.0)  # å±‚æ•°è¶Šå°‘è¶Šé€‚åˆ
        elif mode_name == 'parallel_division':
            # Parallelé€‚åˆå®½åº¦å¢åŠ ï¼Œä½†éœ€è¦è¶³å¤Ÿçš„å‚æ•°é¢„ç®—
            return min(1.0, total_params / 1e6)  # å‚æ•°è¶Šå¤šè¶Šé€‚åˆ
        else:  # hybrid_division
            # Hybridé€‚åˆä¸­ç­‰å¤æ‚åº¦
            param_fit = 1.0 - abs(total_params / 1e6 - 0.5) * 2  # 0.5Må‚æ•°æœ€é€‚åˆ
            depth_fit = 1.0 - abs(layer_depth - 25) / 25.0  # 25å±‚æœ€é€‚åˆ
            return (param_fit + depth_fit) / 2.0

    def _calculate_base_mutation_gain(self, current_accuracy: float, leak_severity: float) -> float:
        """è®¡ç®—åŸºç¡€å˜å¼‚æ”¶ç›Š"""
        # åŸºç¡€æ”¶ç›Šä¸å‡†ç¡®ç‡è·ç¦»ä¸Šé™å’Œæ¼ç‚¹ä¸¥é‡ç¨‹åº¦æˆæ­£æ¯”
        headroom = (0.95 - current_accuracy) / 0.95
        base_gain = headroom * 0.1 * (1 + leak_severity)
        return max(0.005, base_gain)  # æœ€å°æ”¶ç›Šä¿éšœ

    def _get_mode_multiplier(self, mode_name: str, leak_type: str, accuracy_stage: str) -> float:
        """è·å–æ¨¡å¼æ”¶ç›Šå€æ•°"""
        mode_config = self.prior_knowledge['mutation_mode_priors'].get(mode_name, {})
        
        # åŸºç¡€å€æ•°
        base_multiplier = 1.0
        
        # ç“¶é¢ˆç±»å‹é€‚é…åŠ æˆ
        if leak_type in mode_config.get('best_for', []):
            base_multiplier *= 1.3
        
        # å‡†ç¡®ç‡é˜¶æ®µé€‚é…åŠ æˆ
        stage_fit = mode_config.get('accuracy_preference', {}).get(accuracy_stage, 0.5)
        base_multiplier *= stage_fit
        
        return base_multiplier

    def _calculate_mode_risk(self, mode_name: str, model_complexity: Dict[str, float], 
                           current_accuracy: float) -> float:
        """è®¡ç®—æ¨¡å¼é£é™©"""
        base_risk = {
            'serial_division': 0.3,    # ç›¸å¯¹ç¨³å®š
            'parallel_division': 0.5,  # ä¸­ç­‰é£é™©
            'hybrid_division': 0.7     # é«˜é£é™©é«˜æ”¶ç›Š
        }.get(mode_name, 0.5)
        
        # é«˜å‡†ç¡®ç‡æ—¶é£é™©å¢åŠ 
        if current_accuracy > 0.9:
            base_risk *= 1.5
        
        # é«˜å¤æ‚åº¦æ—¶é£é™©å¢åŠ 
        if model_complexity.get('total_parameters', 0) > 5e6:
            base_risk *= 1.2
        
        return base_risk

    def _predict_combination_benefit(self, config: Dict[str, float], 
                                   layer1_type: str, layer2_type: str,
                                   leak_type: str, mutation_mode: str,
                                   current_accuracy: float, combo_type: str) -> Dict[str, Any]:
        """é¢„æµ‹ç‰¹å®šå±‚ç»„åˆçš„æ”¶ç›Š"""
        
        # åŸºç¡€æ•ˆæœå’Œç¨³å®šæ€§
        effectiveness = config.get('effectiveness', 0.5)
        stability = config.get('stability', 0.5)
        
        # è·å–å±‚æ“ä½œç‰¹æ€§
        layer1_props = self.prior_knowledge['layer_operation_priors'].get(layer1_type, {})
        layer2_props = self.prior_knowledge['layer_operation_priors'].get(layer2_type, {})
        
        # è®¡ç®—ååŒæ•ˆåº”
        synergy = self._calculate_layer_synergy(layer1_props, layer2_props, leak_type)
        
        # è®¡ç®—æœŸæœ›æ”¶ç›Š
        base_gain = self._calculate_base_mutation_gain(current_accuracy, 0.5)
        expected_gain = base_gain * effectiveness * synergy
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = stability * synergy
        
        # è®¡ç®—å®æ–½æˆæœ¬
        implementation_cost = self._calculate_implementation_cost(
            layer1_type, layer2_type, mutation_mode
        )
        
        return {
            'expected_gain': float(expected_gain),
            'confidence': float(confidence),
            'effectiveness': float(effectiveness),
            'stability': float(stability),
            'synergy': float(synergy),
            'implementation_cost': float(implementation_cost),
            'combination': f"{layer1_type}+{layer2_type}",
            'type': combo_type
        }

    def _calculate_layer_synergy(self, layer1_props: Dict[str, float], 
                               layer2_props: Dict[str, float], leak_type: str) -> float:
        """è®¡ç®—å±‚é—´ååŒæ•ˆåº”"""
        
        # åŸºç¡€ååŒåˆ†æ•°
        synergy_factors = []
        
        # ç‰¹å¾æå–èƒ½åŠ›ååŒ
        feat_synergy = (layer1_props.get('feature_extraction_boost', 0.5) + 
                       layer2_props.get('feature_extraction_boost', 0.5)) / 2
        synergy_factors.append(feat_synergy)
        
        # å‚æ•°æ•ˆç‡ååŒ
        param_synergy = (layer1_props.get('parameter_efficiency', 0.5) + 
                        layer2_props.get('parameter_efficiency', 0.5)) / 2
        synergy_factors.append(param_synergy)
        
        # è®¡ç®—æˆæœ¬ååŒ
        cost_synergy = 1.0 - abs(layer1_props.get('computation_cost', 0.5) - 
                                layer2_props.get('computation_cost', 0.5))
        synergy_factors.append(cost_synergy)
        
        # ç‰¹æ®Šèƒ½åŠ›äº’è¡¥
        special_abilities = ['stability_boost', 'overfitting_prevention', 
                           'long_range_dependency', 'gradient_flow']
        complementary_bonus = 0.0
        
        for ability in special_abilities:
            if (ability in layer1_props and ability not in layer2_props) or \
               (ability not in layer1_props and ability in layer2_props):
                complementary_bonus += 0.1
        
        base_synergy = np.mean(synergy_factors)
        final_synergy = min(1.0, base_synergy + complementary_bonus)
        
        return final_synergy

    def _calculate_implementation_cost(self, layer1_type: str, layer2_type: str, 
                                     mutation_mode: str) -> float:
        """è®¡ç®—å®æ–½æˆæœ¬"""
        
        # åŸºç¡€æˆæœ¬
        layer_costs = {
            'conv2d': 0.6, 'linear': 0.4, 'batch_norm': 0.2,
            'dropout': 0.1, 'attention': 0.8, 'pool': 0.2,
            'depthwise_conv': 0.5, 'residual_connection': 0.7
        }
        
        cost1 = layer_costs.get(layer1_type, 0.5)
        cost2 = layer_costs.get(layer2_type, 0.5)
        
        # ç»„åˆæˆæœ¬
        if layer1_type == layer2_type:
            combo_cost = cost1 * 1.5  # åŒç§å±‚å¤åˆ¶æˆæœ¬è¾ƒä½
        else:
            combo_cost = cost1 + cost2  # å¼‚ç§å±‚éœ€è¦æ›´å¤šé€‚é…
        
        # æ¨¡å¼æˆæœ¬
        mode_cost_multiplier = {
            'serial_division': 1.0,
            'parallel_division': 1.3,
            'hybrid_division': 1.5
        }.get(mutation_mode, 1.0)
        
        return combo_cost * mode_cost_multiplier

    def _select_best_combination(self, combination_predictions: Dict[str, Any]) -> Dict[str, Any]:
        """é€‰æ‹©æœ€ä½³å±‚ç»„åˆ"""
        
        best_combo = None
        best_score = -1.0
        
        # è¯„ä¼°åŒç§å±‚ç»„åˆ
        if 'homogeneous' in combination_predictions:
            homo = combination_predictions['homogeneous']
            score = (homo['expected_gain'] * homo['confidence']) / (homo['implementation_cost'] + 0.1)
            if score > best_score:
                best_score = score
                best_combo = homo
        
        # è¯„ä¼°å¼‚ç§å±‚ç»„åˆ
        if 'heterogeneous' in combination_predictions:
            for combo_name, hetero in combination_predictions['heterogeneous'].items():
                score = (hetero['expected_gain'] * hetero['confidence']) / (hetero['implementation_cost'] + 0.1)
                if score > best_score:
                    best_score = score
                    best_combo = hetero
        
        return best_combo if best_combo else {'type': 'fallback', 'expected_gain': 0.01}

    def _get_layer_type(self, model: nn.Module, layer_name: str) -> str:
        """è·å–å±‚ç±»å‹"""
        try:
            module = dict(model.named_modules())[layer_name]
            if isinstance(module, nn.Conv2d):
                return 'conv2d'
            elif isinstance(module, nn.Linear):
                return 'linear'
            elif isinstance(module, nn.BatchNorm2d):
                return 'batch_norm'
            elif isinstance(module, nn.Dropout):
                return 'dropout'
            else:
                return 'unknown'
        except:
            return 'unknown'

    def _predict_optimal_parameters(self, layer_analysis: Dict[str, Any], 
                                  mutation_mode: str, best_combination: Dict[str, Any],
                                  current_accuracy: float, model_complexity: Dict[str, float]) -> Dict[str, Any]:
        """é¢„æµ‹æœ€ä¼˜å‚æ•°é…ç½®"""
        
        # åŸºäºå˜å¼‚æ¨¡å¼å’Œå±‚ç»„åˆé¢„æµ‹å‚æ•°
        params = {
            'parameter_scaling_factor': 1.5,  # é»˜è®¤å‚æ•°æ‰©å±•å› å­
            'depth_increase': 1,              # æ·±åº¦å¢åŠ 
            'width_multiplier': 1.0,          # å®½åº¦å€æ•°
            'learning_rate_adjustment': 1.0    # å­¦ä¹ ç‡è°ƒæ•´
        }
        
        # æ ¹æ®å˜å¼‚æ¨¡å¼è°ƒæ•´
        if mutation_mode == 'serial_division':
            params['depth_increase'] = 2
            params['parameter_scaling_factor'] = 1.3
        elif mutation_mode == 'parallel_division':
            params['width_multiplier'] = 2.0
            params['parameter_scaling_factor'] = 1.8
        else:  # hybrid_division
            params['depth_increase'] = 1
            params['width_multiplier'] = 1.5
            params['parameter_scaling_factor'] = 2.0
        
        # æ ¹æ®å½“å‰å‡†ç¡®ç‡è°ƒæ•´
        if current_accuracy > 0.9:
            # é«˜å‡†ç¡®ç‡æ—¶æ›´ä¿å®ˆ
            params['parameter_scaling_factor'] *= 0.8
            params['learning_rate_adjustment'] = 0.5
        
        return params

    def _calculate_comprehensive_score(self, mode_pred: Dict[str, Any], 
                                     combo_pred: Dict[str, Any], 
                                     param_pred: Dict[str, Any]) -> float:
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        
        mode_score = mode_pred['expected_improvement'] * mode_pred['confidence']
        combo_score = combo_pred['recommended_combination']['expected_gain'] * \
                     combo_pred['recommended_combination']['confidence']
        
        # å‚æ•°å¤æ‚åº¦æƒ©ç½š
        param_penalty = param_pred['parameter_scaling_factor'] * 0.1
        
        comprehensive_score = (mode_score + combo_score) / 2.0 - param_penalty
        
        return max(0.0, comprehensive_score)

    def _generate_implementation_details(self, mode_pred: Dict[str, Any], 
                                       combo_pred: Dict[str, Any], 
                                       param_pred: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆå®æ–½ç»†èŠ‚"""
        
        return {
            'mutation_sequence': self._plan_mutation_sequence(mode_pred, combo_pred),
            'parameter_adjustments': param_pred,
            'expected_timeline': self._estimate_implementation_time(mode_pred, combo_pred),
            'resource_requirements': self._estimate_resource_needs(param_pred),
            'rollback_strategy': self._plan_rollback_strategy(mode_pred, combo_pred)
        }

    def _plan_mutation_sequence(self, mode_pred: Dict[str, Any], combo_pred: Dict[str, Any]) -> List[str]:
        """è§„åˆ’å˜å¼‚åºåˆ—"""
        return [
            f"1. å‡†å¤‡{mode_pred['recommended_mode']}å˜å¼‚",
            f"2. å®æ–½{combo_pred['recommended_combination']['combination']}å±‚ç»„åˆ",
            "3. å‚æ•°åˆå§‹åŒ–å’Œå¾®è°ƒ",
            "4. æ¸è¿›å¼è®­ç»ƒéªŒè¯"
        ]

    def _estimate_implementation_time(self, mode_pred: Dict[str, Any], combo_pred: Dict[str, Any]) -> str:
        """ä¼°ç®—å®æ–½æ—¶é—´"""
        base_time = 10  # åŸºç¡€10ä¸ªepoch
        
        if mode_pred['recommended_mode'] == 'hybrid_division':
            base_time *= 1.5
        
        if combo_pred['recommended_combination']['type'] == 'heterogeneous':
            base_time *= 1.2
        
        return f"{int(base_time)} epochs"

    def _estimate_resource_needs(self, param_pred: Dict[str, Any]) -> Dict[str, float]:
        """ä¼°ç®—èµ„æºéœ€æ±‚"""
        scaling = param_pred['parameter_scaling_factor']
        
        return {
            'memory_increase': scaling * 1.2,
            'computation_increase': scaling * 1.5,
            'storage_increase': scaling * 1.1
        }

    def _plan_rollback_strategy(self, mode_pred: Dict[str, Any], combo_pred: Dict[str, Any]) -> List[str]:
        """è§„åˆ’å›æ»šç­–ç•¥"""
        return [
            "1. ä¿å­˜å˜å¼‚å‰æ¨¡å‹æ£€æŸ¥ç‚¹",
            "2. ç›‘æ§å…³é”®æ€§èƒ½æŒ‡æ ‡",
            "3. è®¾ç½®æ€§èƒ½ä¸‹é™é˜ˆå€¼ (2%)",
            "4. è‡ªåŠ¨å›æ»šæœºåˆ¶"
        ]

    def _fallback_mode_prediction(self, current_accuracy: float) -> Dict[str, Any]:
        """æ¨¡å¼é¢„æµ‹fallback"""
        return {
            'recommended_mode': 'serial_division',
            'confidence': 0.5,
            'expected_improvement': 0.01,
            'reasoning': 'Fallback to conservative serial division'
        }

    def _fallback_combination_prediction(self, target_layer_type: str) -> Dict[str, Any]:
        """å±‚ç»„åˆé¢„æµ‹fallback"""
        return {
            'recommended_combination': {
                'combination': f"{target_layer_type}+batch_norm",
                'type': 'heterogeneous',
                'expected_gain': 0.005,
                'confidence': 0.4
            }
        }

    def _fallback_comprehensive_prediction(self, target_layer_name: str) -> Dict[str, Any]:
        """ç»¼åˆé¢„æµ‹fallback"""
        return {
            'mutation_mode': 'serial_division',
            'layer_combination': {
                'combination': 'conv2d+batch_norm',
                'type': 'heterogeneous'
            },
            'expected_total_gain': 0.005,
            'confidence': 0.3
        }

    def _generate_mode_reasoning(self, best_mode: tuple, leak_type: str, accuracy_stage: str) -> str:
        """ç”Ÿæˆæ¨¡å¼é€‰æ‹©æ¨ç†"""
        mode_name, mode_data = best_mode
        
        return (f"{mode_name}æœ€é€‚åˆå½“å‰æƒ…å†µ: "
               f"ç“¶é¢ˆç±»å‹={leak_type}, å‡†ç¡®ç‡é˜¶æ®µ={accuracy_stage}, "
               f"æœŸæœ›æ”¶ç›Š={mode_data['expected_accuracy_gain']:.4f}")

    def _generate_combination_analysis(self, best_combo: Dict[str, Any], 
                                     all_predictions: Dict[str, Any], 
                                     leak_type: str) -> Dict[str, Any]:
        """ç”Ÿæˆç»„åˆåˆ†æ"""
        return {
            'selected_combination': best_combo['combination'],
            'selection_reason': f"æœ€é«˜ç»¼åˆè¯„åˆ†ï¼Œé€‚åˆ{leak_type}ç“¶é¢ˆ",
            'alternative_options': list(all_predictions.get('heterogeneous', {}).keys())[:3],
            'synergy_analysis': f"ååŒæ•ˆåº”è¯„åˆ†: {best_combo.get('synergy', 0.5):.3f}"
        }