import torch
import torch.nn as nn
from torch.autograd import Variable
import numpy as np
from typing import Optional
import gc

from .architect import Architect
from ..utils.minimal_monitor import MinimalMonitor

class SimpleArchitect(Architect):
    """
    æç®€æ¶æ„æœç´¢å™¨
    å»é™¤æ‰€æœ‰å¤æ‚çš„åŠ¨æ€è°ƒæ•´é€»è¾‘ï¼Œä¸“æ³¨äºæ ¸å¿ƒåŠŸèƒ½
    """
    
    def __init__(self, model, args, monitor: Optional[MinimalMonitor] = None):
        super().__init__(model, args)
        
        # ç›‘æ§
        self.monitor = monitor
        
        # ç®€åŒ–çš„å‚æ•°
        self.arch_update_freq = getattr(args, 'arch_update_freq', 50)
        self.warmup_epochs = getattr(args, 'warmup_epochs', 5)
        self.use_first_order = getattr(args, 'use_first_order', True)
        
        # çŠ¶æ€è·Ÿè¸ª
        self.current_epoch = 0
        self.step_count = 0
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = torch.optim.Adam(
            self.model.arch_parameters(),
            lr=args.arch_learning_rate,
            betas=(0.5, 0.999),
            weight_decay=args.arch_weight_decay
        )
        
        if self.monitor:
            self.monitor.log(f"SimpleArchitect initialized: freq={self.arch_update_freq}, warmup={self.warmup_epochs}")
    
    def set_epoch(self, epoch: int):
        """è®¾ç½®å½“å‰epoch"""
        self.current_epoch = epoch
    
    def should_update_arch(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ›´æ–°æ¶æ„"""
        # é¢„çƒ­æœŸè·³è¿‡
        if self.current_epoch < self.warmup_epochs:
            self.step_count += 1  # ä»ç„¶å¢åŠ è®¡æ•°ä½†ä¸æ›´æ–°
            return False
        
        # ç®€å•çš„é¢‘ç‡æ§åˆ¶
        self.step_count += 1
        should_update = self.step_count % self.arch_update_freq == 0
        
        if self.monitor:
            self.monitor.log(f"Step {self.step_count}: should_update_arch = {should_update}")
        
        return should_update
    
    def step(self, input_train, target_train, input_valid, target_valid, eta, network_optimizer, unrolled):
        """æ‰§è¡Œæ¶æ„æ›´æ–°æ­¥éª¤"""
        if not self.should_update_arch():
            return
        
        if self.criterion is None:
            return
        
        # ä¿å­˜æ›´æ–°å‰çš„æ¶æ„å‚æ•°ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        prev_alphas_normal = None
        prev_alphas_reduce = None
        if hasattr(self.model, 'alphas_normal'):
            prev_alphas_normal = self.model.alphas_normal.data.clone()
            prev_alphas_reduce = self.model.alphas_reduce.data.clone()
        
        # ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼šåœ¨æ¶æ„æ›´æ–°å‰æ¸…ç†ä¸å¿…è¦çš„ç¼“å­˜
        self._pre_arch_update_cleanup()
        
        # ç®€å•çš„ä¸€é˜¶æ›´æ–°
        self.optimizer.zero_grad()
        
        try:
            # ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹å‡å°‘å†…å­˜ä½¿ç”¨
            if hasattr(self.model, 'use_gradient_optimized') and self.model.use_gradient_optimized:
                # å¯ç”¨æ¢¯åº¦ä¼˜åŒ–æ¨¡å¼çš„æ¶æ„æŸå¤±è®¡ç®—
                with torch.cuda.amp.autocast():  # ä½¿ç”¨æ··åˆç²¾åº¦åŠ é€Ÿ
                    loss = self._gradient_optimized_loss(input_valid, target_valid)
            else:
                # æ ‡å‡†æ¶æ„æŸå¤±è®¡ç®—
                loss = self.criterion(self.model(input_valid), target_valid)
            
            loss.backward()
            
            # æ™ºèƒ½æ¢¯åº¦è£å‰ªï¼šæ ¹æ®æ¨¡å‹å¤§å°åŠ¨æ€è°ƒæ•´
            total_norm = self._adaptive_gradient_clip()
            
            # æ›´æ–°å‚æ•°
            self.optimizer.step()
            
            # æ£€æŸ¥æ¶æ„å˜åŒ–
            if prev_alphas_normal is not None:
                normal_change = torch.norm(self.model.alphas_normal.data - prev_alphas_normal).item()
                reduce_change = torch.norm(self.model.alphas_reduce.data - prev_alphas_reduce).item()
                
                if normal_change > 0.01 or reduce_change > 0.01:
                    print(f"    ğŸ”„ Architecture parameters updated!")
                    print(f"       Normal change: {normal_change:.4f}")
                    print(f"       Reduce change: {reduce_change:.4f}")
                    print(f"       Validation loss: {loss.item():.4f}")
                    print(f"       Gradient norm: {total_norm:.4f}")
            
            if self.monitor:
                self.monitor.count('arch_updates')
                self.monitor.log(f"Architecture updated at step {self.step_count}, loss={loss.item():.4f}")
            
            # æ¶æ„æ›´æ–°åçš„æ¸…ç†å·¥ä½œ
            self._post_arch_update_cleanup()
            
        except Exception as e:
            if self.monitor:
                self.monitor.log(f"Architecture update failed: {e}")
            print(f"    âŒ Architecture update failed: {e}")
    
    def _pre_arch_update_cleanup(self):
        """æ¶æ„æ›´æ–°å‰çš„å†…å­˜æ¸…ç†"""
        # æ¸…ç†MixedOpç¼“å­˜
        for module in self.model.modules():
            if hasattr(module, 'clear_cache'):
                module.clear_cache()
        
        # æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def _post_arch_update_cleanup(self):
        """æ¶æ„æ›´æ–°åçš„æ¸…ç†å·¥ä½œ"""
        # é‡ç½®MixedOpç»Ÿè®¡
        for module in self.model.modules():
            if hasattr(module, '_stats'):
                # é‡ç½®ç»Ÿè®¡è®¡æ•°å™¨é¿å…ç´¯ç§¯
                if hasattr(module, '_forward_count'):
                    module._forward_count = 0
    
    def _gradient_optimized_loss(self, input_valid, target_valid):
        """ä½¿ç”¨æ¢¯åº¦ä¼˜åŒ–çš„æŸå¤±è®¡ç®—"""
        # ä½¿ç”¨gradient checkpointingå‡å°‘å†…å­˜ä½¿ç”¨
        import torch.utils.checkpoint as cp
        
        def forward_wrapper(x):
            return self.model(x)
        
        # åˆ†æ‰¹å¤„ç†ä»¥å‡å°‘å†…å­˜å³°å€¼
        batch_size = input_valid.size(0)
        if batch_size > 16:
            # åˆ†æˆè¾ƒå°çš„æ‰¹æ¬¡
            losses = []
            chunk_size = 8
            for i in range(0, batch_size, chunk_size):
                end_idx = min(i + chunk_size, batch_size)
                chunk_input = input_valid[i:end_idx]
                chunk_target = target_valid[i:end_idx]
                
                chunk_output = cp.checkpoint(forward_wrapper, chunk_input, use_reentrant=False)
                chunk_loss = self.criterion(chunk_output, chunk_target)
                losses.append(chunk_loss * chunk_input.size(0))
            
            # åŠ æƒå¹³å‡
            total_loss = sum(losses) / batch_size
            return total_loss
        else:
            # å°æ‰¹æ¬¡ç›´æ¥å¤„ç†
            output = cp.checkpoint(forward_wrapper, input_valid, use_reentrant=False)
            return self.criterion(output, target_valid)
    
    def _adaptive_gradient_clip(self) -> float:
        """è‡ªé€‚åº”æ¢¯åº¦è£å‰ª"""
        total_norm = 0.0
        param_count = 0
        
        for p in self.model.arch_parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
                param_count += p.numel()
        
        total_norm = total_norm ** 0.5
        
        # æ ¹æ®å‚æ•°æ•°é‡åŠ¨æ€è°ƒæ•´è£å‰ªé˜ˆå€¼
        adaptive_clip = max(1.0, 5.0 * (param_count / 10000) ** 0.5)
        
        if total_norm > adaptive_clip:
            clip_coef = adaptive_clip / (total_norm + 1e-6)
            for p in self.model.arch_parameters():
                if p.grad is not None:
                    p.grad.data.mul_(clip_coef)
        
        return total_norm
    
    def cleanup_gradients(self):
        """æ¸…ç†æ¢¯åº¦ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            self.model.zero_grad()
            for param in self.model.arch_parameters():
                if param.grad is not None:
                    param.grad.data.zero_()
        except Exception as e:
            if self.monitor:
                self.monitor.log(f"Gradient cleanup error: {e}")
        
        # é¿å…é¢‘ç¹è°ƒç”¨ empty_cacheï¼Œå¯èƒ½å¯¼è‡´æ­»é”
        # gc.collect()
        # if torch.cuda.is_available():
        #     torch.cuda.empty_cache()
    
    def get_stats(self):
        """è·å–ç®€å•çš„ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'step_count': self.step_count,
            'current_epoch': self.current_epoch,
            'arch_update_freq': self.arch_update_freq
        } 