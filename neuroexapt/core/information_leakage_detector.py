"""
ä¿¡æ¯æ³„æ¼æ£€æµ‹å™¨
ç²¾å‡†è¯†åˆ«ç½‘ç»œä¸­ä¿¡æ¯ä¸¢å¤±å’Œç‰¹å¾æ³„æ¼çš„å…³é”®å±‚
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Any, Tuple, Optional
import logging
from collections import defaultdict
import torch.nn.functional as F

logger = logging.getLogger(__name__)


class InformationLeakageDetector:
    """
    ä¿¡æ¯æ³„æ¼æ£€æµ‹å™¨
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    1. é€šè¿‡ä¿¡æ¯è®ºæŒ‡æ ‡æ£€æµ‹ä¿¡æ¯ä¸¢å¤±
    2. åˆ†æç‰¹å¾è¡¨ç¤ºçš„è´¨é‡å’Œå¤šæ ·æ€§
    3. è¯†åˆ«æ¢¯åº¦æµé˜»å¡å’Œä¿¡æ¯ç“¶é¢ˆ
    4. å®šä½çœŸæ­£éœ€è¦å˜å¼‚çš„å…³é”®å±‚
    """
    
    def __init__(self):
        self.activation_cache = {}
        self.gradient_cache = {}
        self.information_metrics = {}
        
    def detect_information_leakage(self, 
                                  model: nn.Module,
                                  activations: Dict[str, torch.Tensor],
                                  gradients: Dict[str, torch.Tensor],
                                  targets: Optional[torch.Tensor] = None) -> Dict[str, Any]:
        """
        æ£€æµ‹ä¿¡æ¯æ³„æ¼å’Œç‰¹å¾ä¸¢å¤±
        
        Returns:
            DictåŒ…å«æ³„æ¼ç‚¹åˆ†æå’Œä¿®å¤å»ºè®®
        """
        
        logger.info("ğŸ” å¼€å§‹ä¿¡æ¯æ³„æ¼æ£€æµ‹åˆ†æ...")
        
        # 1. ä¿¡æ¯ç†µåˆ†æ
        entropy_analysis = self._analyze_information_entropy(activations)
        
        # 2. ç‰¹å¾å¤šæ ·æ€§åˆ†æ  
        diversity_analysis = self._analyze_feature_diversity(activations)
        
        # 3. æ¢¯åº¦æµåˆ†æ
        gradient_flow_analysis = self._analyze_gradient_flow(gradients)
        
        # 4. å±‚é—´ä¿¡æ¯ä¼ é€’åˆ†æ
        information_flow_analysis = self._analyze_information_flow(activations)
        
        # 5. è¡¨ç¤ºè´¨é‡åˆ†æ
        representation_quality = self._analyze_representation_quality(activations, targets)
        
        # 6. ç»¼åˆåˆ†ææ‰¾å‡ºçœŸæ­£çš„æ³„æ¼ç‚¹
        leakage_points = self._identify_critical_leakage_points(
            entropy_analysis, diversity_analysis, gradient_flow_analysis, 
            information_flow_analysis, representation_quality
        )
        
        # 7. ç”Ÿæˆç²¾å‡†çš„ä¿®å¤å»ºè®®
        repair_suggestions = self._generate_targeted_repair_suggestions(leakage_points)
        
        return {
            'leakage_points': leakage_points,
            'repair_suggestions': repair_suggestions,
            'detailed_analysis': {
                'entropy_analysis': entropy_analysis,
                'diversity_analysis': diversity_analysis,
                'gradient_flow_analysis': gradient_flow_analysis,
                'information_flow_analysis': information_flow_analysis,
                'representation_quality': representation_quality
            },
            'summary': self._generate_leakage_summary(leakage_points)
        }
    
    def _analyze_information_entropy(self, activations: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """åˆ†æå„å±‚çš„ä¿¡æ¯ç†µ"""
        
        entropy_metrics = {}
        
        for layer_name, activation in activations.items():
            if activation is None or activation.numel() == 0:
                continue
                
            try:
                # å°†æ¿€æ´»å€¼è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
                if len(activation.shape) > 2:
                    # å¯¹äºå·ç§¯å±‚ï¼Œè®¡ç®—ç©ºé—´ç»´åº¦çš„å¹³å‡
                    flat_activation = activation.flatten(2).mean(dim=2)  # [batch, channels]
                else:
                    flat_activation = activation
                
                # è®¡ç®—æ¯ä¸ªé€šé“çš„ç†µ
                channel_entropies = []
                for channel_idx in range(flat_activation.shape[1]):
                    channel_data = flat_activation[:, channel_idx]
                    
                    # å°†æ•°æ®ç¦»æ•£åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ
                    hist, _ = torch.histogram(channel_data, bins=50, density=True)
                    hist = hist + 1e-8  # é¿å…é›¶æ¦‚ç‡
                    hist = hist / hist.sum()
                    
                    # è®¡ç®—ç†µ
                    entropy = -torch.sum(hist * torch.log2(hist + 1e-8))
                    channel_entropies.append(entropy.item())
                
                avg_entropy = np.mean(channel_entropies)
                entropy_std = np.std(channel_entropies)
                
                # æ£€æµ‹ä¿¡æ¯ä¸¢å¤±
                info_loss_score = max(0, 1 - avg_entropy / 4.0)  # 4bitä½œä¸ºå‚è€ƒ
                
                entropy_metrics[layer_name] = {
                    'average_entropy': avg_entropy,
                    'entropy_std': entropy_std,
                    'channel_entropies': channel_entropies,
                    'information_loss_score': info_loss_score,
                    'is_information_bottleneck': info_loss_score > 0.6
                }
                
            except Exception as e:
                logger.warning(f"å±‚ {layer_name} ç†µè®¡ç®—å¤±è´¥: {e}")
                continue
        
        return entropy_metrics
    
    def _analyze_feature_diversity(self, activations: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """åˆ†æç‰¹å¾å¤šæ ·æ€§"""
        
        diversity_metrics = {}
        
        for layer_name, activation in activations.items():
            if activation is None or activation.numel() == 0:
                continue
                
            try:
                if len(activation.shape) > 2:
                    # å¯¹äºå·ç§¯å±‚
                    flat_activation = activation.flatten(2).mean(dim=2)
                else:
                    flat_activation = activation
                
                # è®¡ç®—ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ
                if flat_activation.shape[1] > 1:
                    correlation_matrix = torch.corrcoef(flat_activation.T)
                    
                    # è®¡ç®—å¹³å‡ç›¸å…³æ€§ï¼ˆæ’é™¤å¯¹è§’çº¿ï¼‰
                    mask = ~torch.eye(correlation_matrix.shape[0], dtype=bool)
                    avg_correlation = torch.abs(correlation_matrix[mask]).mean().item()
                    
                    # è®¡ç®—ç‰¹å¾å†—ä½™åº¦
                    redundancy_score = min(1.0, avg_correlation * 2)
                    
                    # è®¡ç®—æœ‰æ•ˆç§©ï¼ˆç‰¹å¾ç‹¬ç«‹æ€§æŒ‡æ ‡ï¼‰
                    try:
                        _, s, _ = torch.svd(flat_activation.T)
                        normalized_s = s / s.max()
                        effective_rank = (normalized_s > 0.01).sum().item() / s.shape[0]
                    except:
                        effective_rank = 1.0
                    
                    diversity_metrics[layer_name] = {
                        'average_correlation': avg_correlation,
                        'redundancy_score': redundancy_score,
                        'effective_rank': effective_rank,
                        'feature_collapse': redundancy_score > 0.7,
                        'diversity_loss': 1 - effective_rank
                    }
                    
            except Exception as e:
                logger.warning(f"å±‚ {layer_name} å¤šæ ·æ€§åˆ†æå¤±è´¥: {e}")
                continue
        
        return diversity_metrics
    
    def _analyze_gradient_flow(self, gradients: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """åˆ†ææ¢¯åº¦æµé˜»å¡"""
        
        gradient_metrics = {}
        
        for layer_name, gradient in gradients.items():
            if gradient is None or gradient.numel() == 0:
                continue
                
            try:
                # è®¡ç®—æ¢¯åº¦èŒƒæ•°
                grad_norm = torch.norm(gradient).item()
                
                # è®¡ç®—æ¢¯åº¦æ–¹å·®ï¼ˆè¡¡é‡æ¢¯åº¦å¤šæ ·æ€§ï¼‰
                grad_var = torch.var(gradient).item()
                
                # æ£€æµ‹æ¢¯åº¦æ¶ˆå¤±
                vanishing_threshold = 1e-6
                vanishing_score = max(0, 1 - grad_norm / 0.1)  # 0.1ä½œä¸ºå¥åº·æ¢¯åº¦é˜ˆå€¼
                
                # æ£€æµ‹æ¢¯åº¦çˆ†ç‚¸
                exploding_threshold = 10.0
                exploding_score = max(0, (grad_norm - exploding_threshold) / exploding_threshold)
                
                # æ¢¯åº¦å¥åº·åº¦
                health_score = 1 - max(vanishing_score, exploding_score)
                
                gradient_metrics[layer_name] = {
                    'gradient_norm': grad_norm,
                    'gradient_variance': grad_var,
                    'vanishing_score': vanishing_score,
                    'exploding_score': exploding_score,
                    'health_score': health_score,
                    'is_gradient_blocked': vanishing_score > 0.8 or exploding_score > 0.5
                }
                
            except Exception as e:
                logger.warning(f"å±‚ {layer_name} æ¢¯åº¦åˆ†æå¤±è´¥: {e}")
                continue
        
        return gradient_metrics
    
    def _analyze_information_flow(self, activations: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """åˆ†æå±‚é—´ä¿¡æ¯ä¼ é€’æ•ˆç‡"""
        
        layer_names = list(activations.keys())
        flow_metrics = {}
        
        for i in range(len(layer_names) - 1):
            current_layer = layer_names[i]
            next_layer = layer_names[i + 1]
            
            current_activation = activations[current_layer]
            next_activation = activations[next_layer]
            
            if current_activation is None or next_activation is None:
                continue
                
            try:
                # è®¡ç®—ä¿¡æ¯ä¼ é€’æ•ˆç‡
                current_info = self._compute_information_content(current_activation)
                next_info = self._compute_information_content(next_activation)
                
                # ä¿¡æ¯ä¿æŒç‡
                information_retention = next_info / max(current_info, 1e-8)
                
                # ä¿¡æ¯ä¸¢å¤±ç‡
                information_loss = max(0, 1 - information_retention)
                
                flow_metrics[f"{current_layer}->{next_layer}"] = {
                    'current_info': current_info,
                    'next_info': next_info,
                    'retention_rate': information_retention,
                    'loss_rate': information_loss,
                    'is_bottleneck': information_loss > 0.3
                }
                
            except Exception as e:
                logger.warning(f"ä¿¡æ¯æµåˆ†æå¤±è´¥ {current_layer}->{next_layer}: {e}")
                continue
        
        return flow_metrics
    
    def _compute_information_content(self, activation: torch.Tensor) -> float:
        """è®¡ç®—æ¿€æ´»çš„ä¿¡æ¯å«é‡"""
        
        try:
            if len(activation.shape) > 2:
                flat_activation = activation.flatten(2).mean(dim=2)
            else:
                flat_activation = activation
            
            # ä½¿ç”¨ç‰¹å¾çš„æ–¹å·®ä½œä¸ºä¿¡æ¯å«é‡çš„ä»£ç†
            feature_vars = torch.var(flat_activation, dim=0)
            mean_var = torch.mean(feature_vars).item()
            
            return mean_var
            
        except:
            return 0.0
    
    def _analyze_representation_quality(self, 
                                      activations: Dict[str, torch.Tensor],
                                      targets: Optional[torch.Tensor] = None) -> Dict[str, Any]:
        """åˆ†æè¡¨ç¤ºè´¨é‡"""
        
        quality_metrics = {}
        
        for layer_name, activation in activations.items():
            if activation is None or activation.numel() == 0:
                continue
                
            try:
                if len(activation.shape) > 2:
                    flat_activation = activation.flatten(2).mean(dim=2)
                else:
                    flat_activation = activation
                
                # 1. æ¿€æ´»åˆ†å¸ƒå¥åº·åº¦
                activation_mean = torch.mean(flat_activation, dim=0)
                activation_std = torch.std(flat_activation, dim=0)
                
                # æ£€æµ‹æ­»ç¥ç»å…ƒ
                dead_neurons = (activation_std < 1e-6).sum().item()
                dead_ratio = dead_neurons / activation_std.shape[0]
                
                # æ£€æµ‹é¥±å’Œç¥ç»å…ƒ
                saturated_neurons = (torch.abs(activation_mean) > 2.0).sum().item()
                saturated_ratio = saturated_neurons / activation_mean.shape[0]
                
                # 2. è¡¨ç¤ºåˆ†ç¦»åº¦
                if targets is not None and flat_activation.shape[0] > 1:
                    try:
                        # è®¡ç®—ä¸åŒç±»åˆ«é—´çš„è¡¨ç¤ºè·ç¦»
                        unique_targets = torch.unique(targets)
                        if len(unique_targets) > 1:
                            inter_class_distance = self._compute_inter_class_distance(
                                flat_activation, targets, unique_targets
                            )
                        else:
                            inter_class_distance = 0.0
                    except:
                        inter_class_distance = 0.0
                else:
                    inter_class_distance = 0.0
                
                # ç»¼åˆè´¨é‡è¯„åˆ†
                quality_score = (
                    (1 - dead_ratio) * 0.4 +
                    (1 - saturated_ratio) * 0.3 +
                    min(1.0, inter_class_distance) * 0.3
                )
                
                quality_metrics[layer_name] = {
                    'dead_neuron_ratio': dead_ratio,
                    'saturated_neuron_ratio': saturated_ratio,
                    'inter_class_distance': inter_class_distance,
                    'quality_score': quality_score,
                    'needs_repair': quality_score < 0.6
                }
                
            except Exception as e:
                logger.warning(f"å±‚ {layer_name} è´¨é‡åˆ†æå¤±è´¥: {e}")
                continue
        
        return quality_metrics
    
    def _compute_inter_class_distance(self, 
                                    activation: torch.Tensor,
                                    targets: torch.Tensor,
                                    unique_targets: torch.Tensor) -> float:
        """è®¡ç®—ç±»åˆ«é—´è¡¨ç¤ºè·ç¦»"""
        
        try:
            class_centers = []
            for target in unique_targets:
                mask = targets == target
                if mask.sum() > 0:
                    center = activation[mask].mean(dim=0)
                    class_centers.append(center)
            
            if len(class_centers) < 2:
                return 0.0
            
            # è®¡ç®—ç±»åˆ«ä¸­å¿ƒé—´çš„å¹³å‡è·ç¦»
            distances = []
            for i in range(len(class_centers)):
                for j in range(i + 1, len(class_centers)):
                    dist = torch.norm(class_centers[i] - class_centers[j]).item()
                    distances.append(dist)
            
            return np.mean(distances) if distances else 0.0
            
        except:
            return 0.0
    
    def _identify_critical_leakage_points(self, 
                                        entropy_analysis: Dict,
                                        diversity_analysis: Dict,
                                        gradient_flow_analysis: Dict,
                                        information_flow_analysis: Dict,
                                        representation_quality: Dict) -> List[Dict[str, Any]]:
        """è¯†åˆ«å…³é”®çš„ä¿¡æ¯æ³„æ¼ç‚¹"""
        
        leakage_points = []
        
        # æ”¶é›†æ‰€æœ‰å±‚çš„é—®é¢˜
        layer_problems = defaultdict(list)
        
        # 1. ä»ç†µåˆ†æä¸­è¯†åˆ«ä¿¡æ¯ç“¶é¢ˆ
        for layer_name, metrics in entropy_analysis.items():
            if metrics.get('is_information_bottleneck', False):
                layer_problems[layer_name].append({
                    'type': 'information_bottleneck',
                    'severity': metrics['information_loss_score'],
                    'description': f"ä¿¡æ¯ç†µè¿‡ä½ ({metrics['average_entropy']:.2f})"
                })
        
        # 2. ä»å¤šæ ·æ€§åˆ†æä¸­è¯†åˆ«ç‰¹å¾åå¡Œ
        for layer_name, metrics in diversity_analysis.items():
            if metrics.get('feature_collapse', False):
                layer_problems[layer_name].append({
                    'type': 'feature_collapse',
                    'severity': metrics['redundancy_score'],
                    'description': f"ç‰¹å¾å†—ä½™åº¦è¿‡é«˜ ({metrics['redundancy_score']:.2f})"
                })
        
        # 3. ä»æ¢¯åº¦åˆ†æä¸­è¯†åˆ«æ¢¯åº¦é˜»å¡
        for layer_name, metrics in gradient_flow_analysis.items():
            if metrics.get('is_gradient_blocked', False):
                layer_problems[layer_name].append({
                    'type': 'gradient_blocked',
                    'severity': 1 - metrics['health_score'],
                    'description': f"æ¢¯åº¦æµå¼‚å¸¸ (å¥åº·åº¦: {metrics['health_score']:.2f})"
                })
        
        # 4. ä»è¡¨ç¤ºè´¨é‡ä¸­è¯†åˆ«è¡¨ç¤ºé€€åŒ–
        for layer_name, metrics in representation_quality.items():
            if metrics.get('needs_repair', False):
                layer_problems[layer_name].append({
                    'type': 'representation_degradation',
                    'severity': 1 - metrics['quality_score'],
                    'description': f"è¡¨ç¤ºè´¨é‡ä½ä¸‹ (è¯„åˆ†: {metrics['quality_score']:.2f})"
                })
        
        # 5. ç»¼åˆè¯„ä¼°å¹¶æ’åº
        for layer_name, problems in layer_problems.items():
            if problems:
                # è®¡ç®—ç»¼åˆä¸¥é‡æ€§
                total_severity = sum(p['severity'] for p in problems)
                avg_severity = total_severity / len(problems)
                
                leakage_point = {
                    'layer_name': layer_name,
                    'problems': problems,
                    'total_severity': total_severity,
                    'average_severity': avg_severity,
                    'problem_count': len(problems),
                    'priority': self._compute_repair_priority(layer_name, problems)
                }
                
                leakage_points.append(leakage_point)
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        leakage_points.sort(key=lambda x: x['priority'], reverse=True)
        
        return leakage_points
    
    def _compute_repair_priority(self, layer_name: str, problems: List[Dict]) -> float:
        """è®¡ç®—ä¿®å¤ä¼˜å…ˆçº§"""
        
        # åŸºç¡€ä¸¥é‡æ€§
        severity_score = sum(p['severity'] for p in problems)
        
        # é—®é¢˜ç±»å‹æƒé‡
        type_weights = {
            'information_bottleneck': 1.0,
            'feature_collapse': 0.9,
            'gradient_blocked': 0.8,
            'representation_degradation': 0.7
        }
        
        weighted_severity = sum(
            p['severity'] * type_weights.get(p['type'], 0.5) 
            for p in problems
        )
        
        # å±‚ä½ç½®æƒé‡ï¼ˆä¸­é—´å±‚æ›´å…³é”®ï¼‰
        if 'classifier' in layer_name:
            position_weight = 1.2  # åˆ†ç±»å™¨å±‚å¾ˆé‡è¦
        elif any(x in layer_name for x in ['feature', 'conv', 'block']):
            position_weight = 1.0  # ç‰¹å¾å±‚æ­£å¸¸æƒé‡
        else:
            position_weight = 0.8  # å…¶ä»–å±‚
        
        priority = weighted_severity * position_weight
        
        return priority
    
    def _generate_targeted_repair_suggestions(self, leakage_points: List[Dict]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆé’ˆå¯¹æ€§çš„ä¿®å¤å»ºè®®"""
        
        suggestions = []
        
        for point in leakage_points:
            layer_name = point['layer_name']
            problems = point['problems']
            
            # æ ¹æ®é—®é¢˜ç±»å‹ç”Ÿæˆå…·ä½“å»ºè®®
            repair_actions = []
            
            for problem in problems:
                if problem['type'] == 'information_bottleneck':
                    repair_actions.extend([
                        'width_expansion',  # å¢åŠ é€šé“æ•°
                        'residual_connection',  # æ·»åŠ è·³è·ƒè¿æ¥
                        'attention_enhancement'  # å¢å¼ºä¿¡æ¯æµ
                    ])
                elif problem['type'] == 'feature_collapse':
                    repair_actions.extend([
                        'depth_expansion',  # å¢åŠ å±‚æ·±åº¦
                        'parallel_division',  # å¹¶è¡Œåˆ†æ”¯
                        'batch_norm_insertion'  # è§„èŒƒåŒ–
                    ])
                elif problem['type'] == 'gradient_blocked':
                    repair_actions.extend([
                        'residual_connection',  # æ”¹å–„æ¢¯åº¦æµ
                        'batch_norm_insertion',  # ç¨³å®šè®­ç»ƒ
                        'layer_norm'  # å±‚å½’ä¸€åŒ–
                    ])
                elif problem['type'] == 'representation_degradation':
                    repair_actions.extend([
                        'width_expansion',  # å¢å¼ºè¡¨ç¤ºèƒ½åŠ›
                        'attention_enhancement',  # æ³¨æ„åŠ›æœºåˆ¶
                        'information_enhancement'  # ä¿¡æ¯å¢å¼º
                    ])
            
            # å»é‡å¹¶æ’åº
            unique_actions = list(set(repair_actions))
            
            suggestion = {
                'layer_name': layer_name,
                'priority': point['priority'],
                'recommended_actions': unique_actions,
                'primary_action': unique_actions[0] if unique_actions else 'width_expansion',
                'rationale': f"æ£€æµ‹åˆ°{len(problems)}ä¸ªé—®é¢˜: " + 
                           ', '.join(p['description'] for p in problems),
                'expected_improvement': min(1.0, point['total_severity'] * 0.5)
            }
            
            suggestions.append(suggestion)
        
        return suggestions
    
    def _generate_leakage_summary(self, leakage_points: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆæ³„æ¼åˆ†ææ‘˜è¦"""
        
        if not leakage_points:
            return {
                'total_leakage_points': 0,
                'average_severity': 0.0,
                'most_critical_layer': None,
                'summary': "æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„ä¿¡æ¯æ³„æ¼é—®é¢˜"
            }
        
        total_points = len(leakage_points)
        avg_severity = np.mean([p['average_severity'] for p in leakage_points])
        most_critical = leakage_points[0] if leakage_points else None
        
        # é—®é¢˜ç±»å‹ç»Ÿè®¡
        problem_types = defaultdict(int)
        for point in leakage_points:
            for problem in point['problems']:
                problem_types[problem['type']] += 1
        
        summary_text = f"æ£€æµ‹åˆ°{total_points}ä¸ªæ³„æ¼ç‚¹ï¼Œå¹³å‡ä¸¥é‡æ€§{avg_severity:.2f}ã€‚"
        if most_critical:
            summary_text += f"æœ€å…³é”®å±‚: {most_critical['layer_name']}"
        
        return {
            'total_leakage_points': total_points,
            'average_severity': avg_severity,
            'most_critical_layer': most_critical['layer_name'] if most_critical else None,
            'problem_type_distribution': dict(problem_types),
            'summary': summary_text
        }