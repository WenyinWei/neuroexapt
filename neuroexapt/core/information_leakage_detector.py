"""
ä¿¡æ¯æ³„æ¼æ£€æµ‹å™¨ - ç®€åŒ–ç‰ˆæœ¬
ä½¿ç”¨ä¸“æ³¨çš„åˆ†æå™¨å­æ¨¡å—ï¼Œå‡å°‘å¤æ‚æ€§
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Any, Optional
import logging
from .leakage_analyzers import (
    EntropyAnalyzer, DiversityAnalyzer, GradientFlowAnalyzer,
    InformationFlowAnalyzer, RepresentationQualityAnalyzer, LeakagePointIdentifier
)

logger = logging.getLogger(__name__)


class InformationLeakageDetector:
    """
    ä¿¡æ¯æ³„æ¼æ£€æµ‹å™¨ - é‡æ„ç‰ˆæœ¬
    
    ä½¿ç”¨ä¸“æ³¨çš„åˆ†æå™¨å­æ¨¡å—ï¼Œé™ä½ç±»çš„å¤æ‚æ€§
    """
    
    def __init__(self):
        self.activation_cache = {}
        self.gradient_cache = {}
        self.information_metrics = {}
        
        # åˆå§‹åŒ–ä¸“æ³¨çš„åˆ†æå™¨ç»„ä»¶
        self.entropy_analyzer = EntropyAnalyzer()
        self.diversity_analyzer = DiversityAnalyzer()
        self.gradient_analyzer = GradientFlowAnalyzer()
        self.flow_analyzer = InformationFlowAnalyzer()
        self.quality_analyzer = RepresentationQualityAnalyzer()
        self.leakage_identifier = LeakagePointIdentifier()
        
    def detect_information_leakage(self, 
                                  model: nn.Module,
                                  activations: Dict[str, torch.Tensor],
                                  gradients: Dict[str, torch.Tensor],
                                  targets: Optional[torch.Tensor] = None) -> Dict[str, Any]:
        """
        æ£€æµ‹ä¿¡æ¯æ³„æ¼å’Œç‰¹å¾ä¸¢å¤±
        
        Returns:
            DictåŒ…å«æ³„æ¼ç‚¹åˆ†æå’Œä¿®å¤å»ºè®®
        """
        
        logger.info("ğŸ” å¼€å§‹ä¿¡æ¯æ³„æ¼æ£€æµ‹åˆ†æ...")
        
        # 1. ä¿¡æ¯ç†µåˆ†æ - ä½¿ç”¨ä¸“æ³¨çš„åˆ†æå™¨
        entropy_analysis = self.entropy_analyzer.analyze_information_entropy(activations)
        
        # 2. ç‰¹å¾å¤šæ ·æ€§åˆ†æ - ä½¿ç”¨ä¸“æ³¨çš„åˆ†æå™¨
        diversity_analysis = self.diversity_analyzer.analyze_feature_diversity(activations)
        
        # 3. æ¢¯åº¦æµåˆ†æ - ä½¿ç”¨ä¸“æ³¨çš„åˆ†æå™¨
        gradient_flow_analysis = self.gradient_analyzer.analyze_gradient_flow(gradients)
        
        # 4. å±‚é—´ä¿¡æ¯ä¼ é€’åˆ†æ - ä½¿ç”¨ä¸“æ³¨çš„åˆ†æå™¨
        information_flow_analysis = self.flow_analyzer.analyze_information_flow(activations)
        
        # 5. è¡¨ç¤ºè´¨é‡åˆ†æ - ä½¿ç”¨ä¸“æ³¨çš„åˆ†æå™¨
        representation_quality = self.quality_analyzer.analyze_representation_quality(activations, targets)
        
        # 6. ç»¼åˆåˆ†ææ‰¾å‡ºçœŸæ­£çš„æ³„æ¼ç‚¹ - ä½¿ç”¨ä¸“æ³¨çš„è¯†åˆ«å™¨
        leakage_points = self.leakage_identifier.identify_critical_leakage_points(
            entropy_analysis, diversity_analysis, gradient_flow_analysis, 
            information_flow_analysis, representation_quality
        )
        
        # 7. ç”Ÿæˆç²¾å‡†çš„ä¿®å¤å»ºè®®
        repair_suggestions = self._generate_targeted_repair_suggestions(leakage_points)
        
        return {
            'leakage_points': leakage_points,
            'repair_suggestions': repair_suggestions,
            'detailed_analysis': {
                'entropy_analysis': entropy_analysis,
                'diversity_analysis': diversity_analysis,
                'gradient_flow_analysis': gradient_flow_analysis,
                'information_flow_analysis': information_flow_analysis,
                'representation_quality': representation_quality
            },
            'summary': self._generate_leakage_summary(leakage_points)
        }
    
    def _generate_targeted_repair_suggestions(self, leakage_points: List[Dict]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆé’ˆå¯¹æ€§çš„ä¿®å¤å»ºè®®"""
        
        suggestions = []
        
        for point in leakage_points:
            layer_name = point['layer_name']
            problems = point['problems']
            
            # æ ¹æ®é—®é¢˜ç±»å‹ç”Ÿæˆå…·ä½“å»ºè®®
            repair_actions = []
            
            for problem in problems:
                if problem['type'] == 'information_bottleneck':
                    repair_actions.extend([
                        'width_expansion',  # å¢åŠ é€šé“æ•°
                        'residual_connection',  # æ·»åŠ è·³è·ƒè¿æ¥
                        'attention_enhancement'  # å¢å¼ºä¿¡æ¯æµ
                    ])
                elif problem['type'] == 'feature_collapse':
                    repair_actions.extend([
                        'depth_expansion',  # å¢åŠ å±‚æ·±åº¦
                        'parallel_division',  # å¹¶è¡Œåˆ†æ”¯
                        'batch_norm_insertion'  # è§„èŒƒåŒ–
                    ])
                elif problem['type'] == 'gradient_blocked':
                    repair_actions.extend([
                        'residual_connection',  # æ”¹å–„æ¢¯åº¦æµ
                        'batch_norm_insertion',  # ç¨³å®šè®­ç»ƒ
                        'layer_norm'  # å±‚å½’ä¸€åŒ–
                    ])
                elif problem['type'] == 'representation_degradation':
                    repair_actions.extend([
                        'width_expansion',  # å¢å¼ºè¡¨ç¤ºèƒ½åŠ›
                        'attention_enhancement',  # æ³¨æ„åŠ›æœºåˆ¶
                        'information_enhancement'  # ä¿¡æ¯å¢å¼º
                    ])
            
            # å»é‡å¹¶æ’åº
            unique_actions = list(set(repair_actions))
            
            suggestion = {
                'layer_name': layer_name,
                'priority': point['priority'],
                'recommended_actions': unique_actions,
                'primary_action': unique_actions[0] if unique_actions else 'width_expansion',
                'rationale': f"æ£€æµ‹åˆ°{len(problems)}ä¸ªé—®é¢˜: " + 
                           ', '.join(p['description'] for p in problems),
                'expected_improvement': min(1.0, point['total_severity'] * 0.5)
            }
            
            suggestions.append(suggestion)
        
        return suggestions
    
    def _generate_leakage_summary(self, leakage_points: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆæ³„æ¼åˆ†ææ‘˜è¦"""
        
        if not leakage_points:
            return {
                'total_leakage_points': 0,
                'average_severity': 0.0,
                'most_critical_layer': None,
                'summary': "æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„ä¿¡æ¯æ³„æ¼é—®é¢˜"
            }
        
        total_points = len(leakage_points)
        avg_severity = np.mean([p['average_severity'] for p in leakage_points])
        most_critical = leakage_points[0] if leakage_points else None
        
        # é—®é¢˜ç±»å‹ç»Ÿè®¡
        from collections import defaultdict
        problem_types = defaultdict(int)
        for point in leakage_points:
            for problem in point['problems']:
                problem_types[problem['type']] += 1
        
        summary_text = f"æ£€æµ‹åˆ°{total_points}ä¸ªæ³„æ¼ç‚¹ï¼Œå¹³å‡ä¸¥é‡æ€§{avg_severity:.2f}ã€‚"
        if most_critical:
            summary_text += f"æœ€å…³é”®å±‚: {most_critical['layer_name']}"
        
        return {
            'total_leakage_points': total_points,
            'average_severity': avg_severity,
            'most_critical_layer': most_critical['layer_name'] if most_critical else None,
            'problem_type_distribution': dict(problem_types),
            'summary': summary_text
        }