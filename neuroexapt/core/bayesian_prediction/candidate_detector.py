"""
è´å¶æ–¯å€™é€‰ç‚¹æ£€æµ‹å™¨

ä¸“é—¨è´Ÿè´£å‘ç°å’Œè¯„ä¼°å˜å¼‚å€™é€‰ç‚¹
"""

import numpy as np
from typing import Dict, List, Any, Optional, Tuple
import logging

logger = logging.getLogger(__name__)


class BayesianCandidateDetector:
    """è´å¶æ–¯å€™é€‰ç‚¹æ£€æµ‹å™¨"""
    
    def __init__(self, config=None):
        from .bayesian_config import BayesianConfig
        self.config = config if config else BayesianConfig()
        
    def detect_candidates(self, features: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ£€æµ‹å˜å¼‚å€™é€‰ç‚¹"""
        
        candidates = []
        
        # 1. åŸºäºæ¿€æ´»çš„å€™é€‰ç‚¹æ£€æµ‹
        activation_candidates = self._detect_activation_candidates(features)
        candidates.extend(activation_candidates)
        
        # 2. åŸºäºæ¢¯åº¦çš„å€™é€‰ç‚¹æ£€æµ‹
        gradient_candidates = self._detect_gradient_candidates(features)
        candidates.extend(gradient_candidates)
        
        # 3. åŸºäºæ¶æ„çš„å€™é€‰ç‚¹æ£€æµ‹
        architecture_candidates = self._detect_architecture_candidates(features)
        candidates.extend(architecture_candidates)
        
        # 4. åŸºäºæ€§èƒ½çš„å€™é€‰ç‚¹æ£€æµ‹
        performance_candidates = self._detect_performance_candidates(features)
        candidates.extend(performance_candidates)
        
        # 5. å»é‡å’Œä¼˜å…ˆçº§æ’åº
        unique_candidates = self._deduplicate_and_prioritize(candidates)
        
        logger.info(f"ğŸ” å€™é€‰ç‚¹æ£€æµ‹å®Œæˆ: å‘ç°{len(unique_candidates)}ä¸ªå€™é€‰ç‚¹")
        
        return unique_candidates
    
    def _detect_activation_candidates(self, features: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åŸºäºæ¿€æ´»çš„å€™é€‰ç‚¹æ£€æµ‹"""
        
        activation_features = features.get('activation_features', {})
        if not activation_features.get('available'):
            return []
        
        candidates = []
        layer_features = activation_features.get('layer_features', {})
        global_features = activation_features.get('global_features', {})
        
        # æ£€æµ‹ä½æ¿€æ´»å±‚ï¼ˆå¯èƒ½çš„ç“¶é¢ˆï¼‰
        avg_activation = global_features.get('avg_activation', 0)
        for layer_name, layer_data in layer_features.items():
            layer_mean = layer_data.get('mean', 0)
            
            # æ¿€æ´»è¿‡ä½çš„å±‚
            if layer_mean < avg_activation * 0.3:
                candidates.append({
                    'layer_name': layer_name,
                    'detection_method': 'low_activation',
                    'priority': 0.7,
                    'rationale': f'æ¿€æ´»è¿‡ä½ ({layer_mean:.4f})',
                    'suggested_mutations': ['width_expansion', 'attention_enhancement'],
                    'confidence': 0.6
                })
            
            # æ¿€æ´»é¥±å’Œçš„å±‚ï¼ˆå¯èƒ½éœ€è¦æ­£åˆ™åŒ–ï¼‰
            sparsity = layer_data.get('zeros_ratio', 0)
            if sparsity > 0.8:
                candidates.append({
                    'layer_name': layer_name,
                    'detection_method': 'high_sparsity',
                    'priority': 0.6,
                    'rationale': f'æ¿€æ´»ç¨€ç–åº¦è¿‡é«˜ ({sparsity:.4f})',
                    'suggested_mutations': ['batch_norm_insertion', 'layer_norm'],
                    'confidence': 0.5
                })
        
        return candidates
    
    def _detect_gradient_candidates(self, features: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åŸºäºæ¢¯åº¦çš„å€™é€‰ç‚¹æ£€æµ‹"""
        
        gradient_features = features.get('gradient_features', {})
        if not gradient_features.get('available'):
            return []
        
        candidates = []
        layer_features = gradient_features.get('layer_features', {})
        global_features = gradient_features.get('global_features', {})
        
        avg_grad_norm = global_features.get('avg_grad_norm', 0)
        
        for layer_name, layer_data in layer_features.items():
            grad_norm = layer_data.get('norm', 0)
            
            # æ¢¯åº¦æ¶ˆå¤±
            if grad_norm < avg_grad_norm * 0.1 and avg_grad_norm > 0:
                candidates.append({
                    'layer_name': layer_name,
                    'detection_method': 'gradient_vanishing',
                    'priority': 0.8,
                    'rationale': f'æ¢¯åº¦æ¶ˆå¤± (norm: {grad_norm:.6f})',
                    'suggested_mutations': ['residual_connection', 'batch_norm_insertion'],
                    'confidence': 0.7
                })
            
            # æ¢¯åº¦çˆ†ç‚¸
            elif grad_norm > avg_grad_norm * 10 and avg_grad_norm > 0:
                candidates.append({
                    'layer_name': layer_name,
                    'detection_method': 'gradient_explosion',
                    'priority': 0.9,
                    'rationale': f'æ¢¯åº¦çˆ†ç‚¸ (norm: {grad_norm:.6f})',
                    'suggested_mutations': ['layer_norm', 'batch_norm_insertion'],
                    'confidence': 0.8
                })
        
        return candidates
    
    def _detect_architecture_candidates(self, features: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åŸºäºæ¶æ„çš„å€™é€‰ç‚¹æ£€æµ‹"""
        
        architecture_info = features.get('architecture_info', {})
        layer_relationship = features.get('layer_relationship_features', {})
        
        candidates = []
        
        # æ£€æµ‹å‚æ•°ä¸å¹³è¡¡
        layer_info = architecture_info.get('layer_info', [])
        if layer_info:
            param_counts = [info['param_count'] for info in layer_info]
            avg_params = np.mean(param_counts)
            
            for info in layer_info:
                # å‚æ•°è¿‡å°‘çš„å±‚
                if info['param_count'] < avg_params * 0.1 and avg_params > 0:
                    candidates.append({
                        'layer_name': info['name'],
                        'detection_method': 'low_parameter_count',
                        'priority': 0.5,
                        'rationale': f'å‚æ•°é‡è¿‡å°‘ ({info["param_count"]})',
                        'suggested_mutations': ['width_expansion'],
                        'confidence': 0.4
                    })
        
        # æ£€æµ‹è¿æ¥å¤æ‚åº¦
        complexity = layer_relationship.get('connection_complexity', 0)
        skip_connections = layer_relationship.get('skip_connections', 0)
        
        if skip_connections == 0 and len(layer_info) > 5:
            # ç¼ºå°‘è·³è·ƒè¿æ¥çš„æ·±å±‚ç½‘ç»œ
            # é€‰æ‹©ä¸­é—´å±‚æ·»åŠ æ®‹å·®è¿æ¥
            middle_layers = layer_info[len(layer_info)//3:2*len(layer_info)//3]
            for info in middle_layers:
                candidates.append({
                    'layer_name': info['name'],
                    'detection_method': 'missing_skip_connections',
                    'priority': 0.6,
                    'rationale': 'æ·±å±‚ç½‘ç»œç¼ºå°‘è·³è·ƒè¿æ¥',
                    'suggested_mutations': ['residual_connection'],
                    'confidence': 0.5
                })
        
        return candidates
    
    def _detect_performance_candidates(self, features: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åŸºäºæ€§èƒ½çš„å€™é€‰ç‚¹æ£€æµ‹"""
        
        performance_features = features.get('performance_features', {})
        if not performance_features.get('available'):
            return []
        
        candidates = []
        
        # æ£€æµ‹æ€§èƒ½åœæ»
        trend = performance_features.get('short_term_trend', 0)
        improvement_ratio = performance_features.get('improvement_ratio', 1)
        
        if abs(trend) < 0.001 and improvement_ratio < 0.3:
            # æ€§èƒ½åœæ»ï¼Œå»ºè®®æ¢ç´¢æ€§å˜å¼‚
            architecture_info = features.get('architecture_info', {})
            layer_info = architecture_info.get('layer_info', [])
            
            if layer_info:
                # éšæœºé€‰æ‹©ä¸€äº›å±‚è¿›è¡Œæ¢ç´¢æ€§å˜å¼‚
                import random
                selected_layers = random.sample(layer_info, min(3, len(layer_info)))
                
                for info in selected_layers:
                    candidates.append({
                        'layer_name': info['name'],
                        'detection_method': 'performance_stagnation',
                        'priority': 0.4,
                        'rationale': 'æ€§èƒ½åœæ»ï¼Œå»ºè®®æ¢ç´¢æ€§å˜å¼‚',
                        'suggested_mutations': ['attention_enhancement', 'channel_attention'],
                        'confidence': 0.3
                    })
        
        # æ£€æµ‹æ€§èƒ½ä¸‹é™
        elif trend < -0.005:
            # æ€§èƒ½ä¸‹é™ï¼Œéœ€è¦ç´§æ€¥ä¿®å¤
            candidates.append({
                'layer_name': 'global',  # å…¨å±€æ€§é—®é¢˜
                'detection_method': 'performance_degradation',
                'priority': 0.9,
                'rationale': f'æ€§èƒ½ä¸‹é™è¶‹åŠ¿ ({trend:.6f})',
                'suggested_mutations': ['batch_norm_insertion', 'layer_norm'],
                'confidence': 0.7
            })
        
        return candidates
    
    def _deduplicate_and_prioritize(self, candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡å’Œä¼˜å…ˆçº§æ’åº"""
        
        # æŒ‰å±‚åå»é‡ï¼Œä¿ç•™ä¼˜å…ˆçº§æœ€é«˜çš„
        layer_candidates = {}
        
        for candidate in candidates:
            layer_name = candidate['layer_name']
            priority = candidate.get('priority', 0)
            
            if (layer_name not in layer_candidates or 
                priority > layer_candidates[layer_name].get('priority', 0)):
                layer_candidates[layer_name] = candidate
        
        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŒ‰ä¼˜å…ˆçº§æ’åº
        unique_candidates = list(layer_candidates.values())
        unique_candidates.sort(key=lambda x: x.get('priority', 0), reverse=True)
        
        # æ·»åŠ å…¨å±€æ’å
        for i, candidate in enumerate(unique_candidates):
            candidate['rank'] = i + 1
            candidate['total_candidates'] = len(unique_candidates)
        
        return unique_candidates
    
    def evaluate_candidate_quality(self, candidate: Dict[str, Any], features: Dict[str, Any]) -> Dict[str, Any]:
        """è¯„ä¼°å€™é€‰ç‚¹è´¨é‡"""
        
        base_confidence = candidate.get('confidence', 0.5)
        
        # è´¨é‡è¯„ä¼°å› å­
        quality_factors = []
        
        # 1. æ£€æµ‹æ–¹æ³•ç½®ä¿¡åº¦
        detection_method = candidate.get('detection_method', '')
        method_confidence = {
            'gradient_vanishing': 0.8,
            'gradient_explosion': 0.9,
            'low_activation': 0.6,
            'high_sparsity': 0.5,
            'missing_skip_connections': 0.6,
            'performance_degradation': 0.8,
            'performance_stagnation': 0.3,
            'low_parameter_count': 0.4
        }
        
        quality_factors.append(method_confidence.get(detection_method, 0.5))
        
        # 2. ä¼˜å…ˆçº§æƒé‡
        priority = candidate.get('priority', 0)
        quality_factors.append(priority)
        
        # 3. æ•°æ®å¯ç”¨æ€§æƒé‡
        feature_summary = features.get('feature_summary', {})
        data_quality = 0.5
        if feature_summary.get('has_gradients'):
            data_quality += 0.2
        if feature_summary.get('has_activations'):
            data_quality += 0.2
        if feature_summary.get('has_performance_history'):
            data_quality += 0.1
        
        quality_factors.append(data_quality)
        
        # è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°
        quality_score = np.mean(quality_factors)
        adjusted_confidence = base_confidence * quality_score
        
        return {
            'quality_score': quality_score,
            'adjusted_confidence': adjusted_confidence,
            'quality_factors': quality_factors,
            'data_quality': data_quality,
            'recommendation': 'high_quality' if quality_score > 0.7 else 'medium_quality' if quality_score > 0.5 else 'low_quality'
        }