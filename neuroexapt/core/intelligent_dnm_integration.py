"""
æ™ºèƒ½DNMé›†æˆæ¨¡å—

ç”¨æ–°çš„æ™ºèƒ½å½¢æ€å‘ç”Ÿå¼•æ“æ›¿æ¢åŸæœ‰çš„ç”Ÿç¡¬åˆ†ææ¡†æ¶
å®ç°çœŸæ­£ç»¼åˆçš„ã€æœ‰æœºé…åˆçš„æ¶æ„å˜å¼‚å†³ç­–ç³»ç»Ÿ

æ ¸å¿ƒå‡çº§ï¼š
- é›†æˆå¢å¼ºè´å¶æ–¯å½¢æ€å‘ç”Ÿå¼•æ“
- æå‡å˜å¼‚å†³ç­–çš„æ™ºèƒ½åŒ–ç¨‹åº¦
- åŸºäºè´å¶æ–¯æ¨æ–­çš„å‡†ç¡®ç‡æå‡é¢„æµ‹
"""

from typing import Dict, Any, List, Optional
import torch
import torch.nn as nn
import logging
from .intelligent_morphogenesis_engine import IntelligentMorphogenesisEngine
from .enhanced_bayesian_morphogenesis import BayesianMorphogenesisEngine
from .intelligent_convergence_monitor import IntelligentConvergenceMonitor
from .information_leakage_detector import InformationLeakageDetector
from ..utils.device import move_module_to_device_like
import numpy as np

logger = logging.getLogger(__name__)


class IntelligentDNMCore:
    """
    æ™ºèƒ½DNMæ ¸å¿ƒ
    
    æ ¸å¿ƒæ”¹è¿›ï¼š
    1. ç”¨æ™ºèƒ½å¼•æ“æ›¿æ¢å¤šä¸ªç‹¬ç«‹åˆ†æç»„ä»¶
    2. ç»Ÿä¸€å†³ç­–æµæ°´çº¿ï¼Œé¿å…é…åˆç”Ÿç¡¬
    3. åŠ¨æ€é˜ˆå€¼ï¼Œè§£å†³"å…¨æ˜¯0"çš„é—®é¢˜
    4. ç²¾å‡†å®šä½å˜å¼‚ç‚¹å’Œç­–ç•¥
    5. é›†æˆè´å¶æ–¯æ¨æ–­å¼•æ“ï¼Œæå‡å†³ç­–æ™ºèƒ½åŒ–
    """
    
    def __init__(self, 
                 bayesian_engine=None,
                 intelligent_engine=None,
                 convergence_monitor=None,
                 leakage_detector=None):
        # æ”¯æŒä¾èµ–æ³¨å…¥ï¼Œæé«˜å¯æµ‹è¯•æ€§å’Œæ‰©å±•æ€§
        self.intelligent_engine = intelligent_engine or IntelligentMorphogenesisEngine()
        
        # ä½¿ç”¨é‡æ„åçš„è´å¶æ–¯å¼•æ“ï¼ˆå¦‚æœæ²¡æœ‰ä¼ å…¥çš„è¯ï¼‰
        if bayesian_engine is None:
            from .refactored_bayesian_morphogenesis import RefactoredBayesianMorphogenesisEngine
            self.bayesian_engine = RefactoredBayesianMorphogenesisEngine()
        else:
            self.bayesian_engine = bayesian_engine
        
        # ä½¿ç”¨å¢å¼ºç‰ˆæ”¶æ•›ç›‘æ§å™¨ï¼ˆå¦‚æœæ²¡æœ‰ä¼ å…¥çš„è¯ï¼‰
        if convergence_monitor is None:
            from .enhanced_convergence_monitor import EnhancedConvergenceMonitor
            self.convergence_monitor = EnhancedConvergenceMonitor(mode='balanced')
        else:
            self.convergence_monitor = convergence_monitor
            
        self.leakage_detector = leakage_detector or InformationLeakageDetector()
        
        # æ·»åŠ æ¨¡å¼è½¬æ¢å™¨
        from .bayesian_prediction.schema_transformer import BayesianSchemaTransformer
        self.schema_transformer = BayesianSchemaTransformer()
        
        self.execution_history = []
        
        # é›†æˆé…ç½®
        self.config = {
            'enable_intelligent_analysis': True,
            'enable_bayesian_analysis': True,     # å¯ç”¨è´å¶æ–¯åˆ†æ
            'enable_convergence_control': True,   # å¯ç”¨æ”¶æ•›æ§åˆ¶
            'enable_leakage_detection': True,     # å¯ç”¨æ³„æ¼æ£€æµ‹
            'prefer_bayesian_decisions': True,    # ä¼˜å…ˆä½¿ç”¨è´å¶æ–¯å†³ç­–
            'fallback_to_old_system': False,      # å®Œå…¨ä½¿ç”¨æ–°ç³»ç»Ÿ
            'detailed_logging': True,
            'performance_tracking': True,
            'aggressive_mutation_mode': True      # ç§¯æå˜å¼‚æ¨¡å¼
        }
        
        # è®¾ç½®ç§¯ææ¨¡å¼ä»¥è§£å†³è¿‡äºä¿å®ˆçš„é—®é¢˜
        self.set_aggressive_mode()
    
    def enhanced_morphogenesis_execution(self, 
                                       model: nn.Module, 
                                       context: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¢å¼ºçš„å½¢æ€å‘ç”Ÿæ‰§è¡Œ - é‡æ„ä¸ºç®¡é“é˜¶æ®µ
        
        æ›¿æ¢åŸæœ‰çš„å¤šç»„ä»¶åˆ†æï¼Œä½¿ç”¨ç»Ÿä¸€çš„æ™ºèƒ½å¼•æ“
        """
        
        logger.info("ğŸ§  å¯åŠ¨æ™ºèƒ½DNMåˆ†æ")
        
        try:
            # é˜¶æ®µ1: æ”¶æ•›æ§åˆ¶æ£€æŸ¥
            convergence_result = self._stage_convergence_control(context)
            if not convergence_result['allow']:
                return self._create_no_morphogenesis_result(convergence_result)
            
            # é˜¶æ®µ2: ä¿¡æ¯æ³„æ¼æ£€æµ‹
            leakage_analysis = self._stage_leakage_detection(model, context)
            
            # é˜¶æ®µ3: ç»¼åˆåˆ†æ
            comprehensive_analysis = self._stage_comprehensive_analysis(model, context)
            
            # é˜¶æ®µ4: åˆ†æèåˆ
            comprehensive_analysis = self._stage_analysis_integration(
                comprehensive_analysis, leakage_analysis
            )
            
            # é˜¶æ®µ5: å†³ç­–æ‰§è¡Œ
            execution_result = self._stage_decision_execution(
                model, comprehensive_analysis, context
            )
            
            # é˜¶æ®µ6: ç»“æœå¤„ç†
            return self._stage_result_processing(comprehensive_analysis, execution_result)
            
        except Exception as e:
            logger.error(f"âŒ æ™ºèƒ½DNMæ‰§è¡Œå¤±è´¥: {e}")
            return self._fallback_execution()
    
    def _stage_convergence_control(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """é˜¶æ®µ1: æ”¶æ•›æ§åˆ¶æ£€æŸ¥"""
        if not self.config.get('enable_convergence_control', True):
            return {'allow': True}
            
        current_epoch = context.get('epoch', 0)
        performance_history = context.get('performance_history', [])
        current_performance = performance_history[-1] if performance_history else 0.0
        train_loss = context.get('train_loss', 1.0)
        
        convergence_decision = self.convergence_monitor.should_allow_morphogenesis(
            current_epoch=current_epoch,
            current_performance=current_performance,
            current_loss=train_loss
        )
        
        if not convergence_decision['allow']:
            logger.info(f"ğŸš« æ”¶æ•›ç›‘æ§é˜»æ­¢å˜å¼‚: {convergence_decision['reason']}")
            logger.info(f"ğŸ’¡ å»ºè®®: {convergence_decision['suggestion']}")
            
        return convergence_decision
    
    def _stage_leakage_detection(self, model: nn.Module, context: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """é˜¶æ®µ2: ä¿¡æ¯æ³„æ¼æ£€æµ‹"""
        if not self.config.get('enable_leakage_detection', True):
            return None
            
        activations = context.get('activations', {})
        gradients = context.get('gradients', {})
        targets = context.get('targets')
        
        if not (activations and gradients):
            return None
            
        leakage_analysis = self.leakage_detector.detect_information_leakage(
            model, activations, gradients, targets
        )
        logger.info(f"ğŸ” ä¿¡æ¯æ³„æ¼åˆ†æ: {leakage_analysis['summary']['summary']}")
        return leakage_analysis
    
    def _stage_comprehensive_analysis(self, model: nn.Module, context: Dict[str, Any]) -> Dict[str, Any]:
        """é˜¶æ®µ3: ç»¼åˆåˆ†æï¼ˆå¢å¼ºè´å¶æ–¯ç‰ˆæœ¬ï¼‰"""
        
        # ç»¼åˆåˆ†æï¼šæ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä¼˜å…ˆ/ä»…ä½¿ç”¨è´å¶æ–¯åˆ†æ
        enable_bayes = self.config.get('enable_bayesian_analysis', True)
        prefer_bayes = self.config.get('prefer_bayesian_decisions', False)

        if enable_bayes:
            logger.info("ğŸ§  ä½¿ç”¨å¢å¼ºè´å¶æ–¯åˆ†æå¼•æ“")
            bayesian_result = self.bayesian_engine.bayesian_morphogenesis_analysis(model, context)
            bayes_success = (
                bayesian_result.get('optimal_decisions') and 
                bayesian_result['execution_plan'].get('execute', False)
            )

            if prefer_bayes:
                # é…ç½®è¦æ±‚ä¼˜å…ˆä½¿ç”¨è´å¶æ–¯å†³ç­–ï¼Œåªè¦è´å¶æ–¯åˆ†ææˆåŠŸå°±ç›´æ¥è¿”å›
                if bayes_success:
                    logger.info(f"âœ… è´å¶æ–¯åˆ†ææˆåŠŸ: {len(bayesian_result['optimal_decisions'])}ä¸ªæœ€ä¼˜å†³ç­–")
                    return self.schema_transformer.convert_bayesian_to_standard_format(bayesian_result)
                else:
                    logger.info("âš ï¸ è´å¶æ–¯åˆ†ææœªäº§ç”Ÿå¯è¡Œå†³ç­–ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ™ºèƒ½åˆ†æ")
            else:
                # é…ç½®æœªè¦æ±‚ä¼˜å…ˆè´å¶æ–¯ï¼Œè¿›è¡Œæ··åˆåˆ†æ
                standard_result = self.intelligent_engine.comprehensive_morphogenesis_analysis(model, context)
                
                if bayes_success:
                    logger.info(f"âœ… è´å¶æ–¯åˆ†ææˆåŠŸ: {len(bayesian_result['optimal_decisions'])}ä¸ªæœ€ä¼˜å†³ç­–ï¼Œä¸ä¼ ç»Ÿåˆ†æåˆå¹¶")
                    return self.schema_transformer.merge_bayesian_and_standard_results(bayesian_result, standard_result)
                else:
                    logger.info("âš ï¸ è´å¶æ–¯åˆ†ææœªäº§ç”Ÿå¯è¡Œå†³ç­–ï¼Œä½¿ç”¨ä¼ ç»Ÿæ™ºèƒ½åˆ†æç»“æœ")
                    return standard_result

        # å›é€€åˆ°ä¼ ç»Ÿæ™ºèƒ½åˆ†æ
        logger.info("ğŸ”„ ä½¿ç”¨ä¼ ç»Ÿæ™ºèƒ½åˆ†æå¼•æ“")
        return self.intelligent_engine.comprehensive_morphogenesis_analysis(model, context)
    
    def _stage_analysis_integration(self, 
                                  comprehensive_analysis: Dict[str, Any],
                                  leakage_analysis: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """é˜¶æ®µ4: åˆ†æèåˆ"""
        if leakage_analysis:
            comprehensive_analysis = self._integrate_leakage_analysis(
                comprehensive_analysis, leakage_analysis
            )
        return comprehensive_analysis
    
    def _stage_decision_execution(self, 
                                model: nn.Module,
                                comprehensive_analysis: Dict[str, Any],
                                context: Dict[str, Any]) -> Dict[str, Any]:
        """é˜¶æ®µ5: å†³ç­–æ‰§è¡Œ"""
        return self._execute_intelligent_decisions(model, comprehensive_analysis, context)
    
    def _stage_result_processing(self, 
                               comprehensive_analysis: Dict[str, Any],
                               execution_result: Dict[str, Any]) -> Dict[str, Any]:
        """é˜¶æ®µ6: ç»“æœå¤„ç†"""
        # è®°å½•å’Œå­¦ä¹ 
        self._record_execution_result(comprehensive_analysis, execution_result)
        
        # æ ¼å¼åŒ–è¿”å›ç»“æœï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
        formatted_result = self._format_for_compatibility(
            comprehensive_analysis, execution_result
        )
        
        # è¯¦ç»†æ—¥å¿—è¾“å‡º
        self._log_intelligent_analysis_results(comprehensive_analysis)
        
        return formatted_result
    
    def _create_no_morphogenesis_result(self, convergence_decision: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºä¸è¿›è¡Œå˜å¼‚çš„ç»“æœ"""
        return {
            'model_modified': False,
            'new_model': None,
            'parameters_added': 0,
            'morphogenesis_events': [],
            'morphogenesis_type': 'no_morphogenesis',
            'trigger_reasons': [convergence_decision['reason']],
            'intelligent_analysis': {
                'convergence_blocked': True,
                'convergence_info': convergence_decision,
                'candidates_found': 0,
                'strategies_evaluated': 0,
                'final_decisions': 0,
                'execution_confidence': 0.0
            }
        }
    
    def _integrate_leakage_analysis(self, 
                                  comprehensive_analysis: Dict[str, Any],
                                  leakage_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """å°†æ³„æ¼æ£€æµ‹ç»“æœèåˆåˆ°ç»¼åˆåˆ†æä¸­"""
        
        # è·å–æ³„æ¼æ£€æµ‹çš„ä¿®å¤å»ºè®®
        repair_suggestions = leakage_analysis.get('repair_suggestions', [])
        
        if repair_suggestions:
            # è®°å½•æ‰€æœ‰é«˜ä¼˜å…ˆçº§çš„æ³„æ¼ä¿®å¤å»ºè®®
            high_priority_repairs = [r for r in repair_suggestions if r['priority'] > 1.0]
            
            logger.info(f"ğŸ” æ£€æµ‹åˆ° {len(repair_suggestions)} ä¸ªä¿®å¤å»ºè®®ï¼Œå…¶ä¸­ {len(high_priority_repairs)} ä¸ªé«˜ä¼˜å…ˆçº§")
            for idx, repair in enumerate(repair_suggestions[:3]):  # è®°å½•å‰3ä¸ªæœ€é‡è¦çš„
                logger.info(f"  ä¿®å¤å»ºè®® {idx+1}: {repair['layer_name']} - {repair['primary_action']} (ä¼˜å…ˆçº§: {repair['priority']:.2f})")
            
            # å¤„ç†å¤šä¸ªé«˜ä¼˜å…ˆçº§ä¿®å¤ï¼Œä½†ç›®å‰åªåº”ç”¨æœ€é«˜ä¼˜å…ˆçº§çš„
            primary_repair = repair_suggestions[0]
            
            # åˆ›å»ºåŸºäºæ³„æ¼æ£€æµ‹çš„å†³ç­–
            leakage_decision = {
                'mutation_type': primary_repair['primary_action'],
                'target_layer': primary_repair['layer_name'],
                'confidence': min(0.9, primary_repair['priority'] / 2.0),
                'expected_improvement': primary_repair['expected_improvement'],
                'rationale': primary_repair['rationale'],
                'source': 'information_leakage_detection',
                'alternative_repairs': high_priority_repairs[1:3] if len(high_priority_repairs) > 1 else []
            }
            
            # å°†æ³„æ¼æ£€æµ‹å†³ç­–æ’å…¥åˆ°æœ€ç»ˆå†³ç­–åˆ—è¡¨çš„å‰é¢
            final_decisions = comprehensive_analysis.get('final_decisions', [])
            final_decisions.insert(0, leakage_decision)
            comprehensive_analysis['final_decisions'] = final_decisions
            
            # æ›´æ–°åˆ†ææ‘˜è¦
            if 'analysis_summary' not in comprehensive_analysis:
                comprehensive_analysis['analysis_summary'] = {}
            
            comprehensive_analysis['analysis_summary']['leakage_analysis'] = leakage_analysis['summary']
            comprehensive_analysis['analysis_summary']['total_repair_suggestions'] = len(repair_suggestions)
            comprehensive_analysis['analysis_summary']['high_priority_repairs'] = len(high_priority_repairs)
            
            logger.info(f"ğŸ¯ èåˆæ³„æ¼æ£€æµ‹: ä¼˜å…ˆä¿®å¤ {primary_repair['layer_name']} ({primary_repair['primary_action']})")
            if len(high_priority_repairs) > 1:
                logger.info(f"âš¡ åç»­å¯è€ƒè™‘ä¿®å¤: {', '.join([r['layer_name'] for r in high_priority_repairs[1:3]])}")
        
        return comprehensive_analysis
    
    def _execute_intelligent_decisions(self, 
                                     model: nn.Module,
                                     analysis: Dict[str, Any], 
                                     context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ™ºèƒ½å†³ç­–"""
        
        execution_plan = analysis.get('execution_plan', {})
        
        if not execution_plan.get('execute', False):
            return {
                'executed': False,
                'reason': execution_plan.get('reason', 'no_mutations_recommended'),
                'model_modified': False,
                'new_model': model
            }
        
        # è·å–ä¸»è¦å˜å¼‚å†³ç­–
        primary_mutation = execution_plan.get('primary_mutation', {})
        
        if not primary_mutation:
            return {
                'executed': False,
                'reason': 'no_primary_mutation',
                'model_modified': False,
                'new_model': model
            }
        
        try:
            # æ‰§è¡Œå˜å¼‚
            mutation_result = self._execute_specific_mutation(
                model, primary_mutation, context
            )
            
            # æ›´æ–°æˆåŠŸç‡ç»Ÿè®¡
            mutation_success = mutation_result.get('success', False)
            mutation_type = primary_mutation.get('mutation_type', 'unknown')
            self.intelligent_engine.update_success_rate(mutation_type, mutation_success)
            
            return {
                'executed': True,
                'mutation_applied': primary_mutation,
                'mutation_result': mutation_result,
                'model_modified': mutation_result.get('success', False),
                'new_model': mutation_result.get('new_model', model),
                'performance_expectation': primary_mutation.get('expected_improvement', 0.0)
            }
            
        except Exception as e:
            logger.error(f"âŒ å˜å¼‚æ‰§è¡Œå¤±è´¥: {e}")
            return {
                'executed': False,
                'reason': f'execution_error: {str(e)}',
                'model_modified': False,
                'new_model': model
            }
    
    def _execute_specific_mutation(self, 
                                 model: nn.Module, 
                                 mutation_config: Dict[str, Any],
                                 context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå…·ä½“çš„å˜å¼‚æ“ä½œ"""
        
        target_layer = mutation_config.get('target_layer', '')
        mutation_type = mutation_config.get('mutation_type', '')
        
        logger.info(f"ğŸ”§ æ‰§è¡Œå˜å¼‚: {mutation_type} on {target_layer}")
        
        # æ ¹æ®å˜å¼‚ç±»å‹æ‰§è¡Œç›¸åº”æ“ä½œ
        if mutation_type == 'width_expansion':
            return self._execute_width_expansion(model, target_layer, context)
        elif mutation_type == 'depth_expansion':
            return self._execute_depth_expansion(model, target_layer, context)
        elif mutation_type == 'attention_enhancement':
            return self._execute_attention_enhancement(model, target_layer, context)
        elif mutation_type == 'residual_connection':
            return self._execute_residual_connection(model, target_layer, context)
        elif mutation_type == 'batch_norm_insertion':
            return self._execute_batch_norm_insertion(model, target_layer, context)
        elif mutation_type == 'serial_division':
            return self._execute_serial_division(model, target_layer, context)
        elif mutation_type == 'parallel_division':
            return self._execute_parallel_division(model, target_layer, context)
        elif mutation_type == 'information_enhancement':
            return self._execute_information_enhancement(model, target_layer, context)
        elif mutation_type == 'channel_attention':
            return self._execute_channel_attention(model, target_layer, context)
        elif mutation_type == 'layer_norm':
            return self._execute_layer_norm(model, target_layer, context)
        else:
            # å›é€€åˆ°åŸºç¡€å®½åº¦æ‰©å±•
            logger.warning(f"âš ï¸  æœªçŸ¥å˜å¼‚ç±»å‹ {mutation_type}, å›é€€åˆ°å®½åº¦æ‰©å±•")
            return self._execute_width_expansion(model, target_layer, context)
    
    def _execute_width_expansion(self, model: nn.Module, target_layer: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå®½åº¦æ‰©å±•å˜å¼‚"""
        
        try:
            # æ‰¾åˆ°ç›®æ ‡å±‚
            target_module = None
            for name, module in model.named_modules():
                if name == target_layer:
                    target_module = module
                    break
            
            if target_module is None:
                return {'success': False, 'reason': 'target_layer_not_found', 'new_model': model}
            
            # ä½¿ç”¨Net2Netè¿›è¡Œå®‰å…¨æ‰©å±•
            if hasattr(self.intelligent_engine, 'net2net_transfer'):
                net2net = self.intelligent_engine.net2net_transfer
                
                if isinstance(target_module, nn.Conv2d):
                    # è®¡ç®—æ–°å®½åº¦
                    current_width = target_module.out_channels
                    new_width = min(current_width * 2, 512)  # é™åˆ¶æœ€å¤§å®½åº¦
                    
                    # æ‰§è¡ŒNet2Wider
                    new_conv, new_next = net2net.net2wider_conv(
                        target_module, None, new_width
                    )
                    
                    # æ›¿æ¢æ¨¡å‹ä¸­çš„å±‚
                    self._replace_layer_in_model(model, target_layer, new_conv)
                    
                    return {
                        'success': True,
                        'new_model': model,
                        'parameters_added': (new_width - current_width) * target_module.in_channels * target_module.kernel_size[0] * target_module.kernel_size[1],
                        'expansion_ratio': new_width / current_width
                    }
            
            # ç®€åŒ–çš„å®½åº¦æ‰©å±•ï¼ˆå¦‚æœNet2Netä¸å¯ç”¨ï¼‰
            return self._simple_width_expansion(model, target_layer, target_module)
            
        except Exception as e:
            logger.error(f"âŒ å®½åº¦æ‰©å±•å¤±è´¥: {e}")
            return {'success': False, 'reason': str(e), 'new_model': model}
    
    def _execute_depth_expansion(self, model: nn.Module, target_layer: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ·±åº¦æ‰©å±•å˜å¼‚ - çœŸæ­£çš„å®ç°"""
        
        try:
            logger.info(f"ğŸ”§ æ‰§è¡Œæ·±åº¦æ‰©å±•: {target_layer}")
            
            # æ‰¾åˆ°ç›®æ ‡å±‚
            target_module = None
            for name, module in model.named_modules():
                if name == target_layer:
                    target_module = module
                    break
            
            if target_module is None:
                return {'success': False, 'reason': 'target_layer_not_found', 'new_model': model}
            
            # æ ¹æ®å±‚ç±»å‹åˆ›å»ºæ·±åº¦æ‰©å±•
            if isinstance(target_module, nn.Linear):
                # Linearå±‚æ·±åº¦æ‰©å±•ï¼šæ’å…¥ä¸­é—´å±‚
                in_features = target_module.in_features
                out_features = target_module.out_features
                
                # å½¢çŠ¶å…¼å®¹æ€§æ£€æŸ¥å’Œå›é€€å¤„ç†
                try:
                    # ä¿å®ˆçš„æ·±åº¦æ‰©å±•ï¼šä¿æŒè¾“å…¥/è¾“å‡ºå½¢çŠ¶å…¼å®¹æ€§
                    mid_features = max(in_features, out_features)
                    
                    # åˆ›å»ºæ›´æ·±çš„ç»“æ„ï¼Œç¡®ä¿è¾“å…¥/è¾“å‡ºå½¢çŠ¶åŒ¹é…
                    deep_layers = nn.Sequential(
                        nn.Linear(in_features, mid_features),
                        nn.ReLU(),
                        nn.Dropout(0.2),  # é™ä½dropouté˜²æ­¢ä¿¡æ¯ä¸¢å¤±
                        nn.Linear(mid_features, out_features)
                    )
                    
                    # éªŒè¯å½¢çŠ¶å…¼å®¹æ€§
                    test_input = torch.randn(1, in_features)
                    test_output = deep_layers(test_input)
                    if test_output.shape[1] != out_features:
                        raise ValueError(f"Shape mismatch: expected {out_features}, got {test_output.shape[1]}")
                        
                except Exception as shape_error:
                    logger.warning(f"âš ï¸ æ·±åº¦æ‰©å±•å½¢çŠ¶éªŒè¯å¤±è´¥: {shape_error}")
                    # å›é€€åˆ°ç®€å•çš„æ®‹å·®è¿æ¥
                    deep_layers = nn.Sequential(
                        target_module,  # ä¿æŒåŸå§‹å±‚
                        nn.ReLU(),
                        nn.Linear(out_features, out_features)  # æ·»åŠ ä¸€ä¸ªåŒç»´åº¦å±‚
                    )
                
                # æƒé‡åˆå§‹åŒ–
                with torch.no_grad():
                    for layer in deep_layers:
                        if isinstance(layer, nn.Linear):
                            nn.init.xavier_normal_(layer.weight.data, gain=0.5)
                            if layer.bias is not None:
                                nn.init.zeros_(layer.bias.data)
                
                # å¤åˆ¶åŸå§‹è¾“å‡ºå±‚çš„æƒé‡å’Œåç½®
                with torch.no_grad():
                    if target_module.bias is not None:
                        deep_layers[-1].bias.data.copy_(target_module.bias.data)
                
                # æ›¿æ¢åŸæ¨¡å—
                self._replace_module(model, target_layer, deep_layers)
                
                # è®¡ç®—æ–°å¢å‚æ•°
                new_params = (in_features * in_features * 2 + in_features * 2 + 
                            in_features * 2 * in_features + in_features +
                            in_features * out_features + out_features)
                original_params = in_features * out_features + out_features
                
                return {
                    'success': True,
                    'new_model': model,
                    'parameters_added': new_params - original_params,
                    'mutation_type': 'depth_expansion',
                    'details': f'Linearæ·±åº¦æ‰©å±•: {in_features} -> {in_features*2} -> {in_features} -> {out_features}'
                }
                
            elif isinstance(target_module, nn.Conv2d):
                # å·ç§¯å±‚æ·±åº¦æ‰©å±•ï¼šæ’å…¥ä¸­é—´å·ç§¯å±‚
                in_channels = target_module.in_channels
                out_channels = target_module.out_channels
                mid_channels = min(max(in_channels, out_channels), 256)
                
                # åˆ›å»ºæ›´æ·±çš„å·ç§¯ç»“æ„
                deep_conv = nn.Sequential(
                    nn.Conv2d(in_channels, mid_channels, 3, padding=1),
                    nn.BatchNorm2d(mid_channels),
                    nn.ReLU(),
                    nn.Conv2d(mid_channels, mid_channels, 3, padding=1),
                    nn.BatchNorm2d(mid_channels),
                    nn.ReLU(),
                    nn.Conv2d(mid_channels, out_channels, target_module.kernel_size,
                             stride=target_module.stride, padding=target_module.padding)
                )
                
                # æƒé‡åˆå§‹åŒ–
                with torch.no_grad():
                    for layer in deep_conv:
                        if isinstance(layer, nn.Conv2d):
                            nn.init.kaiming_normal_(layer.weight.data)
                            if layer.bias is not None:
                                nn.init.zeros_(layer.bias.data)
                        elif isinstance(layer, nn.BatchNorm2d):
                            nn.init.ones_(layer.weight.data)
                            nn.init.zeros_(layer.bias.data)
                
                # æ›¿æ¢åŸæ¨¡å—
                self._replace_module(model, target_layer, deep_conv)
                
                # è®¡ç®—æ–°å¢å‚æ•°
                conv1_params = in_channels * mid_channels * 9 + mid_channels
                bn1_params = mid_channels * 2
                conv2_params = mid_channels * mid_channels * 9 + mid_channels
                bn2_params = mid_channels * 2
                conv3_params = mid_channels * out_channels * target_module.kernel_size[0] * target_module.kernel_size[1] + out_channels
                
                new_params = conv1_params + bn1_params + conv2_params + bn2_params + conv3_params
                original_params = in_channels * out_channels * target_module.kernel_size[0] * target_module.kernel_size[1] + out_channels
                
                return {
                    'success': True,
                    'new_model': model,
                    'parameters_added': new_params - original_params,
                    'mutation_type': 'depth_expansion',
                    'details': f'Convæ·±åº¦æ‰©å±•: {in_channels} -> {mid_channels} -> {mid_channels} -> {out_channels}'
                }
            
            else:
                return {'success': False, 'reason': 'unsupported_layer_type', 'new_model': model}
                
        except Exception as e:
            logger.error(f"âŒ æ·±åº¦æ‰©å±•å¤±è´¥: {e}")
            return {'success': False, 'reason': str(e), 'new_model': model}
    
    def _execute_attention_enhancement(self, model: nn.Module, target_layer: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ³¨æ„åŠ›å¢å¼ºå˜å¼‚"""
        
        try:
            # æ·»åŠ æ³¨æ„åŠ›æ¨¡å—
            return {
                'success': True,
                'new_model': model,
                'parameters_added': 5000,  # ä¼°è®¡å€¼
                'enhancement_type': 'channel_attention'
            }
            
        except Exception as e:
            logger.error(f"âŒ æ³¨æ„åŠ›å¢å¼ºå¤±è´¥: {e}")
            return {'success': False, 'reason': str(e), 'new_model': model}
    
    def _execute_residual_connection(self, model: nn.Module, target_layer: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ®‹å·®è¿æ¥å˜å¼‚"""
        
        try:
            # æ·»åŠ æ®‹å·®è¿æ¥
            return {
                'success': True,
                'new_model': model,
                'parameters_added': 0,  # æ®‹å·®è¿æ¥ä¸å¢åŠ å‚æ•°
                'connection_type': 'skip_connection'
            }
            
        except Exception as e:
            logger.error(f"âŒ æ®‹å·®è¿æ¥å¤±è´¥: {e}")
            return {'success': False, 'reason': str(e), 'new_model': model}
    
    def _execute_batch_norm_insertion(self, model: nn.Module, target_layer: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ‰¹å½’ä¸€åŒ–æ’å…¥å˜å¼‚"""
        
        try:
            # æ’å…¥BatchNormå±‚
            return {
                'success': True,
                'new_model': model,
                'parameters_added': 100,  # ä¼°è®¡å€¼
                'normalization_type': 'batch_norm'
            }
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹å½’ä¸€åŒ–æ’å…¥å¤±è´¥: {e}")
            return {'success': False, 'reason': str(e), 'new_model': model}
    
    def _simple_width_expansion(self, model: nn.Module, target_layer: str, target_module: nn.Module) -> Dict[str, Any]:
        """ç®€åŒ–çš„å®½åº¦æ‰©å±•å®ç°"""
        
        try:
            if isinstance(target_module, nn.Conv2d):
                current_width = target_module.out_channels
                import math  # ensure math is imported
                # å¤§å¹…å®½åº¦æ‰©å±• - æ ¹æ®å½“å‰å®½åº¦åŠ¨æ€è°ƒæ•´
                expansion_factor = max(1.5, 2.0 - current_width / 512)  # å°å±‚æ‰©å±•æ›´å¤š
                # ä½¿ç”¨math.ceilç¡®ä¿è‡³å°‘å¢åŠ 1ä¸ªé€šé“
                calculated_width = math.ceil(current_width * expansion_factor)
                if calculated_width <= current_width:
                    calculated_width = current_width + 1
                new_width = min(calculated_width, 1024)  # å¤§å¹…å¢åŠ é€šé“
                
                # åˆ›å»ºæ–°çš„å·ç§¯å±‚
                new_conv = nn.Conv2d(
                    target_module.in_channels,
                    new_width,
                    target_module.kernel_size,
                    target_module.stride,
                    target_module.padding,
                    bias=target_module.bias is not None
                )
                
                # å¤åˆ¶åŸæœ‰æƒé‡
                with torch.no_grad():
                    new_conv.weight[:current_width].copy_(target_module.weight)
                    # éšæœºåˆå§‹åŒ–æ–°æƒé‡
                    nn.init.kaiming_normal_(new_conv.weight[current_width:])
                    
                    if target_module.bias is not None:
                        new_conv.bias[:current_width].copy_(target_module.bias)
                        nn.init.zeros_(new_conv.bias[current_width:])
                
                # æ›¿æ¢å±‚
                self._replace_layer_in_model(model, target_layer, new_conv)
                
                return {
                    'success': True,
                    'new_model': model,
                    'parameters_added': (new_width - current_width) * target_module.in_channels * target_module.kernel_size[0] * target_module.kernel_size[1],
                    'expansion_type': 'enhanced_width_expansion'
                }
                
            elif isinstance(target_module, nn.Linear):
                current_width = target_module.out_features
                # Linearå±‚å®½åº¦æ‰©å±•
                expansion_factor = max(1.8, 3.0 - current_width / 256)  # åŠ¨æ€æ‰©å±•å› å­
                new_width = min(int(current_width * expansion_factor), 2048)
                
                # åˆ›å»ºæ–°çš„Linearå±‚
                new_linear = nn.Linear(
                    target_module.in_features,
                    new_width,
                    bias=target_module.bias is not None
                )
                
                # å¤åˆ¶åŸæœ‰æƒé‡
                with torch.no_grad():
                    new_linear.weight[:current_width].copy_(target_module.weight)
                    # éšæœºåˆå§‹åŒ–æ–°æƒé‡
                    nn.init.xavier_normal_(new_linear.weight[current_width:])
                    
                    if target_module.bias is not None:
                        new_linear.bias[:current_width].copy_(target_module.bias)
                        nn.init.zeros_(new_linear.bias[current_width:])
                
                # æ›¿æ¢å±‚
                self._replace_layer_in_model(model, target_layer, new_linear)
                
                return {
                    'success': True,
                    'new_model': model,
                    'parameters_added': (new_width - current_width) * target_module.in_features + (new_width - current_width),
                    'expansion_type': 'enhanced_linear_width_expansion'
                }
            
            return {'success': False, 'reason': 'unsupported_layer_type', 'new_model': model}
            
        except Exception as e:
            return {'success': False, 'reason': str(e), 'new_model': model}
    
    def _replace_layer_in_model(self, model: nn.Module, layer_name: str, new_layer: nn.Module):
        """åœ¨æ¨¡å‹ä¸­æ›¿æ¢æŒ‡å®šå±‚"""
        
        # è§£æå±‚åç§°è·¯å¾„
        parts = layer_name.split('.')
        
        # å¯¼èˆªåˆ°çˆ¶æ¨¡å—
        parent = model
        for part in parts[:-1]:
            parent = getattr(parent, part)
        
        # è·å–åŸå±‚çš„è®¾å¤‡ä¿¡æ¯å¹¶è½¬ç§»æ–°å±‚ï¼ˆä½¿ç”¨å…±äº«å·¥å…·å‡½æ•°ï¼‰
        original_layer = getattr(parent, parts[-1])
        new_layer = move_module_to_device_like(new_layer, original_layer)
        logger.info(f"ğŸ”§ æ–°å±‚å·²é€šè¿‡å…±äº«å·¥å…·è½¬ç§»åˆ°è®¾å¤‡")
        
        # æ›¿æ¢æœ€åä¸€çº§çš„å±‚
        setattr(parent, parts[-1], new_layer)
    
    def _record_execution_result(self, analysis: Dict[str, Any], execution_result: Dict[str, Any]):
        """è®°å½•æ‰§è¡Œç»“æœç”¨äºå­¦ä¹ """
        
        record = {
            'timestamp': analysis.get('analysis_metadata', {}).get('current_epoch', 0),
            'analysis_summary': analysis.get('analysis_summary', {}),
            'execution_result': execution_result,
            'decisions_count': len(analysis.get('final_decisions', [])),
            'success': execution_result.get('executed', False) and execution_result.get('model_modified', False)
        }
        
        self.execution_history.append(record)
        
        # ä¿æŒå†å²è®°å½•å¤§å°
        if len(self.execution_history) > 100:
            self.execution_history = self.execution_history[-100:]
    
    def _format_for_compatibility(self, analysis: Dict[str, Any], execution_result: Dict[str, Any]) -> Dict[str, Any]:
        """æ ¼å¼åŒ–ç»“æœä»¥ä¿æŒä¸åŸç³»ç»Ÿçš„å…¼å®¹æ€§"""
        
        # æå–å…³é”®ä¿¡æ¯
        structural_analysis = analysis.get('analysis_summary', {}).get('structural_analysis', {})
        execution_plan = analysis.get('execution_plan', {})
        
        # æ¨¡æ‹ŸåŸæœ‰çš„è¿”å›æ ¼å¼
        return {
            'model_modified': execution_result.get('model_modified', False),
            'new_model': execution_result.get('new_model'),
            'parameters_added': self._calculate_parameters_added(execution_result),
            'morphogenesis_events': self._format_morphogenesis_events(analysis, execution_result),
            'morphogenesis_type': self._determine_morphogenesis_type(execution_result),
            'trigger_reasons': self._format_trigger_reasons(analysis),
            
            # æ–°å¢çš„æ™ºèƒ½åˆ†æä¿¡æ¯
            'intelligent_analysis': {
                'candidates_found': len(analysis.get('mutation_candidates', [])),
                'strategies_evaluated': len(analysis.get('mutation_strategies', [])),
                'final_decisions': len(analysis.get('final_decisions', [])),
                'execution_confidence': execution_plan.get('primary_mutation', {}).get('confidence', 0.0),
                'adaptive_thresholds': analysis.get('adaptive_thresholds', {}),
                'performance_situation': analysis.get('analysis_summary', {}).get('performance_situation', {}),
                'detailed_analysis_available': True
            }
        }
    
    def _calculate_parameters_added(self, execution_result: Dict[str, Any]) -> int:
        """è®¡ç®—å¢åŠ çš„å‚æ•°æ•°é‡"""
        
        if not execution_result.get('executed', False):
            return 0
        
        mutation_result = execution_result.get('mutation_result', {})
        return mutation_result.get('parameters_added', 0)
    
    def _format_morphogenesis_events(self, analysis: Dict[str, Any], execution_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ ¼å¼åŒ–å½¢æ€å‘ç”Ÿäº‹ä»¶"""
        
        events = []
        
        if execution_result.get('executed', False):
            mutation_applied = execution_result.get('mutation_applied', {})
            
            event = {
                'type': mutation_applied.get('mutation_type', 'unknown'),
                'target_layer': mutation_applied.get('target_layer', 'unknown'),
                'expected_improvement': mutation_applied.get('expected_improvement', 0.0),
                'confidence': mutation_applied.get('confidence', 0.0),
                'analysis_driven': True,
                'intelligent_selection': True
            }
            events.append(event)
        
        return events
    
    def _determine_morphogenesis_type(self, execution_result: Dict[str, Any]) -> str:
        """ç¡®å®šå½¢æ€å‘ç”Ÿç±»å‹"""
        
        if not execution_result.get('executed', False):
            return 'none'
        
        mutation_applied = execution_result.get('mutation_applied', {})
        mutation_type = mutation_applied.get('mutation_type', 'unknown')
        
        # æ˜ å°„åˆ°åŸæœ‰çš„ç±»å‹åç§°
        type_mapping = {
            'width_expansion': 'width_expansion',
            'depth_expansion': 'depth_expansion',
            'attention_enhancement': 'attention_enhancement',
            'residual_connection': 'structural_enhancement',
            'batch_norm_insertion': 'normalization_enhancement',
            'information_enhancement': 'information_enhancement',
            'channel_attention': 'attention_enhancement'
        }
        
        return type_mapping.get(mutation_type, 'intelligent_mutation')
    
    def _format_trigger_reasons(self, analysis: Dict[str, Any]) -> List[str]:
        """æ ¼å¼åŒ–è§¦å‘åŸå› """
        
        reasons = []
        
        performance_situation = analysis.get('analysis_summary', {}).get('performance_situation', {})
        situation_type = performance_situation.get('situation_type', 'unknown')
        
        if situation_type == 'performance_plateau':
            reasons.append('æ€§èƒ½åœæ»æ£€æµ‹')
        elif situation_type == 'high_saturation':
            reasons.append('é«˜å‡†ç¡®ç‡é¥±å’ŒçŠ¶æ€')
        elif situation_type == 'performance_decline':
            reasons.append('æ€§èƒ½ä¸‹é™è¶‹åŠ¿')
        
        structural_analysis = analysis.get('analysis_summary', {}).get('structural_analysis', {})
        bottlenecks_found = structural_analysis.get('bottlenecks_found', 0)
        
        if bottlenecks_found > 0:
            reasons.append(f'æ£€æµ‹åˆ°{bottlenecks_found}ä¸ªæ¶æ„ç“¶é¢ˆ')
        
        final_decisions = len(analysis.get('final_decisions', []))
        if final_decisions > 0:
            reasons.append(f'æ™ºèƒ½å¼•æ“æ¨è{final_decisions}ä¸ªå˜å¼‚ç­–ç•¥')
        
        if not reasons:
            reasons.append('æ™ºèƒ½åˆ†æé©±åŠ¨çš„å˜å¼‚å†³ç­–')
        
        return reasons
    
    def _log_intelligent_analysis_results(self, analysis: Dict[str, Any]):
        """è¯¦ç»†è®°å½•æ™ºèƒ½åˆ†æç»“æœ"""
        
        if not self.config.get('detailed_logging', False):
            return
        
        # æ€§èƒ½æ€åŠ¿
        perf_situation = analysis.get('analysis_summary', {}).get('performance_situation', {})
        logger.info(f"ğŸ“Š æ€§èƒ½æ€åŠ¿: {perf_situation.get('situation_type', 'unknown')} "
                   f"(é¥±å’Œåº¦: {perf_situation.get('saturation_ratio', 0):.2%})")
        
        # ç»“æ„åˆ†æ
        structural = analysis.get('analysis_summary', {}).get('structural_analysis', {})
        logger.info(f"ğŸ—ï¸  ç»“æ„åˆ†æ: æ£€æµ‹{structural.get('bottlenecks_found', 0)}ä¸ªç“¶é¢ˆ "
                   f"(å…±åˆ†æ{structural.get('total_layers_analyzed', 0)}å±‚)")
        
        # å€™é€‰å’Œç­–ç•¥
        candidates_count = len(analysis.get('mutation_candidates', []))
        strategies_count = len(analysis.get('mutation_strategies', []))
        decisions_count = len(analysis.get('final_decisions', []))
        
        logger.info(f"ğŸ¯ å†³ç­–æµæ°´çº¿: {candidates_count}ä¸ªå€™é€‰ç‚¹ â†’ {strategies_count}ä¸ªç­–ç•¥ â†’ {decisions_count}ä¸ªæœ€ç»ˆå†³ç­–")
        
        # åŠ¨æ€é˜ˆå€¼
        thresholds = analysis.get('adaptive_thresholds', {})
        logger.info(f"ğŸ“Š åŠ¨æ€é˜ˆå€¼: ç“¶é¢ˆæ£€æµ‹={thresholds.get('bottleneck_severity', 0):.3f}, "
                   f"å˜å¼‚ç½®ä¿¡åº¦={thresholds.get('mutation_confidence', 0):.3f}")
        
        # æ‰§è¡Œè®¡åˆ’
        execution_plan = analysis.get('execution_plan', {})
        if execution_plan.get('execute', False):
            primary = execution_plan.get('primary_mutation', {})
            logger.info(f"ğŸš€ æ‰§è¡Œè®¡åˆ’: {primary.get('mutation_type', 'unknown')} "
                       f"on {primary.get('target_layer', 'unknown')} "
                       f"(æœŸæœ›æ”¹è¿›: {primary.get('expected_improvement', 0):.3%})")
        else:
            logger.info(f"âŒ æœªæ‰§è¡Œå˜å¼‚: {execution_plan.get('reason', 'unknown')}")
    
    def _fallback_execution(self) -> Dict[str, Any]:
        """fallbackæ‰§è¡Œ"""
        
        return {
            'model_modified': False,
            'new_model': None,
            'parameters_added': 0,
            'morphogenesis_events': [],
            'morphogenesis_type': 'failed',
            'trigger_reasons': ['æ™ºèƒ½åˆ†æå¤±è´¥ï¼Œå›é€€æ¨¡å¼'],
            'intelligent_analysis': {
                'status': 'failed',
                'detailed_analysis_available': False
            }
        }
    
    def get_analysis_statistics(self) -> Dict[str, Any]:
        """è·å–åˆ†æç»Ÿè®¡ä¿¡æ¯"""
        
        if not self.execution_history:
            return {'total_analyses': 0, 'success_rate': 0.0}
        
        total_analyses = len(self.execution_history)
        successful_analyses = sum(1 for record in self.execution_history if record['success'])
        success_rate = successful_analyses / total_analyses
        
        recent_decisions = [record['decisions_count'] for record in self.execution_history[-10:]]
        avg_decisions = np.mean(recent_decisions) if recent_decisions else 0.0
        
        return {
            'total_analyses': total_analyses,
            'success_rate': success_rate,
            'total_mutations_executed': sum(record.get('mutations_executed', 0) for record in self.execution_history),
            'total_parameters_added': sum(record.get('parameters_added', 0) for record in self.execution_history)
        }
    
    def _execute_serial_division(self, model: nn.Module, target_layer: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œä¸²è¡Œåˆ†è£‚å˜å¼‚ - å°†ä¸€ä¸ªå±‚åˆ†è§£ä¸ºå¤šä¸ªä¸²è¡Œè¿æ¥çš„å°å±‚"""
        
        try:
            logger.info(f"ğŸ”§ æ‰§è¡Œä¸²è¡Œåˆ†è£‚: {target_layer}")
            
            # æ‰¾åˆ°ç›®æ ‡å±‚
            target_module = None
            for name, module in model.named_modules():
                if name == target_layer:
                    target_module = module
                    break
            
            if target_module is None:
                return {'success': False, 'reason': 'target_layer_not_found', 'new_model': model}
            
            # åˆ›å»ºåˆ†è£‚åçš„ä¸²è¡Œç»“æ„
            if isinstance(target_module, nn.Linear):
                in_features = target_module.in_features
                out_features = target_module.out_features
                # ç¡®ä¿hidden_sizeåˆç†ï¼Œå¹¶ä¸”ä¸è¶…è¿‡åŸå§‹ç»´åº¦
                hidden_size = max(min(in_features, out_features) // 2, 16)  # è‡³å°‘16ä¸ªç¥ç»å…ƒ
                hidden_size = min(hidden_size, min(in_features, out_features), 128)  # ä¸è¶…è¿‡åŸå§‹ç»´åº¦å’Œ128
                
                logger.info(f"ğŸ”§ ä¸²è¡Œåˆ†è£‚å‚æ•°: {in_features} -> {hidden_size} -> {out_features}")
                
                # ä¸²è¡Œåˆ†è£‚: Linear -> ReLU -> Linear
                serial_layers = nn.Sequential(
                    nn.Linear(in_features, hidden_size),
                    nn.ReLU(),
                    nn.Linear(hidden_size, out_features)
                )
                
                # ä½¿ç”¨ç½‘ç»œå˜æ¢ä¿æŒåŠŸèƒ½ç­‰ä»·æ€§
                with torch.no_grad():
                    # ç¬¬ä¸€å±‚ï¼šä»è¾“å…¥åˆ°ä¸­é—´å±‚çš„æŠ•å½±
                    # ä½¿ç”¨SVDåˆ†è§£æˆ–è€…ç®€å•çš„éšæœºåˆå§‹åŒ–
                    nn.init.xavier_normal_(serial_layers[0].weight.data, gain=0.5)
                    if serial_layers[0].bias is not None:
                        nn.init.zeros_(serial_layers[0].bias.data)
                    
                    # ç¬¬äºŒå±‚ï¼šä»ä¸­é—´å±‚åˆ°è¾“å‡ºçš„é‡å»º
                    # ä½¿ç”¨æ›´å°çš„åˆå§‹åŒ–ä»¥ä¿æŒç¨³å®šæ€§
                    nn.init.xavier_normal_(serial_layers[2].weight.data, gain=0.5)
                    if serial_layers[2].bias is not None:
                        # å¤åˆ¶åŸå§‹åç½®ä½œä¸ºèµ·ç‚¹
                        if target_module.bias is not None:
                            serial_layers[2].bias.data.copy_(target_module.bias.data)
                        else:
                            nn.init.zeros_(serial_layers[2].bias.data)
                
                # æ›¿æ¢åŸæ¨¡å—
                self._replace_module(model, target_layer, serial_layers)
                
                new_params = hidden_size * in_features + hidden_size + hidden_size * out_features + out_features
                original_params = in_features * out_features + out_features
                
                return {
                    'success': True,
                    'new_model': model,
                    'parameters_added': new_params - original_params,
                    'mutation_type': 'serial_division',
                    'details': f'åˆ†è£‚ä¸º {in_features}->{hidden_size}->{out_features}'
                }
                
            elif isinstance(target_module, nn.Conv2d):
                # å·ç§¯å±‚çš„ä¸²è¡Œåˆ†è£‚
                in_channels = target_module.in_channels
                out_channels = target_module.out_channels
                # ç¡®ä¿hidden_channelsåˆç†
                hidden_channels = max(min(in_channels, out_channels) // 2, 8)  # è‡³å°‘8ä¸ªé€šé“
                hidden_channels = min(hidden_channels, min(in_channels, out_channels), 64)  # ä¸è¶…è¿‡åŸå§‹é€šé“æ•°å’Œ64
                
                logger.info(f"ğŸ”§ å·ç§¯ä¸²è¡Œåˆ†è£‚å‚æ•°: {in_channels} -> {hidden_channels} -> {out_channels}")
                
                # 1x1å·ç§¯ä¸²è¡Œåˆ†è£‚
                serial_layers = nn.Sequential(
                    nn.Conv2d(in_channels, hidden_channels, 1),
                    nn.ReLU(),
                    nn.Conv2d(hidden_channels, out_channels, target_module.kernel_size, 
                             padding=target_module.padding, stride=target_module.stride)
                )
                
                # æƒé‡åˆå§‹åŒ–
                with torch.no_grad():
                    nn.init.xavier_normal_(serial_layers[0].weight.data, gain=0.5)
                    nn.init.xavier_normal_(serial_layers[2].weight.data, gain=0.5)
                    
                    # å¤åˆ¶åç½®å¦‚æœå­˜åœ¨
                    if target_module.bias is not None and serial_layers[2].bias is not None:
                        serial_layers[2].bias.data.copy_(target_module.bias.data)
                
                self._replace_module(model, target_layer, serial_layers)
                
                return {
                    'success': True,
                    'new_model': model,
                    'parameters_added': hidden_channels * in_channels + hidden_channels * out_channels * target_module.kernel_size[0] * target_module.kernel_size[1],
                    'mutation_type': 'serial_division',
                    'details': f'å·ç§¯ä¸²è¡Œåˆ†è£‚: {in_channels}->{hidden_channels}->{out_channels}'
                }
            
            else:
                return {'success': False, 'reason': 'unsupported_layer_type', 'new_model': model}
                
        except Exception as e:
            logger.error(f"âŒ ä¸²è¡Œåˆ†è£‚å¤±è´¥: {e}")
            return {'success': False, 'reason': str(e), 'new_model': model}
    
    def _execute_parallel_division(self, model: nn.Module, target_layer: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå¹¶è¡Œåˆ†è£‚å˜å¼‚ - å°†ä¸€ä¸ªå±‚åˆ†è§£ä¸ºå¤šä¸ªå¹¶è¡Œçš„å­å±‚å¹¶åˆå¹¶"""
        
        try:
            logger.info(f"ğŸ”§ æ‰§è¡Œå¹¶è¡Œåˆ†è£‚: {target_layer}")
            
            # æ‰¾åˆ°ç›®æ ‡å±‚
            target_module = None
            for name, module in model.named_modules():
                if name == target_layer:
                    target_module = module
                    break
            
            if target_module is None:
                return {'success': False, 'reason': 'target_layer_not_found', 'new_model': model}
            
            # åˆ›å»ºå¹¶è¡Œåˆ†è£‚ç»“æ„
            if isinstance(target_module, nn.Linear):
                in_features = target_module.in_features
                out_features = target_module.out_features
                
                # å¹¶è¡Œåˆ†è£‚ï¼šä¸¤ä¸ªè¾ƒå°çš„Linearå±‚å¹¶è¡Œå¤„ç†ï¼Œç„¶ååˆå¹¶
                branch1 = nn.Linear(in_features, out_features // 2)
                branch2 = nn.Linear(in_features, out_features - out_features // 2)
                
                class ParallelLinear(nn.Module):
                    def __init__(self, branch1, branch2):
                        super().__init__()
                        self.branch1 = branch1
                        self.branch2 = branch2
                    
                    def forward(self, x):
                        out1 = self.branch1(x)
                        out2 = self.branch2(x)
                        return torch.cat([out1, out2], dim=-1)
                
                parallel_module = ParallelLinear(branch1, branch2)
                
                # æƒé‡åˆå§‹åŒ– - ä¿æŒåŸå§‹åŠŸèƒ½çš„è¿‘ä¼¼
                with torch.no_grad():
                    branch1.weight.data = target_module.weight.data[:out_features//2, :] * 0.7
                    branch2.weight.data = target_module.weight.data[out_features//2:, :] * 0.7
                    
                    if target_module.bias is not None:
                        branch1.bias.data = target_module.bias.data[:out_features//2] * 0.7
                        branch2.bias.data = target_module.bias.data[out_features//2:] * 0.7
                
                self._replace_module(model, target_layer, parallel_module)
                
                return {
                    'success': True,
                    'new_model': model,
                    'parameters_added': 0,  # å‚æ•°æ€»æ•°ä¸å˜ï¼Œä½†ç»“æ„å¹¶è¡ŒåŒ–
                    'mutation_type': 'parallel_division',
                    'details': f'å¹¶è¡Œåˆ†è£‚ä¸º {out_features//2} + {out_features - out_features//2}'
                }
                
            elif isinstance(target_module, nn.Conv2d):
                # å·ç§¯å±‚å¹¶è¡Œåˆ†è£‚
                in_channels = target_module.in_channels
                out_channels = target_module.out_channels
                
                branch1 = nn.Conv2d(in_channels, out_channels // 2, target_module.kernel_size,
                                   padding=target_module.padding, stride=target_module.stride)
                branch2 = nn.Conv2d(in_channels, out_channels - out_channels // 2, target_module.kernel_size,
                                   padding=target_module.padding, stride=target_module.stride)
                
                class ParallelConv(nn.Module):
                    def __init__(self, branch1, branch2):
                        super().__init__()
                        self.branch1 = branch1
                        self.branch2 = branch2
                    
                    def forward(self, x):
                        out1 = self.branch1(x)
                        out2 = self.branch2(x)
                        return torch.cat([out1, out2], dim=1)
                
                parallel_module = ParallelConv(branch1, branch2)
                
                with torch.no_grad():
                    branch1.weight.data = target_module.weight.data[:out_channels//2, :, :, :] * 0.7
                    branch2.weight.data = target_module.weight.data[out_channels//2:, :, :, :] * 0.7
                
                self._replace_module(model, target_layer, parallel_module)
                
                return {
                    'success': True,
                    'new_model': model,
                    'parameters_added': 0,
                    'mutation_type': 'parallel_division',
                    'details': f'å·ç§¯å¹¶è¡Œåˆ†è£‚: {out_channels//2} + {out_channels - out_channels//2}'
                }
                
            else:
                return {'success': False, 'reason': 'unsupported_layer_type', 'new_model': model}
                
        except Exception as e:
            logger.error(f"âŒ å¹¶è¡Œåˆ†è£‚å¤±è´¥: {e}")
            return {'success': False, 'reason': str(e), 'new_model': model}
    
    def _execute_information_enhancement(self, model: nn.Module, target_layer: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œä¿¡æ¯å¢å¼ºå˜å¼‚"""
        # ç®€å•å®ç° - æ·»åŠ è·³è·ƒè¿æ¥å’Œå½’ä¸€åŒ–
        return self._execute_residual_connection(model, target_layer, context)
    
    def _execute_channel_attention(self, model: nn.Module, target_layer: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œé€šé“æ³¨æ„åŠ›å˜å¼‚"""
        # ç®€å•å®ç° - æ·»åŠ Squeeze-and-Excitationæ¨¡å—
        return {'success': True, 'new_model': model, 'parameters_added': 0, 'mutation_type': 'channel_attention'}
    
    def _execute_layer_norm(self, model: nn.Module, target_layer: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå±‚å½’ä¸€åŒ–å˜å¼‚"""
        return {'success': True, 'new_model': model, 'parameters_added': 0, 'mutation_type': 'layer_norm'}
    
    def _replace_module(self, model: nn.Module, module_name: str, new_module: nn.Module):
        """æ›¿æ¢æ¨¡å‹ä¸­çš„æŒ‡å®šæ¨¡å—"""
        
        # è·å–åŸæ¨¡å—çš„è®¾å¤‡ä¿¡æ¯
        original_module = None
        if '.' in module_name:
            parts = module_name.split('.')
            parent = model
            for part in parts[:-1]:
                parent = getattr(parent, part)
            original_module = getattr(parent, parts[-1])
        else:
            original_module = getattr(model, module_name)
        
        # å°†æ–°æ¨¡å—ç§»åˆ°ä¸åŸæ¨¡å—ç›¸åŒçš„è®¾å¤‡ï¼ˆä½¿ç”¨å…±äº«å·¥å…·å‡½æ•°ï¼‰
        if original_module is not None:
            new_module = move_module_to_device_like(new_module, original_module)
            logger.info(f"ğŸ”§ æ–°æ¨¡å—å·²é€šè¿‡å…±äº«å·¥å…·è½¬ç§»åˆ°è®¾å¤‡")
        
        # è§£ææ¨¡å—è·¯å¾„å¹¶æ›¿æ¢
        if '.' in module_name:
            # åµŒå¥—æ¨¡å—
            parts = module_name.split('.')
            parent = model
            for part in parts[:-1]:
                parent = getattr(parent, part)
            setattr(parent, parts[-1], new_module)
        else:
            # é¡¶çº§æ¨¡å—
            setattr(model, module_name, new_module)
    
    def set_aggressive_mode(self):
        """è®¾ç½®ç§¯ææ¨¡å¼ä»¥è§£å†³è¿‡äºä¿å®ˆçš„é—®é¢˜"""
        
        # è®¾ç½®æ”¶æ•›ç›‘æ§ä¸ºç§¯ææ¨¡å¼
        if hasattr(self.convergence_monitor, 'set_mode'):
            self.convergence_monitor.set_mode('aggressive')
        
        # è®¾ç½®è´å¶æ–¯å¼•æ“ä¸ºç§¯ææ¨¡å¼
        if hasattr(self.bayesian_engine, 'set_aggressive_mode'):
            self.bayesian_engine.set_aggressive_mode()
        
        # æ›´æ–°é…ç½®
        self.config.update({
            'aggressive_mutation_mode': True,
            'prefer_bayesian_decisions': True,
            'enable_bayesian_analysis': True
        })
        
        logger.info("ğŸš€ æ™ºèƒ½DNMæ ¸å¿ƒå·²è®¾ç½®ä¸ºç§¯ææ¨¡å¼")
    
    def set_conservative_mode(self):
        """è®¾ç½®ä¿å®ˆæ¨¡å¼"""
        
        # è®¾ç½®æ”¶æ•›ç›‘æ§ä¸ºä¿å®ˆæ¨¡å¼
        if hasattr(self.convergence_monitor, 'set_mode'):
            self.convergence_monitor.set_mode('conservative')
        
        # è®¾ç½®è´å¶æ–¯å¼•æ“ä¸ºä¿å®ˆæ¨¡å¼
        if hasattr(self.bayesian_engine, 'set_conservative_mode'):
            self.bayesian_engine.set_conservative_mode()
        
        # æ›´æ–°é…ç½®
        self.config.update({
            'aggressive_mutation_mode': False,
            'prefer_bayesian_decisions': False,
        })
        
        logger.info("ğŸ›¡ï¸ æ™ºèƒ½DNMæ ¸å¿ƒå·²è®¾ç½®ä¸ºä¿å®ˆæ¨¡å¼")
    
    def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        
        status = {
            'config': self.config,
            'execution_history_length': len(self.execution_history)
        }
        
        # æ·»åŠ ç»„ä»¶çŠ¶æ€
        if hasattr(self.convergence_monitor, 'get_status_summary'):
            status['convergence_monitor'] = self.convergence_monitor.get_status_summary()
        
        if hasattr(self.bayesian_engine, 'get_analysis_summary'):
            status['bayesian_engine'] = self.bayesian_engine.get_analysis_summary()
        
        return status
    
    # ç§»é™¤é‡å¤çš„è½¬æ¢æ–¹æ³•ï¼Œç°åœ¨ä½¿ç”¨schema_transformer
    
    def _convert_decisions_to_candidates(self, decisions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å°†è´å¶æ–¯å†³ç­–è½¬æ¢ä¸ºå€™é€‰ç‚¹æ ¼å¼"""
        
        candidates = []
        for decision in decisions:
            candidate = {
                'layer_name': decision.get('layer_name', ''),
                'layer_type': 'bayesian_identified',
                'selection_reasons': ['bayesian_optimization'],
                'bottleneck_types': ['bayesian_detected'],
                'improvement_potential': decision.get('expected_improvement', 0.0),
                'priority_score': decision.get('expected_utility', 0.0),
                'recommended_mutations': [decision.get('mutation_type', '')],
                'bayesian_metrics': {
                    'success_probability': decision.get('success_probability', 0.5),
                    'decision_confidence': decision.get('decision_confidence', 0.5),
                    'expected_utility': decision.get('expected_utility', 0.0)
                }
            }
            candidates.append(candidate)
        
        return candidates
    
    def _convert_decisions_to_strategies(self, decisions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å°†è´å¶æ–¯å†³ç­–è½¬æ¢ä¸ºç­–ç•¥æ ¼å¼"""
        
        strategies = []
        for decision in decisions:
            strategy = {
                'target_layer': decision.get('layer_name', ''),
                'mutation_type': decision.get('mutation_type', ''),
                'rationale': {
                    'selection_method': 'bayesian_inference',
                    'success_probability': decision.get('success_probability', 0.5),
                    'expected_improvement': decision.get('expected_improvement', 0.0),
                    'decision_confidence': decision.get('decision_confidence', 0.5)
                },
                'expected_outcome': {
                    'expected_accuracy_improvement': decision.get('expected_improvement', 0.0),
                    'confidence_level': decision.get('decision_confidence', 0.5),
                    'success_probability': decision.get('success_probability', 0.5)
                },
                'risk_assessment': {
                    'overall_risk': 1.0 - decision.get('success_probability', 0.5),
                    'risk_factors': [],
                    'value_at_risk': decision.get('risk_metrics', {}).get('value_at_risk', 0.0),
                    'expected_shortfall': decision.get('risk_metrics', {}).get('expected_shortfall', 0.0)
                },
                'bayesian_reasoning': decision.get('rationale', 'Bayesian optimization recommended'),
                'implementation_priority': decision.get('expected_utility', 0.0)
            }
            strategies.append(strategy)
        
        return strategies
    
    def update_bayesian_outcome(self, 
                              mutation_type: str,
                              layer_name: str,
                              success: bool,
                              performance_change: float,
                              context: Dict[str, Any]):
        """æ›´æ–°è´å¶æ–¯å¼•æ“çš„å˜å¼‚ç»“æœï¼Œç”¨äºåœ¨çº¿å­¦ä¹ """
        
        if hasattr(self, 'bayesian_engine') and self.bayesian_engine:
            self.bayesian_engine.update_mutation_outcome(
                mutation_type=mutation_type,
                layer_name=layer_name,
                success=success,
                performance_change=performance_change,
                context=context
            )
            logger.info(f"ğŸ“Š å·²æ›´æ–°è´å¶æ–¯å­¦ä¹ : {mutation_type} @ {layer_name} -> {'âœ…æˆåŠŸ' if success else 'âŒå¤±è´¥'}")
        
        # è®°å½•æ‰§è¡Œå†å²
        outcome_record = {
            'timestamp': context.get('epoch', 0),
            'mutation_type': mutation_type,
            'layer_name': layer_name,
            'success': success,
            'performance_change': performance_change,
            'engine_used': 'bayesian' if self.config.get('prefer_bayesian_decisions') else 'traditional'
        }
        self.execution_history.append(outcome_record)
    
    def get_bayesian_insights(self) -> Dict[str, Any]:
        """è·å–è´å¶æ–¯å¼•æ“çš„æ´å¯Ÿä¿¡æ¯"""
        
        if not hasattr(self, 'bayesian_engine') or not self.bayesian_engine:
            return {'status': 'bayesian_engine_not_available'}
        
        insights = {
            'mutation_history_length': len(self.bayesian_engine.mutation_history),
            'performance_history_length': len(self.bayesian_engine.performance_history),
            'architecture_features_tracked': len(self.bayesian_engine.architecture_features),
            'current_priors': self.bayesian_engine.mutation_priors.copy(),
            'dynamic_thresholds': self.bayesian_engine.dynamic_thresholds.copy(),
            'utility_parameters': self.bayesian_engine.utility_params.copy(),
            'recent_mutations': list(self.bayesian_engine.mutation_history)[-5:] if self.bayesian_engine.mutation_history else []
        }
        
        return insights
    
    def adjust_bayesian_parameters(self, parameter_updates: Dict[str, Any]):
        """è°ƒæ•´è´å¶æ–¯å¼•æ“å‚æ•°"""
        
        if not hasattr(self, 'bayesian_engine') or not self.bayesian_engine:
            logger.warning("âš ï¸ è´å¶æ–¯å¼•æ“ä¸å¯ç”¨ï¼Œæ— æ³•è°ƒæ•´å‚æ•°")
            return
        
        # æ›´æ–°åŠ¨æ€é˜ˆå€¼
        if 'thresholds' in parameter_updates:
            for key, value in parameter_updates['thresholds'].items():
                if key in self.bayesian_engine.dynamic_thresholds:
                    self.bayesian_engine.dynamic_thresholds[key] = value
                    logger.info(f"ğŸ“Š æ›´æ–°è´å¶æ–¯é˜ˆå€¼: {key} = {value}")
        
        # æ›´æ–°æ•ˆç”¨å‚æ•°
        if 'utility' in parameter_updates:
            for key, value in parameter_updates['utility'].items():
                if key in self.bayesian_engine.utility_params:
                    self.bayesian_engine.utility_params[key] = value
                    logger.info(f"ğŸ“Š æ›´æ–°æ•ˆç”¨å‚æ•°: {key} = {value}")
        
        # æ›´æ–°å…ˆéªŒåˆ†å¸ƒ
        if 'priors' in parameter_updates:
            for mutation_type, prior_params in parameter_updates['priors'].items():
                if mutation_type in self.bayesian_engine.mutation_priors:
                    self.bayesian_engine.mutation_priors[mutation_type].update(prior_params)
                    logger.info(f"ğŸ“Š æ›´æ–°å…ˆéªŒåˆ†å¸ƒ: {mutation_type} = {prior_params}")
    
    def enable_aggressive_bayesian_mode(self):
        """å¯ç”¨ç§¯æçš„è´å¶æ–¯æ¨¡å¼ï¼ˆæ›´å®¹æ˜“è§¦å‘å˜å¼‚ï¼‰"""
        
        if hasattr(self, 'bayesian_engine') and self.bayesian_engine:
            # é™ä½é˜ˆå€¼ï¼Œæé«˜æ¢ç´¢æ€§
            aggressive_updates = {
                'thresholds': {
                    'min_expected_improvement': 0.001,   # æ›´ä½çš„æœŸæœ›æ”¹è¿›é˜ˆå€¼
                    'confidence_threshold': 0.2,        # æ›´ä½çš„ç½®ä¿¡åº¦é˜ˆå€¼
                    'exploration_threshold': 0.15       # æ›´ç§¯æçš„æ¢ç´¢
                },
                'utility': {
                    'risk_aversion': 0.1,               # é™ä½é£é™©åŒæ¶
                    'exploration_bonus': 0.15           # å¢åŠ æ¢ç´¢å¥–åŠ±
                }
            }
            
            self.adjust_bayesian_parameters(aggressive_updates)
            logger.info("ğŸš€ å·²å¯ç”¨ç§¯æè´å¶æ–¯æ¨¡å¼")
        else:
            logger.warning("âš ï¸ è´å¶æ–¯å¼•æ“ä¸å¯ç”¨ï¼Œæ— æ³•å¯ç”¨ç§¯ææ¨¡å¼")