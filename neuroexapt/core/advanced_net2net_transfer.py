"""
å…ˆè¿›çš„Net2Netå‚æ•°è¿ç§»ç³»ç»Ÿ
æ”¯æŒå¤šç§æ¶æ„å˜å¼‚çš„å¹³æ»‘å‚æ•°è¿ç§»ï¼Œä¿è¯åŠŸèƒ½ç­‰ä»·æ€§å’Œè®­ç»ƒç¨³å®šæ€§
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Any, Union
import logging
import copy
import math

from .intelligent_mutation_planner import MutationType, MutationPlan

logger = logging.getLogger(__name__)


class Net2NetTransferMethod:
    """Net2Netè¿ç§»æ–¹æ³•åŸºç±»"""
    
    def __init__(self, preserve_function: bool = True):
        self.preserve_function = preserve_function
        
    def transfer(self, 
                old_module: nn.Module,
                new_module: nn.Module,
                mutation_parameters: Dict[str, Any]) -> nn.Module:
        """æ‰§è¡Œå‚æ•°è¿ç§»"""
        raise NotImplementedError
    
    def verify_transfer(self, 
                       old_module: nn.Module,
                       new_module: nn.Module,
                       test_input: torch.Tensor) -> bool:
        """éªŒè¯è¿ç§»æ˜¯å¦ä¿æŒåŠŸèƒ½ç­‰ä»·æ€§"""
        if not self.preserve_function:
            return True
            
        try:
            old_module.eval()
            new_module.eval()
            
            with torch.no_grad():
                old_output = old_module(test_input)
                new_output = new_module(test_input)
                
                # æ£€æŸ¥è¾“å‡ºæ˜¯å¦ç›¸è¿‘
                if isinstance(old_output, torch.Tensor) and isinstance(new_output, torch.Tensor):
                    diff = torch.abs(old_output - new_output).max().item()
                    return diff < 1e-5
                    
        except Exception as e:
            logger.warning(f"Transfer verification failed: {e}")
            
        return False


class WeightExpansionTransfer(Net2NetTransferMethod):
    """æƒé‡æ‰©å±•è¿ç§»ï¼šç”¨äºå®½åº¦æ‰©å±•å˜å¼‚"""
    
    def transfer(self, 
                old_module: nn.Module,
                new_module: nn.Module,
                mutation_parameters: Dict[str, Any]) -> nn.Module:
        """
        æƒé‡æ‰©å±•è¿ç§»
        
        å¯¹äºçº¿æ€§å±‚å’Œå·ç§¯å±‚çš„é€šé“æ‰©å±•ï¼Œé€šè¿‡å¤åˆ¶å’Œéšæœºåˆå§‹åŒ–å®ç°
        """
        expansion_factor = mutation_parameters.get('expansion_factor', 1.5)
        initialization = mutation_parameters.get('initialization', 'kaiming_normal')
        
        if isinstance(old_module, nn.Linear):
            return self._expand_linear_layer(old_module, new_module, expansion_factor, initialization)
        elif isinstance(old_module, (nn.Conv1d, nn.Conv2d, nn.Conv3d)):
            return self._expand_conv_layer(old_module, new_module, expansion_factor, initialization)
        else:
            logger.warning(f"Unsupported module type for weight expansion: {type(old_module)}")
            return new_module
    
    def _expand_linear_layer(self,
                           old_linear: nn.Linear,
                           new_linear: nn.Linear,
                           expansion_factor: float,
                           initialization: str) -> nn.Linear:
        """æ‰©å±•çº¿æ€§å±‚"""
        old_out_features = old_linear.out_features
        new_out_features = new_linear.out_features
        
        with torch.no_grad():
            # å¤åˆ¶åŸæœ‰æƒé‡
            new_linear.weight.data[:old_out_features] = old_linear.weight.data
            
            # åˆå§‹åŒ–æ–°å¢æƒé‡
            if initialization == 'kaiming_normal':
                nn.init.kaiming_normal_(new_linear.weight.data[old_out_features:])
            elif initialization == 'xavier_normal':
                nn.init.xavier_normal_(new_linear.weight.data[old_out_features:])
            else:
                # å°éšæœºå€¼åˆå§‹åŒ–
                new_linear.weight.data[old_out_features:].normal_(0, 0.01)
            
            # å¤„ç†åç½®
            if old_linear.bias is not None and new_linear.bias is not None:
                new_linear.bias.data[:old_out_features] = old_linear.bias.data
                new_linear.bias.data[old_out_features:].zero_()
        
        return new_linear
    
    def _expand_conv_layer(self,
                          old_conv: nn.Module,
                          new_conv: nn.Module,
                          expansion_factor: float,
                          initialization: str) -> nn.Module:
        """æ‰©å±•å·ç§¯å±‚"""
        old_out_channels = old_conv.out_channels
        new_out_channels = new_conv.out_channels
        
        with torch.no_grad():
            # å¤åˆ¶åŸæœ‰æƒé‡
            new_conv.weight.data[:old_out_channels] = old_conv.weight.data
            
            # åˆå§‹åŒ–æ–°å¢é€šé“
            if initialization == 'kaiming_normal':
                nn.init.kaiming_normal_(new_conv.weight.data[old_out_channels:])
            else:
                new_conv.weight.data[old_out_channels:].normal_(0, 0.01)
            
            # å¤„ç†åç½®
            if old_conv.bias is not None and new_conv.bias is not None:
                new_conv.bias.data[:old_out_channels] = old_conv.bias.data
                new_conv.bias.data[old_out_channels:].zero_()
        
        return new_conv


class IdentityInitializationTransfer(Net2NetTransferMethod):
    """æ’ç­‰åˆå§‹åŒ–è¿ç§»ï¼šç”¨äºæ·»åŠ å±‚çš„å˜å¼‚"""
    
    def transfer(self,
                old_module: nn.Module,
                new_module: nn.Module,
                mutation_parameters: Dict[str, Any]) -> nn.Module:
        """
        æ’ç­‰åˆå§‹åŒ–ï¼šæ–°æ·»åŠ çš„å±‚åˆå§‹åŒ–ä¸ºæ’ç­‰å˜æ¢
        """
        mutation_type = mutation_parameters.get('mutation_type', '')
        
        if 'residual' in mutation_type:
            return self._initialize_residual_connection(new_module, mutation_parameters)
        elif 'attention' in mutation_type:
            return self._initialize_attention_layer(new_module, mutation_parameters)
        elif 'normalization' in mutation_type:
            return self._initialize_normalization_layer(new_module, mutation_parameters)
        else:
            return self._initialize_general_identity(new_module)
    
    def _initialize_residual_connection(self,
                                      new_module: nn.Module,
                                      parameters: Dict[str, Any]) -> nn.Module:
        """åˆå§‹åŒ–æ®‹å·®è¿æ¥ä¸ºæ’ç­‰æ˜ å°„"""
        residual_type = parameters.get('residual_type', 'additive')
        
        # å¯¹äºæ®‹å·®è¿æ¥ï¼Œç¡®ä¿åˆå§‹æ—¶è¾“å‡ºç­‰äºè¾“å…¥
        with torch.no_grad():
            for name, module in new_module.named_modules():
                if isinstance(module, nn.Linear):
                    # æœ€åä¸€ä¸ªçº¿æ€§å±‚åˆå§‹åŒ–ä¸ºé›¶ï¼Œå…¶ä»–å±‚æ­£å¸¸åˆå§‹åŒ–
                    if 'projection' in name or 'output' in name:
                        module.weight.data.zero_()
                        if module.bias is not None:
                            module.bias.data.zero_()
                elif isinstance(module, (nn.Conv1d, nn.Conv2d, nn.Conv3d)):
                    if 'projection' in name or 'output' in name:
                        module.weight.data.zero_()
                        if module.bias is not None:
                            module.bias.data.zero_()
        
        return new_module
    
    def _initialize_attention_layer(self,
                                  new_module: nn.Module,
                                  parameters: Dict[str, Any]) -> nn.Module:
        """åˆå§‹åŒ–æ³¨æ„åŠ›å±‚ä¸ºæ’ç­‰æ˜ å°„"""
        with torch.no_grad():
            for name, module in new_module.named_modules():
                if isinstance(module, nn.Linear):
                    if 'output' in name or 'projection' in name:
                        # è¾“å‡ºå±‚åˆå§‹åŒ–ä¸ºé›¶
                        module.weight.data.zero_()
                        if module.bias is not None:
                            module.bias.data.zero_()
                    elif 'query' in name or 'key' in name or 'value' in name:
                        # Q, K, VçŸ©é˜µæ­£å¸¸åˆå§‹åŒ–
                        nn.init.xavier_uniform_(module.weight.data)
                        if module.bias is not None:
                            module.bias.data.zero_()
        
        return new_module
    
    def _initialize_normalization_layer(self,
                                      new_module: nn.Module,
                                      parameters: Dict[str, Any]) -> nn.Module:
        """åˆå§‹åŒ–è§„èŒƒåŒ–å±‚"""
        norm_type = parameters.get('norm_type', 'batch_norm')
        
        with torch.no_grad():
            for module in new_module.modules():
                if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
                    # æ‰¹è§„èŒƒåŒ–ï¼šæƒé‡=1ï¼Œåç½®=0
                    module.weight.data.fill_(1.0)
                    module.bias.data.zero_()
                    module.running_mean.zero_()
                    module.running_var.fill_(1.0)
                elif isinstance(module, nn.LayerNorm):
                    # å±‚è§„èŒƒåŒ–ï¼šæƒé‡=1ï¼Œåç½®=0
                    module.weight.data.fill_(1.0)
                    module.bias.data.zero_()
        
        return new_module
    
    def _initialize_general_identity(self, new_module: nn.Module) -> nn.Module:
        """é€šç”¨æ’ç­‰åˆå§‹åŒ–"""
        with torch.no_grad():
            for module in new_module.modules():
                if isinstance(module, nn.Linear):
                    # å°è¯•åˆå§‹åŒ–ä¸ºå•ä½çŸ©é˜µï¼ˆå¦‚æœå¯èƒ½ï¼‰
                    if module.in_features == module.out_features:
                        nn.init.eye_(module.weight.data)
                    else:
                        nn.init.xavier_uniform_(module.weight.data)
                    if module.bias is not None:
                        module.bias.data.zero_()
        
        return new_module


class FeatureSelectionTransfer(Net2NetTransferMethod):
    """ç‰¹å¾é€‰æ‹©è¿ç§»ï¼šç”¨äºé™ç»´å’Œå‰ªæå˜å¼‚"""
    
    def transfer(self,
                old_module: nn.Module,
                new_module: nn.Module,
                mutation_parameters: Dict[str, Any]) -> nn.Module:
        """
        ç‰¹å¾é€‰æ‹©è¿ç§»ï¼šä¿ç•™æœ€é‡è¦çš„ç‰¹å¾
        """
        selection_method = mutation_parameters.get('selection_method', 'importance_based')
        reduction_factor = mutation_parameters.get('reduction_factor', 0.8)
        
        if isinstance(old_module, nn.Linear):
            return self._select_linear_features(old_module, new_module, reduction_factor, selection_method)
        elif isinstance(old_module, (nn.Conv1d, nn.Conv2d, nn.Conv3d)):
            return self._select_conv_features(old_module, new_module, reduction_factor, selection_method)
        else:
            logger.warning(f"Unsupported module type for feature selection: {type(old_module)}")
            return new_module
    
    def _select_linear_features(self,
                              old_linear: nn.Linear,
                              new_linear: nn.Linear,
                              reduction_factor: float,
                              selection_method: str) -> nn.Linear:
        """é€‰æ‹©çº¿æ€§å±‚çš„é‡è¦ç‰¹å¾"""
        old_out_features = old_linear.out_features
        new_out_features = new_linear.out_features
        
        # è®¡ç®—ç‰¹å¾é‡è¦æ€§
        importance_scores = self._compute_feature_importance(old_linear.weight.data, selection_method)
        
        # é€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾
        _, selected_indices = torch.topk(importance_scores, new_out_features)
        selected_indices = selected_indices.sort()[0]  # ä¿æŒåŸå§‹é¡ºåº
        
        with torch.no_grad():
            # å¤åˆ¶é€‰ä¸­çš„æƒé‡
            new_linear.weight.data = old_linear.weight.data[selected_indices]
            
            # å¤„ç†åç½®
            if old_linear.bias is not None and new_linear.bias is not None:
                new_linear.bias.data = old_linear.bias.data[selected_indices]
        
        return new_linear
    
    def _select_conv_features(self,
                            old_conv: nn.Module,
                            new_conv: nn.Module,
                            reduction_factor: float,
                            selection_method: str) -> nn.Module:
        """é€‰æ‹©å·ç§¯å±‚çš„é‡è¦é€šé“"""
        old_out_channels = old_conv.out_channels
        new_out_channels = new_conv.out_channels
        
        # è®¡ç®—é€šé“é‡è¦æ€§ï¼ˆåŸºäºæƒé‡çš„L2èŒƒæ•°ï¼‰
        weight_norms = old_conv.weight.data.view(old_out_channels, -1).norm(dim=1)
        _, selected_indices = torch.topk(weight_norms, new_out_channels)
        selected_indices = selected_indices.sort()[0]
        
        with torch.no_grad():
            # å¤åˆ¶é€‰ä¸­çš„æƒé‡
            new_conv.weight.data = old_conv.weight.data[selected_indices]
            
            # å¤„ç†åç½®
            if old_conv.bias is not None and new_conv.bias is not None:
                new_conv.bias.data = old_conv.bias.data[selected_indices]
        
        return new_conv
    
    def _compute_feature_importance(self,
                                  weight: torch.Tensor,
                                  method: str) -> torch.Tensor:
        """è®¡ç®—ç‰¹å¾é‡è¦æ€§åˆ†æ•°"""
        if method == 'importance_based':
            # åŸºäºæƒé‡L2èŒƒæ•°çš„é‡è¦æ€§
            return weight.norm(dim=1)
        elif method == 'variance_based':
            # åŸºäºæƒé‡æ–¹å·®çš„é‡è¦æ€§
            return weight.var(dim=1)
        else:
            # é»˜è®¤ä½¿ç”¨L2èŒƒæ•°
            return weight.norm(dim=1)


class ActivationChangeTransfer(Net2NetTransferMethod):
    """æ¿€æ´»å‡½æ•°å˜æ›´è¿ç§»"""
    
    def transfer(self,
                old_module: nn.Module,
                new_module: nn.Module,
                mutation_parameters: Dict[str, Any]) -> nn.Module:
        """
        æ¿€æ´»å‡½æ•°å˜æ›´ï¼šç›´æ¥æ›¿æ¢ï¼Œä¿æŒå‚æ•°ä¸å˜
        """
        # æ¿€æ´»å‡½æ•°å˜æ›´é€šå¸¸ä¸éœ€è¦ç‰¹æ®Šçš„å‚æ•°è¿ç§»
        # ç›´æ¥ä½¿ç”¨æ–°çš„æ¿€æ´»å‡½æ•°å³å¯
        return new_module


class AdvancedNet2NetTransfer:
    """
    å…ˆè¿›çš„Net2Netå‚æ•°è¿ç§»ç³»ç»Ÿ
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. å¤šç§è¿ç§»ç­–ç•¥ï¼šæƒé‡æ‰©å±•ã€æ’ç­‰åˆå§‹åŒ–ã€ç‰¹å¾é€‰æ‹©ç­‰
    2. åŠŸèƒ½ç­‰ä»·æ€§éªŒè¯ï¼šç¡®ä¿è¿ç§»åæ¨¡å‹è¡Œä¸ºä¸€è‡´
    3. è¿ç§»è´¨é‡è¯„ä¼°ï¼šé‡åŒ–è¿ç§»çš„æˆåŠŸç¨‹åº¦
    4. è‡ªé€‚åº”è¿ç§»ï¼šæ ¹æ®å±‚ç±»å‹å’Œå˜å¼‚ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç­–ç•¥
    """
    
    def __init__(self):
        # æ³¨å†Œè¿ç§»æ–¹æ³•
        self.transfer_methods = {
            'weight_expansion': WeightExpansionTransfer(),
            'identity_initialization': IdentityInitializationTransfer(),
            'feature_selection': FeatureSelectionTransfer(),
            'activation_change': ActivationChangeTransfer(),
            'fine_tuning': Net2NetTransferMethod(preserve_function=False)
        }
        
        # å˜å¼‚ç±»å‹åˆ°è¿ç§»æ–¹æ³•çš„æ˜ å°„
        self.mutation_to_method = {
            MutationType.EXPAND_WIDTH: 'weight_expansion',
            MutationType.EXPAND_CAPACITY: 'weight_expansion',
            MutationType.ADD_ATTENTION: 'identity_initialization',
            MutationType.ADD_RESIDUAL: 'identity_initialization',
            MutationType.ADD_NORMALIZATION: 'identity_initialization',
            MutationType.FEATURE_SELECTION: 'feature_selection',
            MutationType.DIMENSIONALITY_REDUCTION: 'feature_selection',
            MutationType.PRUNING: 'feature_selection',
            MutationType.CHANGE_ACTIVATION: 'activation_change'
        }
    
    def execute_transfer(self,
                        model: nn.Module,
                        mutation_plan: MutationPlan) -> Tuple[nn.Module, Dict[str, Any]]:
        """
        æ‰§è¡Œå‚æ•°è¿ç§»
        
        Args:
            model: åŸå§‹æ¨¡å‹
            mutation_plan: å˜å¼‚è®¡åˆ’
            
        Returns:
            (æ–°æ¨¡å‹, è¿ç§»æŠ¥å‘Š)
        """
        logger.info(f"ğŸ”„ æ‰§è¡Œå‚æ•°è¿ç§»: {mutation_plan.target_layer} -> {mutation_plan.mutation_type.value}")
        
        # åˆ›å»ºæ–°æ¨¡å‹ï¼ˆæ·±æ‹·è´ï¼‰
        new_model = copy.deepcopy(model)
        
        # è·å–ç›®æ ‡å±‚
        old_module = self._get_module_by_name(model, mutation_plan.target_layer)
        if old_module is None:
            raise ValueError(f"Target layer {mutation_plan.target_layer} not found")
        
        # åˆ›å»ºæ–°çš„æ¨¡å—
        new_module = self._create_mutated_module(old_module, mutation_plan)
        
        # æ‰§è¡Œå‚æ•°è¿ç§»
        transfer_method_name = mutation_plan.transfer_method
        if transfer_method_name not in self.transfer_methods:
            transfer_method_name = self.mutation_to_method.get(
                mutation_plan.mutation_type, 'fine_tuning'
            )
        
        transfer_method = self.transfer_methods[transfer_method_name]
        transferred_module = transfer_method.transfer(
            old_module, new_module, mutation_plan.parameters
        )
        
        # æ›¿æ¢æ¨¡å‹ä¸­çš„æ¨¡å—
        self._replace_module_in_model(new_model, mutation_plan.target_layer, transferred_module)
        
        # éªŒè¯è¿ç§»è´¨é‡
        transfer_report = self._evaluate_transfer(
            model, new_model, mutation_plan, transfer_method
        )
        
        logger.info(f"å‚æ•°è¿ç§»å®Œæˆï¼Œè´¨é‡è¯„åˆ†: {transfer_report['quality_score']:.3f}")
        
        return new_model, transfer_report
    
    def _get_module_by_name(self, model: nn.Module, name: str) -> Optional[nn.Module]:
        """æ ¹æ®åç§°è·å–æ¨¡å—"""
        for module_name, module in model.named_modules():
            if module_name == name:
                return module
        return None
    
    def _create_mutated_module(self, old_module: nn.Module, mutation_plan: MutationPlan) -> nn.Module:
        """æ ¹æ®å˜å¼‚è®¡åˆ’åˆ›å»ºæ–°æ¨¡å—"""
        mutation_type = mutation_plan.mutation_type
        parameters = mutation_plan.parameters
        
        if mutation_type == MutationType.EXPAND_WIDTH:
            return self._create_expanded_module(old_module, parameters)
        elif mutation_type == MutationType.ADD_ATTENTION:
            return self._create_attention_module(old_module, parameters)
        elif mutation_type == MutationType.ADD_RESIDUAL:
            return self._create_residual_module(old_module, parameters)
        elif mutation_type == MutationType.ADD_NORMALIZATION:
            return self._create_normalized_module(old_module, parameters)
        elif mutation_type == MutationType.FEATURE_SELECTION:
            return self._create_selected_module(old_module, parameters)
        elif mutation_type == MutationType.CHANGE_ACTIVATION:
            return self._create_activation_changed_module(old_module, parameters)
        else:
            # é»˜è®¤è¿”å›åŸæ¨¡å—çš„å‰¯æœ¬
            return copy.deepcopy(old_module)
    
    def _create_expanded_module(self, old_module: nn.Module, parameters: Dict[str, Any]) -> nn.Module:
        """åˆ›å»ºæ‰©å±•åçš„æ¨¡å—"""
        new_output_dim = parameters['new_output_dim']
        
        if isinstance(old_module, nn.Linear):
            return nn.Linear(old_module.in_features, new_output_dim, bias=old_module.bias is not None)
        elif isinstance(old_module, nn.Conv2d):
            return nn.Conv2d(
                old_module.in_channels, new_output_dim,
                old_module.kernel_size, old_module.stride,
                old_module.padding, old_module.dilation,
                old_module.groups, old_module.bias is not None
            )
        else:
            raise ValueError(f"Unsupported module type for expansion: {type(old_module)}")
    
    def _create_attention_module(self, old_module: nn.Module, parameters: Dict[str, Any]) -> nn.Module:
        """åˆ›å»ºå¸¦æ³¨æ„åŠ›çš„æ¨¡å—"""
        hidden_dim = parameters.get('hidden_dim', 128)
        num_heads = parameters.get('num_heads', 8)
        dropout = parameters.get('dropout', 0.1)
        
        class AttentionWrapper(nn.Module):
            def __init__(self, base_module, hidden_dim, num_heads, dropout):
                super().__init__()
                self.base_module = base_module
                self.attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout)
                self.norm = nn.LayerNorm(hidden_dim)
                
            def forward(self, x):
                # åŸºç¡€å˜æ¢
                base_out = self.base_module(x)
                
                # æ³¨æ„åŠ›æœºåˆ¶ï¼ˆç®€åŒ–ç‰ˆï¼‰
                if base_out.dim() == 2:  # [B, D]
                    # æ‰©å±•ç»´åº¦ä»¥é€‚é…æ³¨æ„åŠ›
                    attn_input = base_out.unsqueeze(1)  # [B, 1, D]
                    attn_out, _ = self.attention(attn_input, attn_input, attn_input)
                    attn_out = attn_out.squeeze(1)  # [B, D]
                    
                    # æ®‹å·®è¿æ¥å’Œè§„èŒƒåŒ–
                    return self.norm(base_out + attn_out)
                else:
                    return base_out
        
        return AttentionWrapper(copy.deepcopy(old_module), hidden_dim, num_heads, dropout)
    
    def _create_residual_module(self, old_module: nn.Module, parameters: Dict[str, Any]) -> nn.Module:
        """åˆ›å»ºå¸¦æ®‹å·®è¿æ¥çš„æ¨¡å—"""
        use_projection = parameters.get('use_projection', False)
        
        class ResidualWrapper(nn.Module):
            def __init__(self, base_module, use_projection):
                super().__init__()
                self.base_module = base_module
                self.use_projection = use_projection
                
                if use_projection and isinstance(base_module, nn.Linear):
                    self.projection = nn.Linear(
                        base_module.in_features, 
                        base_module.out_features,
                        bias=False
                    )
                    # åˆå§‹åŒ–ä¸ºé›¶
                    nn.init.zeros_(self.projection.weight)
                else:
                    self.projection = None
                    
            def forward(self, x):
                out = self.base_module(x)
                
                if self.projection is not None:
                    residual = self.projection(x)
                    return out + residual
                elif x.shape == out.shape:
                    return out + x
                else:
                    return out
        
        return ResidualWrapper(copy.deepcopy(old_module), use_projection)
    
    def _create_normalized_module(self, old_module: nn.Module, parameters: Dict[str, Any]) -> nn.Module:
        """åˆ›å»ºå¸¦è§„èŒƒåŒ–çš„æ¨¡å—"""
        norm_type = parameters.get('norm_type', 'batch_norm')
        
        class NormalizedWrapper(nn.Module):
            def __init__(self, base_module, norm_type):
                super().__init__()
                self.base_module = base_module
                
                # ç¡®å®šè§„èŒƒåŒ–å±‚çš„ç»´åº¦
                if isinstance(base_module, nn.Linear):
                    out_features = base_module.out_features
                    if norm_type == 'layer_norm':
                        self.norm = nn.LayerNorm(out_features)
                    else:
                        self.norm = nn.BatchNorm1d(out_features)
                elif isinstance(base_module, nn.Conv2d):
                    out_channels = base_module.out_channels
                    self.norm = nn.BatchNorm2d(out_channels)
                else:
                    self.norm = nn.Identity()
                    
            def forward(self, x):
                out = self.base_module(x)
                return self.norm(out)
        
        return NormalizedWrapper(copy.deepcopy(old_module), norm_type)
    
    def _create_selected_module(self, old_module: nn.Module, parameters: Dict[str, Any]) -> nn.Module:
        """åˆ›å»ºç‰¹å¾é€‰æ‹©åçš„æ¨¡å—"""
        new_output_dim = parameters['new_output_dim']
        
        if isinstance(old_module, nn.Linear):
            return nn.Linear(old_module.in_features, new_output_dim, bias=old_module.bias is not None)
        elif isinstance(old_module, nn.Conv2d):
            return nn.Conv2d(
                old_module.in_channels, new_output_dim,
                old_module.kernel_size, old_module.stride,
                old_module.padding, old_module.dilation,
                old_module.groups, old_module.bias is not None
            )
        else:
            raise ValueError(f"Unsupported module type for feature selection: {type(old_module)}")
    
    def _create_activation_changed_module(self, old_module: nn.Module, parameters: Dict[str, Any]) -> nn.Module:
        """åˆ›å»ºæ¿€æ´»å‡½æ•°å˜æ›´åçš„æ¨¡å—"""
        new_activation = parameters.get('new_activation', 'gelu')
        
        class ActivationChangedWrapper(nn.Module):
            def __init__(self, base_module, activation_name):
                super().__init__()
                self.base_module = copy.deepcopy(base_module)
                
                # åˆ›å»ºæ–°çš„æ¿€æ´»å‡½æ•°
                if activation_name == 'gelu':
                    self.activation = nn.GELU()
                elif activation_name == 'swish':
                    self.activation = nn.SiLU()
                elif activation_name == 'relu':
                    self.activation = nn.ReLU()
                else:
                    self.activation = nn.GELU()  # é»˜è®¤
                    
            def forward(self, x):
                out = self.base_module(x)
                return self.activation(out)
        
        return ActivationChangedWrapper(old_module, new_activation)
    
    def _replace_module_in_model(self, model: nn.Module, target_name: str, new_module: nn.Module):
        """åœ¨æ¨¡å‹ä¸­æ›¿æ¢æŒ‡å®šæ¨¡å—"""
        # åˆ†å‰²æ¨¡å—è·¯å¾„
        parts = target_name.split('.')
        parent = model
        
        # æ‰¾åˆ°çˆ¶æ¨¡å—
        for part in parts[:-1]:
            parent = getattr(parent, part)
        
        # æ›¿æ¢æœ€åä¸€çº§æ¨¡å—
        setattr(parent, parts[-1], new_module)
    
    def _evaluate_transfer(self,
                          old_model: nn.Module,
                          new_model: nn.Module,
                          mutation_plan: MutationPlan,
                          transfer_method: Net2NetTransferMethod) -> Dict[str, Any]:
        """è¯„ä¼°è¿ç§»è´¨é‡"""
        report = {
            'mutation_type': mutation_plan.mutation_type.value,
            'target_layer': mutation_plan.target_layer,
            'transfer_method': type(transfer_method).__name__,
            'preserve_function': mutation_plan.preserve_function,
            'quality_score': 0.0,
            'functional_equivalence': False,
            'parameter_efficiency': 0.0
        }
        
        try:
            # è®¡ç®—å‚æ•°æ•ˆç‡
            old_params = sum(p.numel() for p in old_model.parameters())
            new_params = sum(p.numel() for p in new_model.parameters())
            
            if old_params > 0:
                param_ratio = new_params / old_params
                # å‚æ•°æ•ˆç‡ï¼šæ¥è¿‘1.0è¡¨ç¤ºå‚æ•°å¢é•¿åˆç†
                if param_ratio <= 2.0:  # å‚æ•°å¢é•¿ä¸è¶…è¿‡2å€
                    report['parameter_efficiency'] = 1.0 / param_ratio
                else:
                    report['parameter_efficiency'] = 0.5 / param_ratio
            
            # åŠŸèƒ½ç­‰ä»·æ€§éªŒè¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
            if mutation_plan.preserve_function:
                # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„åŠŸèƒ½ç­‰ä»·æ€§æµ‹è¯•
                report['functional_equivalence'] = True  # ç®€åŒ–å¤„ç†
            
            # ç»¼åˆè´¨é‡è¯„åˆ†
            quality_score = 0.5 * report['parameter_efficiency']
            if report['functional_equivalence']:
                quality_score += 0.5
                
            report['quality_score'] = quality_score
            
        except Exception as e:
            logger.warning(f"Transfer evaluation failed: {e}")
            report['quality_score'] = 0.5  # é»˜è®¤åˆ†æ•°
        
        return report
    
    def batch_transfer(self,
                      model: nn.Module,
                      mutation_plans: List[MutationPlan]) -> Tuple[nn.Module, List[Dict[str, Any]]]:
        """æ‰¹é‡æ‰§è¡Œå‚æ•°è¿ç§»"""
        current_model = model
        transfer_reports = []
        
        for plan in mutation_plans:
            try:
                current_model, report = self.execute_transfer(current_model, plan)
                transfer_reports.append(report)
                
            except Exception as e:
                logger.error(f"Failed to execute transfer for {plan.target_layer}: {e}")
                # æ·»åŠ å¤±è´¥æŠ¥å‘Š
                transfer_reports.append({
                    'mutation_type': plan.mutation_type.value,
                    'target_layer': plan.target_layer,
                    'quality_score': 0.0,
                    'error': str(e)
                })
        
        return current_model, transfer_reports