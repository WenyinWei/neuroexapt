#!/usr/bin/env python3
"""
Dynamic Neural Morphogenesis - è¿æ¥ç”Ÿé•¿æ¨¡å—

åŸºäºæ¢¯åº¦å¼•å¯¼çš„è¿æ¥åŠ¨æ€ç”Ÿé•¿æœºåˆ¶ï¼š
1. åˆ†æè·¨å±‚æ¢¯åº¦ç›¸å…³æ€§ï¼Œå‘ç°æ½œåœ¨çš„æœ‰ç›Šè¿æ¥
2. åŠ¨æ€æ·»åŠ è·³è·ƒè¿æ¥ã€æ³¨æ„åŠ›æœºåˆ¶
3. å®ç°å±‚é—´ä¿¡æ¯æµä¼˜åŒ–
4. æ”¯æŒResNetå¼è·³è·ƒè¿æ¥å’ŒTransformerå¼æ³¨æ„åŠ›è¿æ¥
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
from typing import Dict, List, Tuple, Optional, Any, Set
from collections import defaultdict, deque
import logging

logger = logging.getLogger(__name__)


class GradientCorrelationAnalyzer:
    """æ¢¯åº¦ç›¸å…³æ€§åˆ†æå™¨"""
    
    def __init__(self, correlation_threshold=0.1, history_length=10):
        self.correlation_threshold = correlation_threshold
        self.history_length = history_length
        self.gradient_history = defaultdict(deque)
        self.correlation_cache = {}
        
    def collect_gradients(self, model: nn.Module) -> Dict[str, torch.Tensor]:
        """æ”¶é›†æ¨¡å‹çš„æ¢¯åº¦ä¿¡æ¯"""
        gradients = {}
        
        for name, param in model.named_parameters():
            if param.grad is not None and param.requires_grad:
                # åªæ”¶é›†å·ç§¯å’Œçº¿æ€§å±‚çš„æ¢¯åº¦
                if any(layer_type in name for layer_type in ['conv', 'linear', 'fc']):
                    gradients[name] = param.grad.clone().detach()
        
        return gradients
    
    def update_gradient_history(self, gradients: Dict[str, torch.Tensor]) -> None:
        """æ›´æ–°æ¢¯åº¦å†å²è®°å½•"""
        for name, grad in gradients.items():
            # å°†æ¢¯åº¦å±•å¹³å¹¶æ·»åŠ åˆ°å†å²è®°å½•
            flat_grad = grad.view(-1)
            
            if len(self.gradient_history[name]) >= self.history_length:
                self.gradient_history[name].popleft()
            
            self.gradient_history[name].append(flat_grad)
    
    def calculate_layer_correlation(self, layer1_name: str, layer2_name: str) -> float:
        """è®¡ç®—ä¸¤å±‚ä¹‹é—´çš„æ¢¯åº¦ç›¸å…³æ€§"""
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = tuple(sorted([layer1_name, layer2_name]))
        if cache_key in self.correlation_cache:
            return self.correlation_cache[cache_key]
        
        # è·å–æ¢¯åº¦å†å²
        grad_history1 = self.gradient_history.get(layer1_name, [])
        grad_history2 = self.gradient_history.get(layer2_name, [])
        
        if len(grad_history1) < 3 or len(grad_history2) < 3:
            return 0.0
        
        # è®¡ç®—æ—¶é—´åºåˆ—ç›¸å…³æ€§
        correlations = []
        min_length = min(len(grad_history1), len(grad_history2))
        
        for i in range(min_length):
            grad1 = grad_history1[i]
            grad2 = grad_history2[i]
            
            # è°ƒæ•´åˆ°ç›¸åŒç»´åº¦
            min_size = min(grad1.size(0), grad2.size(0))
            if min_size < 10:  # æ¢¯åº¦å¤ªå°ï¼Œè·³è¿‡
                continue
                
            grad1_sample = grad1[:min_size]
            grad2_sample = grad2[:min_size]
            
            # è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°
            correlation = self._pearson_correlation(grad1_sample, grad2_sample)
            if not math.isnan(correlation):
                correlations.append(abs(correlation))  # ä½¿ç”¨ç»å¯¹å€¼
        
        # è®¡ç®—å¹³å‡ç›¸å…³æ€§
        avg_correlation = np.mean(correlations) if correlations else 0.0
        
        # ç¼“å­˜ç»“æœ
        self.correlation_cache[cache_key] = avg_correlation
        
        return avg_correlation
    
    def _pearson_correlation(self, x: torch.Tensor, y: torch.Tensor) -> float:
        """è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°"""
        try:
            # æ ‡å‡†åŒ–
            x_centered = x - torch.mean(x)
            y_centered = y - torch.mean(y)
            
            # è®¡ç®—ç›¸å…³ç³»æ•°
            numerator = torch.sum(x_centered * y_centered)
            denominator = torch.sqrt(torch.sum(x_centered ** 2) * torch.sum(y_centered ** 2))
            
            if denominator > 1e-8:
                correlation = numerator / denominator
                return correlation.item()
            else:
                return 0.0
                
        except Exception as e:
            logger.debug(f"Correlation calculation failed: {e}")
            return 0.0
    
    def find_beneficial_connections(self, layer_names: List[str]) -> List[Dict]:
        """æ‰¾åˆ°æœ‰ç›Šçš„è¿æ¥å€™é€‰"""
        beneficial_connections = []
        
        for i, layer1 in enumerate(layer_names):
            for j, layer2 in enumerate(layer_names):
                if i >= j:  # é¿å…é‡å¤å’Œè‡ªè¿æ¥
                    continue
                
                # è®¡ç®—ç›¸å…³æ€§
                correlation = self.calculate_layer_correlation(layer1, layer2)
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰ç›Šè¿æ¥
                if correlation > self.correlation_threshold:
                    # åˆ†æè¿æ¥ç±»å‹å’Œå±‚é—´è·ç¦»
                    layer_distance = abs(i - j)
                    connection_type = self._determine_connection_type(layer1, layer2, layer_distance)
                    
                    beneficial_connections.append({
                        'source_layer': layer1,
                        'target_layer': layer2,
                        'correlation': correlation,
                        'layer_distance': layer_distance,
                        'connection_type': connection_type,
                        'priority': self._calculate_connection_priority(correlation, layer_distance)
                    })
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        beneficial_connections.sort(key=lambda x: x['priority'], reverse=True)
        
        return beneficial_connections
    
    def _determine_connection_type(self, layer1: str, layer2: str, distance: int) -> str:
        """ç¡®å®šè¿æ¥ç±»å‹"""
        
        # åŸºäºå±‚åç§°å’Œè·ç¦»ç¡®å®šè¿æ¥ç±»å‹
        if distance == 1:
            return 'adjacent'  # ç›¸é‚»å±‚ï¼Œé€šå¸¸ä¸éœ€è¦é¢å¤–è¿æ¥
        elif distance == 2:
            return 'skip_connection'  # è·³è·ƒè¿æ¥
        elif distance >= 3:
            if 'conv' in layer1 and 'conv' in layer2:
                return 'feature_pyramid'  # ç‰¹å¾é‡‘å­—å¡”è¿æ¥
            elif 'fc' in layer1 and 'fc' in layer2:
                return 'dense_connection'  # å¯†é›†è¿æ¥
            else:
                return 'cross_stage'  # è·¨é˜¶æ®µè¿æ¥
        else:
            return 'unknown'
    
    def _calculate_connection_priority(self, correlation: float, distance: int) -> float:
        """è®¡ç®—è¿æ¥ä¼˜å…ˆçº§"""
        
        # åŸºç¡€ä¼˜å…ˆçº§ = ç›¸å…³æ€§å¼ºåº¦
        base_priority = correlation
        
        # è·ç¦»æƒ©ç½šï¼šè·ç¦»è¶Šè¿œï¼Œä¼˜å…ˆçº§ç•¥å¾®é™ä½
        distance_penalty = 1.0 / (1.0 + 0.1 * distance)
        
        # æœ€ç»ˆä¼˜å…ˆçº§
        priority = base_priority * distance_penalty
        
        return priority


class ConnectionBuilder:
    """è¿æ¥æ„å»ºå™¨ - å®é™…æ„å»ºç¥ç»ç½‘ç»œè¿æ¥"""
    
    def __init__(self):
        self.built_connections = set()
        self.connection_modules = {}
        
    def build_skip_connection(self, 
                            model: nn.Module, 
                            source_layer: str, 
                            target_layer: str,
                            connection_id: str) -> bool:
        """æ„å»ºè·³è·ƒè¿æ¥"""
        
        try:
            # è·å–æºå±‚å’Œç›®æ ‡å±‚
            source_module = self._get_module_by_name(model, source_layer)
            target_module = self._get_module_by_name(model, target_layer)
            
            if source_module is None or target_module is None:
                logger.warning(f"Cannot find modules: {source_layer} or {target_layer}")
                return False
            
            # åˆ†æå±‚çš„è¾“å‡ºç»´åº¦
            source_shape = self._get_layer_output_shape(source_module)
            target_shape = self._get_layer_output_shape(target_module)
            
            if source_shape is None or target_shape is None:
                logger.warning(f"Cannot determine shapes for {source_layer} -> {target_layer}")
                return False
            
            # åˆ›å»ºé€‚é…å™¨æ¨¡å—
            adapter = self._create_shape_adapter(source_shape, target_shape)
            
            if adapter is not None:
                # å°†é€‚é…å™¨æ·»åŠ åˆ°æ¨¡å‹
                adapter_name = f"skip_adapter_{connection_id}"
                self._add_module_to_model(model, adapter_name, adapter)
                
                # è®°å½•è¿æ¥ä¿¡æ¯
                self.built_connections.add(connection_id)
                self.connection_modules[connection_id] = {
                    'type': 'skip_connection',
                    'source': source_layer,
                    'target': target_layer,
                    'adapter': adapter_name
                }
                
                logger.info(f"Built skip connection: {source_layer} -> {target_layer}")
                return True
            
        except Exception as e:
            logger.error(f"Failed to build skip connection {source_layer} -> {target_layer}: {e}")
            return False
        
        return False
    
    def build_attention_connection(self, 
                                 model: nn.Module, 
                                 source_layer: str, 
                                 target_layer: str,
                                 connection_id: str) -> bool:
        """æ„å»ºæ³¨æ„åŠ›è¿æ¥"""
        
        try:
            # è·å–å±‚ä¿¡æ¯
            source_module = self._get_module_by_name(model, source_layer)
            target_module = self._get_module_by_name(model, target_layer)
            
            if source_module is None or target_module is None:
                return False
            
            # åˆ†æç‰¹å¾ç»´åº¦
            source_shape = self._get_layer_output_shape(source_module)
            target_shape = self._get_layer_output_shape(target_module)
            
            # åˆ›å»ºæ³¨æ„åŠ›æ¨¡å—
            attention_module = self._create_attention_module(source_shape, target_shape)
            
            if attention_module is not None:
                # æ·»åŠ åˆ°æ¨¡å‹
                attention_name = f"attention_{connection_id}"
                self._add_module_to_model(model, attention_name, attention_module)
                
                # è®°å½•è¿æ¥
                self.built_connections.add(connection_id)
                self.connection_modules[connection_id] = {
                    'type': 'attention_connection',
                    'source': source_layer,
                    'target': target_layer,
                    'attention': attention_name
                }
                
                logger.info(f"Built attention connection: {source_layer} -> {target_layer}")
                return True
                
        except Exception as e:
            logger.error(f"Failed to build attention connection {source_layer} -> {target_layer}: {e}")
            return False
        
        return False
    
    def _get_module_by_name(self, model: nn.Module, module_name: str) -> Optional[nn.Module]:
        """æ ¹æ®åç§°è·å–æ¨¡å—"""
        try:
            # å°è¯•ç›´æ¥è·å–
            if hasattr(model, 'get_submodule'):
                return model.get_submodule(module_name)
            else:
                # å…¼å®¹è€ç‰ˆæœ¬
                parts = module_name.split('.')
                module = model
                for part in parts:
                    module = getattr(module, part)
                return module
        except Exception:
            return None
    
    def _get_layer_output_shape(self, module: nn.Module) -> Optional[Tuple]:
        """è·å–å±‚çš„è¾“å‡ºå½¢çŠ¶"""
        if isinstance(module, nn.Conv2d):
            return ('conv', module.out_channels, module.kernel_size, module.stride, module.padding)
        elif isinstance(module, nn.Linear):
            return ('linear', module.out_features)
        elif isinstance(module, nn.BatchNorm2d):
            return ('bn2d', module.num_features)
        elif isinstance(module, nn.BatchNorm1d):
            return ('bn1d', module.num_features)
        else:
            return None
    
    def _create_shape_adapter(self, source_shape: Tuple, target_shape: Tuple) -> Optional[nn.Module]:
        """åˆ›å»ºå½¢çŠ¶é€‚é…å™¨"""
        
        # Conv2d åˆ° Conv2d çš„é€‚é…
        if source_shape[0] == 'conv' and target_shape[0] == 'conv':
            source_channels = source_shape[1]
            target_channels = target_shape[1]
            
            if source_channels != target_channels:
                # é€šé“æ•°é€‚é…
                return nn.Conv2d(source_channels, target_channels, kernel_size=1, bias=False)
            else:
                # å½¢çŠ¶ç›¸åŒï¼Œä½¿ç”¨æ’ç­‰æ˜ å°„
                return nn.Identity()
        
        # Linear åˆ° Linear çš„é€‚é…
        elif source_shape[0] == 'linear' and target_shape[0] == 'linear':
            source_features = source_shape[1]
            target_features = target_shape[1]
            
            if source_features != target_features:
                return nn.Linear(source_features, target_features, bias=False)
            else:
                return nn.Identity()
        
        # Conv2d åˆ° Linear çš„é€‚é… (éœ€è¦å…¨å±€æ± åŒ–)
        elif source_shape[0] == 'conv' and target_shape[0] == 'linear':
            source_channels = source_shape[1]
            target_features = target_shape[1]
            
            return nn.Sequential(
                nn.AdaptiveAvgPool2d(1),
                nn.Flatten(),
                nn.Linear(source_channels, target_features, bias=False)
            )
        
        else:
            logger.warning(f"Unsupported shape adaptation: {source_shape} -> {target_shape}")
            return None
    
    def _create_attention_module(self, source_shape: Tuple, target_shape: Tuple) -> Optional[nn.Module]:
        """åˆ›å»ºæ³¨æ„åŠ›æ¨¡å—"""
        
        # ç®€åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶
        if source_shape[0] == 'conv' and target_shape[0] == 'conv':
            source_channels = source_shape[1]
            target_channels = target_shape[1]
            
            # é€šé“æ³¨æ„åŠ›
            return ChannelAttention(source_channels, target_channels)
        
        elif source_shape[0] == 'linear' and target_shape[0] == 'linear':
            source_features = source_shape[1]
            target_features = target_shape[1]
            
            # ç‰¹å¾æ³¨æ„åŠ›
            return FeatureAttention(source_features, target_features)
        
        else:
            return None
    
    def _add_module_to_model(self, model: nn.Module, module_name: str, module: nn.Module) -> None:
        """å‘æ¨¡å‹æ·»åŠ æ–°æ¨¡å—"""
        # å°†æ¨¡å—æ·»åŠ åˆ°æ¨¡å‹çš„ä¸€ä¸ªç‰¹æ®Šå®¹å™¨ä¸­
        if not hasattr(model, '_dnm_connections'):
            model._dnm_connections = nn.ModuleDict()
        
        model._dnm_connections[module_name] = module


class ChannelAttention(nn.Module):
    """é€šé“æ³¨æ„åŠ›æ¨¡å—"""
    
    def __init__(self, source_channels: int, target_channels: int, reduction_ratio: int = 16):
        super().__init__()
        
        self.source_channels = source_channels
        self.target_channels = target_channels
        
        # å…¨å±€å¹³å‡æ± åŒ–
        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)
        
        # æ³¨æ„åŠ›ç½‘ç»œ
        hidden_channels = max(source_channels // reduction_ratio, 1)
        self.attention_net = nn.Sequential(
            nn.Linear(source_channels, hidden_channels),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_channels, target_channels),
            nn.Sigmoid()
        )
        
        # é€šé“é€‚é…å™¨
        if source_channels != target_channels:
            self.channel_adapter = nn.Conv2d(source_channels, target_channels, 1, bias=False)
        else:
            self.channel_adapter = nn.Identity()
    
    def forward(self, source_features: torch.Tensor, target_features: torch.Tensor) -> torch.Tensor:
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        b, c, h, w = source_features.size()
        
        # å…¨å±€æ± åŒ–å¾—åˆ°é€šé“æè¿°ç¬¦
        channel_descriptor = self.global_avg_pool(source_features).view(b, c)
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attention_weights = self.attention_net(channel_descriptor).view(b, self.target_channels, 1, 1)
        
        # é€‚é…æºç‰¹å¾çš„é€šé“æ•°
        adapted_source = self.channel_adapter(source_features)
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        attended_features = adapted_source * attention_weights
        
        # ä¸ç›®æ ‡ç‰¹å¾èåˆ
        if attended_features.shape == target_features.shape:
            return target_features + attended_features
        else:
            # å°ºå¯¸ä¸åŒ¹é…æ—¶ä½¿ç”¨è‡ªé€‚åº”æ± åŒ–
            attended_features = F.adaptive_avg_pool2d(attended_features, target_features.shape[2:])
            return target_features + attended_features


class FeatureAttention(nn.Module):
    """ç‰¹å¾æ³¨æ„åŠ›æ¨¡å—"""
    
    def __init__(self, source_features: int, target_features: int):
        super().__init__()
        
        self.source_features = source_features
        self.target_features = target_features
        
        # æ³¨æ„åŠ›æƒé‡è®¡ç®—
        hidden_size = max(min(source_features, target_features) // 4, 1)
        self.attention_net = nn.Sequential(
            nn.Linear(source_features, hidden_size),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_size, target_features),
            nn.Sigmoid()
        )
        
        # ç‰¹å¾é€‚é…å™¨
        if source_features != target_features:
            self.feature_adapter = nn.Linear(source_features, target_features, bias=False)
        else:
            self.feature_adapter = nn.Identity()
    
    def forward(self, source_features: torch.Tensor, target_features: torch.Tensor) -> torch.Tensor:
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attention_weights = self.attention_net(source_features)
        
        # é€‚é…æºç‰¹å¾
        adapted_source = self.feature_adapter(source_features)
        
        # åº”ç”¨æ³¨æ„åŠ›
        attended_features = adapted_source * attention_weights
        
        # ä¸ç›®æ ‡ç‰¹å¾èåˆ
        return target_features + attended_features


class DNMConnectionGrowth:
    """DNMè¿æ¥ç”Ÿé•¿ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or self._default_config()
        self.analyzer = GradientCorrelationAnalyzer(**self.config['analyzer'])
        self.builder = ConnectionBuilder()
        self.connection_statistics = defaultdict(int)
        self.growth_history = []
        
    def _default_config(self) -> Dict:
        """é»˜è®¤é…ç½®"""
        return {
            'analyzer': {
                'correlation_threshold': 0.15,
                'history_length': 8
            },
            'growth': {
                'max_new_connections': 3,
                'min_correlation_threshold': 0.1,
                'growth_frequency': 8,  # æ¯8ä¸ªepochåˆ†æä¸€æ¬¡
                'connection_types': ['skip_connection', 'attention_connection']
            },
            'filtering': {
                'min_layer_distance': 2,  # æœ€å°å±‚é—´è·ç¦»
                'max_layer_distance': 6,  # æœ€å¤§å±‚é—´è·ç¦»
                'avoid_redundant_connections': True
            }
        }
    
    def collect_and_analyze_gradients(self, model: nn.Module) -> None:
        """æ”¶é›†å¹¶åˆ†ææ¢¯åº¦"""
        # æ”¶é›†å½“å‰æ¢¯åº¦
        gradients = self.analyzer.collect_gradients(model)
        
        # æ›´æ–°æ¢¯åº¦å†å²
        self.analyzer.update_gradient_history(gradients)
        
        logger.debug(f"Collected gradients from {len(gradients)} layers")
    
    def analyze_and_grow_connections(self, model: nn.Module, epoch: int) -> Dict[str, Any]:
        """åˆ†æå¹¶ç”Ÿé•¿æ–°è¿æ¥"""
        
        # æ£€æŸ¥æ˜¯å¦åˆ°äº†ç”Ÿé•¿æ—¶æœº
        if epoch % self.config['growth']['growth_frequency'] != 0:
            return {'connections_grown': 0, 'message': 'Not growth epoch'}
        
        # è·å–æ‰€æœ‰å±‚åç§°
        layer_names = []
        for name, module in model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)) and any(t in name for t in ['conv', 'fc', 'linear']):
                layer_names.append(name)
        
        if len(layer_names) < 2:
            return {'connections_grown': 0, 'message': 'Not enough layers for connections'}
        
        # æ‰¾åˆ°æœ‰ç›Šè¿æ¥
        beneficial_connections = self.analyzer.find_beneficial_connections(layer_names)
        
        # è¿‡æ»¤è¿æ¥
        filtered_connections = self._filter_connections(beneficial_connections)
        
        # æ‰§è¡Œè¿æ¥ç”Ÿé•¿
        connections_grown = self._grow_connections(model, filtered_connections)
        
        # è®°å½•ç”Ÿé•¿å†å²
        growth_record = {
            'epoch': epoch,
            'candidates_found': len(beneficial_connections),
            'connections_grown': connections_grown,
            'grown_connections': [conn for conn in filtered_connections[:connections_grown]]
        }
        self.growth_history.append(growth_record)
        
        # æ›´æ–°ç»Ÿè®¡
        for conn in filtered_connections[:connections_grown]:
            self.connection_statistics[conn['connection_type']] += 1
        
        result = {
            'connections_grown': connections_grown,
            'beneficial_connections_found': len(beneficial_connections),
            'connection_candidates': filtered_connections[:5],  # å‰5ä¸ªå€™é€‰
            'growth_history': growth_record,
            'message': f'Successfully grown {connections_grown} connections'
        }
        
        logger.info(f"DNM Connection Growth completed: {connections_grown} connections grown")
        return result
    
    def _filter_connections(self, connections: List[Dict]) -> List[Dict]:
        """è¿‡æ»¤è¿æ¥å€™é€‰"""
        filtered = []
        
        min_distance = self.config['filtering']['min_layer_distance']
        max_distance = self.config['filtering']['max_layer_distance']
        max_connections = self.config['growth']['max_new_connections']
        
        for conn in connections:
            # è·ç¦»è¿‡æ»¤
            if not (min_distance <= conn['layer_distance'] <= max_distance):
                continue
            
            # ç›¸å…³æ€§è¿‡æ»¤
            if conn['correlation'] < self.config['growth']['min_correlation_threshold']:
                continue
            
            # é¿å…é‡å¤è¿æ¥
            connection_id = f"{conn['source_layer']}_to_{conn['target_layer']}"
            if self.config['filtering']['avoid_redundant_connections']:
                if connection_id in self.builder.built_connections:
                    continue
            
            # æ·»åŠ è¿æ¥ID
            conn['connection_id'] = connection_id
            filtered.append(conn)
            
            # é™åˆ¶æ•°é‡
            if len(filtered) >= max_connections:
                break
        
        return filtered
    
    def _grow_connections(self, model: nn.Module, connections: List[Dict]) -> int:
        """æ‰§è¡Œè¿æ¥ç”Ÿé•¿"""
        grown_count = 0
        
        for conn in connections:
            try:
                connection_type = conn['connection_type']
                connection_id = conn['connection_id']
                source_layer = conn['source_layer']
                target_layer = conn['target_layer']
                
                success = False
                
                # æ ¹æ®è¿æ¥ç±»å‹ç”Ÿé•¿
                if connection_type in ['skip_connection', 'dense_connection']:
                    success = self.builder.build_skip_connection(
                        model, source_layer, target_layer, connection_id
                    )
                elif connection_type in ['feature_pyramid', 'cross_stage']:
                    success = self.builder.build_attention_connection(
                        model, source_layer, target_layer, connection_id
                    )
                
                if success:
                    grown_count += 1
                    logger.info(f"Successfully grown {connection_type}: {source_layer} -> {target_layer}")
                else:
                    logger.warning(f"Failed to grow connection: {source_layer} -> {target_layer}")
                    
            except Exception as e:
                logger.error(f"Error growing connection {conn.get('connection_id', 'unknown')}: {e}")
                continue
        
        return grown_count
    
    def get_growth_summary(self) -> Dict[str, Any]:
        """è·å–è¿æ¥ç”Ÿé•¿æ€»ç»“"""
        return {
            'total_connections_grown': sum(self.connection_statistics.values()),
            'connections_by_type': dict(self.connection_statistics),
            'growth_history': self.growth_history,
            'built_connections': list(self.builder.built_connections),
            'connection_modules': self.builder.connection_modules,
            'config': self.config
        }


# æµ‹è¯•å‡½æ•°
def test_connection_growth():
    """æµ‹è¯•è¿æ¥ç”Ÿé•¿åŠŸèƒ½"""
    print("ğŸ”— Testing DNM Connection Growth")
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    class TestModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
            self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
            self.conv3 = nn.Conv2d(64, 128, 3, padding=1)
            self.fc1 = nn.Linear(128 * 4 * 4, 256)
            self.fc2 = nn.Linear(256, 10)
            
        def forward(self, x):
            x = F.relu(self.conv1(x))
            x = F.max_pool2d(x, 2)
            x = F.relu(self.conv2(x))
            x = F.max_pool2d(x, 2)
            x = F.relu(self.conv3(x))
            x = F.max_pool2d(x, 2)
            x = x.view(x.size(0), -1)
            x = F.relu(self.fc1(x))
            x = self.fc2(x)
            return x
    
    model = TestModel()
    connection_growth = DNMConnectionGrowth()
    
    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ï¼Œæ”¶é›†æ¢¯åº¦
    dummy_input = torch.randn(8, 3, 32, 32)
    dummy_target = torch.randint(0, 10, (8,))
    criterion = nn.CrossEntropyLoss()
    
    for epoch in range(16):
        # å‰å‘ä¼ æ’­
        output = model(dummy_input)
        loss = criterion(output, dummy_target)
        
        # åå‘ä¼ æ’­
        model.zero_grad()
        loss.backward()
        
        # æ”¶é›†æ¢¯åº¦
        connection_growth.collect_and_analyze_gradients(model)
        
        # å°è¯•ç”Ÿé•¿è¿æ¥
        if epoch % 8 == 0 and epoch > 0:
            result = connection_growth.analyze_and_grow_connections(model, epoch)
            print(f"Epoch {epoch}: {result}")
    
    # è·å–æ€»ç»“
    summary = connection_growth.get_growth_summary()
    print(f"Growth summary: {summary}")
    
    return model, connection_growth


if __name__ == "__main__":
    test_connection_growth()