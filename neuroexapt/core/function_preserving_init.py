"""
"""
defgroup group_function_preserving_init Function Preserving Init
ingroup core
Function Preserving Init module for NeuroExapt framework.
"""


å‡½æ•°ä¿æŒåˆå§‹åŒ– (Function-Preserving Initialization)

ASO-SEæ¡†æ¶çš„æ ¸å¿ƒæœºåˆ¶ä¹‹ä¸€ï¼šç¡®ä¿æ¶æ„çªå˜æ—¶æ–°æ¶æ„çš„è¾“å‡ºä¸æ—§æ¶æ„å®Œå…¨ä¸€è‡´ï¼Œ
é¿å…æŸå¤±å‡½æ•°å‰§çƒˆéœ‡è¡ï¼Œå®ç°å¹³æ»‘çš„æ¶æ„è¿‡æ¸¡ã€‚

æ”¯æŒçš„æ“ä½œç±»å‹ï¼š
1. å±‚å¢åŠ  (Layer Addition) - æ’ç­‰æ˜ å°„åˆå§‹åŒ–
2. é€šé“æ‰©å±• (Channel Expansion) - æƒé‡å¤åˆ¶ä¸å¾®è°ƒ
3. åˆ†æ”¯æ·»åŠ  (Branch Addition) - é›¶æƒé‡åˆå§‹åŒ–
"""

import torch
import torch.nn as nn
import copy
from typing import Dict, List, Optional, Tuple, Union
import logging

logger = logging.getLogger(__name__)

class FunctionPreservingInitializer:
    """
    å‡½æ•°ä¿æŒåˆå§‹åŒ–å™¨
    
    ç¡®ä¿æ¶æ„å˜æ›´æ—¶ç½‘ç»œè¾“å‡ºä¿æŒä¸å˜çš„æ ¸å¿ƒå·¥å…·
    """
    
    def __init__(self, preserve_ratio: float = 0.95, noise_scale: float = 1e-4):
        """
        Args:
            preserve_ratio: æƒé‡ä¿æŒçš„æ¯”ä¾‹ï¼Œç”¨äºé€šé“æ‰©å±•æ—¶çš„æƒé‡å¤åˆ¶
            noise_scale: æ·»åŠ çš„å¾®å°éšæœºå™ªå£°è§„æ¨¡ï¼Œé¿å…å®Œå…¨å¯¹ç§°
        """
        self.preserve_ratio = preserve_ratio
        self.noise_scale = noise_scale
        
    def identity_layer_init(self, layer: nn.Module, input_shape: Optional[Tuple[int, ...]] = None) -> nn.Module:
        """
        æ’ç­‰æ˜ å°„åˆå§‹åŒ–
        
        å°†æ–°å¢å±‚åˆå§‹åŒ–ä¸ºæ’ç­‰æ˜ å°„ï¼Œç¡®ä¿å±‚çš„è¾“å‡º = è¾“å…¥
        
        Args:
            layer: è¦åˆå§‹åŒ–çš„å±‚
            input_shape: è¾“å…¥å¼ é‡çš„å½¢çŠ¶
            
        Returns:
            åˆå§‹åŒ–åçš„å±‚
        """
        if isinstance(layer, nn.Conv2d):
            return self._identity_conv2d_init(layer)
        elif isinstance(layer, nn.Linear):
            return self._identity_linear_init(layer)
        elif isinstance(layer, nn.BatchNorm2d):
            return self._identity_batchnorm_init(layer)
        else:
            logger.warning(f"Identity initialization not implemented for {type(layer)}")
            return layer
    
    def _identity_conv2d_init(self, conv: nn.Conv2d) -> nn.Conv2d:
        """å·ç§¯å±‚æ’ç­‰æ˜ å°„åˆå§‹åŒ–"""
        with torch.no_grad():
            # é‡ç½®æƒé‡
            conv.weight.zero_()
            
            # è®¾ç½®ä¸­å¿ƒæƒé‡ä¸º1 (å¯¹äº3x3å·ç§¯ï¼Œä¸­å¿ƒä¸º(1,1))
            if conv.kernel_size == (3, 3):
                center = (1, 1)
            elif conv.kernel_size == (1, 1):
                center = (0, 0)
            else:
                center = tuple(k//2 for k in conv.kernel_size)
            
            # åªåœ¨è¾“å…¥è¾“å‡ºé€šé“æ•°ç›¸åŒæ—¶è®¾ç½®æ’ç­‰
            min_channels = min(conv.in_channels, conv.out_channels)
            for i in range(min_channels):
                conv.weight[i, i, center[0], center[1]] = 1.0
                
            # å¦‚æœæœ‰åç½®ï¼Œè®¾ä¸ºé›¶
            if conv.bias is not None:
                conv.bias.zero_()
                
        logger.debug(f"Initialized Conv2d {conv.in_channels}->{conv.out_channels} as identity")
        return conv
    
    def _identity_linear_init(self, linear: nn.Linear) -> nn.Linear:
        """çº¿æ€§å±‚æ’ç­‰æ˜ å°„åˆå§‹åŒ–"""
        with torch.no_grad():
            # é‡ç½®ä¸ºé›¶
            linear.weight.zero_()
            
            # è®¾ç½®å¯¹è§’çº¿ä¸º1
            min_dim = min(linear.in_features, linear.out_features)
            for i in range(min_dim):
                linear.weight[i, i] = 1.0
                
            # åç½®è®¾ä¸ºé›¶
            if linear.bias is not None:
                linear.bias.zero_()
                
        logger.debug(f"Initialized Linear {linear.in_features}->{linear.out_features} as identity")
        return linear
    
    def _identity_batchnorm_init(self, bn: nn.BatchNorm2d) -> nn.BatchNorm2d:
        """BatchNormå±‚æ’ç­‰æ˜ å°„åˆå§‹åŒ–"""
        with torch.no_grad():
            # è®¾ç½®ä¸ºæ’ç­‰å˜æ¢
            bn.weight.fill_(1.0)
            bn.bias.zero_()
            
            # é‡ç½®è¿è¡Œç»Ÿè®¡
            if bn.running_mean is not None:
                bn.running_mean.zero_()
            if bn.running_var is not None:
                bn.running_var.fill_(1.0)
                
        logger.debug(f"Initialized BatchNorm2d as identity")
        return bn
    
    def expand_channels_preserving(self, old_layer: nn.Module, new_channels: int, 
                                 expansion_strategy: str = "replicate") -> nn.Module:
        """
        é€šé“æ‰©å±•çš„å‡½æ•°ä¿æŒåˆå§‹åŒ–
        
        Args:
            old_layer: åŸå§‹å±‚
            new_channels: æ–°çš„é€šé“æ•°
            expansion_strategy: æ‰©å±•ç­–ç•¥ ("replicate", "interpolate", "zero_pad")
            
        Returns:
            æ‰©å±•åçš„å±‚
        """
        if isinstance(old_layer, nn.Conv2d):
            return self._expand_conv2d_channels(old_layer, new_channels, expansion_strategy)
        elif isinstance(old_layer, nn.BatchNorm2d):
            return self._expand_batchnorm_channels(old_layer, new_channels, expansion_strategy)
        else:
            logger.warning(f"Channel expansion not implemented for {type(old_layer)}")
            return old_layer
    
    def _expand_conv2d_channels(self, old_conv: nn.Conv2d, new_out_channels: int, 
                               strategy: str) -> nn.Conv2d:
        """å·ç§¯å±‚é€šé“æ‰©å±•"""
        old_out_channels = old_conv.out_channels
        
        if new_out_channels <= old_out_channels:
            logger.warning("New channel count should be larger than old count")
            return old_conv
        
        # åˆ›å»ºæ–°çš„å·ç§¯å±‚
        new_conv = nn.Conv2d(
            old_conv.in_channels, 
            new_out_channels,
            old_conv.kernel_size,
            old_conv.stride,
            old_conv.padding,
            old_conv.dilation,
            old_conv.groups,
            old_conv.bias is not None
        )
        
        with torch.no_grad():
            # å¤åˆ¶åŸæœ‰æƒé‡
            new_conv.weight[:old_out_channels] = old_conv.weight.clone()
            
            # æ‰©å±•ç­–ç•¥
            if strategy == "replicate":
                # å¤åˆ¶ç°æœ‰é€šé“
                expand_count = new_out_channels - old_out_channels
                replicate_indices = torch.randperm(old_out_channels)[:expand_count]
                new_conv.weight[old_out_channels:] = old_conv.weight[replicate_indices].clone()
                
                # æ·»åŠ å¾®å°å™ªå£°é¿å…å®Œå…¨å¯¹ç§°
                noise = torch.randn_like(new_conv.weight[old_out_channels:]) * self.noise_scale
                new_conv.weight[old_out_channels:] += noise
                
            elif strategy == "zero_pad":
                # æ–°é€šé“åˆå§‹åŒ–ä¸ºé›¶
                new_conv.weight[old_out_channels:].zero_()
                
            elif strategy == "interpolate":
                # ä½¿ç”¨ç°æœ‰é€šé“çš„æ’å€¼
                for i in range(old_out_channels, new_out_channels):
                    # åœ¨ç°æœ‰é€šé“é—´æ’å€¼
                    alpha = (i - old_out_channels) / (new_out_channels - old_out_channels)
                    idx1 = min(int(alpha * old_out_channels), old_out_channels - 1)
                    idx2 = min(idx1 + 1, old_out_channels - 1)
                    weight1, weight2 = 1 - alpha, alpha
                    
                    new_conv.weight[i] = (weight1 * old_conv.weight[idx1] + 
                                        weight2 * old_conv.weight[idx2])
            
            # å¤åˆ¶åç½®
            if old_conv.bias is not None:
                new_conv.bias[:old_out_channels] = old_conv.bias.clone()
                if strategy == "replicate":
                    new_conv.bias[old_out_channels:] = old_conv.bias[replicate_indices].clone()
                else:
                    new_conv.bias[old_out_channels:].zero_()
        
        logger.info(f"Expanded Conv2d channels: {old_out_channels} -> {new_out_channels} "
                   f"using {strategy} strategy")
        return new_conv
    
    def _expand_batchnorm_channels(self, old_bn: nn.BatchNorm2d, new_channels: int, 
                                  strategy: str) -> nn.BatchNorm2d:
        """BatchNormå±‚é€šé“æ‰©å±•"""
        old_channels = old_bn.num_features
        
        if new_channels <= old_channels:
            return old_bn
            
        new_bn = nn.BatchNorm2d(new_channels, old_bn.eps, old_bn.momentum, 
                               old_bn.affine, old_bn.track_running_stats)
        
        with torch.no_grad():
            if old_bn.affine:
                # å¤åˆ¶æƒé‡å’Œåç½®
                new_bn.weight[:old_channels] = old_bn.weight.clone()
                new_bn.bias[:old_channels] = old_bn.bias.clone()
                
                # æ–°é€šé“åˆå§‹åŒ–
                new_bn.weight[old_channels:].fill_(1.0)
                new_bn.bias[old_channels:].zero_()
            
            if old_bn.track_running_stats:
                # å¤åˆ¶è¿è¡Œç»Ÿè®¡
                new_bn.running_mean[:old_channels] = old_bn.running_mean.clone()
                new_bn.running_var[:old_channels] = old_bn.running_var.clone()
                
                # æ–°é€šé“ç»Ÿè®¡åˆå§‹åŒ–
                new_bn.running_mean[old_channels:].zero_()
                new_bn.running_var[old_channels:].fill_(1.0)
                
                new_bn.num_batches_tracked = old_bn.num_batches_tracked
        
        logger.info(f"Expanded BatchNorm channels: {old_channels} -> {new_channels}")
        return new_bn
    
    def zero_branch_init(self, branch: nn.Module) -> nn.Module:
        """
        åˆ†æ”¯é›¶åˆå§‹åŒ–
        
        å°†æ–°å¢åˆ†æ”¯çš„æœ€åä¸€å±‚åˆå§‹åŒ–ä¸ºé›¶æƒé‡ï¼Œä½¿å…¶åœ¨è¯ç”Ÿç¬é—´ä¸å½±å“ä¸»å¹²æµ
        """
        # æ‰¾åˆ°åˆ†æ”¯çš„æœ€åä¸€å±‚
        last_layer = None
        for module in branch.modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)):
                last_layer = module
        
        if last_layer is not None:
            with torch.no_grad():
                last_layer.weight.zero_()
                if last_layer.bias is not None:
                    last_layer.bias.zero_()
            logger.info(f"Zero-initialized branch output layer: {type(last_layer)}")
        
        return branch
    
    def smooth_parameter_transfer(self, old_model: nn.Module, new_model: nn.Module,
                                layer_mapping: Dict[str, str]) -> nn.Module:
        """
        å¹³æ»‘å‚æ•°ä¼ é€’
        
        å°†æ—§æ¨¡å‹çš„å‚æ•°å¹³æ»‘ä¼ é€’åˆ°æ–°æ¨¡å‹ä¸­ï¼Œæ”¯æŒå½¢çŠ¶å˜åŒ–çš„æƒ…å†µ
        
        Args:
            old_model: æ—§æ¨¡å‹
            new_model: æ–°æ¨¡å‹  
            layer_mapping: å±‚åç§°æ˜ å°„ {"new_layer_name": "old_layer_name"}
            
        Returns:
            å‚æ•°ä¼ é€’åçš„æ–°æ¨¡å‹
        """
        old_state_dict = old_model.state_dict()
        new_state_dict = new_model.state_dict()
        
        transferred_count = 0
        
        for new_name, old_name in layer_mapping.items():
            if old_name in old_state_dict and new_name in new_state_dict:
                old_param = old_state_dict[old_name]
                new_param = new_state_dict[new_name]
                
                if old_param.shape == new_param.shape:
                    # ç›´æ¥å¤åˆ¶
                    new_state_dict[new_name] = old_param.clone()
                    transferred_count += 1
                    
                elif len(old_param.shape) == len(new_param.shape):
                    # å½¢çŠ¶ä¸åŒä½†ç»´åº¦ç›¸åŒï¼Œå°è¯•éƒ¨åˆ†å¤åˆ¶
                    min_shape = tuple(min(old_param.shape[i], new_param.shape[i]) 
                                    for i in range(len(old_param.shape)))
                    
                    # åˆ›å»ºç´¢å¼•åˆ‡ç‰‡
                    slices = tuple(slice(0, s) for s in min_shape)
                    
                    with torch.no_grad():
                        new_param[slices] = old_param[slices]
                    
                    transferred_count += 1
                    logger.info(f"Partially transferred {old_name} -> {new_name}: "
                              f"{old_param.shape} -> {new_param.shape}")
                else:
                    logger.warning(f"Cannot transfer {old_name} -> {new_name}: "
                                 f"incompatible shapes {old_param.shape} vs {new_param.shape}")
        
        new_model.load_state_dict(new_state_dict)
        logger.info(f"Successfully transferred {transferred_count} parameters")
        
        return new_model


def create_identity_residual_block(in_channels: int, out_channels: int) -> nn.Module:
    """
    åˆ›å»ºæ’ç­‰æ®‹å·®å—
    
    ç”¨äºæ·»åŠ æ–°å±‚æ—¶çš„å‡½æ•°ä¿æŒåˆå§‹åŒ–
    """
    if in_channels == out_channels:
        # ç›´æ¥æ’ç­‰è¿æ¥
        return nn.Identity()
    else:
        # éœ€è¦é€šé“è°ƒæ•´çš„æ’ç­‰è¿æ¥
        conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)
        
        # å‡½æ•°ä¿æŒåˆå§‹åŒ–
        initializer = FunctionPreservingInitializer()
        conv = initializer.identity_layer_init(conv, None)
        
        return nn.Sequential(
            conv,
            nn.BatchNorm2d(out_channels)
        )


def test_function_preserving_init():
    """æµ‹è¯•å‡½æ•°ä¿æŒåˆå§‹åŒ–çš„åŠŸèƒ½"""
    print("ğŸ§ª Testing Function-Preserving Initialization...")
    
    initializer = FunctionPreservingInitializer()
    
    # æµ‹è¯•æ’ç­‰æ˜ å°„åˆå§‹åŒ–
    conv = nn.Conv2d(32, 32, 3, padding=1)
    x = torch.randn(1, 32, 16, 16)
    
    # åˆå§‹åŒ–å‰åçš„è¾“å‡ºåº”è¯¥ç›¸åŒï¼ˆå¯¹äºæ’ç­‰æ˜ å°„ï¼‰
    conv_identity = initializer.identity_layer_init(conv, x.shape)
    
    # æµ‹è¯•é€šé“æ‰©å±•
    old_conv = nn.Conv2d(32, 64, 3, padding=1)
    torch.nn.init.xavier_uniform_(old_conv.weight)
    
    new_conv = initializer.expand_channels_preserving(old_conv, 128, "replicate")
    
    print(f"âœ… Original conv output channels: {old_conv.out_channels}")
    print(f"âœ… Expanded conv output channels: {new_conv.out_channels}")
    
    # éªŒè¯å‰64ä¸ªé€šé“çš„æƒé‡ä¿æŒä¸å˜
    assert torch.allclose(old_conv.weight, new_conv.weight[:64])
    print("âœ… Channel expansion preserves original weights")
    
    print("ğŸ‰ Function-Preserving Initialization tests passed!")


if __name__ == "__main__":
    test_function_preserving_init() 