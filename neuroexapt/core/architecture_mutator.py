"""
defgroup group_architecture_mutator Architecture Mutator
ingroup core
Architecture Mutator module for NeuroExapt framework.
"""

æ¶æ„çªå˜å™¨ (Architecture Mutator)

ASO-SEæ¡†æ¶çš„æ ¸å¿ƒç»„ä»¶ï¼šè´Ÿè´£æ‰§è¡Œæ¶æ„çš„åŠ¨æ€å˜åŒ–ï¼ŒåŒ…æ‹¬ï¼š
1. å±‚å¢åŠ  (Layer Addition) - æ·»åŠ æ–°çš„è®¡ç®—å±‚
2. é€šé“æ‰©å±• (Channel Expansion) - å¢åŠ ç½‘ç»œå®½åº¦
3. åˆ†æ”¯æ·»åŠ  (Branch Addition) - å¼•å…¥æ–°çš„è®¡ç®—è·¯å¾„
4. æ“ä½œæ›¿æ¢ (Operation Replacement) - æ›¿æ¢ç°æœ‰æ“ä½œ

æ‰€æœ‰å˜åŒ–éƒ½ä¸å‡½æ•°ä¿æŒåˆå§‹åŒ–é›†æˆï¼Œç¡®ä¿æ¶æ„å˜åŒ–æ—¶çš„å¹³æ»‘è¿‡æ¸¡ã€‚
"""

import torch
import torch.nn as nn
import copy
import logging
from typing import Dict, List, Optional, Tuple, Union, Any
from .function_preserving_init import FunctionPreservingInitializer, create_identity_residual_block
from .genotypes import Genotype, PRIMITIVES

logger = logging.getLogger(__name__)

class ArchitectureMutator:
    """
    æ¶æ„çªå˜å™¨
    
    è´Ÿè´£æ‰§è¡Œå„ç§æ¶æ„å˜åŒ–æ“ä½œï¼Œä¸å‡½æ•°ä¿æŒåˆå§‹åŒ–ç´§å¯†é›†æˆ
    """
    
    def __init__(self, preserve_function: bool = True, mutation_strength: float = 0.3):
        """
        Args:
            preserve_function: æ˜¯å¦ä½¿ç”¨å‡½æ•°ä¿æŒåˆå§‹åŒ–
            mutation_strength: çªå˜å¼ºåº¦ï¼Œæ§åˆ¶å˜åŒ–å¹…åº¦
        """
        self.preserve_function = preserve_function
        self.mutation_strength = mutation_strength
        self.initializer = FunctionPreservingInitializer() if preserve_function else None
        
        # çªå˜å†å²
        self.mutation_history = []
        self.mutation_count = 0
        
        logger.info(f"ğŸ§¬ Architecture Mutator initialized: "
                   f"preserve_function={preserve_function}, "
                   f"mutation_strength={mutation_strength}")
    
    def add_layer(self, model: nn.Module, target_position: int, 
                  layer_type: str = "conv", **layer_kwargs) -> nn.Module:
        """
        æ·»åŠ æ–°å±‚
        
        Args:
            model: ç›®æ ‡æ¨¡å‹
            target_position: æ’å…¥ä½ç½®
            layer_type: å±‚ç±»å‹ ("conv", "linear", "residual")
            **layer_kwargs: å±‚å‚æ•°
            
        Returns:
            ä¿®æ”¹åçš„æ¨¡å‹
        """
        logger.info(f"ğŸ—ï¸ Adding {layer_type} layer at position {target_position}")
        
        if isinstance(model, nn.Sequential):
            return self._add_layer_to_sequential(model, target_position, layer_type, **layer_kwargs)
        else:
            # For non-Sequential models, treat as Sequential for simplicity
            logger.warning("Non-Sequential model, converting to Sequential for layer addition")
            return self._add_layer_to_sequential(nn.Sequential(*list(model.children())), target_position, layer_type, **layer_kwargs)
    
    def _add_layer_to_sequential(self, model: nn.Sequential, position: int, 
                               layer_type: str, **kwargs) -> nn.Sequential:
        """å‘Sequentialæ¨¡å‹æ·»åŠ å±‚"""
        layers = list(model.children())
        
        if layer_type == "conv":
            new_layer = self._create_conv_layer(**kwargs)
        elif layer_type == "linear":
            new_layer = self._create_linear_layer(**kwargs)
        elif layer_type == "residual":
            new_layer = self._create_residual_layer(**kwargs)
        else:
            raise ValueError(f"Unsupported layer type: {layer_type}")
        
        # å‡½æ•°ä¿æŒåˆå§‹åŒ–
        if self.preserve_function and self.initializer:
            new_layer = self.initializer.identity_layer_init(new_layer)
        
        # æ’å…¥æ–°å±‚
        layers.insert(position, new_layer)
        new_model = nn.Sequential(*layers)
        
        self._record_mutation("add_layer", {
            "position": position,
            "layer_type": layer_type,
            "kwargs": kwargs
        })
        
        return new_model
    
    def expand_channels(self, model: nn.Module, target_layer: str, 
                       expansion_factor: float = 1.5, 
                       strategy: str = "replicate") -> nn.Module:
        """
        æ‰©å±•é€šé“æ•°
        
        Args:
            model: ç›®æ ‡æ¨¡å‹
            target_layer: ç›®æ ‡å±‚åç§°
            expansion_factor: æ‰©å±•å€æ•°
            strategy: æ‰©å±•ç­–ç•¥
            
        Returns:
            ä¿®æ”¹åçš„æ¨¡å‹
        """
        logger.info(f"ğŸ“ˆ Expanding channels in {target_layer} by {expansion_factor}x")
        
        new_model = copy.deepcopy(model)
        
        # æŸ¥æ‰¾ç›®æ ‡å±‚
        target_module = self._find_module_by_name(new_model, target_layer)
        if target_module is None:
            logger.warning(f"Target layer {target_layer} not found")
            return model
        
        # æ‰©å±•é€šé“
        if isinstance(target_module, nn.Conv2d):
            new_channels = int(target_module.out_channels * expansion_factor)
            if self.preserve_function and self.initializer:
                expanded_layer = self.initializer.expand_channels_preserving(
                    target_module, new_channels, strategy
                )
            else:
                expanded_layer = self._create_expanded_conv(target_module, new_channels)
            
            # æ›¿æ¢å±‚
            self._replace_module_by_name(new_model, target_layer, expanded_layer)
            
            # æ›´æ–°åç»­å±‚çš„è¾“å…¥é€šé“
            self._update_subsequent_layers(new_model, target_layer, new_channels)
        
        self._record_mutation("expand_channels", {
            "target_layer": target_layer,
            "expansion_factor": expansion_factor,
            "strategy": strategy
        })
        
        return new_model
    
    def add_branch(self, model: nn.Module, branch_point: str, 
                   branch_structure: List[Dict], merge_strategy: str = "add") -> nn.Module:
        """
        æ·»åŠ åˆ†æ”¯ç»“æ„
        
        Args:
            model: ç›®æ ‡æ¨¡å‹
            branch_point: åˆ†æ”¯ç‚¹
            branch_structure: åˆ†æ”¯ç»“æ„å®šä¹‰
            merge_strategy: åˆå¹¶ç­–ç•¥ ("add", "concat", "attention")
            
        Returns:
            ä¿®æ”¹åçš„æ¨¡å‹
        """
        logger.info(f"ğŸŒ¿ Adding branch at {branch_point} with {len(branch_structure)} layers")
        
        # åˆ›å»ºåˆ†æ”¯æ¨¡å—
        branch = self._create_branch_from_structure(branch_structure)
        
        # å‡½æ•°ä¿æŒåˆå§‹åŒ–ï¼šæ–°åˆ†æ”¯åˆå§‹ä¸ºé›¶è¾“å‡º
        if self.preserve_function and self.initializer:
            branch = self.initializer.zero_branch_init(branch)
        
        # åˆ›å»ºå¸¦åˆ†æ”¯çš„æ–°æ¨¡å‹
        new_model = self._integrate_branch(model, branch_point, branch, merge_strategy)
        
        self._record_mutation("add_branch", {
            "branch_point": branch_point,
            "branch_structure": branch_structure,
            "merge_strategy": merge_strategy
        })
        
        return new_model
    
    def replace_operation(self, model: nn.Module, target_layer: str, 
                         new_operation: str) -> nn.Module:
        """
        æ›¿æ¢æ“ä½œ
        
        Args:
            model: ç›®æ ‡æ¨¡å‹
            target_layer: ç›®æ ‡å±‚
            new_operation: æ–°æ“ä½œåç§°
            
        Returns:
            ä¿®æ”¹åçš„æ¨¡å‹
        """
        logger.info(f"ğŸ”„ Replacing operation in {target_layer} with {new_operation}")
        
        new_model = copy.deepcopy(model)
        target_module = self._find_module_by_name(new_model, target_layer)
        
        if target_module is None:
            return model
        
        # åˆ›å»ºæ–°æ“ä½œ
        new_op = self._create_operation(new_operation, target_module)
        
        # å‡½æ•°ä¿æŒåˆå§‹åŒ–
        if self.preserve_function and self.initializer:
            new_op = self._preserve_function_in_replacement(target_module, new_op)
        
        # æ›¿æ¢æ“ä½œ
        self._replace_module_by_name(new_model, target_layer, new_op)
        
        self._record_mutation("replace_operation", {
            "target_layer": target_layer,
            "new_operation": new_operation
        })
        
        return new_model
    
    def mutate_genotype(self, current_genotype: Genotype, 
                       mutation_type: str = "random") -> Genotype:
        """
        çªå˜åŸºå› å‹
        
        Args:
            current_genotype: å½“å‰åŸºå› å‹
            mutation_type: çªå˜ç±»å‹ ("random", "guided", "conservative")
            
        Returns:
            çªå˜åçš„åŸºå› å‹
        """
        logger.info(f"ğŸ§¬ Mutating genotype with {mutation_type} strategy")
        
        if mutation_type == "random":
            return self._random_genotype_mutation(current_genotype)
        elif mutation_type == "guided":
            return self._guided_genotype_mutation(current_genotype)
        elif mutation_type == "conservative":
            return self._conservative_genotype_mutation(current_genotype)
        else:
            raise ValueError(f"Unknown mutation type: {mutation_type}")
    
    def _create_conv_layer(self, in_channels: int, out_channels: int, 
                          kernel_size: int = 3, **kwargs) -> nn.Module:
        """åˆ›å»ºå·ç§¯å±‚"""
        return nn.Conv2d(in_channels, out_channels, kernel_size, 
                        padding=kernel_size//2, **kwargs)
    
    def _create_linear_layer(self, in_features: int, out_features: int, **kwargs) -> nn.Module:
        """åˆ›å»ºçº¿æ€§å±‚"""
        return nn.Linear(in_features, out_features, **kwargs)
    
    def _create_residual_layer(self, in_channels: int, out_channels: int, **kwargs) -> nn.Module:
        """åˆ›å»ºæ®‹å·®å±‚"""
        return create_identity_residual_block(in_channels, out_channels)
    
    def _create_expanded_conv(self, original_conv: nn.Conv2d, new_channels: int) -> nn.Conv2d:
        """åˆ›å»ºæ‰©å±•é€šé“çš„å·ç§¯å±‚"""
        new_conv = nn.Conv2d(
            original_conv.in_channels,
            new_channels,
            original_conv.kernel_size,
            original_conv.stride,
            original_conv.padding,
            original_conv.dilation,
            original_conv.groups,
            original_conv.bias is not None
        )
        return new_conv
    
    def _find_module_by_name(self, model: nn.Module, name: str) -> Optional[nn.Module]:
        """æ ¹æ®åç§°æŸ¥æ‰¾æ¨¡å—"""
        for module_name, module in model.named_modules():
            if module_name == name:
                return module
        return None
    
    def _replace_module_by_name(self, model: nn.Module, name: str, new_module: nn.Module):
        """æ ¹æ®åç§°æ›¿æ¢æ¨¡å—"""
        parts = name.split('.')
        parent = model
        
        for part in parts[:-1]:
            parent = getattr(parent, part)
        
        setattr(parent, parts[-1], new_module)
    
    def _update_subsequent_layers(self, model: nn.Module, changed_layer: str, new_channels: int):
        """æ›´æ–°åç»­å±‚ä»¥é€‚åº”é€šé“å˜åŒ–"""
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“æ¨¡å‹ç»“æ„å®ç°
        # ç®€åŒ–ç‰ˆæœ¬ï¼šå‡è®¾ä¸‹ä¸€å±‚æ˜¯éœ€è¦æ›´æ–°è¾“å…¥é€šé“çš„å±‚
        pass
    
    def _create_branch_from_structure(self, structure: List[Dict]) -> nn.Module:
        """ä»ç»“æ„å®šä¹‰åˆ›å»ºåˆ†æ”¯"""
        layers = []
        for layer_def in structure:
            layer_type = layer_def.get("type", "conv")
            if layer_type == "conv":
                layers.append(self._create_conv_layer(**layer_def.get("params", {})))
            elif layer_type == "linear":
                layers.append(self._create_linear_layer(**layer_def.get("params", {})))
            # å¯ä»¥æ·»åŠ æ›´å¤šå±‚ç±»å‹
        
        return nn.Sequential(*layers)
    
    def _integrate_branch(self, model: nn.Module, branch_point: str, 
                         branch: nn.Module, merge_strategy: str) -> nn.Module:
        """å°†åˆ†æ”¯é›†æˆåˆ°æ¨¡å‹ä¸­"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼šåˆ›å»ºåŒ…è£…å™¨
        class BranchedModel(nn.Module):
            def __init__(self, main_model, branch, branch_point, merge_strategy):
                super().__init__()
                self.main_model = main_model
                self.branch = branch
                self.branch_point = branch_point
                self.merge_strategy = merge_strategy
            
            def forward(self, x):
                # è¿™é‡Œéœ€è¦å®ç°åˆ†æ”¯é€»è¾‘
                main_out = self.main_model(x)
                branch_out = self.branch(x)
                
                if self.merge_strategy == "add":
                    return main_out + branch_out
                elif self.merge_strategy == "concat":
                    return torch.cat([main_out, branch_out], dim=1)
                else:
                    return main_out  # é»˜è®¤åªè¿”å›ä¸»è·¯å¾„
        
        return BranchedModel(model, branch, branch_point, merge_strategy)
    
    def _random_genotype_mutation(self, genotype: Genotype) -> Genotype:
        """éšæœºåŸºå› å‹çªå˜"""
        import random
        
        # å¤åˆ¶å½“å‰åŸºå› å‹
        normal_ops = list(genotype.normal)
        reduce_ops = list(genotype.reduce)
        
        # éšæœºçªå˜å‡ ä¸ªæ“ä½œ
        num_mutations = max(1, int(len(normal_ops) * self.mutation_strength))
        
        for _ in range(num_mutations):
            if random.random() < 0.5 and normal_ops:  # çªå˜normalæ“ä½œ
                idx = random.randint(0, len(normal_ops) - 1)
                new_op = random.choice(PRIMITIVES)
                normal_ops[idx] = (new_op, normal_ops[idx][1])
            elif reduce_ops:  # çªå˜reduceæ“ä½œ
                idx = random.randint(0, len(reduce_ops) - 1)
                new_op = random.choice(PRIMITIVES)
                reduce_ops[idx] = (new_op, reduce_ops[idx][1])
        
        return Genotype(
            normal=normal_ops,
            normal_concat=genotype.normal_concat,
            reduce=reduce_ops,
            reduce_concat=genotype.reduce_concat
        )
    
    def _guided_genotype_mutation(self, genotype: Genotype) -> Genotype:
        """å¼•å¯¼å¼åŸºå› å‹çªå˜ï¼ˆåŸºäºæ€§èƒ½å†å²ï¼‰"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼šåå‘é€‰æ‹©æ€§èƒ½æ›´å¥½çš„æ“ä½œ
        return self._random_genotype_mutation(genotype)  # ç›®å‰ä½¿ç”¨éšæœºçªå˜
    
    def _conservative_genotype_mutation(self, genotype: Genotype) -> Genotype:
        """ä¿å®ˆå¼åŸºå› å‹çªå˜ï¼ˆå°å¹…åº¦å˜åŒ–ï¼‰"""
        # å‡å°‘çªå˜å¼ºåº¦
        original_strength = self.mutation_strength
        self.mutation_strength *= 0.5
        
        result = self._random_genotype_mutation(genotype)
        
        # æ¢å¤åŸå§‹å¼ºåº¦
        self.mutation_strength = original_strength
        return result
    
    def _create_operation(self, op_name: str, reference_module: nn.Module) -> nn.Module:
        """æ ¹æ®æ“ä½œåç§°åˆ›å»ºæ–°æ“ä½œ"""
        # è¿™é‡Œéœ€è¦æ ¹æ®PRIMITIVESå®ç°å…·ä½“æ“ä½œåˆ›å»º
        # ç®€åŒ–ç‰ˆæœ¬ï¼šè¿”å›ç›¸åŒç±»å‹çš„æ¨¡å—
        return copy.deepcopy(reference_module)
    
    def _preserve_function_in_replacement(self, old_module: nn.Module, 
                                        new_module: nn.Module) -> nn.Module:
        """åœ¨æ“ä½œæ›¿æ¢æ—¶ä¿æŒå‡½æ•°"""
        if self.initializer:
            # å°è¯•ä¼ é€’å‚æ•°
            layer_mapping = {"new": "old"}  # ç®€åŒ–æ˜ å°„
            # è¿™é‡Œéœ€è¦æ›´å¤æ‚çš„é€»è¾‘æ¥å¤„ç†ä¸åŒç±»å‹çš„å±‚ä¹‹é—´çš„å‚æ•°ä¼ é€’
        return new_module
    
    def _record_mutation(self, mutation_type: str, details: Dict[str, Any]):
        """è®°å½•çªå˜å†å²"""
        self.mutation_count += 1
        mutation_record = {
            "id": self.mutation_count,
            "type": mutation_type,
            "details": details,
            "timestamp": self.mutation_count  # ç®€åŒ–æ—¶é—´æˆ³
        }
        self.mutation_history.append(mutation_record)
        logger.debug(f"Recorded mutation #{self.mutation_count}: {mutation_type}")
    
    def get_mutation_report(self) -> Dict:
        """è·å–çªå˜å†å²æŠ¥å‘Š"""
        if not self.mutation_history:
            return {"message": "No mutations performed yet"}
        
        mutation_types = {}
        for mutation in self.mutation_history:
            mut_type = mutation["type"]
            mutation_types[mut_type] = mutation_types.get(mut_type, 0) + 1
        
        return {
            "total_mutations": len(self.mutation_history),
            "mutation_types": mutation_types,
            "recent_mutations": self.mutation_history[-5:],  # æœ€è¿‘5æ¬¡çªå˜
            "mutation_strength": self.mutation_strength,
            "preserve_function": self.preserve_function
        }


class GradualArchitectureGrowth:
    """
    æ¸è¿›å¼æ¶æ„ç”Ÿé•¿
    
    å®ç°ç½‘ç»œæ¶æ„çš„æ¸è¿›å¼å¢é•¿ï¼Œé¿å…å‰§çƒˆå˜åŒ–
    """
    
    def __init__(self, mutator: ArchitectureMutator, 
                 growth_schedule: Optional[List[Dict]] = None):
        """
        Args:
            mutator: æ¶æ„çªå˜å™¨
            growth_schedule: ç”Ÿé•¿è®¡åˆ’
        """
        self.mutator = mutator
        self.growth_schedule = growth_schedule or self._default_growth_schedule()
        self.current_stage = 0
        
        logger.info(f"ğŸŒ± Gradual Architecture Growth initialized with {len(self.growth_schedule)} stages")
    
    def _default_growth_schedule(self) -> List[Dict]:
        """é»˜è®¤ç”Ÿé•¿è®¡åˆ’"""
        return [
            {"epoch": 10, "action": "expand_channels", "params": {"expansion_factor": 1.2}},
            {"epoch": 20, "action": "add_layer", "params": {"layer_type": "residual"}},
            {"epoch": 30, "action": "expand_channels", "params": {"expansion_factor": 1.5}},
            {"epoch": 40, "action": "add_branch", "params": {"merge_strategy": "add"}}
        ]
    
    def should_grow(self, current_epoch: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç”Ÿé•¿"""
        if self.current_stage >= len(self.growth_schedule):
            return False
        
        target_epoch = self.growth_schedule[self.current_stage]["epoch"]
        return current_epoch >= target_epoch
    
    def perform_growth(self, model: nn.Module, current_epoch: int) -> Optional[nn.Module]:
        """æ‰§è¡Œç”Ÿé•¿æ“ä½œ"""
        if not self.should_grow(current_epoch):
            return None
        
        stage_config = self.growth_schedule[self.current_stage]
        action = stage_config["action"]
        params = stage_config.get("params", {})
        
        logger.info(f"ğŸŒ¿ Performing growth stage {self.current_stage}: {action}")
        
        if action == "expand_channels":
            result = self.mutator.expand_channels(model, "conv1", **params)
        elif action == "add_layer":
            result = self.mutator.add_layer(model, -1, **params)
        elif action == "add_branch":
            result = self.mutator.add_branch(model, "conv1", [{"type": "conv"}], **params)
        else:
            logger.warning(f"Unknown growth action: {action}")
            result = model
        
        self.current_stage += 1
        return result


def test_architecture_mutator():
    """æµ‹è¯•æ¶æ„çªå˜å™¨åŠŸèƒ½"""
    print("ğŸ§ª Testing Architecture Mutator...")
    
    # åˆ›å»ºçªå˜å™¨
    mutator = ArchitectureMutator(preserve_function=True, mutation_strength=0.3)
    
    # åˆ›å»ºç®€å•æµ‹è¯•æ¨¡å‹
    test_model = nn.Sequential(
        nn.Conv2d(3, 16, 3, padding=1),
        nn.ReLU(),
        nn.Conv2d(16, 32, 3, padding=1),
        nn.ReLU()
    )
    
    print(f"ğŸ“Š Original model layers: {len(list(test_model.children()))}")
    
    # æµ‹è¯•æ·»åŠ å±‚
    new_model = mutator.add_layer(test_model, 2, "conv", in_channels=16, out_channels=16)
    print(f"âœ… After adding layer: {len(list(new_model.children()))} layers")
    
    # æµ‹è¯•é€šé“æ‰©å±•
    expanded_model = mutator.expand_channels(test_model, "0", expansion_factor=1.5)
    print(f"âœ… Channel expansion completed")
    
    # æµ‹è¯•åŸºå› å‹çªå˜
    from .genotypes import Genotype
    test_genotype = Genotype(
        normal=[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)],
        normal_concat=[2],
        reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1)],
        reduce_concat=[2]
    )
    
    mutated_genotype = mutator.mutate_genotype(test_genotype, "random")
    print(f"âœ… Genotype mutation completed")
    
    # æµ‹è¯•æ¸è¿›å¼ç”Ÿé•¿
    growth = GradualArchitectureGrowth(mutator)
    should_grow = growth.should_grow(10)
    print(f"âœ… Growth check at epoch 10: {should_grow}")
    
    # è·å–æŠ¥å‘Š
    report = mutator.get_mutation_report()
    print(f"âœ… Mutation report: {report['total_mutations']} mutations performed")
    
    print("ğŸ‰ Architecture Mutator tests passed!")


if __name__ == "__main__":
    test_architecture_mutator() 