"""
@defgroup group_fast_operations Fast Operations
@ingroup core
Fast Operations module for NeuroExapt framework.

é«˜æ€§èƒ½æ“ä½œæ¨¡å— - é’ˆå¯¹ASO-SEæ¶æ„æœç´¢ä¼˜åŒ–

ä¸»è¦ä¼˜åŒ–ï¼š
1. æ™ºèƒ½æ“ä½œé€‰æ‹©ï¼šåªè®¡ç®—æƒé‡å¤§çš„æ“ä½œ
2. æ“ä½œç¼“å­˜ï¼šé¿å…é‡å¤è®¡ç®—
3. æ‰¹é‡ä¼˜åŒ–ï¼šå‘é‡åŒ–æ¶æ„å‚æ•°æ›´æ–°
4. å†…å­˜æ± ï¼šå‡å°‘å†…å­˜åˆ†é…å¼€é”€
5. è®¾å¤‡ä¼˜åŒ–ï¼šæœ€å°åŒ–æ•°æ®ä¼ è¾“
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Union
import math
import time
from collections import OrderedDict
import numpy as np

class FastMixedOp(nn.Module):
    """
    é«˜æ€§èƒ½æ··åˆæ“ä½œ
    
    æ ¸å¿ƒä¼˜åŒ–ï¼š
    1. æƒé‡é˜ˆå€¼è¿‡æ»¤ï¼šåªè®¡ç®—æƒé‡>thresholdçš„æ“ä½œ
    2. Top-Ké€‰æ‹©ï¼šåªä¿ç•™å‰Kä¸ªæœ€å¤§æƒé‡çš„æ“ä½œ
    3. æ“ä½œç¼“å­˜ï¼šç¼“å­˜æ˜‚è´µæ“ä½œçš„ç»“æœ
    4. åŠ¨æ€ç²¾åº¦ï¼šæ ¹æ®æƒé‡å¤§å°é€‰æ‹©è®¡ç®—ç²¾åº¦
    """
    
    def __init__(self, C, stride, primitives=None, weight_threshold=0.01, top_k=3):
        super().__init__()
        
        if primitives is None:
            from .genotypes import PRIMITIVES
            primitives = PRIMITIVES
        
        self.C = C
        self.stride = stride
        self.weight_threshold = weight_threshold
        self.top_k = min(top_k, len(primitives))
        
        # æ„å»ºæ“ä½œå­—å…¸
        self._ops = nn.ModuleDict()
        self._op_names = list(primitives)
        
        # æŒ‰è®¡ç®—æˆæœ¬æ’åºæ“ä½œï¼ˆä¾¿å®œçš„æ“ä½œä¼˜å…ˆï¼‰
        self._op_costs = self._calculate_operation_costs(primitives)
        self._sorted_ops = sorted(enumerate(primitives), key=lambda x: self._op_costs[x[1]])
        
        for primitive in primitives:
            op = self._create_operation(primitive, C, stride)
            self._ops[primitive] = op
        
        # ç¼“å­˜ç›¸å…³
        self._cache = {}
        self._cache_hits = 0
        self._cache_misses = 0
        
        # æ€§èƒ½ç»Ÿè®¡
        self.forward_count = 0
        self.active_ops_avg = 0.0
        
    def _calculate_operation_costs(self, primitives):
        """è®¡ç®—æ“ä½œçš„ç›¸å¯¹æˆæœ¬"""
        costs = {
            'none': 0.1,
            'skip_connect': 0.2,
            'avg_pool_3x3': 0.3,
            'max_pool_3x3': 0.3,
            'sep_conv_3x3': 1.0,
            'sep_conv_5x5': 1.5,
            'sep_conv_7x7': 2.5,
            'dil_conv_3x3': 1.2,
            'dil_conv_5x5': 1.8,
            'conv_7x1_1x7': 1.3,
        }
        return {op: costs.get(op, 1.0) for op in primitives}
    
    def _create_operation(self, primitive, C, stride):
        """åˆ›å»ºä¼˜åŒ–çš„æ“ä½œ"""
        from .operations import OPS
        
        op = OPS[primitive](C, stride, False)
        
        # ä¸ºæ± åŒ–æ“ä½œæ·»åŠ BN
        if 'pool' in primitive:
            op = nn.Sequential(op, nn.BatchNorm2d(C, affine=False))
        
        # æ“ä½œèåˆä¼˜åŒ–
        if isinstance(op, nn.Sequential):
            op = self._fuse_operations(op)
        
        return op
    
    def _fuse_operations(self, sequential_op):
        """æ“ä½œèåˆä¼˜åŒ–"""
        # ç®€å•çš„èåˆï¼šåˆå¹¶è¿ç»­çš„Conv+BN+ReLU
        layers = list(sequential_op.children())
        fused_layers = []
        
        i = 0
        while i < len(layers):
            if (i < len(layers) - 2 and 
                isinstance(layers[i], nn.ReLU) and
                isinstance(layers[i+1], nn.Conv2d) and
                isinstance(layers[i+2], nn.BatchNorm2d)):
                
                # åˆ›å»ºèåˆå±‚
                conv = layers[i+1]
                bn = layers[i+2]
                fused = nn.Sequential(
                    nn.ReLU(inplace=True),  # ä½¿ç”¨inplaceèŠ‚çœå†…å­˜
                    conv,
                    bn
                )
                fused_layers.append(fused)
                i += 3
            else:
                fused_layers.append(layers[i])
                i += 1
        
        return nn.Sequential(*fused_layers)
    
    def forward(self, x, weights, training=True):
        """
        é«˜æ€§èƒ½å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥å¼ é‡
            weights: æ¶æ„æƒé‡ [num_ops]
            training: æ˜¯å¦è®­ç»ƒæ¨¡å¼
        """
        self.forward_count += 1
        
        if not training:
            # æ¨ç†æ—¶åªä½¿ç”¨æœ€å¤§æƒé‡çš„æ“ä½œ
            max_idx = weights.argmax()
            op_name = self._op_names[max_idx]
            return self._ops[op_name](x)
        
        # è®­ç»ƒæ—¶çš„æ™ºèƒ½é€‰æ‹©ç­–ç•¥
        return self._forward_training(x, weights)
    
    def _forward_training(self, x, weights):
        """è®­ç»ƒæ—¶çš„ä¼˜åŒ–å‰å‘ä¼ æ’­"""
        device = x.device
        
        # ç­–ç•¥1ï¼šæƒé‡é˜ˆå€¼è¿‡æ»¤
        active_indices = (weights > self.weight_threshold).nonzero(as_tuple=True)[0]
        
        if len(active_indices) == 0:
            # æ‰€æœ‰æƒé‡éƒ½å¾ˆå°ï¼Œä½¿ç”¨æƒé‡æœ€å¤§çš„æ“ä½œ
            active_indices = weights.topk(1)[1]
        elif len(active_indices) > self.top_k:
            # å¤ªå¤šæ´»è·ƒæ“ä½œï¼Œåªä¿ç•™å‰top_kä¸ª
            top_weights, top_indices = weights[active_indices].topk(self.top_k)
            active_indices = active_indices[top_indices]
        
        # æ›´æ–°ç»Ÿè®¡
        self.active_ops_avg = 0.9 * self.active_ops_avg + 0.1 * len(active_indices)
        
        # è®¡ç®—æ´»è·ƒæ“ä½œçš„è¾“å‡º
        outputs = []
        active_weights = []
        
        for idx in active_indices:
            op_name = self._op_names[idx.item()]
            weight = weights[idx]
            
            # ç¼“å­˜é”®
            cache_key = self._get_cache_key(x, op_name)
            
            if cache_key in self._cache:
                output = self._cache[cache_key]
                self._cache_hits += 1
            else:
                output = self._ops[op_name](x)
                # åªç¼“å­˜è®¡ç®—æˆæœ¬é«˜çš„æ“ä½œ
                if self._op_costs[op_name] > 1.0:
                    self._cache[cache_key] = output.clone()
                    # é™åˆ¶ç¼“å­˜å¤§å°
                    if len(self._cache) > 100:
                        self._cache.clear()
                self._cache_misses += 1
            
            outputs.append(output)
            active_weights.append(weight)
        
        # å½’ä¸€åŒ–æƒé‡
        active_weights = torch.stack(active_weights)
        active_weights = active_weights / (active_weights.sum() + 1e-8)
        
        # åŠ æƒæ±‚å’Œ
        result = sum(w * out for w, out in zip(active_weights, outputs))
        
        return result
    
    def _get_cache_key(self, x, op_name):
        """ç”Ÿæˆç¼“å­˜é”®"""
        # ç®€åŒ–çš„ç¼“å­˜é”®ï¼šåŸºäºè¾“å…¥å½¢çŠ¶å’Œæ“ä½œå
        return f"{op_name}_{x.shape}_{x.device}"
    
    def get_performance_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        cache_hit_rate = self._cache_hits / max(self._cache_hits + self._cache_misses, 1)
        return {
            'forward_count': self.forward_count,
            'active_ops_avg': self.active_ops_avg,
            'cache_hit_rate': cache_hit_rate,
            'cache_size': len(self._cache)
        }
    
    def clear_cache(self):
        """æ¸…é™¤ç¼“å­˜"""
        self._cache.clear()
        self._cache_hits = 0
        self._cache_misses = 0

class BatchedArchitectureUpdate(nn.Module):
    """
    æ‰¹é‡åŒ–æ¶æ„å‚æ•°æ›´æ–°
    
    å°†å¤šä¸ªæ¶æ„å‚æ•°çš„æ›´æ–°æ“ä½œæ‰¹é‡åŒ–ï¼Œå‡å°‘GPU kernelè°ƒç”¨æ¬¡æ•°
    """
    
    def __init__(self, num_layers, num_ops_per_layer):
        super().__init__()
        self.num_layers = num_layers
        self.num_ops_per_layer = num_ops_per_layer
        
        # æ‰¹é‡åŒ–çš„æ¶æ„å‚æ•°
        self.arch_params = nn.Parameter(
            torch.randn(num_layers, num_ops_per_layer) * 0.1
        )
        
        # Gumbel-Softmaxå‚æ•°
        self.temperature = 5.0
        self.min_temperature = 0.1
        self.anneal_rate = 0.98
    
    def forward(self, layer_idx=None):
        """
        è·å–æ¶æ„æƒé‡
        
        Args:
            layer_idx: å±‚ç´¢å¼•ï¼ŒNoneè¡¨ç¤ºè¿”å›æ‰€æœ‰å±‚
        """
        if layer_idx is not None:
            logits = self.arch_params[layer_idx]
        else:
            logits = self.arch_params
        
        if self.training:
            return self._gumbel_softmax(logits)
        else:
            return F.softmax(logits, dim=-1)
    
    def _gumbel_softmax(self, logits):
        """æ‰¹é‡åŒ–Gumbel-Softmaxé‡‡æ ·"""
        # ç”ŸæˆGumbelå™ªå£°
        gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) + 1e-8) + 1e-8)
        
        # åŠ å…¥å™ªå£°å¹¶é™¤ä»¥æ¸©åº¦
        noisy_logits = (logits + gumbel_noise) / self.temperature
        
        # Softmax
        soft_weights = F.softmax(noisy_logits, dim=-1)
        
        # ç¡¬é‡‡æ ·ï¼ˆå‰å‘æ—¶ç¦»æ•£ï¼Œåå‘æ—¶è¿ç»­ï¼‰
        hard_weights = F.one_hot(soft_weights.argmax(dim=-1), soft_weights.size(-1)).float()
        
        # ä½¿ç”¨straight-through estimator
        return hard_weights - soft_weights.detach() + soft_weights
    
    def anneal_temperature(self):
        """é€€ç«æ¸©åº¦"""
        self.temperature = max(self.min_temperature, self.temperature * self.anneal_rate)
        return self.temperature
    
    def get_dominant_ops(self, threshold=0.5):
        """è·å–å ä¸»å¯¼åœ°ä½çš„æ“ä½œ"""
        with torch.no_grad():
            probs = F.softmax(self.arch_params, dim=-1)
            dominant = (probs > threshold).float()
            return dominant

class MemoryEfficientCell(nn.Module):
    """
    å†…å­˜é«˜æ•ˆçš„Cellå®ç°
    
    ä¼˜åŒ–ç­–ç•¥ï¼š
    1. æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼šæƒè¡¡è®¡ç®—å’Œå†…å­˜
    2. è¾“å‡ºç¼“å­˜ï¼šé¿å…é‡å¤è®¡ç®—
    3. åŠ¨æ€å½¢çŠ¶ï¼šæ ¹æ®éœ€è¦è°ƒæ•´
    """
    
    def __init__(self, C_in, C_out, num_nodes=4, num_ops=8, use_checkpoint=True):
        super().__init__()
        
        self.C_in = C_in
        self.C_out = C_out
        self.num_nodes = num_nodes
        self.num_ops = num_ops
        self.use_checkpoint = use_checkpoint
        
        # é¢„å¤„ç†å±‚
        self.preprocess0 = self._preprocess_layer(C_in, C_out)
        self.preprocess1 = self._preprocess_layer(C_in, C_out)
        
        # æ··åˆæ“ä½œ
        self.ops = nn.ModuleList()
        for i in range(num_nodes):
            for j in range(2 + i):  # æ¯ä¸ªèŠ‚ç‚¹è¿æ¥å‰é¢çš„æ‰€æœ‰èŠ‚ç‚¹
                op = FastMixedOp(C_out, stride=1)
                self.ops.append(op)
        
        # æ¶æ„å‚æ•°æ›´æ–°å™¨
        num_edges = sum(range(2, 2 + num_nodes))
        self.arch_updater = BatchedArchitectureUpdate(num_edges, num_ops)
        
        # è¾“å‡ºå¤„ç†
        self.output_conv = nn.Conv2d(num_nodes * C_out, C_out, 1, bias=False)
        self.output_bn = nn.BatchNorm2d(C_out)
        
        # æ€§èƒ½ç›‘æ§
        self.memory_usage = []
        self.compute_time = []
    
    def _preprocess_layer(self, C_in, C_out):
        """é¢„å¤„ç†å±‚"""
        if C_in == C_out:
            return nn.Sequential(
                nn.ReLU(inplace=True),
                nn.BatchNorm2d(C_in)
            )
        else:
            return nn.Sequential(
                nn.ReLU(inplace=True),
                nn.Conv2d(C_in, C_out, 1, bias=False),
                nn.BatchNorm2d(C_out)
            )
    
    def forward(self, s0, s1):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            s0, s1: å‰ä¸¤ä¸ªèŠ‚ç‚¹çš„è¾“å‡º
        """
        # é¢„å¤„ç†
        s0 = self.preprocess0(s0)
        s1 = self.preprocess1(s1)
        
        states = [s0, s1]
        
        # è·å–æ‰€æœ‰æ¶æ„æƒé‡
        arch_weights = self.arch_updater()
        
        op_idx = 0
        for i in range(self.num_nodes):
            # è®¡ç®—æ–°èŠ‚ç‚¹
            if self.use_checkpoint and self.training:
                # ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹èŠ‚çœå†…å­˜
                new_state = torch.utils.checkpoint.checkpoint(
                    self._compute_node, i, states, arch_weights, op_idx
                )
            else:
                new_state = self._compute_node(i, states, arch_weights, op_idx)
            
            states.append(new_state)
            op_idx += len(states) - 1
        
        # æ‹¼æ¥æ‰€æœ‰ä¸­é—´èŠ‚ç‚¹çš„è¾“å‡º
        intermediate_states = states[2:]  # æ’é™¤è¾“å…¥èŠ‚ç‚¹
        output = torch.cat(intermediate_states, dim=1)
        
        # è¾“å‡ºå¤„ç†
        output = self.output_conv(output)
        output = self.output_bn(output)
        
        return output
    
    def _compute_node(self, node_idx, states, arch_weights, start_op_idx):
        """è®¡ç®—å•ä¸ªèŠ‚ç‚¹"""
        node_inputs = []
        
        for j, state in enumerate(states):
            op_idx = start_op_idx + j
            if op_idx < len(arch_weights) and op_idx < len(self.ops):
                weight = arch_weights[op_idx]
                
                # ä½¿ç”¨FastMixedOp
                op_output = self.ops[op_idx](state, weight, self.training)
                node_inputs.append(op_output)
        
        # æ±‚å’Œæ‰€æœ‰è¾“å…¥
        return sum(node_inputs) if node_inputs else states[0]
    
    def get_memory_usage(self):
        """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if torch.cuda.is_available():
            return torch.cuda.memory_allocated() / 1024**2  # MB
        return 0

class FastDeviceManager:
    """
    å¿«é€Ÿè®¾å¤‡ç®¡ç†å™¨
    
    ä¼˜åŒ–ç­–ç•¥ï¼š
    1. è®¾å¤‡äº²å’Œæ€§ï¼šå°†ç›¸å…³æ“ä½œæ”¾åœ¨åŒä¸€è®¾å¤‡
    2. å†…å­˜æ± ï¼šé¢„åˆ†é…å†…å­˜å‡å°‘åˆ†é…å¼€é”€
    3. å¼‚æ­¥ä¼ è¾“ï¼šé‡å è®¡ç®—å’Œæ•°æ®ä¼ è¾“
    4. æ‰¹é‡æ“ä½œï¼šå‡å°‘kernelè°ƒç”¨
    """
    
    def __init__(self, device=None, memory_pool_size=1024):
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.memory_pool_size = memory_pool_size  # MB
        
        # å†…å­˜æ± 
        self._init_memory_pool()
        
        # æ€§èƒ½ç»Ÿè®¡
        self.transfer_time = 0.0
        self.transfer_count = 0
        
        print(f"ğŸš€ FastDeviceManager initialized on {self.device}")
    
    def _init_memory_pool(self):
        """åˆå§‹åŒ–å†…å­˜æ± """
        if self.device.type == 'cuda':
            # é¢„åˆ†é…ä¸€äº›å¸¸ç”¨å¤§å°çš„å¼ é‡
            self.memory_pool = {}
            common_shapes = [
                (1, 32, 32, 32), (1, 64, 16, 16), (1, 128, 8, 8),
                (16, 32, 32, 32), (16, 64, 16, 16), (16, 128, 8, 8)
            ]
            
            for shape in common_shapes:
                self.memory_pool[shape] = torch.empty(shape, device=self.device)
    
    def to_device(self, tensor, non_blocking=True):
        """é«˜æ•ˆè®¾å¤‡è½¬ç§»"""
        if tensor.device == self.device:
            return tensor
        
        start_time = time.time()
        result = tensor.to(self.device, non_blocking=non_blocking)
        self.transfer_time += time.time() - start_time
        self.transfer_count += 1
        
        return result
    
    def get_tensor_from_pool(self, shape, dtype=torch.float32):
        """ä»å†…å­˜æ± è·å–å¼ é‡"""
        if shape in self.memory_pool:
            return self.memory_pool[shape].clone()
        else:
            return torch.empty(shape, dtype=dtype, device=self.device)
    
    def get_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        avg_transfer_time = self.transfer_time / max(self.transfer_count, 1)
        return {
            'device': str(self.device),
            'total_transfer_time': self.transfer_time,
            'transfer_count': self.transfer_count,
            'avg_transfer_time': avg_transfer_time
        }

# å…¨å±€è®¾å¤‡ç®¡ç†å™¨å®ä¾‹
_fast_device_manager = None

def get_fast_device_manager():
    """è·å–å…¨å±€å¿«é€Ÿè®¾å¤‡ç®¡ç†å™¨"""
    global _fast_device_manager
    if _fast_device_manager is None:
        _fast_device_manager = FastDeviceManager()
    return _fast_device_manager

class OperationProfiler:
    """
    æ“ä½œæ€§èƒ½åˆ†æå™¨
    
    ç”¨äºåˆ†æä¸åŒæ“ä½œçš„è®¡ç®—æˆæœ¬ï¼ŒæŒ‡å¯¼ä¼˜åŒ–å†³ç­–
    """
    
    def __init__(self):
        self.operation_times = {}
        self.operation_memory = {}
        self.operation_count = {}
    
    def profile_operation(self, op_name, operation, input_tensor, num_runs=10):
        """åˆ†ææ“ä½œæ€§èƒ½"""
        device = input_tensor.device
        
        # é¢„çƒ­
        for _ in range(3):
            _ = operation(input_tensor)
        
        if device.type == 'cuda':
            torch.cuda.synchronize()
        
        # è®¡æ—¶
        start_time = time.time()
        start_memory = torch.cuda.memory_allocated() if device.type == 'cuda' else 0
        
        for _ in range(num_runs):
            output = operation(input_tensor)
        
        if device.type == 'cuda':
            torch.cuda.synchronize()
        
        end_time = time.time()
        end_memory = torch.cuda.memory_allocated() if device.type == 'cuda' else 0
        
        # è®°å½•ç»“æœ
        avg_time = (end_time - start_time) / num_runs
        memory_delta = end_memory - start_memory
        
        self.operation_times[op_name] = avg_time
        self.operation_memory[op_name] = memory_delta
        self.operation_count[op_name] = self.operation_count.get(op_name, 0) + 1
    
    def get_operation_ranking(self, criterion='time'):
        """è·å–æ“ä½œæ’å"""
        if criterion == 'time':
            data = self.operation_times
        elif criterion == 'memory':
            data = self.operation_memory
        else:
            raise ValueError(f"Unknown criterion: {criterion}")
        
        return sorted(data.items(), key=lambda x: x[1])
    
    def print_report(self):
        """æ‰“å°æ€§èƒ½æŠ¥å‘Š"""
        print("\nğŸ” Operation Performance Report:")
        print("=" * 50)
        
        print("\nâ±ï¸ Time Ranking (fastest to slowest):")
        for op_name, time_ms in self.get_operation_ranking('time'):
            print(f"  {op_name}: {time_ms*1000:.2f}ms")
        
        print("\nğŸ’¾ Memory Ranking (least to most):")
        for op_name, memory_bytes in self.get_operation_ranking('memory'):
            print(f"  {op_name}: {memory_bytes/1024/1024:.2f}MB")

# å¯¼å‡ºæ¥å£
__all__ = [
    'FastMixedOp',
    'BatchedArchitectureUpdate', 
    'MemoryEfficientCell',
    'FastDeviceManager',
    'get_fast_device_manager',
    'OperationProfiler'
]