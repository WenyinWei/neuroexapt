#!/usr/bin/env python3
"""
DNM Net2Net å‚æ•°å¹³æ»‘è¿ç§»æ¨¡å—

åŸºäºNet2Netè®ºæ–‡æ€æƒ³ï¼Œå®ç°ç¥ç»ç½‘ç»œæ¶æ„å˜å¼‚æ—¶çš„å‚æ•°å¹³æ»‘è¿ç§»ï¼š
1. ç½‘ç»œåŠ å®½(Net2WiderNet): å¢åŠ ç¥ç»å…ƒæ•°é‡
2. ç½‘ç»œåŠ æ·±(Net2DeeperNet): å¢åŠ ç½‘ç»œå±‚æ•°
3. åˆ†æ”¯åˆ†è£‚: ä¸€å±‚åˆ†è£‚æˆå¤šä¸ªå¹¶è¡Œåˆ†æ”¯
4. æ“ä½œå˜å¼‚: å·ç§¯å±‚å˜å¼‚æˆä¸åŒæ“ä½œç»„åˆ

ç¡®ä¿æ¶æ„å˜å¼‚æ—¶ä¿æŒå‡½æ•°ç­‰ä»·æ€§å’Œè®­ç»ƒç¨³å®šæ€§
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import copy
from typing import Dict, List, Tuple, Optional, Union
import logging

logger = logging.getLogger(__name__)


class Net2NetTransformer:
    """Net2Netå˜æ¢å™¨ - å®ç°å¹³æ»‘çš„æ¶æ„å˜å¼‚"""
    
    def __init__(self, noise_scale: float = 1e-5):
        self.noise_scale = noise_scale
        
    def wider_conv2d(self, layer: nn.Conv2d, new_out_channels: int, 
                     next_layer: Optional[nn.Module] = None) -> Tuple[nn.Conv2d, Optional[nn.Module]]:
        """
        Net2WiderNet: æ‰©å±•å·ç§¯å±‚çš„è¾“å‡ºé€šé“æ•°
        
        Args:
            layer: åŸå§‹å·ç§¯å±‚
            new_out_channels: æ–°çš„è¾“å‡ºé€šé“æ•°
            next_layer: ä¸‹ä¸€å±‚(ç”¨äºæƒé‡è°ƒæ•´)
            
        Returns:
            (æ–°å·ç§¯å±‚, è°ƒæ•´åçš„ä¸‹ä¸€å±‚)
        """
        assert new_out_channels > layer.out_channels, "æ–°é€šé“æ•°å¿…é¡»å¤§äºåŸé€šé“æ•°"
        
        # åˆ›å»ºæ–°çš„å·ç§¯å±‚
        new_layer = nn.Conv2d(
            in_channels=layer.in_channels,
            out_channels=new_out_channels,
            kernel_size=layer.kernel_size,
            stride=layer.stride,
            padding=layer.padding,
            dilation=layer.dilation,
            groups=layer.groups,
            bias=layer.bias is not None,
            padding_mode=layer.padding_mode
        )
        
        # åˆå§‹åŒ–æƒé‡
        with torch.no_grad():
            # å¤åˆ¶åŸæœ‰æƒé‡
            new_layer.weight[:layer.out_channels] = layer.weight.data
            if layer.bias is not None:
                new_layer.bias[:layer.out_channels] = layer.bias.data
            
            # ä¸ºæ–°å¢é€šé“éšæœºé€‰æ‹©å¤åˆ¶æº
            additional_channels = new_out_channels - layer.out_channels
            for i in range(additional_channels):
                # éšæœºé€‰æ‹©ä¸€ä¸ªåŸæœ‰é€šé“è¿›è¡Œå¤åˆ¶
                source_idx = np.random.randint(0, layer.out_channels)
                target_idx = layer.out_channels + i
                
                # å¤åˆ¶æƒé‡å¹¶æ·»åŠ å°æ‰°åŠ¨
                new_layer.weight[target_idx] = layer.weight[source_idx].clone()
                new_layer.weight[target_idx] += torch.randn_like(new_layer.weight[target_idx]) * self.noise_scale
                
                if layer.bias is not None:
                    new_layer.bias[target_idx] = layer.bias[source_idx].clone()
                    new_layer.bias[target_idx] += torch.randn_like(new_layer.bias[target_idx]) * self.noise_scale
        
        # è°ƒæ•´ä¸‹ä¸€å±‚
        new_next_layer = None
        if next_layer is not None:
            new_next_layer = self._adjust_next_layer_for_wider(next_layer, layer.out_channels, new_out_channels)
        
        logger.info(f"Net2WiderNet: {layer.out_channels} -> {new_out_channels} channels")
        return new_layer, new_next_layer
    
    def wider_linear(self, layer: nn.Linear, new_out_features: int, 
                     next_layer: Optional[nn.Module] = None) -> Tuple[nn.Linear, Optional[nn.Module]]:
        """
        Net2WiderNet: æ‰©å±•çº¿æ€§å±‚çš„è¾“å‡ºç‰¹å¾æ•°
        """
        assert new_out_features > layer.out_features, "æ–°ç‰¹å¾æ•°å¿…é¡»å¤§äºåŸç‰¹å¾æ•°"
        
        # åˆ›å»ºæ–°çš„çº¿æ€§å±‚
        new_layer = nn.Linear(
            in_features=layer.in_features,
            out_features=new_out_features,
            bias=layer.bias is not None
        )
        
        # åˆå§‹åŒ–æƒé‡
        with torch.no_grad():
            # å¤åˆ¶åŸæœ‰æƒé‡
            new_layer.weight[:layer.out_features] = layer.weight.data
            if layer.bias is not None:
                new_layer.bias[:layer.out_features] = layer.bias.data
            
            # ä¸ºæ–°å¢ç‰¹å¾å¤åˆ¶æƒé‡
            additional_features = new_out_features - layer.out_features
            for i in range(additional_features):
                source_idx = np.random.randint(0, layer.out_features)
                target_idx = layer.out_features + i
                
                new_layer.weight[target_idx] = layer.weight[source_idx].clone()
                new_layer.weight[target_idx] += torch.randn_like(new_layer.weight[target_idx]) * self.noise_scale
                
                if layer.bias is not None:
                    new_layer.bias[target_idx] = layer.bias[source_idx].clone()
        
        # è°ƒæ•´ä¸‹ä¸€å±‚
        new_next_layer = None
        if next_layer is not None:
            new_next_layer = self._adjust_next_layer_for_wider(next_layer, layer.out_features, new_out_features)
        
        logger.info(f"Net2WiderNet: {layer.out_features} -> {new_out_features} features")
        return new_layer, new_next_layer
    
    def deeper_conv2d(self, layer: nn.Conv2d, position: str = 'after') -> nn.Module:
        """
        Net2DeeperNet: åœ¨å·ç§¯å±‚åæ’å…¥æ–°å±‚
        
        Args:
            layer: åŸå§‹å·ç§¯å±‚
            position: 'before' æˆ– 'after'
            
        Returns:
            æ–°æ’å…¥çš„å±‚ï¼ˆæ’ç­‰å˜æ¢ï¼‰
        """
        if position == 'after':
            # åœ¨åé¢æ’å…¥1x1æ’ç­‰å·ç§¯
            new_layer = nn.Conv2d(
                in_channels=layer.out_channels,
                out_channels=layer.out_channels,
                kernel_size=1,
                stride=1,
                padding=0,
                bias=True
            )
            
            # åˆå§‹åŒ–ä¸ºæ’ç­‰å˜æ¢
            with torch.no_grad():
                nn.init.eye_(new_layer.weight.squeeze())
                if new_layer.bias is not None:
                    nn.init.zeros_(new_layer.bias)
                    
        else:  # before
            # åœ¨å‰é¢æ’å…¥æ’ç­‰å·ç§¯
            new_layer = nn.Conv2d(
                in_channels=layer.in_channels,
                out_channels=layer.in_channels,
                kernel_size=layer.kernel_size,
                stride=1,
                padding=layer.padding,
                bias=True
            )
            
            # åˆå§‹åŒ–ä¸ºæ’ç­‰å˜æ¢
            with torch.no_grad():
                # å¯¹äº3x3å·ç§¯ï¼Œä¸­å¿ƒè®¾ä¸º1ï¼Œå…¶ä½™ä¸º0
                nn.init.zeros_(new_layer.weight)
                if layer.kernel_size == (3, 3):
                    for i in range(layer.in_channels):
                        new_layer.weight[i, i, 1, 1] = 1.0
                elif layer.kernel_size == (1, 1):
                    nn.init.eye_(new_layer.weight.squeeze())
                
                if new_layer.bias is not None:
                    nn.init.zeros_(new_layer.bias)
        
        logger.info(f"Net2DeeperNet: Added identity layer {position} conv layer")
        return new_layer
    
    def deeper_linear(self, layer: nn.Linear, position: str = 'after') -> nn.Module:
        """
        Net2DeeperNet: åœ¨çº¿æ€§å±‚é™„è¿‘æ’å…¥æ–°å±‚
        """
        if position == 'after':
            new_layer = nn.Linear(layer.out_features, layer.out_features, bias=True)
        else:
            new_layer = nn.Linear(layer.in_features, layer.in_features, bias=True)
        
        # åˆå§‹åŒ–ä¸ºæ’ç­‰å˜æ¢
        with torch.no_grad():
            nn.init.eye_(new_layer.weight)
            nn.init.zeros_(new_layer.bias)
        
        logger.info(f"Net2DeeperNet: Added identity linear layer {position} existing layer")
        return new_layer
    
    def _adjust_next_layer_for_wider(self, next_layer: nn.Module, old_channels: int, new_channels: int) -> nn.Module:
        """è°ƒæ•´ä¸‹ä¸€å±‚ä»¥é€‚åº”æ‰©å®½çš„å‰ä¸€å±‚"""
        
        if isinstance(next_layer, nn.Conv2d):
            # è°ƒæ•´å·ç§¯å±‚çš„è¾“å…¥é€šé“
            new_next = nn.Conv2d(
                in_channels=new_channels,
                out_channels=next_layer.out_channels,
                kernel_size=next_layer.kernel_size,
                stride=next_layer.stride,
                padding=next_layer.padding,
                dilation=next_layer.dilation,
                groups=next_layer.groups,
                bias=next_layer.bias is not None,
                padding_mode=next_layer.padding_mode
            )
            
            with torch.no_grad():
                # å¤åˆ¶åŸæœ‰æƒé‡
                new_next.weight[:, :old_channels] = next_layer.weight.data
                
                # æ–°å¢é€šé“çš„æƒé‡è®¾ä¸º0ï¼ˆä¿æŒå‡½æ•°ç­‰ä»·æ€§ï¼‰
                new_next.weight[:, old_channels:] = 0
                
                if next_layer.bias is not None:
                    new_next.bias[:] = next_layer.bias.data
            
            return new_next
            
        elif isinstance(next_layer, nn.Linear):
            # è°ƒæ•´çº¿æ€§å±‚çš„è¾“å…¥ç‰¹å¾
            new_next = nn.Linear(
                in_features=new_channels,
                out_features=next_layer.out_features,
                bias=next_layer.bias is not None
            )
            
            with torch.no_grad():
                # å¤åˆ¶åŸæœ‰æƒé‡
                new_next.weight[:, :old_channels] = next_layer.weight.data
                
                # æ–°å¢ç‰¹å¾çš„æƒé‡è®¾ä¸º0
                new_next.weight[:, old_channels:] = 0
                
                if next_layer.bias is not None:
                    new_next.bias[:] = next_layer.bias.data
            
            return new_next
            
        elif isinstance(next_layer, nn.BatchNorm2d):
            # è°ƒæ•´BatchNormå±‚
            new_next = nn.BatchNorm2d(new_channels)
            
            with torch.no_grad():
                # å¤åˆ¶åŸæœ‰å‚æ•°
                new_next.weight[:old_channels] = next_layer.weight.data
                new_next.bias[:old_channels] = next_layer.bias.data
                new_next.running_mean[:old_channels] = next_layer.running_mean.data
                new_next.running_var[:old_channels] = next_layer.running_var.data
                
                # æ–°å¢é€šé“åˆå§‹åŒ–
                new_next.weight[old_channels:] = 1.0
                new_next.bias[old_channels:] = 0.0
                new_next.running_mean[old_channels:] = 0.0
                new_next.running_var[old_channels:] = 1.0
                
                new_next.num_batches_tracked = next_layer.num_batches_tracked
            
            return new_next
        
        return next_layer


class DNMArchitectureMutator:
    """DNMæ¶æ„å˜å¼‚å™¨ - æ™ºèƒ½çš„æ¶æ„å˜å¼‚ç­–ç•¥"""
    
    def __init__(self, transformer: Net2NetTransformer):
        self.transformer = transformer
        
    def split_conv_layer(self, layer: nn.Conv2d, split_type: str = 'parallel') -> nn.Module:
        """
        åˆ†è£‚å·ç§¯å±‚
        
        Args:
            layer: åŸå§‹å·ç§¯å±‚
            split_type: 'parallel' (å¹¶è¡Œåˆ†æ”¯) æˆ– 'sequential' (ä¸²è¡Œåˆ†å±‚)
            
        Returns:
            åˆ†è£‚åçš„æ¨¡å—
        """
        if split_type == 'parallel':
            return self._split_conv_parallel(layer)
        elif split_type == 'sequential':
            return self._split_conv_sequential(layer)
        else:
            raise ValueError(f"Unsupported split_type: {split_type}")
    
    def _split_conv_parallel(self, layer: nn.Conv2d) -> nn.Module:
        """å°†å·ç§¯å±‚åˆ†è£‚æˆä¸¤ä¸ªå¹¶è¡Œåˆ†æ”¯"""
        
        # è®¡ç®—æ¯ä¸ªåˆ†æ”¯çš„é€šé“æ•°
        branch1_channels = layer.out_channels // 2
        branch2_channels = layer.out_channels - branch1_channels
        
        # åˆ›å»ºä¸¤ä¸ªåˆ†æ”¯
        branch1 = nn.Conv2d(
            layer.in_channels, branch1_channels,
            layer.kernel_size, layer.stride, layer.padding,
            layer.dilation, layer.groups, layer.bias is not None
        )
        
        branch2 = nn.Conv2d(
            layer.in_channels, branch2_channels,
            layer.kernel_size, layer.stride, layer.padding,
            layer.dilation, layer.groups, layer.bias is not None
        )
        
        # åˆ†é…æƒé‡
        with torch.no_grad():
            branch1.weight.data = layer.weight[:branch1_channels].clone()
            branch2.weight.data = layer.weight[branch1_channels:].clone()
            
            if layer.bias is not None:
                branch1.bias.data = layer.bias[:branch1_channels].clone()
                branch2.bias.data = layer.bias[branch1_channels:].clone()
        
        # åˆ›å»ºå¹¶è¡Œæ¨¡å—
        class ParallelBranches(nn.Module):
            def __init__(self, branch1, branch2):
                super().__init__()
                self.branch1 = branch1
                self.branch2 = branch2
            
            def forward(self, x):
                out1 = self.branch1(x)
                out2 = self.branch2(x)
                return torch.cat([out1, out2], dim=1)
        
        logger.info(f"Split conv layer into parallel branches: {branch1_channels} + {branch2_channels}")
        return ParallelBranches(branch1, branch2)
    
    def _split_conv_sequential(self, layer: nn.Conv2d) -> nn.Module:
        """å°†å·ç§¯å±‚åˆ†è£‚æˆä¸¤ä¸ªä¸²è¡Œå±‚"""
        
        # è®¡ç®—ä¸­é—´é€šé“æ•°
        intermediate_channels = max(layer.out_channels, layer.in_channels)
        
        # ç¬¬ä¸€å±‚ï¼šè¾“å…¥ -> ä¸­é—´é€šé“
        layer1 = nn.Conv2d(
            layer.in_channels, intermediate_channels,
            kernel_size=1, stride=1, padding=0, bias=False
        )
        
        # ç¬¬äºŒå±‚ï¼šä¸­é—´é€šé“ -> è¾“å‡º
        layer2 = nn.Conv2d(
            intermediate_channels, layer.out_channels,
            layer.kernel_size, layer.stride, layer.padding,
            layer.dilation, bias=layer.bias is not None
        )
        
        # BatchNormå±‚
        bn = nn.BatchNorm2d(intermediate_channels)
        
        # æƒé‡åˆå§‹åŒ–
        with torch.no_grad():
            # ç¬¬ä¸€å±‚ä½¿ç”¨éšæœºåˆå§‹åŒ–
            nn.init.kaiming_normal_(layer1.weight, mode='fan_out', nonlinearity='relu')
            
            # ç¬¬äºŒå±‚å°½é‡ä¿æŒåŸæœ‰åŠŸèƒ½
            if intermediate_channels == layer.out_channels:
                # å¦‚æœé€šé“æ•°åŒ¹é…ï¼Œå¯ä»¥ç›´æ¥å¤åˆ¶æƒé‡
                layer2.weight.data = layer.weight.data.clone()
                if layer.bias is not None:
                    layer2.bias.data = layer.bias.data.clone()
            else:
                # å¦åˆ™ä½¿ç”¨Heåˆå§‹åŒ–
                nn.init.kaiming_normal_(layer2.weight, mode='fan_out', nonlinearity='relu')
                if layer2.bias is not None:
                    nn.init.zeros_(layer2.bias)
        
        # åˆ›å»ºä¸²è¡Œæ¨¡å—
        sequential_module = nn.Sequential(layer1, bn, nn.ReLU(inplace=True), layer2)
        
        logger.info(f"Split conv layer into sequential: {layer.in_channels} -> {intermediate_channels} -> {layer.out_channels}")
        return sequential_module
    
    def mutate_conv_to_depthwise_separable(self, layer: nn.Conv2d) -> nn.Module:
        """å°†æ ‡å‡†å·ç§¯å˜å¼‚ä¸ºæ·±åº¦å¯åˆ†ç¦»å·ç§¯"""
        
        if layer.groups != 1:
            logger.warning("Layer is already grouped, skipping depthwise separable mutation")
            return layer
        
        # æ·±åº¦å·ç§¯
        depthwise = nn.Conv2d(
            layer.in_channels, layer.in_channels,
            layer.kernel_size, layer.stride, layer.padding,
            groups=layer.in_channels, bias=False
        )
        
        # é€ç‚¹å·ç§¯
        pointwise = nn.Conv2d(
            layer.in_channels, layer.out_channels,
            kernel_size=1, stride=1, padding=0,
            bias=layer.bias is not None
        )
        
        # æƒé‡åˆå§‹åŒ–
        with torch.no_grad():
            # æ·±åº¦å·ç§¯ï¼šæ¯ä¸ªè¾“å…¥é€šé“å¯¹åº”ä¸€ä¸ªæ»¤æ³¢å™¨
            for i in range(layer.in_channels):
                if i < layer.out_channels:
                    # ä»åŸå±‚å¤åˆ¶å¯¹åº”çš„æƒé‡
                    depthwise.weight[i, 0] = layer.weight[i % layer.out_channels, i]
                else:
                    # éšæœºåˆå§‹åŒ–
                    nn.init.kaiming_normal_(depthwise.weight[i:i+1], mode='fan_out', nonlinearity='relu')
            
            # é€ç‚¹å·ç§¯åˆå§‹åŒ–
            nn.init.kaiming_normal_(pointwise.weight, mode='fan_out', nonlinearity='relu')
            if pointwise.bias is not None and layer.bias is not None:
                pointwise.bias.data = layer.bias.data.clone()
        
        # ç»„åˆæ¨¡å—
        module = nn.Sequential(depthwise, pointwise)
        
        logger.info(f"Mutated conv to depthwise separable: {layer.in_channels}x{layer.out_channels}")
        return module
    
    def add_residual_connection(self, layer: nn.Module, input_channels: int, output_channels: int) -> nn.Module:
        """ä¸ºå±‚æ·»åŠ æ®‹å·®è¿æ¥"""
        
        # åˆ›å»ºè·³è·ƒè¿æ¥ï¼ˆå¦‚æœé€šé“æ•°ä¸åŒ¹é…ï¼Œä½¿ç”¨1x1å·ç§¯è°ƒæ•´ï¼‰
        if input_channels == output_channels:
            shortcut = nn.Identity()
        else:
            shortcut = nn.Conv2d(input_channels, output_channels, 1, bias=False)
            with torch.no_grad():
                nn.init.kaiming_normal_(shortcut.weight, mode='fan_out', nonlinearity='relu')
        
        # åˆ›å»ºæ®‹å·®æ¨¡å—
        class ResidualBlock(nn.Module):
            def __init__(self, main_layer, shortcut):
                super().__init__()
                self.main_layer = main_layer
                self.shortcut = shortcut
            
            def forward(self, x):
                identity = self.shortcut(x)
                out = self.main_layer(x)
                
                # ç¡®ä¿å°ºå¯¸åŒ¹é…
                if out.shape != identity.shape:
                    # ä½¿ç”¨è‡ªé€‚åº”æ± åŒ–è°ƒæ•´ç©ºé—´å°ºå¯¸
                    identity = F.adaptive_avg_pool2d(identity, out.shape[2:])
                
                return out + identity
        
        logger.info(f"Added residual connection: {input_channels} -> {output_channels}")
        return ResidualBlock(layer, shortcut)


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•å‡½æ•°
def test_net2net_transforms():
    """æµ‹è¯•Net2Netå˜æ¢"""
    print("ğŸ§ª Testing Net2Net Transforms")
    
    transformer = Net2NetTransformer()
    mutator = DNMArchitectureMutator(transformer)
    
    # æµ‹è¯•å·ç§¯å±‚æ‰©å®½
    conv = nn.Conv2d(32, 64, 3, padding=1)
    new_conv, _ = transformer.wider_conv2d(conv, 96)
    print(f"âœ… Conv wider: {conv.out_channels} -> {new_conv.out_channels}")
    
    # æµ‹è¯•å·ç§¯å±‚åŠ æ·±
    deeper_layer = transformer.deeper_conv2d(conv, 'after')
    print(f"âœ… Conv deeper: Added layer after conv")
    
    # æµ‹è¯•å¹¶è¡Œåˆ†è£‚
    parallel_split = mutator.split_conv_layer(conv, 'parallel')
    print(f"âœ… Parallel split: Created branched structure")
    
    # æµ‹è¯•ä¸²è¡Œåˆ†è£‚
    sequential_split = mutator.split_conv_layer(conv, 'sequential')
    print(f"âœ… Sequential split: Created layered structure")
    
    # æµ‹è¯•æ·±åº¦å¯åˆ†ç¦»å·ç§¯å˜å¼‚
    depthwise_sep = mutator.mutate_conv_to_depthwise_separable(conv)
    print(f"âœ… Depthwise separable: Converted standard conv")
    
    print("ğŸ‰ Net2Net transforms test completed!")


if __name__ == "__main__":
    test_net2net_transforms()