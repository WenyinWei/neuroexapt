#!/usr/bin/env python3
"""
"""
defgroup group_separated_training Separated Training
ingroup core
Separated Training module for NeuroExapt framework.
"""


åˆ†ç¦»è®­ç»ƒç­–ç•¥

å®ç°æ¶æ„å‚æ•°ï¼ˆalphasï¼‰å’Œç½‘ç»œæƒé‡å‚æ•°çš„åˆ†ç¦»è®­ç»ƒï¼š
1. å¤§éƒ¨åˆ†epochsè®­ç»ƒç½‘ç»œæƒé‡å‚æ•°ï¼ˆå›ºå®šæ¶æ„å‚æ•°ï¼‰
2. å®šæœŸæ’å…¥æ¶æ„è®­ç»ƒepochï¼ˆå›ºå®šç½‘ç»œæƒé‡ï¼Œè®­ç»ƒæ¶æ„å‚æ•°ï¼‰
3. æ˜¾è‘—å‡å°‘æ€»å‚æ•°é‡å’Œè®¡ç®—å¼€é”€
"""

import torch
import torch.nn as nn
import torch.optim as optim
from typing import Dict, List, Optional, Tuple, Iterator
import time

class SeparatedTrainingStrategy:
    """
    åˆ†ç¦»è®­ç»ƒç­–ç•¥ç±»
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    - ä¸»è¦æ—¶é—´è®­ç»ƒç½‘ç»œæƒé‡ï¼ˆå›ºå®šæ¶æ„ï¼‰â†’ å¿«é€Ÿæ”¶æ•›
    - å¶å°”è®­ç»ƒæ¶æ„å‚æ•°ï¼ˆæŒ‡å¯¼æ¼”åŒ–æ–¹å‘ï¼‰â†’ æ¸è¿›ä¼˜åŒ–
    """
    
    def __init__(self, 
                 weight_training_epochs: int = 4,  # è¿ç»­è®­ç»ƒæƒé‡çš„epochæ•°
                 arch_training_epochs: int = 1,    # æ’å…¥æ¶æ„è®­ç»ƒçš„epochæ•°  
                 total_epochs: int = 20,
                 warmup_epochs: int = 5):          # å‰å‡ ä¸ªepochåªè®­ç»ƒæƒé‡
        
        self.weight_training_epochs = weight_training_epochs
        self.arch_training_epochs = arch_training_epochs  
        self.total_epochs = total_epochs
        self.warmup_epochs = warmup_epochs
        
        # è®­ç»ƒè®¡åˆ’
        self.training_schedule = self._create_schedule()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.weight_training_time = 0.0
        self.arch_training_time = 0.0
        
        print(f"ğŸ§¬ åˆ†ç¦»è®­ç»ƒç­–ç•¥:")
        print(f"   æƒé‡è®­ç»ƒè½®æ¬¡: {weight_training_epochs}")
        print(f"   æ¶æ„è®­ç»ƒè½®æ¬¡: {arch_training_epochs}")
        print(f"   é¢„çƒ­è½®æ¬¡: {warmup_epochs}")
        print(f"   è®­ç»ƒè®¡åˆ’: {self.get_schedule_summary()}")
    
    def _create_schedule(self) -> List[str]:
        """åˆ›å»ºè®­ç»ƒè®¡åˆ’"""
        schedule = []
        
        # é¢„çƒ­é˜¶æ®µï¼šåªè®­ç»ƒæƒé‡
        for i in range(self.warmup_epochs):
            schedule.append('weight')
        
        # ä¸»è®­ç»ƒé˜¶æ®µï¼šäº¤æ›¿è®­ç»ƒ
        remaining_epochs = self.total_epochs - self.warmup_epochs
        cycle_length = self.weight_training_epochs + self.arch_training_epochs
        
        current_epoch = self.warmup_epochs
        while current_epoch < self.total_epochs:
            # æƒé‡è®­ç»ƒé˜¶æ®µ
            for i in range(self.weight_training_epochs):
                if current_epoch >= self.total_epochs:
                    break
                schedule.append('weight')
                current_epoch += 1
            
            # æ¶æ„è®­ç»ƒé˜¶æ®µ
            for i in range(self.arch_training_epochs):
                if current_epoch >= self.total_epochs:
                    break
                schedule.append('arch')
                current_epoch += 1
        
        return schedule
    
    def get_training_mode(self, epoch: int) -> str:
        """è·å–å½“å‰epochçš„è®­ç»ƒæ¨¡å¼"""
        if epoch < len(self.training_schedule):
            return self.training_schedule[epoch]
        return 'weight'  # é»˜è®¤è®­ç»ƒæƒé‡
    
    def get_schedule_summary(self) -> str:
        """è·å–è®­ç»ƒè®¡åˆ’æ‘˜è¦"""
        weight_count = self.training_schedule.count('weight')
        arch_count = self.training_schedule.count('arch')
        return f"æƒé‡{weight_count}è½® + æ¶æ„{arch_count}è½® = æ€»å…±{len(self.training_schedule)}è½®"
    
    def should_train_weights(self, epoch: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è®­ç»ƒç½‘ç»œæƒé‡"""
        return self.get_training_mode(epoch) == 'weight'
    
    def should_train_architecture(self, epoch: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è®­ç»ƒæ¶æ„å‚æ•°"""
        return self.get_training_mode(epoch) == 'arch'

class SeparatedOptimizer:
    """
    åˆ†ç¦»ä¼˜åŒ–å™¨
    
    åˆ†åˆ«ç®¡ç†ç½‘ç»œæƒé‡å’Œæ¶æ„å‚æ•°çš„ä¼˜åŒ–å™¨
    """
    
    def __init__(self, model: nn.Module, weight_lr: float = 0.025, arch_lr: float = 3e-4,
                 weight_momentum: float = 0.9, weight_decay: float = 3e-4):
        
        self.model = model
        
        # åˆ†ç¦»å‚æ•°
        self.weight_params = []
        self.arch_params = []
        
        # è·å–æ¶æ„å‚æ•°
        if hasattr(model, 'arch_parameters'):
            self.arch_params = list(model.arch_parameters())
        
        # è·å–ç½‘ç»œæƒé‡å‚æ•°ï¼ˆæ’é™¤æ¶æ„å‚æ•°ï¼‰
        arch_param_ids = {id(p) for p in self.arch_params}
        for param in model.parameters():
            if id(param) not in arch_param_ids:
                self.weight_params.append(param)
        
        # åˆ›å»ºåˆ†ç¦»çš„ä¼˜åŒ–å™¨
        self.weight_optimizer = optim.SGD(
            self.weight_params,
            lr=weight_lr,
            momentum=weight_momentum,
            weight_decay=weight_decay
        )
        
        self.arch_optimizer = optim.Adam(
            self.arch_params,
            lr=arch_lr,
            weight_decay=1e-3
        )
        
        # ç»Ÿè®¡ä¿¡æ¯
        print(f"ğŸ“Š å‚æ•°ç»Ÿè®¡:")
        print(f"   ç½‘ç»œæƒé‡å‚æ•°: {sum(p.numel() for p in self.weight_params):,}")
        print(f"   æ¶æ„å‚æ•°: {sum(p.numel() for p in self.arch_params):,}")
        
        weight_params_count = sum(p.numel() for p in self.weight_params)
        arch_params_count = sum(p.numel() for p in self.arch_params)
        total_params = weight_params_count + arch_params_count
        
        if total_params > 0:
            arch_ratio = arch_params_count / total_params * 100
            print(f"   æ¶æ„å‚æ•°å æ¯”: {arch_ratio:.2f}%")
    
    def zero_grad_weights(self):
        """æ¸…é›¶ç½‘ç»œæƒé‡æ¢¯åº¦"""
        self.weight_optimizer.zero_grad()
    
    def zero_grad_arch(self):
        """æ¸…é›¶æ¶æ„å‚æ•°æ¢¯åº¦"""
        self.arch_optimizer.zero_grad()
    
    def step_weights(self):
        """æ›´æ–°ç½‘ç»œæƒé‡"""
        self.weight_optimizer.step()
    
    def step_arch(self):
        """æ›´æ–°æ¶æ„å‚æ•°"""
        self.arch_optimizer.step()
    
    def freeze_arch_params(self):
        """å†»ç»“æ¶æ„å‚æ•°"""
        for param in self.arch_params:
            param.requires_grad = False
    
    def unfreeze_arch_params(self):
        """è§£å†»æ¶æ„å‚æ•°"""
        for param in self.arch_params:
            param.requires_grad = True
    
    def freeze_weight_params(self):
        """å†»ç»“ç½‘ç»œæƒé‡å‚æ•°"""
        for param in self.weight_params:
            param.requires_grad = False
    
    def unfreeze_weight_params(self):
        """è§£å†»ç½‘ç»œæƒé‡å‚æ•°"""
        for param in self.weight_params:
            param.requires_grad = True
    
    def get_lr_schedulers(self):
        """è·å–å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        weight_scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.weight_optimizer, T_max=20, eta_min=0.001
        )
        arch_scheduler = optim.lr_scheduler.StepLR(
            self.arch_optimizer, step_size=10, gamma=0.5
        )
        return weight_scheduler, arch_scheduler

class SeparatedTrainer:
    """
    åˆ†ç¦»è®­ç»ƒå™¨
    
    å®ç°å®Œæ•´çš„åˆ†ç¦»è®­ç»ƒé€»è¾‘
    """
    
    def __init__(self, model: nn.Module, strategy: SeparatedTrainingStrategy,
                 optimizer: SeparatedOptimizer, criterion: nn.Module):
        
        self.model = model
        self.strategy = strategy
        self.optimizer = optimizer
        self.criterion = criterion
        
        # è·å–å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.weight_scheduler, self.arch_scheduler = optimizer.get_lr_schedulers()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.epoch_stats = {}
    
    def train_epoch_weights(self, train_loader, epoch: int) -> Dict[str, float]:
        """è®­ç»ƒç½‘ç»œæƒé‡ï¼ˆå›ºå®šæ¶æ„å‚æ•°ï¼‰"""
        start_time = time.time()
        
        # å†»ç»“æ¶æ„å‚æ•°ï¼Œè§£å†»æƒé‡å‚æ•°
        self.optimizer.freeze_arch_params()
        self.optimizer.unfreeze_weight_params()
        
        self.model.train()
        
        total_loss = 0.0
        total_samples = 0
        correct = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.cuda(), target.cuda()
            
            self.optimizer.zero_grad_weights()
            output = self.model(data)
            loss = self.criterion(output, target)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.optimizer.weight_params, 5.0)
            
            self.optimizer.step_weights()
            
            # ç»Ÿè®¡
            total_loss += loss.item() * data.size(0)
            total_samples += data.size(0)
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
        
        epoch_time = time.time() - start_time
        self.strategy.weight_training_time += epoch_time
        
        avg_loss = total_loss / total_samples
        accuracy = 100.0 * correct / total_samples
        
        return {
            'loss': avg_loss,
            'accuracy': accuracy,
            'time': epoch_time
        }
    
    def train_epoch_architecture(self, train_loader, valid_loader, epoch: int) -> Dict[str, float]:
        """è®­ç»ƒæ¶æ„å‚æ•°ï¼ˆå›ºå®šç½‘ç»œæƒé‡ï¼‰"""
        start_time = time.time()
        
        # å†»ç»“æƒé‡å‚æ•°ï¼Œè§£å†»æ¶æ„å‚æ•°
        self.optimizer.freeze_weight_params()
        self.optimizer.unfreeze_arch_params()
        
        self.model.train()
        
        total_loss = 0.0
        total_samples = 0
        arch_updates = 0
        
        valid_iter = iter(valid_loader)
        
        for batch_idx, (data, target) in enumerate(train_loader):
            # è·å–éªŒè¯æ•°æ®ç”¨äºæ¶æ„æœç´¢
            try:
                valid_data, valid_target = next(valid_iter)
            except StopIteration:
                valid_iter = iter(valid_loader)
                valid_data, valid_target = next(valid_iter)
            
            data, target = data.cuda(), target.cuda()
            valid_data, valid_target = valid_data.cuda(), valid_target.cuda()
            
            # æ¶æ„å‚æ•°æ¢¯åº¦æ›´æ–°
            self.optimizer.zero_grad_arch()
            
            # åœ¨éªŒè¯æ•°æ®ä¸Šè¯„ä¼°å½“å‰æ¶æ„
            valid_output = self.model(valid_data)
            arch_loss = self.criterion(valid_output, valid_target)
            
            arch_loss.backward()
            self.optimizer.step_arch()
            
            # ç»Ÿè®¡
            total_loss += arch_loss.item() * valid_data.size(0)
            total_samples += valid_data.size(0)
            arch_updates += 1
            
            # å®šæœŸè¾“å‡ºæ¶æ„æœç´¢è¿›åº¦
            if batch_idx % 50 == 0:
                print(f"    æ¶æ„æœç´¢æ­¥éª¤ {batch_idx}: æŸå¤±={arch_loss.item():.4f}")
        
        epoch_time = time.time() - start_time
        self.strategy.arch_training_time += epoch_time
        
        avg_loss = total_loss / total_samples
        
        return {
            'loss': avg_loss,
            'arch_updates': float(arch_updates),
            'time': epoch_time
        }
    
    def train_epoch(self, train_loader, valid_loader, epoch: int) -> Dict[str, float]:
        """æ ¹æ®ç­–ç•¥è®­ç»ƒä¸€ä¸ªepoch"""
        
        # æ›´æ–°å­¦ä¹ ç‡
        if self.strategy.should_train_weights(epoch):
            self.weight_scheduler.step()
        else:
            self.arch_scheduler.step()
        
        # æ ¹æ®è®­ç»ƒæ¨¡å¼é€‰æ‹©è®­ç»ƒæ–¹æ³•
        if self.strategy.should_train_weights(epoch):
            print(f"ğŸ‹ï¸ Epoch {epoch}: è®­ç»ƒç½‘ç»œæƒé‡å‚æ•°")
            stats = self.train_epoch_weights(train_loader, epoch)
        else:
            print(f"ğŸ§¬ Epoch {epoch}: è®­ç»ƒæ¶æ„å‚æ•°")
            stats = self.train_epoch_architecture(train_loader, valid_loader, epoch)
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        self.epoch_stats[epoch] = stats
        
        return stats
    
    def get_final_statistics(self) -> Dict[str, float]:
        """è·å–æœ€ç»ˆè®­ç»ƒç»Ÿè®¡"""
        weight_epochs = sum(1 for stats in self.epoch_stats.values() if stats['mode'] == 'weight')
        arch_epochs = sum(1 for stats in self.epoch_stats.values() if stats['mode'] == 'arch')
        
        total_time = self.strategy.weight_training_time + self.strategy.arch_training_time
        
        return {
            'weight_epochs': weight_epochs,
            'arch_epochs': arch_epochs,
            'weight_training_time': self.strategy.weight_training_time,
            'arch_training_time': self.strategy.arch_training_time,
            'total_time': total_time,
            'weight_time_ratio': self.strategy.weight_training_time / total_time if total_time > 0 else 0,
            'time_per_weight_epoch': self.strategy.weight_training_time / weight_epochs if weight_epochs > 0 else 0,
            'time_per_arch_epoch': self.strategy.arch_training_time / arch_epochs if arch_epochs > 0 else 0
        }

def create_separated_training_setup(model: nn.Module, 
                                  weight_training_epochs: int = 4,
                                  arch_training_epochs: int = 1,
                                  total_epochs: int = 20) -> Tuple[SeparatedTrainingStrategy, SeparatedOptimizer, SeparatedTrainer]:
    """
    åˆ›å»ºå®Œæ•´çš„åˆ†ç¦»è®­ç»ƒè®¾ç½®
    
    Returns:
        strategy: è®­ç»ƒç­–ç•¥
        optimizer: åˆ†ç¦»ä¼˜åŒ–å™¨  
        trainer: è®­ç»ƒå™¨
    """
    
    # åˆ›å»ºè®­ç»ƒç­–ç•¥
    strategy = SeparatedTrainingStrategy(
        weight_training_epochs=weight_training_epochs,
        arch_training_epochs=arch_training_epochs,
        total_epochs=total_epochs
    )
    
    # åˆ›å»ºåˆ†ç¦»ä¼˜åŒ–å™¨
    optimizer = SeparatedOptimizer(model)
    
    # åˆ›å»ºæŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss().cuda()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = SeparatedTrainer(model, strategy, optimizer, criterion)
    
    return strategy, optimizer, trainer

if __name__ == "__main__":
    # æµ‹è¯•åˆ†ç¦»è®­ç»ƒç­–ç•¥
    strategy = SeparatedTrainingStrategy(
        weight_training_epochs=4,
        arch_training_epochs=1, 
        total_epochs=20
    )
    
    print("è®­ç»ƒè®¡åˆ’:")
    for epoch in range(20):
        mode = strategy.get_training_mode(epoch)
        print(f"Epoch {epoch}: {mode}") 