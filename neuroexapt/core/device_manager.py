"""
@defgroup group_device_manager Device Manager
@ingroup core
Device Manager module for NeuroExapt framework.


è®¾å¤‡ç®¡ç†å™¨ (Device Manager)

ç»Ÿä¸€ç®¡ç†ASO-SEæ¡†æ¶ä¸­æ‰€æœ‰æ¨¡å‹å’Œæ•°æ®çš„è®¾å¤‡åˆ†é…ï¼Œç¡®ä¿ï¼š
1. æ‰€æœ‰æ¨¡å‹åœ¨åŒä¸€è®¾å¤‡ä¸Š
2. æ•°æ®è‡ªåŠ¨è½¬ç§»åˆ°æ­£ç¡®è®¾å¤‡
3. å†…å­˜é«˜æ•ˆåˆ©ç”¨
4. å¤šGPUæ”¯æŒï¼ˆæœªæ¥æ‰©å±•ï¼‰
"""

import torch
import torch.nn as nn
import logging
import gc
from typing import Dict, List, Optional, Union, Any
import warnings

logger = logging.getLogger(__name__)

class DeviceManager:
    """
    è®¾å¤‡ç®¡ç†å™¨
    
    ç»Ÿä¸€ç®¡ç†è®¾å¤‡åˆ†é…å’Œå†…å­˜ä¼˜åŒ–
    """
    
    def __init__(self, device: Optional[Union[str, torch.device]] = None, 
                 memory_fraction: float = 0.9):
        """
        Args:
            device: ç›®æ ‡è®¾å¤‡ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©
            memory_fraction: GPUå†…å­˜ä½¿ç”¨æ¯”ä¾‹
        """
        # è®¾å¤‡é€‰æ‹©
        if device is None:
            if torch.cuda.is_available():
                # é€‰æ‹©æ˜¾å­˜æœ€å¤šçš„GPU
                gpu_count = torch.cuda.device_count()
                if gpu_count > 1:
                    best_gpu = self._select_best_gpu()
                    self.device = torch.device(f'cuda:{best_gpu}')
                else:
                    self.device = torch.device('cuda:0')
            else:
                self.device = torch.device('cpu')
                warnings.warn("CUDA not available, using CPU. Performance will be significantly slower.")
        else:
            self.device = torch.device(device)
        
        self.memory_fraction = memory_fraction
        
        # è®¾å¤‡ä¿¡æ¯
        self._log_device_info()
        
        # å†…å­˜ç›‘æ§
        self.peak_memory = 0.0
        self.memory_history = []
        
        # æ¨¡å‹æ³¨å†Œè¡¨
        self.registered_models = {}
        
        logger.info(f"ğŸ”§ Device Manager initialized on {self.device}")
    
    def _select_best_gpu(self) -> int:
        """é€‰æ‹©æœ€ä¼˜GPU"""
        best_gpu = 0
        max_memory = 0
        
        for i in range(torch.cuda.device_count()):
            memory = torch.cuda.get_device_properties(i).total_memory
            if memory > max_memory:
                max_memory = memory
                best_gpu = i
        
        logger.info(f"ğŸ¯ Selected GPU {best_gpu} with {max_memory / 1024**3:.1f}GB memory")
        return best_gpu
    
    def _log_device_info(self):
        """è®°å½•è®¾å¤‡ä¿¡æ¯"""
        if self.device.type == 'cuda':
            props = torch.cuda.get_device_properties(self.device)
            total_memory = props.total_memory / 1024**3
            logger.info(f"ğŸ”§ Using GPU: {props.name}")
            logger.info(f"   Total Memory: {total_memory:.1f}GB")
            logger.info(f"   Compute Capability: {props.major}.{props.minor}")
            
            # è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥
            torch.cuda.set_per_process_memory_fraction(self.memory_fraction, self.device.index)
        else:
            logger.info("ğŸ”§ Using CPU")
    
    def register_model(self, name: str, model: nn.Module) -> nn.Module:
        """
        æ³¨å†Œå¹¶è½¬ç§»æ¨¡å‹åˆ°ç›®æ ‡è®¾å¤‡
        
        Args:
            name: æ¨¡å‹åç§°
            model: æ¨¡å‹å®ä¾‹
            
        Returns:
            è½¬ç§»åˆ°ç›®æ ‡è®¾å¤‡çš„æ¨¡å‹
        """
        logger.info(f"ğŸ“‹ Registering model '{name}' to {self.device}")
        
        # è½¬ç§»åˆ°ç›®æ ‡è®¾å¤‡
        model = model.to(self.device)
        
        # æ³¨å†Œæ¨¡å‹
        self.registered_models[name] = model
        
        # è®°å½•æ¨¡å‹å‚æ•°é‡
        param_count = sum(p.numel() for p in model.parameters())
        param_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / 1024**2
        
        logger.info(f"   Parameters: {param_count:,} ({param_size_mb:.1f}MB)")
        
        return model
    
    def to_device(self, data: Any) -> Any:
        """
        å°†æ•°æ®è½¬ç§»åˆ°ç›®æ ‡è®¾å¤‡
        
        Args:
            data: è¦è½¬ç§»çš„æ•°æ®
            
        Returns:
            è½¬ç§»åˆ°ç›®æ ‡è®¾å¤‡çš„æ•°æ®
        """
        if isinstance(data, torch.Tensor):
            return data.to(self.device, non_blocking=True)
        elif isinstance(data, (list, tuple)):
            return type(data)(self.to_device(item) for item in data)
        elif isinstance(data, dict):
            return {key: self.to_device(value) for key, value in data.items()}
        else:
            return data
    
    def create_tensor(self, *args, **kwargs) -> torch.Tensor:
        """åœ¨æ­£ç¡®è®¾å¤‡ä¸Šåˆ›å»ºå¼ é‡"""
        if 'device' not in kwargs:
            kwargs['device'] = self.device
        return torch.tensor(*args, **kwargs)
    
    def zeros_like(self, tensor: torch.Tensor) -> torch.Tensor:
        """åˆ›å»ºåŒè®¾å¤‡çš„é›¶å¼ é‡"""
        return torch.zeros_like(tensor, device=self.device)
    
    def ones_like(self, tensor: torch.Tensor) -> torch.Tensor:
        """åˆ›å»ºåŒè®¾å¤‡çš„å•ä½å¼ é‡"""
        return torch.ones_like(tensor, device=self.device)
    
    def transfer_model_state(self, source_model: nn.Module, 
                           target_model: nn.Module) -> nn.Module:
        """
        åœ¨åŒä¸€è®¾å¤‡ä¸Šè½¬ç§»æ¨¡å‹çŠ¶æ€
        
        Args:
            source_model: æºæ¨¡å‹
            target_model: ç›®æ ‡æ¨¡å‹
            
        Returns:
            åŠ è½½äº†çŠ¶æ€çš„ç›®æ ‡æ¨¡å‹
        """
        # ç¡®ä¿ä¸¤ä¸ªæ¨¡å‹åœ¨åŒä¸€è®¾å¤‡ä¸Š
        source_model = source_model.to(self.device)
        target_model = target_model.to(self.device)
        
        source_dict = source_model.state_dict()
        target_dict = target_model.state_dict()
        
        # é€å±‚è½¬ç§»å…¼å®¹çš„å‚æ•°
        transferred = 0
        for key in target_dict.keys():
            if key in source_dict:
                source_param = source_dict[key]
                target_param = target_dict[key]
                
                if source_param.shape == target_param.shape:
                    target_dict[key] = source_param.clone()
                    transferred += 1
                else:
                    logger.warning(f"Shape mismatch for {key}: "
                                 f"{source_param.shape} -> {target_param.shape}")
        
        target_model.load_state_dict(target_dict)
        logger.info(f"âœ… Transferred {transferred} parameters between models")
        
        return target_model
    
    def optimize_memory(self):
        """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        if self.device.type == 'cuda':
            # æ¸…ç©ºGPUç¼“å­˜
            torch.cuda.empty_cache()
            
            # è§¦å‘åƒåœ¾å›æ”¶
            gc.collect()
            
            # è®°å½•å†…å­˜ä½¿ç”¨
            current_memory = torch.cuda.memory_allocated(self.device) / 1024**2
            self.memory_history.append(current_memory)
            
            if current_memory > self.peak_memory:
                self.peak_memory = current_memory
            
            logger.debug(f"ğŸ’¾ Memory: {current_memory:.1f}MB (Peak: {self.peak_memory:.1f}MB)")
    
    def get_memory_stats(self) -> Dict[str, Union[float, str]]:
        """è·å–å†…å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if self.device.type == 'cuda':
            allocated = torch.cuda.memory_allocated(self.device) / 1024**2
            cached = torch.cuda.memory_reserved(self.device) / 1024**2
            total = torch.cuda.get_device_properties(self.device).total_memory / 1024**2
            
            return {
                'allocated_mb': allocated,
                'cached_mb': cached,
                'total_mb': total,
                'peak_mb': self.peak_memory,
                'utilization': allocated / total
            }
        else:
            return {'device': 'cpu'}
    
    def create_data_loader_wrapper(self, data_loader):
        """åˆ›å»ºè‡ªåŠ¨è½¬ç§»æ•°æ®çš„DataLoaderåŒ…è£…å™¨"""
        class DeviceDataLoader:
            def __init__(self, loader, device_manager):
                self.loader = loader
                self.device_manager = device_manager
            
            def __iter__(self):
                for batch in self.loader:
                    yield self.device_manager.to_device(batch)
            
            def __len__(self):
                return len(self.loader)
            
            def __getattr__(self, name):
                return getattr(self.loader, name)
        
        return DeviceDataLoader(data_loader, self)
    
    def safe_model_creation(self, model_class, *args, **kwargs) -> nn.Module:
        """å®‰å…¨çš„æ¨¡å‹åˆ›å»ºï¼Œè‡ªåŠ¨å¤„ç†è®¾å¤‡å’Œå†…å­˜"""
        try:
            # ç›´æ¥åœ¨ç›®æ ‡è®¾å¤‡ä¸Šåˆ›å»ºæ¨¡å‹
            model = model_class(*args, **kwargs)
            
            # ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            model = model.to(self.device)
            
            # ä¼˜åŒ–å†…å­˜
            self.optimize_memory()
            
            return model
            
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                logger.error("ğŸš¨ GPU memory insufficient for model creation")
                self.optimize_memory()
                
                # å°è¯•åœ¨CPUä¸Šåˆ›å»º
                logger.warning("ğŸ”„ Falling back to CPU")
                original_device = self.device
                self.device = torch.device('cpu')
                try:
                    model = model_class(*args, **kwargs)
                    return model
                except Exception as e2:
                    # æ¢å¤åŸè®¾å¤‡è®¾ç½®
                    self.device = original_device
                    raise e2
            else:
                raise e
    
    def checkpoint_memory_state(self) -> Dict[str, Any]:
        """æ£€æŸ¥ç‚¹å†…å­˜çŠ¶æ€"""
        if self.device.type == 'cuda':
            return {
                'allocated': torch.cuda.memory_allocated(self.device),
                'cached': torch.cuda.memory_reserved(self.device),
                'peak': self.peak_memory
            }
        return {}
    
    def restore_memory_state(self, state: Dict[str, Any]):
        """æ¢å¤å†…å­˜çŠ¶æ€"""
        if self.device.type == 'cuda' and state:
            # æ¸…ç†åˆ°ç›®æ ‡çŠ¶æ€
            target_allocated = state.get('allocated', 0)
            current_allocated = torch.cuda.memory_allocated(self.device)
            
            if current_allocated > target_allocated * 1.2:  # 20%å®¹å¿åº¦
                logger.info("ğŸ§¹ Cleaning up excess memory")
                self.optimize_memory()
    
    def context_switch_model(self, old_model: nn.Module, new_model: nn.Module) -> nn.Module:
        """
        ä¸Šä¸‹æ–‡åˆ‡æ¢æ¨¡å‹ï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨
        
        Args:
            old_model: æ—§æ¨¡å‹
            new_model: æ–°æ¨¡å‹
            
        Returns:
            ä¼˜åŒ–åçš„æ–°æ¨¡å‹
        """
        # ä¿å­˜å†…å­˜çŠ¶æ€
        memory_state = self.checkpoint_memory_state()
        
        # é‡Šæ”¾æ—§æ¨¡å‹
        if old_model is not None:
            old_model.cpu()  # å…ˆç§»åˆ°CPU
            del old_model
        
        # ä¼˜åŒ–å†…å­˜
        self.optimize_memory()
        
        # ç¡®ä¿æ–°æ¨¡å‹åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        new_model = new_model.to(self.device)
        
        logger.info("ğŸ”„ Model context switch completed")
        return new_model
    
    def get_device_report(self) -> Dict[str, Any]:
        """è·å–è®¾å¤‡ä½¿ç”¨æŠ¥å‘Š"""
        report = {
            'device': str(self.device),
            'device_type': self.device.type,
            'registered_models': list(self.registered_models.keys())
        }
        
        if self.device.type == 'cuda':
            props = torch.cuda.get_device_properties(self.device)
            memory_stats = self.get_memory_stats()
            
            report.update({
                'gpu_name': props.name,
                'gpu_memory_total_gb': props.total_memory / 1024**3,
                'memory_stats': memory_stats,
                'memory_fraction': self.memory_fraction
            })
        
        return report

# å…¨å±€è®¾å¤‡ç®¡ç†å™¨å®ä¾‹
_global_device_manager = None

def get_device_manager(device: Optional[Union[str, torch.device]] = None) -> DeviceManager:
    """è·å–å…¨å±€è®¾å¤‡ç®¡ç†å™¨å®ä¾‹"""
    global _global_device_manager
    
    if _global_device_manager is None:
        _global_device_manager = DeviceManager(device)
    
    return _global_device_manager

def reset_device_manager():
    """é‡ç½®å…¨å±€è®¾å¤‡ç®¡ç†å™¨"""
    global _global_device_manager
    _global_device_manager = None

def auto_device(data: Any) -> Any:
    """è‡ªåŠ¨è½¬ç§»æ•°æ®åˆ°ç®¡ç†çš„è®¾å¤‡"""
    dm = get_device_manager()
    return dm.to_device(data)

def register_model(name: str, model: nn.Module) -> nn.Module:
    """æ³¨å†Œæ¨¡å‹åˆ°è®¾å¤‡ç®¡ç†å™¨"""
    dm = get_device_manager()
    return dm.register_model(name, model)

def optimize_memory():
    """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
    dm = get_device_manager()
    dm.optimize_memory()

def test_device_manager():
    """æµ‹è¯•è®¾å¤‡ç®¡ç†å™¨åŠŸèƒ½"""
    print("ğŸ§ª Testing Device Manager...")
    
    # åˆ›å»ºè®¾å¤‡ç®¡ç†å™¨
    dm = DeviceManager()
    
    # æµ‹è¯•æ¨¡å‹æ³¨å†Œ
    test_model = nn.Sequential(
        nn.Conv2d(3, 16, 3),
        nn.ReLU(),
        nn.Conv2d(16, 32, 3)
    )
    
    registered_model = dm.register_model("test_model", test_model)
    print(f"âœ… Model registered on {next(registered_model.parameters()).device}")
    
    # æµ‹è¯•æ•°æ®è½¬ç§»
    test_data = torch.randn(4, 3, 32, 32)
    device_data = dm.to_device(test_data)
    print(f"âœ… Data transferred to {device_data.device}")
    
    # æµ‹è¯•å†…å­˜ä¼˜åŒ–
    dm.optimize_memory()
    memory_stats = dm.get_memory_stats()
    print(f"âœ… Memory stats: {memory_stats}")
    
    # æµ‹è¯•æŠ¥å‘Š
    report = dm.get_device_report()
    print(f"âœ… Device report: {report['device']}")
    
    print("ğŸ‰ Device Manager tests passed!")

if __name__ == "__main__":
    test_device_manager() 