"""
Gumbel-Softmaxå¼•å¯¼å¼æ¢ç´¢ (Gumbel-Softmax Guided Exploration)

ASO-SEæ¡†æ¶çš„æ ¸å¿ƒæœºåˆ¶ä¹‹äºŒï¼šé€šè¿‡Gumbel-SoftmaxæŠ€å·§è¿›è¡Œå¼•å¯¼å¼æ¶æ„æ¢ç´¢ï¼Œ
é¿å…è´ªå©ªé€‰æ‹©å¯¼è‡´çš„å±€éƒ¨æœ€ä¼˜ï¼Œæ”¯æŒæ¸©åº¦è°ƒèŠ‚å’Œæ¢ç´¢/åˆ©ç”¨å¹³è¡¡ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. Gumbel-Softmaxé‡‡æ · - å¯å¾®çš„éšæœºæ¶æ„é€‰æ‹©
2. æ¸©åº¦é€€ç«ç­–ç•¥ - ä»æ¢ç´¢åˆ°åˆ©ç”¨çš„å¹³æ»‘è¿‡æ¸¡
3. å¤šæ ·æ€§ä¿æŒ - é¿å…è¿‡æ—©æ”¶æ•›åˆ°æ¬¡ä¼˜æ¶æ„
"""

import torch
import torch.nn as nn
try:
    import torch.nn.functional as F
except ImportError:
    # Fallback for linter issues
    from torch.nn import functional as F
import numpy as np
from typing import Dict, List, Optional, Tuple, Union, Callable
import logging
import math

logger = logging.getLogger(__name__)

class GumbelSoftmaxExplorer:
    """
    Gumbel-Softmaxå¼•å¯¼å¼æ¢ç´¢å™¨
    
    é€šè¿‡å¯æ§çš„éšæœºæ€§å®ç°æ¶æ„ç©ºé—´çš„æœ‰æ•ˆæ¢ç´¢
    """
    
    def __init__(self, initial_temp: float = 5.0, min_temp: float = 0.1, 
                 anneal_rate: float = 0.98, exploration_factor: float = 1.0):
        """
        Args:
            initial_temp: åˆå§‹æ¸©åº¦ï¼Œæ§åˆ¶éšæœºæ€§å¼ºåº¦
            min_temp: æœ€å°æ¸©åº¦ï¼Œé˜²æ­¢å®Œå…¨ç¡®å®šæ€§é€‰æ‹©
            anneal_rate: æ¸©åº¦é€€ç«ç‡ï¼Œæ¯è½®è¡°å‡æ¯”ä¾‹
            exploration_factor: æ¢ç´¢å› å­ï¼Œè°ƒèŠ‚æ¢ç´¢å¼ºåº¦
        """
        self.initial_temp = initial_temp
        self.current_temp = initial_temp
        self.min_temp = min_temp
        self.anneal_rate = anneal_rate
        self.exploration_factor = exploration_factor
        
        # æ¢ç´¢å†å²ç»Ÿè®¡
        self.exploration_history = []
        self.diversity_scores = []
        self.step_count = 0
        
        logger.info(f"ğŸŒ¡ï¸ Gumbel-Softmax Explorer initialized: "
                   f"temp={initial_temp:.2f}â†’{min_temp:.2f}, "
                   f"anneal_rate={anneal_rate:.3f}")
    
    def sample_architecture(self, alpha_weights: torch.Tensor, 
                          hard: bool = False) -> Tuple[torch.Tensor, Dict]:
        """
        ä½¿ç”¨Gumbel-Softmaxé‡‡æ ·æ¶æ„
        
        Args:
            alpha_weights: æ¶æ„å‚æ•° [num_edges, num_ops]
            hard: æ˜¯å¦ä½¿ç”¨ç¡¬é‡‡æ ·ï¼ˆstraight-throughï¼‰
            
        Returns:
            é‡‡æ ·ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
        """
        self.step_count += 1
        
        # ç”ŸæˆGumbelå™ªå£°
        gumbel_noise = self._sample_gumbel(alpha_weights.shape)
        
        # åŠ å…¥æ¸©åº¦å’Œå™ªå£°çš„logits
        logits = (alpha_weights + gumbel_noise) / self.current_temp
        
        # Softmaxé‡‡æ ·
        soft_samples = F.softmax(logits, dim=-1)
        
        if hard:
            # ç¡¬é‡‡æ ·ï¼šé€‰æ‹©æœ€å¤§å€¼ï¼Œä½†ä¿æŒæ¢¯åº¦
            hard_samples = self._straight_through_softmax(soft_samples)
            samples = hard_samples
        else:
            samples = soft_samples
        
        # è®¡ç®—æ¢ç´¢ç»Ÿè®¡
        stats = self._compute_exploration_stats(alpha_weights, samples, logits)
        
        # è®°å½•æ¢ç´¢å†å²
        self.exploration_history.append({
            'step': self.step_count,
            'temperature': self.current_temp,
            'entropy': stats['entropy'],
            'diversity': stats['diversity']
        })
        
        return samples, stats
    
    def _sample_gumbel(self, shape: Tuple[int, ...], eps: float = 1e-20) -> torch.Tensor:
        """é‡‡æ ·Gumbelåˆ†å¸ƒå™ªå£°"""
        uniform = torch.rand(shape, device=torch.cuda.current_device() if torch.cuda.is_available() else None)
        gumbel = -torch.log(-torch.log(uniform + eps) + eps)
        return gumbel * self.exploration_factor
    
    def _straight_through_softmax(self, soft_samples: torch.Tensor) -> torch.Tensor:
        """Straight-throughç¡¬é‡‡æ ·ï¼Œä¿æŒæ¢¯åº¦æµ"""
        # ç¡¬é‡‡æ ·ï¼šé€‰æ‹©æœ€å¤§æ¦‚ç‡çš„æ“ä½œ
        hard_samples = torch.zeros_like(soft_samples)
        max_indices = torch.argmax(soft_samples, dim=-1, keepdim=True)
        hard_samples.scatter_(-1, max_indices, 1.0)
        
        # Straight-throughæ¢¯åº¦ï¼šå‰å‘ä½¿ç”¨ç¡¬é‡‡æ ·ï¼Œåå‘ä½¿ç”¨è½¯é‡‡æ ·
        return hard_samples.detach() + soft_samples - soft_samples.detach()
    
    def _compute_exploration_stats(self, alpha_weights: torch.Tensor, 
                                 samples: torch.Tensor, 
                                 logits: torch.Tensor) -> Dict:
        """è®¡ç®—æ¢ç´¢ç›¸å…³ç»Ÿè®¡ä¿¡æ¯"""
        with torch.no_grad():
            # ç†µè®¡ç®— - è¡¡é‡æ¢ç´¢ç¨‹åº¦
            probs = F.softmax(logits, dim=-1)
            entropy = -(probs * torch.log(probs + 1e-8)).sum(dim=-1).mean().item()
            
            # å¤šæ ·æ€§åˆ†æ•° - è¡¡é‡é€‰æ‹©çš„å¤šæ ·æ€§
            diversity = self._compute_diversity_score(samples)
            
            # ä¸è´ªå©ªé€‰æ‹©çš„å·®å¼‚
            greedy_selection = torch.zeros_like(alpha_weights)
            greedy_indices = torch.argmax(alpha_weights, dim=-1, keepdim=True)
            greedy_selection.scatter_(-1, greedy_indices, 1.0)
            
            difference_from_greedy = torch.abs(samples - greedy_selection).sum().item()
            
            return {
                'entropy': entropy,
                'diversity': diversity,
                'temperature': self.current_temp,
                'difference_from_greedy': difference_from_greedy,
                'exploration_strength': entropy / math.log(alpha_weights.size(-1))  # å½’ä¸€åŒ–ç†µ
            }
    
    def _compute_diversity_score(self, samples: torch.Tensor) -> float:
        """è®¡ç®—æ¶æ„é€‰æ‹©çš„å¤šæ ·æ€§åˆ†æ•°"""
        # è®¡ç®—æ¯ä¸ªæ“ä½œè¢«é€‰æ‹©çš„é¢‘ç‡
        op_frequencies = samples.sum(dim=0)
        total_selections = op_frequencies.sum()
        
        if total_selections > 0:
            op_probs = op_frequencies / total_selections
            # ä½¿ç”¨å½’ä¸€åŒ–ç†µä½œä¸ºå¤šæ ·æ€§æŒ‡æ ‡
            diversity = -(op_probs * torch.log(op_probs + 1e-8)).sum().item()
            max_diversity = math.log(len(op_frequencies))
            return diversity / max_diversity if max_diversity > 0 else 0.0
        return 0.0
    
    def update_temperature(self, performance_gain: Optional[float] = None, 
                          force_anneal: bool = False) -> float:
        """
        æ›´æ–°æ¸©åº¦å‚æ•°
        
        Args:
            performance_gain: æ€§èƒ½æå‡ï¼Œç”¨äºè‡ªé€‚åº”æ¸©åº¦è°ƒèŠ‚
            force_anneal: å¼ºåˆ¶é€€ç«
            
        Returns:
            æ–°çš„æ¸©åº¦å€¼
        """
        if force_anneal or self.current_temp > self.min_temp:
            if performance_gain is not None:
                # è‡ªé€‚åº”æ¸©åº¦è°ƒèŠ‚ï¼šæ€§èƒ½æå‡æ—¶é™æ¸©ï¼Œæ€§èƒ½ä¸‹é™æ—¶å‡æ¸©
                if performance_gain > 0:
                    # æ€§èƒ½æå‡ï¼Œå¯ä»¥é€‚å½“é™ä½æ¢ç´¢
                    self.current_temp *= self.anneal_rate
                else:
                    # æ€§èƒ½ä¸‹é™ï¼Œå¢åŠ æ¢ç´¢
                    self.current_temp = min(self.current_temp * 1.05, self.initial_temp)
            else:
                # æ ‡å‡†çº¿æ€§é€€ç«
                self.current_temp = max(self.current_temp * self.anneal_rate, self.min_temp)
        
        logger.debug(f"Temperature updated to {self.current_temp:.3f}")
        return self.current_temp
    
    def get_exploration_schedule(self, total_steps: int) -> List[float]:
        """
        ç”Ÿæˆå®Œæ•´çš„æ¢ç´¢æ¸©åº¦è®¡åˆ’
        
        Args:
            total_steps: æ€»æ­¥æ•°
            
        Returns:
            æ¸©åº¦è®¡åˆ’åˆ—è¡¨
        """
        schedule = []
        temp = self.initial_temp
        
        for step in range(total_steps):
            schedule.append(temp)
            temp = max(temp * self.anneal_rate, self.min_temp)
        
        return schedule
    
    def adaptive_temperature_control(self, validation_loss_history: List[float], 
                                   window_size: int = 5) -> float:
        """
        åŸºäºéªŒè¯æŸå¤±å†å²çš„è‡ªé€‚åº”æ¸©åº¦æ§åˆ¶
        
        Args:
            validation_loss_history: éªŒè¯æŸå¤±å†å²
            window_size: æ»‘åŠ¨çª—å£å¤§å°
            
        Returns:
            è°ƒæ•´åçš„æ¸©åº¦
        """
        if len(validation_loss_history) < window_size:
            return self.current_temp
        
        # è®¡ç®—æœ€è¿‘çª—å£çš„æŸå¤±è¶‹åŠ¿
        recent_losses = validation_loss_history[-window_size:]
        trend = np.polyfit(range(len(recent_losses)), recent_losses, 1)[0]
        
        if trend > 0:  # æŸå¤±ä¸Šå‡ï¼Œå¢åŠ æ¢ç´¢
            self.current_temp = min(self.current_temp * 1.1, self.initial_temp)
            logger.info(f"ğŸ”¥ Increasing exploration: temp={self.current_temp:.3f} (loss trend: +{trend:.4f})")
        elif trend < -0.001:  # æŸå¤±æ˜¾è‘—ä¸‹é™ï¼Œå‡å°‘æ¢ç´¢
            self.current_temp = max(self.current_temp * 0.95, self.min_temp)
            logger.info(f"â„ï¸ Reducing exploration: temp={self.current_temp:.3f} (loss trend: {trend:.4f})")
        
        return self.current_temp
    
    def get_exploration_report(self) -> Dict:
        """è·å–æ¢ç´¢è¿‡ç¨‹çš„è¯¦ç»†æŠ¥å‘Š"""
        if not self.exploration_history:
            return {"message": "No exploration history available"}
        
        history = self.exploration_history
        
        return {
            "total_steps": len(history),
            "current_temperature": self.current_temp,
            "average_entropy": np.mean([h['entropy'] for h in history]),
            "average_diversity": np.mean([h['diversity'] for h in history]),
            "temperature_range": {
                "initial": history[0]['temperature'],
                "current": history[-1]['temperature'],
                "min": min(h['temperature'] for h in history)
            },
            "exploration_trend": {
                "entropy_trend": np.polyfit(range(len(history)), 
                                          [h['entropy'] for h in history], 1)[0],
                "diversity_trend": np.polyfit(range(len(history)), 
                                            [h['diversity'] for h in history], 1)[0]
            }
        }


class MultiObjectiveExplorer(GumbelSoftmaxExplorer):
    """
    å¤šç›®æ ‡Gumbel-Softmaxæ¢ç´¢å™¨
    
    åŒæ—¶è€ƒè™‘å‡†ç¡®ç‡ã€å»¶è¿Ÿã€å‚æ•°é‡ç­‰å¤šä¸ªç›®æ ‡çš„æ¶æ„æ¢ç´¢
    """
    
    def __init__(self, objectives: List[str] = ["accuracy", "latency", "params"], 
                 objective_weights: Optional[List[float]] = None, **kwargs):
        super().__init__(**kwargs)
        
        self.objectives = objectives
        self.objective_weights = objective_weights or [1.0] * len(objectives)
        self.objective_history = {obj: [] for obj in objectives}
        
        logger.info(f"ğŸ¯ Multi-objective explorer: {objectives} with weights {objective_weights}")
    
    def sample_with_objectives(self, alpha_weights: torch.Tensor, 
                             objective_scores: Dict[str, float],
                             hard: bool = False) -> Tuple[torch.Tensor, Dict]:
        """
        åŸºäºå¤šç›®æ ‡çš„æ¶æ„é‡‡æ ·
        
        Args:
            alpha_weights: æ¶æ„å‚æ•°
            objective_scores: å„ç›®æ ‡çš„å½“å‰åˆ†æ•°
            hard: æ˜¯å¦ç¡¬é‡‡æ ·
            
        Returns:
            é‡‡æ ·ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
        """
        # æ›´æ–°ç›®æ ‡å†å²
        for obj, score in objective_scores.items():
            if obj in self.objective_history:
                self.objective_history[obj].append(score)
        
        # è®¡ç®—å¤šç›®æ ‡æƒé‡è°ƒæ•´
        objective_adjustment = self._compute_objective_adjustment(objective_scores)
        
        # è°ƒæ•´æ¶æ„å‚æ•°æƒé‡
        adjusted_alpha = alpha_weights + objective_adjustment
        
        return self.sample_architecture(adjusted_alpha, hard)
    
    def _compute_objective_adjustment(self, objective_scores: Dict[str, float]) -> torch.Tensor:
        """åŸºäºå¤šç›®æ ‡åˆ†æ•°è®¡ç®—æ¶æ„å‚æ•°è°ƒæ•´"""
        # è¿™é‡Œå¯ä»¥å®ç°å¤æ‚çš„å¤šç›®æ ‡ä¼˜åŒ–ç­–ç•¥
        # ç®€åŒ–ç‰ˆæœ¬ï¼šæ ¹æ®ç›®æ ‡è¾¾æˆæƒ…å†µè°ƒæ•´æ¸©åº¦
        
        weighted_score = sum(score * weight 
                           for score, weight in zip(objective_scores.values(), self.objective_weights))
        
        # æ ¹æ®ç»¼åˆåˆ†æ•°è°ƒæ•´æ¢ç´¢å¼ºåº¦
        if weighted_score > 0.8:  # é«˜åˆ†æ—¶å‡å°‘æ¢ç´¢
            self.current_temp *= 0.98
        elif weighted_score < 0.5:  # ä½åˆ†æ—¶å¢åŠ æ¢ç´¢
            self.current_temp *= 1.02
        
        # è¿”å›é›¶è°ƒæ•´ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”ç”¨ä¸­å¯æ‰©å±•ä¸ºæ›´å¤æ‚çš„è°ƒæ•´ç­–ç•¥ï¼‰
        return torch.zeros(1, 1) * 0.1


def create_annealing_schedule(initial_temp: float, min_temp: float, 
                            total_epochs: int, schedule_type: str = "cosine") -> List[float]:
    """
    åˆ›å»ºæ¸©åº¦é€€ç«è®¡åˆ’
    
    Args:
        initial_temp: åˆå§‹æ¸©åº¦
        min_temp: æœ€å°æ¸©åº¦
        total_epochs: æ€»è½®æ•°
        schedule_type: è®¡åˆ’ç±»å‹ ("linear", "cosine", "exponential")
        
    Returns:
        æ¸©åº¦è®¡åˆ’åˆ—è¡¨
    """
    temperatures = []
    
    for epoch in range(total_epochs):
        if schedule_type == "linear":
            progress = epoch / (total_epochs - 1)
            temp = initial_temp * (1 - progress) + min_temp * progress
            
        elif schedule_type == "cosine":
            progress = epoch / (total_epochs - 1)
            temp = min_temp + (initial_temp - min_temp) * 0.5 * (1 + math.cos(math.pi * progress))
            
        elif schedule_type == "exponential":
            decay_rate = (min_temp / initial_temp) ** (1 / (total_epochs - 1))
            temp = initial_temp * (decay_rate ** epoch)
            
        else:
            raise ValueError(f"Unknown schedule type: {schedule_type}")
        
        temperatures.append(max(temp, min_temp))
    
    return temperatures


def test_gumbel_softmax_explorer():
    """æµ‹è¯•Gumbel-Softmaxæ¢ç´¢å™¨çš„åŠŸèƒ½"""
    print("ğŸ§ª Testing Gumbel-Softmax Explorer...")
    
    # åˆ›å»ºæ¢ç´¢å™¨
    explorer = GumbelSoftmaxExplorer(initial_temp=2.0, min_temp=0.1)
    
    # æ¨¡æ‹Ÿæ¶æ„å‚æ•°
    alpha_weights = torch.randn(10, 8)  # 10æ¡è¾¹ï¼Œ8ç§æ“ä½œ
    alpha_weights = F.softmax(alpha_weights, dim=-1)
    
    print(f"ğŸ“Š Original alpha shape: {alpha_weights.shape}")
    
    # æµ‹è¯•è½¯é‡‡æ ·
    soft_samples, soft_stats = explorer.sample_architecture(alpha_weights, hard=False)
    print(f"âœ… Soft sampling completed, entropy: {soft_stats['entropy']:.3f}")
    
    # æµ‹è¯•ç¡¬é‡‡æ ·
    hard_samples, hard_stats = explorer.sample_architecture(alpha_weights, hard=True)
    print(f"âœ… Hard sampling completed, entropy: {hard_stats['entropy']:.3f}")
    
    # æµ‹è¯•æ¸©åº¦é€€ç«
    original_temp = explorer.current_temp
    explorer.update_temperature()
    print(f"âœ… Temperature annealed: {original_temp:.3f} â†’ {explorer.current_temp:.3f}")
    
    # æµ‹è¯•å¤šç›®æ ‡æ¢ç´¢å™¨
    multi_explorer = MultiObjectiveExplorer(
        objectives=["accuracy", "latency"], 
        objective_weights=[0.7, 0.3]
    )
    
    objective_scores = {"accuracy": 0.85, "latency": 0.6}
    multi_samples, multi_stats = multi_explorer.sample_with_objectives(
        alpha_weights, objective_scores, hard=True
    )
    print(f"âœ… Multi-objective sampling completed")
    
    # æµ‹è¯•é€€ç«è®¡åˆ’
    schedule = create_annealing_schedule(5.0, 0.1, 20, "cosine")
    print(f"âœ… Created annealing schedule: {len(schedule)} steps")
    print(f"   Start: {schedule[0]:.2f}, Mid: {schedule[10]:.2f}, End: {schedule[-1]:.2f}")
    
    print("ğŸ‰ Gumbel-Softmax Explorer tests passed!")


if __name__ == "__main__":
    test_gumbel_softmax_explorer() 