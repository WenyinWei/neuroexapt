#!/usr/bin/env python3
"""
"""
\defgroup group_pareto_optimization Pareto Optimization
\ingroup core
Pareto Optimization module for NeuroExapt framework.
"""


Dynamic Neural Morphogenesis - å¤šç›®æ ‡è¿›åŒ–ä¼˜åŒ–æ¨¡å—

åŸºäºå¸•ç´¯æ‰˜æœ€ä¼˜çš„å¤šç›®æ ‡æ¶æ„æ¼”åŒ–ï¼š
1. åŒæ—¶ä¼˜åŒ–å‡†ç¡®ç‡ã€è®¡ç®—æ•ˆç‡ã€æ¨¡å‹å¤æ‚åº¦
2. ä½¿ç”¨é—ä¼ ç®—æ³•è¿›è¡Œå…¨å±€æ¶æ„æœç´¢
3. å®ç°å¸•ç´¯æ‰˜å‰æ²¿åˆ†æå’Œéæ”¯é…æ’åº
4. æ”¯æŒè‡ªé€‚åº”ç§ç¾¤ç®¡ç†å’Œå¤šæ ·æ€§ä¿æŒ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
import copy
import random
from typing import Dict, List, Tuple, Optional, Any, Callable
from collections import defaultdict
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)


@dataclass
class FitnessMetrics:
    """é€‚åº”åº¦æŒ‡æ ‡"""
    accuracy: float          # å‡†ç¡®ç‡ (æœ€å¤§åŒ–)
    efficiency: float        # è®¡ç®—æ•ˆç‡ (GFLOPS/s, æœ€å¤§åŒ–)
    complexity: float        # æ¨¡å‹å¤æ‚åº¦ (å‚æ•°é‡, æœ€å°åŒ–)
    memory_usage: float      # å†…å­˜ä½¿ç”¨ (MB, æœ€å°åŒ–)
    training_speed: float    # è®­ç»ƒé€Ÿåº¦ (samples/s, æœ€å¤§åŒ–)
    energy_consumption: float # èƒ½è€— (J, æœ€å°åŒ–)
    
    def __post_init__(self):
        """åå¤„ç†ï¼šç¡®ä¿æ‰€æœ‰æŒ‡æ ‡éƒ½æ˜¯æœ‰æ•ˆçš„æ•°å€¼"""
        for field_name in self.__dataclass_fields__:
            value = getattr(self, field_name)
            if math.isnan(value) or math.isinf(value):
                setattr(self, field_name, 0.0)


class ModelFitnessEvaluator:
    """æ¨¡å‹é€‚åº”åº¦è¯„ä¼°å™¨"""
    
    def __init__(self, device='cuda', evaluation_samples=500):
        self.device = device
        self.evaluation_samples = evaluation_samples
        self.baseline_metrics = None
        
    def evaluate_comprehensive_fitness(self, 
                                     model: nn.Module, 
                                     train_loader, 
                                     val_loader,
                                     optimization_objectives: List[str] = None) -> FitnessMetrics:
        """
        ç»¼åˆè¯„ä¼°æ¨¡å‹é€‚åº”åº¦
        
        Args:
            model: å¾…è¯„ä¼°æ¨¡å‹
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            optimization_objectives: ä¼˜åŒ–ç›®æ ‡åˆ—è¡¨
            
        Returns:
            FitnessMetricså¯¹è±¡åŒ…å«æ‰€æœ‰é€‚åº”åº¦æŒ‡æ ‡
        """
        if optimization_objectives is None:
            optimization_objectives = ['accuracy', 'efficiency', 'complexity']
        
        model = model.to(self.device)
        
        # 1. å‡†ç¡®ç‡è¯„ä¼°
        accuracy = self._evaluate_accuracy(model, val_loader)
        
        # 2. è®¡ç®—æ•ˆç‡è¯„ä¼°
        efficiency = self._evaluate_efficiency(model)
        
        # 3. æ¨¡å‹å¤æ‚åº¦è¯„ä¼°
        complexity = self._evaluate_complexity(model)
        
        # 4. å†…å­˜ä½¿ç”¨è¯„ä¼°
        memory_usage = self._evaluate_memory_usage(model)
        
        # 5. è®­ç»ƒé€Ÿåº¦è¯„ä¼°
        training_speed = self._evaluate_training_speed(model, train_loader)
        
        # 6. èƒ½è€—è¯„ä¼°ï¼ˆè¿‘ä¼¼ï¼‰
        energy_consumption = self._estimate_energy_consumption(model, complexity, efficiency)
        
        return FitnessMetrics(
            accuracy=accuracy,
            efficiency=efficiency,
            complexity=complexity,
            memory_usage=memory_usage,
            training_speed=training_speed,
            energy_consumption=energy_consumption
        )
    
    def _evaluate_accuracy(self, model: nn.Module, val_loader) -> float:
        """è¯„ä¼°æ¨¡å‹å‡†ç¡®ç‡"""
        model.eval()
        correct = 0
        total = 0
        samples_processed = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                if samples_processed >= self.evaluation_samples:
                    break
                    
                data, target = data.to(self.device), target.to(self.device)
                output = model(data)
                
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)
                samples_processed += target.size(0)
        
        return 100.0 * correct / total if total > 0 else 0.0
    
    def _evaluate_efficiency(self, model: nn.Module) -> float:
        """è¯„ä¼°è®¡ç®—æ•ˆç‡ (GFLOPS)"""
        try:
            # ä½¿ç”¨torch.profilerè¯„ä¼°FLOPS
            model.eval()
            
            # åˆ›å»ºéšæœºè¾“å…¥
            if hasattr(model, 'input_size'):
                input_size = model.input_size
            else:
                # å‡è®¾æ˜¯CIFAR-10è¾“å…¥
                input_size = (1, 3, 32, 32)
            
            dummy_input = torch.randn(input_size).to(self.device)
            
            # è®¡ç®—FLOPSï¼ˆç®€åŒ–ä¼°ç®—ï¼‰
            flops = self._estimate_flops(model, dummy_input)
            
            # æµ‹é‡å®é™…æ¨ç†æ—¶é—´
            inference_time = self._measure_inference_time(model, dummy_input)
            
            # è®¡ç®—GFLOPS/s
            if inference_time > 0:
                efficiency = flops / (inference_time * 1e9)  # GFLOPS/s
            else:
                efficiency = 0.0
                
            return efficiency
            
        except Exception as e:
            logger.warning(f"Efficiency evaluation failed: {e}")
            return 0.0
    
    def _estimate_flops(self, model: nn.Module, input_tensor: torch.Tensor) -> float:
        """ä¼°ç®—æ¨¡å‹çš„FLOPS"""
        total_flops = 0
        
        def flop_count_hook(module, input, output):
            nonlocal total_flops
            
            if isinstance(module, nn.Conv2d):
                # å·ç§¯å±‚FLOPS
                batch_size = output.shape[0]
                output_height, output_width = output.shape[2], output.shape[3]
                kernel_flops = module.kernel_size[0] * module.kernel_size[1]
                output_elements = batch_size * output_height * output_width
                filters = module.out_channels
                flops = output_elements * kernel_flops * filters * module.in_channels
                total_flops += flops
                
            elif isinstance(module, nn.Linear):
                # å…¨è¿æ¥å±‚FLOPS
                batch_size = input[0].shape[0]
                flops = batch_size * module.in_features * module.out_features
                total_flops += flops
        
        # æ³¨å†Œhooks
        hooks = []
        for module in model.modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)):
                hook = module.register_forward_hook(flop_count_hook)
                hooks.append(hook)
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            _ = model(input_tensor)
        
        # æ¸…ç†hooks
        for hook in hooks:
            hook.remove()
        
        return total_flops
    
    def _measure_inference_time(self, model: nn.Module, input_tensor: torch.Tensor, 
                              warmup_runs=10, measure_runs=100) -> float:
        """æµ‹é‡æ¨ç†æ—¶é—´"""
        model.eval()
        
        # é¢„çƒ­
        with torch.no_grad():
            for _ in range(warmup_runs):
                _ = model(input_tensor)
        
        if self.device == 'cuda':
            torch.cuda.synchronize()
        
        # æµ‹é‡
        import time
        start_time = time.perf_counter()
        
        with torch.no_grad():
            for _ in range(measure_runs):
                _ = model(input_tensor)
        
        if self.device == 'cuda':
            torch.cuda.synchronize()
            
        end_time = time.perf_counter()
        
        avg_time = (end_time - start_time) / measure_runs
        return avg_time
    
    def _evaluate_complexity(self, model: nn.Module) -> float:
        """è¯„ä¼°æ¨¡å‹å¤æ‚åº¦ï¼ˆå‚æ•°é‡ï¼‰"""
        total_params = sum(p.numel() for p in model.parameters())
        return float(total_params)
    
    def _evaluate_memory_usage(self, model: nn.Module) -> float:
        """è¯„ä¼°å†…å­˜ä½¿ç”¨ï¼ˆMBï¼‰"""
        try:
            # è®¡ç®—æ¨¡å‹å‚æ•°å ç”¨çš„å†…å­˜
            param_memory = sum(p.numel() * p.element_size() for p in model.parameters())
            
            # è®¡ç®—ç¼“å†²åŒºå ç”¨çš„å†…å­˜
            buffer_memory = sum(b.numel() * b.element_size() for b in model.buffers())
            
            total_memory = param_memory + buffer_memory
            memory_mb = total_memory / (1024 * 1024)  # è½¬æ¢ä¸ºMB
            
            return memory_mb
            
        except Exception as e:
            logger.warning(f"Memory evaluation failed: {e}")
            return 0.0
    
    def _evaluate_training_speed(self, model: nn.Module, train_loader) -> float:
        """è¯„ä¼°è®­ç»ƒé€Ÿåº¦ï¼ˆsamples/sï¼‰"""
        try:
            model.train()
            optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
            criterion = nn.CrossEntropyLoss()
            
            total_samples = 0
            import time
            start_time = time.time()
            
            batch_count = 0
            for batch_idx, (data, target) in enumerate(train_loader):
                if batch_idx >= 3:  # åªæµ‹è¯•å‡ ä¸ªbatch
                    break
                    
                try:
                    data, target = data.to(self.device), target.to(self.device)
                    
                    optimizer.zero_grad()
                    output = model(data)
                    loss = criterion(output, target)
                    loss.backward()
                    optimizer.step()
                    
                    total_samples += data.size(0)
                    batch_count += 1
                    
                except Exception as e:
                    logger.debug(f"Batch {batch_idx} failed in speed evaluation: {e}")
                    continue
            
            elapsed_time = time.time() - start_time
            speed = total_samples / elapsed_time if elapsed_time > 0 and total_samples > 0 else 1.0
            
            return max(speed, 1.0)  # ç¡®ä¿è¿”å›æ­£å€¼
            
        except Exception as e:
            logger.warning(f"Training speed evaluation failed: {e}")
            return 1.0  # è¿”å›é»˜è®¤å€¼è€Œä¸æ˜¯0
    
    def _estimate_energy_consumption(self, model: nn.Module, complexity: float, efficiency: float) -> float:
        """ä¼°ç®—èƒ½è€—ï¼ˆç„¦è€³ï¼‰"""
        # ç®€åŒ–çš„èƒ½è€—æ¨¡å‹ï¼šåŸºäºå‚æ•°é‡å’Œè®¡ç®—é‡
        base_energy = complexity * 1e-9  # åŸºç¡€èƒ½è€—
        compute_energy = (1.0 / (efficiency + 1e-6)) * 1e-3  # è®¡ç®—èƒ½è€—
        
        total_energy = base_energy + compute_energy
        return total_energy


class ParetoOptimizer:
    """å¸•ç´¯æ‰˜ä¼˜åŒ–å™¨"""
    
    def __init__(self, objectives: List[str], maximize_objectives: List[str] = None):
        """
        Args:
            objectives: ä¼˜åŒ–ç›®æ ‡åˆ—è¡¨
            maximize_objectives: éœ€è¦æœ€å¤§åŒ–çš„ç›®æ ‡ï¼ˆå…¶ä½™ä¸ºæœ€å°åŒ–ï¼‰
        """
        self.objectives = objectives
        self.maximize_objectives = maximize_objectives or ['accuracy', 'efficiency', 'training_speed']
        self.minimize_objectives = [obj for obj in objectives if obj not in self.maximize_objectives]
        
    def is_dominated(self, fitness1: FitnessMetrics, fitness2: FitnessMetrics) -> bool:
        """
        åˆ¤æ–­fitness1æ˜¯å¦è¢«fitness2æ”¯é…
        
        å¦‚æœfitness2åœ¨æ‰€æœ‰ç›®æ ‡ä¸Šéƒ½ä¸å·®äºfitness1ï¼Œä¸”è‡³å°‘åœ¨ä¸€ä¸ªç›®æ ‡ä¸Šæ›´å¥½ï¼Œ
        åˆ™fitness1è¢«fitness2æ”¯é…
        """
        better_in_any = False
        
        for obj in self.objectives:
            val1 = getattr(fitness1, obj)
            val2 = getattr(fitness2, obj)
            
            if obj in self.maximize_objectives:
                # æœ€å¤§åŒ–ç›®æ ‡
                if val2 < val1:
                    return False  # fitness2åœ¨æ­¤ç›®æ ‡ä¸Šæ›´å·®
                elif val2 > val1:
                    better_in_any = True
            else:
                # æœ€å°åŒ–ç›®æ ‡
                if val2 > val1:
                    return False  # fitness2åœ¨æ­¤ç›®æ ‡ä¸Šæ›´å·®
                elif val2 < val1:
                    better_in_any = True
        
        return better_in_any
    
    def non_dominated_sort(self, population_fitness: List[FitnessMetrics]) -> List[List[int]]:
        """
        éæ”¯é…æ’åº
        
        Returns:
            æ¯ä¸ªå‰æ²¿çš„ä¸ªä½“ç´¢å¼•åˆ—è¡¨
        """
        n = len(population_fitness)
        domination_count = [0] * n  # æ¯ä¸ªä¸ªä½“è¢«æ”¯é…çš„æ¬¡æ•°
        dominated_solutions = [[] for _ in range(n)]  # æ¯ä¸ªä¸ªä½“æ”¯é…çš„è§£é›†åˆ
        fronts = [[]]
        
        # è®¡ç®—æ”¯é…å…³ç³»
        for i in range(n):
            for j in range(n):
                if i != j:
                    if self.is_dominated(population_fitness[j], population_fitness[i]):
                        # iæ”¯é…j
                        dominated_solutions[i].append(j)
                    elif self.is_dominated(population_fitness[i], population_fitness[j]):
                        # jæ”¯é…i
                        domination_count[i] += 1
            
            if domination_count[i] == 0:
                fronts[0].append(i)
        
        # æ„å»ºåç»­å‰æ²¿
        front_idx = 0
        while len(fronts[front_idx]) > 0:
            next_front = []
            for i in fronts[front_idx]:
                for j in dominated_solutions[i]:
                    domination_count[j] -= 1
                    if domination_count[j] == 0:
                        next_front.append(j)
            
            if next_front:
                fronts.append(next_front)
            front_idx += 1
        
        return fronts[:-1] if not fronts[-1] else fronts
    
    def calculate_crowding_distance(self, front: List[int], 
                                  population_fitness: List[FitnessMetrics]) -> List[float]:
        """
        è®¡ç®—æ‹¥æŒ¤è·ç¦»
        
        Args:
            front: å‰æ²¿ä¸­ä¸ªä½“çš„ç´¢å¼•
            population_fitness: ç§ç¾¤é€‚åº”åº¦
            
        Returns:
            æ¯ä¸ªä¸ªä½“çš„æ‹¥æŒ¤è·ç¦»
        """
        distances = [0.0] * len(front)
        
        if len(front) <= 2:
            return [float('inf')] * len(front)
        
        for obj in self.objectives:
            # æŒ‰ç›®æ ‡å€¼æ’åº
            front_sorted = sorted(front, key=lambda x: getattr(population_fitness[x], obj))
            
            # è¾¹ç•Œä¸ªä½“è®¾ç½®ä¸ºæ— ç©·å¤§
            idx_min = front.index(front_sorted[0])
            idx_max = front.index(front_sorted[-1])
            distances[idx_min] = float('inf')
            distances[idx_max] = float('inf')
            
            # è®¡ç®—ç›®æ ‡å€¼èŒƒå›´
            obj_min = getattr(population_fitness[front_sorted[0]], obj)
            obj_max = getattr(population_fitness[front_sorted[-1]], obj)
            obj_range = obj_max - obj_min
            
            if obj_range > 0:
                # è®¡ç®—ä¸­é—´ä¸ªä½“çš„æ‹¥æŒ¤è·ç¦»
                for i in range(1, len(front_sorted) - 1):
                    idx = front.index(front_sorted[i])
                    obj_prev = getattr(population_fitness[front_sorted[i-1]], obj)
                    obj_next = getattr(population_fitness[front_sorted[i+1]], obj)
                    distances[idx] += (obj_next - obj_prev) / obj_range
        
        return distances


class MultiObjectiveEvolution:
    """å¤šç›®æ ‡è¿›åŒ–ç®—æ³•"""
    
    def __init__(self, 
                 population_size: int = 20,
                 max_generations: int = 50,
                 mutation_rate: float = 0.3,
                 crossover_rate: float = 0.7,
                 elitism_ratio: float = 0.1):
        
        self.population_size = population_size
        self.max_generations = max_generations
        self.mutation_rate = mutation_rate
        self.crossover_rate = crossover_rate
        self.elitism_ratio = elitism_ratio
        
        self.optimizer = ParetoOptimizer(['accuracy', 'efficiency', 'complexity'])
        self.evaluator = ModelFitnessEvaluator()
        
        # è¿›åŒ–å†å²
        self.evolution_history = []
        self.pareto_fronts_history = []
        
    def evolve_population(self, 
                         initial_models: List[nn.Module],
                         train_loader,
                         val_loader,
                         target_generation: int = None) -> Dict[str, Any]:
        """
        è¿›åŒ–ç§ç¾¤
        
        Args:
            initial_models: åˆå§‹æ¨¡å‹ç§ç¾¤
            train_loader: è®­ç»ƒæ•°æ®
            val_loader: éªŒè¯æ•°æ®
            target_generation: ç›®æ ‡ä»£æ•°
            
        Returns:
            è¿›åŒ–ç»“æœ
        """
        if target_generation is None:
            target_generation = self.max_generations
        
        # ç¡®ä¿ç§ç¾¤å¤§å°
        population = self._ensure_population_size(initial_models)
        
        logger.info(f"Starting multi-objective evolution with {len(population)} individuals")
        
        for generation in range(target_generation):
            logger.info(f"Generation {generation + 1}/{target_generation}")
            
            # è¯„ä¼°ç§ç¾¤é€‚åº”åº¦
            population_fitness = self._evaluate_population(population, train_loader, val_loader)
            
            # éæ”¯é…æ’åº
            fronts = self.optimizer.non_dominated_sort(population_fitness)
            
            # è®°å½•å¸•ç´¯æ‰˜å‰æ²¿
            pareto_front = [population[i] for i in fronts[0]]
            pareto_fitness = [population_fitness[i] for i in fronts[0]]
            
            self.pareto_fronts_history.append({
                'generation': generation,
                'front_size': len(fronts[0]),
                'fitness_metrics': pareto_fitness
            })
            
            # é€‰æ‹©å’Œç¹æ®–
            if generation < target_generation - 1:
                population = self._selection_and_reproduction(population, population_fitness, fronts)
            
            # è®°å½•è¿›åŒ–å†å²
            generation_stats = self._calculate_generation_statistics(population_fitness)
            self.evolution_history.append({
                'generation': generation,
                'stats': generation_stats,
                'pareto_front_size': len(fronts[0])
            })
            
            logger.info(f"Generation {generation + 1} completed: "
                       f"Pareto front size: {len(fronts[0])}, "
                       f"Best accuracy: {generation_stats['best_accuracy']:.2f}%")
        
        # è¿”å›æœ€ç»ˆç»“æœ
        final_fitness = self._evaluate_population(population, train_loader, val_loader)
        final_fronts = self.optimizer.non_dominated_sort(final_fitness)
        
        best_models = [population[i] for i in final_fronts[0]]
        best_fitness = [final_fitness[i] for i in final_fronts[0]]
        
        return {
            'best_models': best_models,
            'best_fitness': best_fitness,
            'evolution_history': self.evolution_history,
            'pareto_fronts_history': self.pareto_fronts_history,
            'final_population': population,
            'final_fitness': final_fitness
        }
    
    def _ensure_population_size(self, models: List[nn.Module]) -> List[nn.Module]:
        """ç¡®ä¿ç§ç¾¤å¤§å°"""
        if len(models) == self.population_size:
            return models
        elif len(models) > self.population_size:
            return models[:self.population_size]
        else:
            # é€šè¿‡å¤åˆ¶å’Œå˜å¼‚æ‰©å±•ç§ç¾¤
            population = models.copy()
            while len(population) < self.population_size:
                base_model = random.choice(models)
                mutated_model = self._mutate_model(copy.deepcopy(base_model))
                population.append(mutated_model)
            return population
    
    def _evaluate_population(self, 
                           population: List[nn.Module], 
                           train_loader, 
                           val_loader) -> List[FitnessMetrics]:
        """è¯„ä¼°ç§ç¾¤é€‚åº”åº¦"""
        fitness_list = []
        
        for i, model in enumerate(population):
            try:
                fitness = self.evaluator.evaluate_comprehensive_fitness(
                    model, train_loader, val_loader
                )
                fitness_list.append(fitness)
                
                logger.debug(f"Model {i}: Accuracy={fitness.accuracy:.2f}%, "
                           f"Efficiency={fitness.efficiency:.2f}, "
                           f"Complexity={fitness.complexity:.0f}")
                           
            except Exception as e:
                logger.warning(f"Failed to evaluate model {i}: {e}")
                # ä½¿ç”¨é»˜è®¤çš„å·®é€‚åº”åº¦
                fitness_list.append(FitnessMetrics(0.0, 0.0, 1e9, 1e9, 0.0, 1e9))
        
        return fitness_list
    
    def _selection_and_reproduction(self, 
                                  population: List[nn.Module],
                                  fitness: List[FitnessMetrics],
                                  fronts: List[List[int]]) -> List[nn.Module]:
        """é€‰æ‹©å’Œç¹æ®–"""
        new_population = []
        
        # ç²¾è‹±ä¿ç•™
        elite_count = max(1, int(self.population_size * self.elitism_ratio))
        
        # ä»å¸•ç´¯æ‰˜å‰æ²¿é€‰æ‹©ç²¾è‹±
        elite_indices = fronts[0][:elite_count]
        for idx in elite_indices:
            new_population.append(copy.deepcopy(population[idx]))
        
        # ç¹æ®–å‰©ä½™ä¸ªä½“
        while len(new_population) < self.population_size:
            # é”¦æ ‡èµ›é€‰æ‹©çˆ¶æ¯
            parent1 = self._tournament_selection(population, fitness, fronts)
            parent2 = self._tournament_selection(population, fitness, fronts)
            
            # äº¤å‰
            if random.random() < self.crossover_rate:
                child = self._crossover(parent1, parent2)
            else:
                child = copy.deepcopy(parent1)
            
            # å˜å¼‚
            if random.random() < self.mutation_rate:
                child = self._mutate_model(child)
            
            new_population.append(child)
        
        return new_population[:self.population_size]
    
    def _tournament_selection(self, 
                            population: List[nn.Module],
                            fitness: List[FitnessMetrics],
                            fronts: List[List[int]],
                            tournament_size: int = 3) -> nn.Module:
        """é”¦æ ‡èµ›é€‰æ‹©"""
        candidates = random.sample(range(len(population)), min(tournament_size, len(population)))
        
        # æ‰¾åˆ°æœ€å¥½çš„å€™é€‰è€…ï¼ˆåŸºäºå‰æ²¿æ’åå’Œæ‹¥æŒ¤è·ç¦»ï¼‰
        best_candidate = candidates[0]
        best_front_rank = float('inf')
        
        for front_idx, front in enumerate(fronts):
            for candidate in candidates:
                if candidate in front:
                    if front_idx < best_front_rank:
                        best_candidate = candidate
                        best_front_rank = front_idx
                    elif front_idx == best_front_rank:
                        # åŒä¸€å‰æ²¿ï¼Œæ¯”è¾ƒæ‹¥æŒ¤è·ç¦»
                        distances = self.optimizer.calculate_crowding_distance(front, fitness)
                        candidate_distance = distances[front.index(candidate)]
                        best_distance = distances[front.index(best_candidate)]
                        if candidate_distance > best_distance:
                            best_candidate = candidate
        
        return copy.deepcopy(population[best_candidate])
    
    def _crossover(self, parent1: nn.Module, parent2: nn.Module) -> nn.Module:
        """æ¨¡å‹äº¤å‰"""
        # ç®€åŒ–çš„äº¤å‰ï¼šéšæœºé€‰æ‹©æ¯å±‚çš„å‚æ•°æ¥æº
        child = copy.deepcopy(parent1)
        
        for (name1, param1), (name2, param2) in zip(parent1.named_parameters(), parent2.named_parameters()):
            if name1 == name2 and param1.shape == param2.shape:
                if random.random() < 0.5:
                    # ä½¿ç”¨parent2çš„å‚æ•°
                    child_param = dict(child.named_parameters())[name1]
                    child_param.data = param2.data.clone()
        
        return child
    
    def _mutate_model(self, model: nn.Module) -> nn.Module:
        """æ¨¡å‹å˜å¼‚"""
        # ç®€åŒ–çš„å˜å¼‚ï¼šæ·»åŠ é«˜æ–¯å™ªå£°åˆ°å‚æ•°
        for param in model.parameters():
            if random.random() < 0.1:  # 10%çš„å‚æ•°è¿›è¡Œå˜å¼‚
                noise = torch.randn_like(param) * 0.01
                param.data += noise
        
        return model
    
    def _calculate_generation_statistics(self, fitness: List[FitnessMetrics]) -> Dict[str, float]:
        """è®¡ç®—ä»£é™…ç»Ÿè®¡ä¿¡æ¯"""
        accuracies = [f.accuracy for f in fitness]
        efficiencies = [f.efficiency for f in fitness]
        complexities = [f.complexity for f in fitness]
        
        return {
            'best_accuracy': max(accuracies),
            'avg_accuracy': np.mean(accuracies),
            'best_efficiency': max(efficiencies),
            'avg_efficiency': np.mean(efficiencies),
            'min_complexity': min(complexities),
            'avg_complexity': np.mean(complexities)
        }


class DNMMultiObjectiveOptimization:
    """DNMå¤šç›®æ ‡ä¼˜åŒ–ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or self._default_config()
        self.evolution = MultiObjectiveEvolution(**self.config['evolution'])
        self.optimization_history = []
        
    def _default_config(self) -> Dict:
        """é»˜è®¤é…ç½®"""
        return {
            'evolution': {
                'population_size': 15,
                'max_generations': 20,
                'mutation_rate': 0.3,
                'crossover_rate': 0.7,
                'elitism_ratio': 0.15
            },
            'objectives': {
                'primary': ['accuracy', 'efficiency', 'complexity'],
                'secondary': ['memory_usage', 'training_speed'],
                'weights': {
                    'accuracy': 0.4,
                    'efficiency': 0.3,
                    'complexity': 0.2,
                    'memory_usage': 0.05,
                    'training_speed': 0.05
                }
            },
            'optimization': {
                'trigger_frequency': 15,  # æ¯15ä¸ªepochè§¦å‘ä¸€æ¬¡
                'performance_plateau_threshold': 0.01,
                'min_improvement_epochs': 5
            }
        }
    
    def optimize_architecture_population(self, 
                                       base_models: List[nn.Module],
                                       train_loader,
                                       val_loader,
                                       epoch: int) -> Dict[str, Any]:
        """ä¼˜åŒ–æ¶æ„ç§ç¾¤"""
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘ä¼˜åŒ–
        if epoch % self.config['optimization']['trigger_frequency'] != 0:
            return {'optimized': False, 'message': 'Not optimization epoch'}
        
        logger.info(f"Starting multi-objective optimization at epoch {epoch}")
        
        # æ‰§è¡Œå¤šç›®æ ‡è¿›åŒ–
        result = self.evolution.evolve_population(
            base_models, train_loader, val_loader,
            target_generation=self.config['evolution']['max_generations']
        )
        
        # åˆ†æç»“æœ
        analysis = self._analyze_optimization_results(result)
        
        # è®°å½•ä¼˜åŒ–å†å²
        optimization_record = {
            'epoch': epoch,
            'initial_population_size': len(base_models),
            'final_pareto_front_size': len(result['best_models']),
            'analysis': analysis,
            'evolution_history': result['evolution_history'][-5:]  # ä¿ç•™æœ€å5ä»£
        }
        
        self.optimization_history.append(optimization_record)
        
        logger.info(f"Multi-objective optimization completed: "
                   f"Pareto front size: {len(result['best_models'])}")
        
        return {
            'optimized': True,
            'best_models': result['best_models'],
            'best_fitness': result['best_fitness'],
            'analysis': analysis,
            'optimization_record': optimization_record
        }
    
    def _analyze_optimization_results(self, evolution_result: Dict) -> Dict[str, Any]:
        """åˆ†æä¼˜åŒ–ç»“æœ"""
        best_fitness = evolution_result['best_fitness']
        
        if not best_fitness:
            return {'error': 'No valid fitness results'}
        
        # è®¡ç®—å¸•ç´¯æ‰˜å‰æ²¿ç»Ÿè®¡
        accuracies = [f.accuracy for f in best_fitness]
        efficiencies = [f.efficiency for f in best_fitness]
        complexities = [f.complexity for f in best_fitness]
        
        analysis = {
            'pareto_front_size': len(best_fitness),
            'accuracy_range': (min(accuracies), max(accuracies)),
            'efficiency_range': (min(efficiencies), max(efficiencies)),
            'complexity_range': (min(complexities), max(complexities)),
            'best_accuracy_model': accuracies.index(max(accuracies)),
            'most_efficient_model': efficiencies.index(max(efficiencies)),
            'simplest_model': complexities.index(min(complexities)),
            'diversity_score': self._calculate_diversity_score(best_fitness)
        }
        
        return analysis
    
    def _calculate_diversity_score(self, fitness_list: List[FitnessMetrics]) -> float:
        """è®¡ç®—ç§ç¾¤å¤šæ ·æ€§è¯„åˆ†"""
        if len(fitness_list) < 2:
            return 0.0
        
        # è®¡ç®—ç›®æ ‡ç©ºé—´ä¸­çš„å¹³å‡è·ç¦»
        distances = []
        objectives = ['accuracy', 'efficiency', 'complexity']
        
        for i in range(len(fitness_list)):
            for j in range(i + 1, len(fitness_list)):
                f1, f2 = fitness_list[i], fitness_list[j]
                
                # è®¡ç®—æ¬§æ°è·ç¦»ï¼ˆæ ‡å‡†åŒ–åï¼‰
                dist = 0.0
                for obj in objectives:
                    val1 = getattr(f1, obj)
                    val2 = getattr(f2, obj)
                    
                    # ç®€å•æ ‡å‡†åŒ–
                    if obj == 'accuracy':
                        norm_diff = abs(val1 - val2) / 100.0
                    elif obj == 'efficiency':
                        norm_diff = abs(val1 - val2) / max(val1 + val2, 1.0)
                    else:  # complexity
                        norm_diff = abs(val1 - val2) / max(val1 + val2, 1.0)
                    
                    dist += norm_diff ** 2
                
                distances.append(math.sqrt(dist))
        
        return np.mean(distances) if distances else 0.0
    
    def get_optimization_summary(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–æ€»ç»“"""
        return {
            'total_optimizations': len(self.optimization_history),
            'optimization_history': self.optimization_history,
            'config': self.config
        }


# æµ‹è¯•å‡½æ•°
def test_multi_objective_optimization():
    """æµ‹è¯•å¤šç›®æ ‡ä¼˜åŒ–"""
    print("ğŸ¯ Testing DNM Multi-Objective Optimization")
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    def create_test_model(hidden_size=64):
        return nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, hidden_size, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(hidden_size, 10)
        )
    
    # åˆ›å»ºåˆå§‹ç§ç¾¤
    initial_models = [
        create_test_model(32),
        create_test_model(64),
        create_test_model(96),
        create_test_model(128)
    ]
    
    # åˆ›å»ºè™šæ‹Ÿæ•°æ®
    from torch.utils.data import TensorDataset, DataLoader
    
    train_data = torch.randn(100, 3, 32, 32)
    train_labels = torch.randint(0, 10, (100,))
    val_data = torch.randn(50, 3, 32, 32)
    val_labels = torch.randint(0, 10, (50,))
    
    train_loader = DataLoader(TensorDataset(train_data, train_labels), batch_size=16)
    val_loader = DataLoader(TensorDataset(val_data, val_labels), batch_size=16)
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = DNMMultiObjectiveOptimization()
    
    # æ‰§è¡Œä¼˜åŒ–
    result = optimizer.optimize_architecture_population(
        initial_models, train_loader, val_loader, epoch=15
    )
    
    print(f"Optimization result: {result}")
    print(f"Summary: {optimizer.get_optimization_summary()}")
    
    return optimizer, result


if __name__ == "__main__":
    test_multi_objective_optimization()