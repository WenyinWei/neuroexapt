#!/usr/bin/env python3
"""
è‡ªé€‚åº”æ¶æ„æ€§èƒ½Profilingå·¥å…·

åˆ†æè‡ªé€‚åº”ç”Ÿé•¿æ¶æ„çš„å„ä¸ªç¯èŠ‚è€—æ—¶ï¼š
1. MixedOpè®¡ç®—å¼€é”€
2. æ¶æ„å‚æ•°æ›´æ–°å¼€é”€  
3. SoftmaxSumä¼˜åŒ–æ•ˆæœ
4. å†…å­˜ä½¿ç”¨åˆ†æ
5. ä¸å›ºå®šæ¶æ„çš„è¯¦ç»†å¯¹æ¯”
"""

import torch
import torch.nn as nn
import torch.optim as optim
import time
import sys
import os
from contextlib import contextmanager
from typing import Dict, List, Tuple, Optional
import threading
import psutil
import gc

# Add project to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from neuroexapt.core.model import Network
from neuroexapt.core.architect import Architect
from neuroexapt.core.operations import MixedOp
from neuroexapt.cuda_ops.softmax_sum import SoftmaxSumFunction

class PerformanceProfiler:
    """æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.timings = {}
        self.memory_stats = {}
        self.call_counts = {}
        
    @contextmanager
    def time_block(self, name: str):
        """æµ‹é‡ä»£ç å—æ‰§è¡Œæ—¶é—´"""
        start_time = time.perf_counter()
        start_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
        
        try:
            yield
        finally:
            end_time = time.perf_counter()
            end_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
            
            elapsed = end_time - start_time
            memory_diff = end_memory - start_memory
            
            if name not in self.timings:
                self.timings[name] = []
                self.memory_stats[name] = []
                self.call_counts[name] = 0
            
            self.timings[name].append(elapsed)
            self.memory_stats[name].append(memory_diff)
            self.call_counts[name] += 1
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ç»“æœ"""
        results = {}
        for name in self.timings:
            times = self.timings[name]
            memories = self.memory_stats[name]
            
            results[name] = {
                'avg_time': sum(times) / len(times),
                'total_time': sum(times),
                'call_count': self.call_counts[name],
                'avg_memory': sum(memories) / len(memories) / 1024 / 1024,  # MB
                'total_memory': sum(memories) / 1024 / 1024,  # MB
            }
        
        return results
    
    def print_report(self):
        """æ‰“å°æ€§èƒ½æŠ¥å‘Š"""
        stats = self.get_stats()
        
        print("\nğŸ” æ€§èƒ½åˆ†ææŠ¥å‘Š")
        print("=" * 80)
        print(f"{'ç»„ä»¶':<20} {'è°ƒç”¨æ¬¡æ•°':<8} {'æ€»æ—¶é—´(s)':<12} {'å¹³å‡æ—¶é—´(ms)':<12} {'å†…å­˜å˜åŒ–(MB)':<12}")
        print("-" * 80)
        
        # æŒ‰æ€»æ—¶é—´æ’åº
        sorted_stats = sorted(stats.items(), key=lambda x: x[1]['total_time'], reverse=True)
        
        for name, stat in sorted_stats:
            print(f"{name:<20} {stat['call_count']:<8} "
                  f"{stat['total_time']:<12.3f} {stat['avg_time']*1000:<12.1f} "
                  f"{stat['avg_memory']:<12.1f}")

def create_test_data(batch_size: int = 96):
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    return (
        torch.randn(batch_size, 3, 32, 32, device='cuda'),
        torch.randint(0, 10, (batch_size,), device='cuda')
    )

def profile_fixed_architecture(profiler: PerformanceProfiler, epochs: int = 3):
    """åˆ†æå›ºå®šæ¶æ„æ€§èƒ½"""
    print("ğŸ“Š åˆ†æå›ºå®šæ¶æ„æ€§èƒ½...")
    
    # åˆ›å»ºå›ºå®šæ¶æ„ï¼ˆç±»ä¼¼basic_classification.pyä¸­çš„FixedNetworkï¼‰
    from examples.basic_classification import FixedNetwork
    
    model = FixedNetwork(
        C=16,
        num_classes=10,
        layers=6
    ).cuda()
    
    criterion = nn.CrossEntropyLoss().cuda()
    optimizer = optim.SGD(model.parameters(), lr=0.025, momentum=0.9, weight_decay=3e-4)
    
    print(f"   å›ºå®šæ¶æ„å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # æµ‹è¯•æ•°æ®
    input_data, target_data = create_test_data()
    
    total_time = 0
    
    for epoch in range(epochs):
        epoch_start = time.time()
        
        for batch in range(10):  # ç®€åŒ–æµ‹è¯•10ä¸ªbatch
            with profiler.time_block("fixed_forward"):
                logits = model(input_data)
            
            with profiler.time_block("fixed_loss"):
                loss = criterion(logits, target_data)
            
            with profiler.time_block("fixed_backward"):
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
        
        epoch_time = time.time() - epoch_start
        total_time += epoch_time
        print(f"   å›ºå®šæ¶æ„ Epoch {epoch}: {epoch_time:.2f}s")
    
    avg_epoch_time = total_time / epochs
    print(f"   å›ºå®šæ¶æ„å¹³å‡æ¯epoch: {avg_epoch_time:.2f}s")
    
    return avg_epoch_time

def profile_adaptive_architecture(profiler: PerformanceProfiler, epochs: int = 3):
    """åˆ†æè‡ªé€‚åº”æ¶æ„æ€§èƒ½"""
    print("\nğŸ“Š åˆ†æè‡ªé€‚åº”æ¶æ„æ€§èƒ½...")
    
    # åˆ›å»ºè‡ªé€‚åº”æ¶æ„
    model = Network(
        C=16,
        num_classes=10,
        layers=6,
        potential_layers=4,
        use_optimized_ops=True,
        use_gradient_optimized=True,
        quiet=True
    ).cuda()
    
    criterion = nn.CrossEntropyLoss().cuda()
    optimizer = optim.SGD(model.parameters(), lr=0.025, momentum=0.9, weight_decay=3e-4)
    
    # æ¶æ„ä¼˜åŒ–å™¨
    class SimpleArgs:
        arch_learning_rate = 3e-4
        arch_weight_decay = 1e-3
        momentum = 0.9
        weight_decay = 3e-4
        learning_rate = 0.025
        
    architect = Architect(model, SimpleArgs())
    architect.criterion = criterion
    
    print(f"   è‡ªé€‚åº”æ¶æ„å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # æµ‹è¯•æ•°æ®
    input_data, target_data = create_test_data()
    input_valid, target_valid = create_test_data()
    
    total_time = 0
    
    for epoch in range(epochs):
        epoch_start = time.time()
        
        for batch in range(10):  # ç®€åŒ–æµ‹è¯•10ä¸ªbatch
            # æ¶æ„æœç´¢æ­¥éª¤ï¼ˆæ¯5ä¸ªbatchä¸€æ¬¡ï¼‰
            if batch % 5 == 0:
                with profiler.time_block("arch_search_step"):
                    architect.step(
                        input_data, target_data, 
                        input_valid, target_valid,
                        0.025, optimizer, False
                    )
            
            # æƒé‡æ›´æ–°æ­¥éª¤
            with profiler.time_block("adaptive_forward"):
                logits = model(input_data)
            
            with profiler.time_block("adaptive_loss"):
                loss = criterion(logits, target_data)
            
            with profiler.time_block("adaptive_backward"):
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
        
        epoch_time = time.time() - epoch_start
        total_time += epoch_time
        print(f"   è‡ªé€‚åº”æ¶æ„ Epoch {epoch}: {epoch_time:.2f}s")
    
    avg_epoch_time = total_time / epochs
    print(f"   è‡ªé€‚åº”æ¶æ„å¹³å‡æ¯epoch: {avg_epoch_time:.2f}s")
    
    return avg_epoch_time

def profile_mixedop_components(profiler: PerformanceProfiler):
    """è¯¦ç»†åˆ†æMixedOpç»„ä»¶æ€§èƒ½"""
    print("\nğŸ”¬ è¯¦ç»†åˆ†æMixedOpç»„ä»¶...")
    
    from neuroexapt.core.operations import OPS
    
    # æµ‹è¯•å•ä¸ªæ“ä½œ
    C = 16
    input_tensor = torch.randn(96, C, 32, 32, device='cuda')
    
    print("   å•ä¸ªæ“ä½œæ€§èƒ½æµ‹è¯•:")
    for op_name in ['sep_conv_3x3', 'sep_conv_5x5', 'dil_conv_3x3', 'avg_pool_3x3']:
        op = OPS[op_name](C, 1, False).cuda()
        
        # é¢„çƒ­
        for _ in range(5):
            _ = op(input_tensor)
        
        # æµ‹è¯•
        times = []
        for _ in range(20):
            torch.cuda.synchronize()
            start = time.perf_counter()
            _ = op(input_tensor)
            torch.cuda.synchronize()
            times.append(time.perf_counter() - start)
        
        avg_time = sum(times) / len(times) * 1000
        print(f"     {op_name:<15}: {avg_time:.2f}ms")
    
    # æµ‹è¯•MixedOpæ•´ä½“æ€§èƒ½
    print("\n   MixedOpæ•´ä½“æ€§èƒ½:")
    
    # æ ‡å‡†MixedOp
    mixedop = MixedOp(C, 1).cuda()
    weights = torch.softmax(torch.randn(len(mixedop._ops), device='cuda'), 0)
    
    with profiler.time_block("mixedop_standard"):
        for _ in range(20):
            _ = mixedop(input_tensor, weights)
    
    # ä¼˜åŒ–MixedOp
    mixedop_opt = MixedOp(C, 1).cuda()
    
    with profiler.time_block("mixedop_optimized"):
        for _ in range(20):
            _ = mixedop_opt(input_tensor, weights)

def profile_softmax_sum_performance(profiler: PerformanceProfiler):
    """åˆ†æSoftmaxSumæ€§èƒ½"""
    print("\nğŸ”¥ åˆ†æSoftmaxSumæ€§èƒ½...")
    
    # ä¸åŒè§„æ¨¡æµ‹è¯•
    test_configs = [
        ("Small", 4, 2, 16, 16, 16),
        ("Medium", 8, 4, 32, 32, 32),
        ("Large", 12, 8, 64, 64, 64),
    ]
    
    def pytorch_baseline(x, logits):
        weights = torch.softmax(logits, 0)
        return (x * weights.view(-1, 1, 1, 1, 1)).sum(dim=0)
    
    print("   SoftmaxSumæ€§èƒ½å¯¹æ¯”:")
    for name, N, B, C, H, W in test_configs:
        x = torch.randn(N, B, C, H, W, device='cuda')
        logits = torch.randn(N, device='cuda')
        
        # PyTorchåŸºçº¿
        times_pt = []
        for _ in range(10):
            torch.cuda.synchronize()
            start = time.perf_counter()
            _ = pytorch_baseline(x, logits)
            torch.cuda.synchronize()
            times_pt.append(time.perf_counter() - start)
        
        # CUDAä¼˜åŒ–ç‰ˆæœ¬
        times_cuda = []
        for _ in range(10):
            torch.cuda.synchronize()
            start = time.perf_counter()
            _ = SoftmaxSumFunction.apply(x, logits)
            torch.cuda.synchronize()
            times_cuda.append(time.perf_counter() - start)
        
        avg_pt = sum(times_pt) / len(times_pt) * 1000
        avg_cuda = sum(times_cuda) / len(times_cuda) * 1000
        speedup = avg_pt / avg_cuda
        
        print(f"     {name:<8}: PyTorch {avg_pt:.2f}ms, CUDA {avg_cuda:.2f}ms, åŠ é€Ÿ {speedup:.2f}x")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ è‡ªé€‚åº”æ¶æ„æ€§èƒ½æ·±åº¦åˆ†æ")
    print("=" * 80)
    
    profiler = PerformanceProfiler()
    
    # æ¸…ç†GPUå†…å­˜
    torch.cuda.empty_cache()
    
    # 1. åˆ†æå›ºå®šæ¶æ„
    fixed_time = profile_fixed_architecture(profiler, epochs=2)
    
    # æ¸…ç†GPUå†…å­˜
    torch.cuda.empty_cache()
    
    # 2. åˆ†æè‡ªé€‚åº”æ¶æ„
    adaptive_time = profile_adaptive_architecture(profiler, epochs=2)
    
    # 3. åˆ†æMixedOpç»„ä»¶
    profile_mixedop_components(profiler)
    
    # 4. åˆ†æSoftmaxSumæ€§èƒ½
    profile_softmax_sum_performance(profiler)
    
    # 5. æ€»ç»“æŠ¥å‘Š
    profiler.print_report()
    
    print(f"\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”æ€»ç»“:")
    print("=" * 50)
    print(f"å›ºå®šæ¶æ„å¹³å‡æ—¶é—´:     {fixed_time:.2f}s/epoch")
    print(f"è‡ªé€‚åº”æ¶æ„å¹³å‡æ—¶é—´:   {adaptive_time:.2f}s/epoch")
    print(f"æ€§èƒ½å·®è·:             {adaptive_time/fixed_time:.1f}x æ…¢")
    
    # åˆ†æåŸå› 
    print(f"\nğŸ” æ€§èƒ½å·®è·åˆ†æ:")
    print("=" * 50)
    
    # å‚æ•°é‡åˆ†æ
    fixed_params = 337866  # ä»ä¹‹å‰çš„è¾“å‡º
    adaptive_params = 10196070  # ä»ä¹‹å‰çš„è¾“å‡º
    param_ratio = adaptive_params / fixed_params
    
    print(f"å‚æ•°é‡å·®å¼‚:           {param_ratio:.1f}x ({adaptive_params:,} vs {fixed_params:,})")
    print(f"è®¡ç®—å¤æ‚åº¦:           MixedOp vs å›ºå®šæ“ä½œ")
    print(f"æ¶æ„æœç´¢å¼€é”€:         é¢å¤–çš„æ¢¯åº¦è®¡ç®—å’Œå‚æ•°æ›´æ–°")
    
    print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    print("1. å‡å°‘MixedOpä¸­çš„å€™é€‰æ“ä½œæ•°é‡")
    print("2. ä½¿ç”¨æ›´é«˜æ•ˆçš„æ¶æ„æœç´¢ç­–ç•¥") 
    print("3. å®ç°åŠ¨æ€æ“ä½œå‰ªæ")
    print("4. ä¼˜åŒ–SoftmaxSumå’ŒTritonå†…æ ¸")

if __name__ == "__main__":
    main() 