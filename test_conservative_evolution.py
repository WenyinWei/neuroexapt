"""
æµ‹è¯•ä¿å®ˆçš„ä¿¡æ¯è®ºè¿›åŒ–ç­–ç•¥
è¿™ä¸ªè„šæœ¬ç”¨äºå¿«é€ŸéªŒè¯è°ƒæ•´åçš„æ¡†æ¶æ˜¯å¦èƒ½å¤Ÿä¿æŒæ›´é«˜çš„å‡†ç¡®ç‡
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import numpy as np
import sys
import os

# Add the current directory to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from neuroexapt.core.structural_evolution import StructuralEvolution
from neuroexapt.core.operators import PruneByEntropy, AddBlock, ExpandWithMI, create_conv_block
from neuroexapt.neuroexapt import NeuroExapt


class TestCNN(nn.Module):
    """ç®€å•çš„æµ‹è¯•CNN"""
    def __init__(self, num_classes=10):
        super(TestCNN, self).__init__()
        
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),    # features.0
            nn.ReLU(inplace=True),             # features.1
            nn.MaxPool2d(2, 2),                # features.2
            nn.Conv2d(32, 64, 3, padding=1),  # features.3
            nn.ReLU(inplace=True),             # features.4
            nn.MaxPool2d(2, 2),                # features.5
        )
        
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(64 * 8 * 8, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(128, num_classes)
        )
        
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x


def load_test_data(batch_size=64):
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])

    trainset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform
    )
    # ä½¿ç”¨å°å­é›†è¿›è¡Œå¿«é€Ÿæµ‹è¯•
    trainset.data = trainset.data[:1000]  # åªä½¿ç”¨1000ä¸ªæ ·æœ¬
    trainset.targets = trainset.targets[:1000]
    
    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)

    testset = torchvision.datasets.CIFAR10(
        root='./data', train=False, download=False, transform=transform
    )
    testset.data = testset.data[:200]  # åªä½¿ç”¨200ä¸ªæµ‹è¯•æ ·æœ¬
    testset.targets = testset.targets[:200]
    
    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)
    
    return trainloader, testloader


def evaluate_model(model, testloader, device):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, targets in testloader:
            data, targets = data.to(device), targets.to(device)
            outputs = model(data)
            _, predicted = torch.max(outputs.data, 1)
            total += targets.size(0)
            correct += (predicted == targets).sum().item()
    
    accuracy = 100 * correct / total
    model.train()
    return accuracy


def main():
    print("ğŸ§ª æµ‹è¯•ä¿å®ˆçš„ä¿¡æ¯è®ºè¿›åŒ–ç­–ç•¥")
    print("=" * 50)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®
    print("ğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®...")
    trainloader, testloader = load_test_data()
    
    # åˆ›å»ºæ¨¡å‹
    print("ğŸ—ï¸ åˆ›å»ºæµ‹è¯•æ¨¡å‹...")
    model = TestCNN().to(device)
    criterion = nn.CrossEntropyLoss()
    
    initial_params = sum(p.numel() for p in model.parameters())
    print(f"åˆå§‹å‚æ•°æ•°é‡: {initial_params:,}")
    
    # è·å–åˆå§‹å‡†ç¡®ç‡
    initial_acc = evaluate_model(model, testloader, device)
    print(f"åˆå§‹å‡†ç¡®ç‡: {initial_acc:.2f}%")
    
    # è®¾ç½®ä¿å®ˆçš„æ“ä½œç¬¦
    operators = [
        # åªé’ˆå¯¹éå…³é”®å±‚è¿›è¡Œå‰ªæ
        PruneByEntropy('features.1'),  # ReLUå±‚
        PruneByEntropy('features.4'),  # ReLUå±‚
        
        # ä¼˜å…ˆæ‰©å±•æ“ä½œ
        AddBlock('features.2', lambda channels: create_conv_block(channels)),
        AddBlock('features.5', lambda channels: create_conv_block(channels)),
        
        # ä¿å®ˆçš„æ‰©å±•
        ExpandWithMI(expansion_factor=1.1),
    ]
    
    # åˆ›å»ºNeuroExaptå®ä¾‹
    neuroexapt = NeuroExapt(
        model=model,
        criterion=criterion,
        dataloader=trainloader,
        operators=operators,
        device=device,
        lambda_entropy=0.005,    # é™ä½æ­£åˆ™åŒ–
        lambda_bayesian=0.002,
        enable_validation=True
    )
    
    print("\nğŸ§  å¼€å§‹è¿›åŒ–æµ‹è¯•...")
    
    # è¿›è¡Œå‡ è½®è®­ç»ƒå’Œè¿›åŒ–
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    for epoch in range(5):  # åªæµ‹è¯•5ä¸ªepoch
        print(f"\nEpoch {epoch + 1}/5")
        
        # ç®€å•è®­ç»ƒ
        model.train()
        for i, (data, targets) in enumerate(trainloader):
            if i >= 5:  # æ¯ä¸ªepochåªè®­ç»ƒ5ä¸ªbatch
                break
                
            data, targets = data.to(device), targets.to(device)
            optimizer.zero_grad()
            
            outputs = model(data)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
        
        # è¯„ä¼°
        current_acc = evaluate_model(model, testloader, device)
        print(f"å½“å‰å‡†ç¡®ç‡: {current_acc:.2f}%")
        
        # å°è¯•è¿›åŒ–
        performance_metrics = {
            'val_accuracy': current_acc,
            'train_accuracy': current_acc,  # ç®€åŒ–æµ‹è¯•
            'loss': loss.item()
        }
        
        prev_params = sum(p.numel() for p in model.parameters())
        evolved_model, action_taken = neuroexapt.evolution_engine.step(
            epoch=epoch + 1,
            performance_metrics=performance_metrics,
            dataloader=trainloader,
            criterion=criterion
        )
        
        if action_taken:
            model = evolved_model
            neuroexapt.model = model
            new_params = sum(p.numel() for p in model.parameters())
            
            print(f"âœ… è¿›åŒ–åŠ¨ä½œ: {action_taken}")
            print(f"ğŸ“Š å‚æ•°å˜åŒ–: {prev_params:,} -> {new_params:,} (Î”{new_params - prev_params:+,})")
            
            # é‡æ–°è¯„ä¼°
            post_evolution_acc = evaluate_model(model, testloader, device)
            print(f"ğŸ¯ è¿›åŒ–åå‡†ç¡®ç‡: {post_evolution_acc:.2f}%")
            
            # é‡ç½®ä¼˜åŒ–å™¨
            optimizer = optim.Adam(model.parameters(), lr=0.001)
        else:
            print("â„¹ï¸ æœªè¿›è¡Œè¿›åŒ–")
    
    # æœ€ç»ˆç»“æœ
    final_acc = evaluate_model(model, testloader, device)
    final_params = sum(p.numel() for p in model.parameters())
    
    print(f"\nğŸ“Š æœ€ç»ˆç»“æœ:")
    print(f"   åˆå§‹å‡†ç¡®ç‡: {initial_acc:.2f}%")
    print(f"   æœ€ç»ˆå‡†ç¡®ç‡: {final_acc:.2f}%")
    print(f"   å‡†ç¡®ç‡å˜åŒ–: {final_acc - initial_acc:+.2f}%")
    print(f"   åˆå§‹å‚æ•°: {initial_params:,}")
    print(f"   æœ€ç»ˆå‚æ•°: {final_params:,}")
    print(f"   å‚æ•°å˜åŒ–: {final_params - initial_params:+,}")
    print(f"   è¿›åŒ–æ¬¡æ•°: {neuroexapt.stats['evolutions']}")
    
    # æ¸…ç†
    neuroexapt.cleanup()
    
    if final_acc >= initial_acc - 2.0:  # å…è®¸å°å¹…ä¸‹é™
        print("âœ… æµ‹è¯•é€šè¿‡ï¼šä¿å®ˆç­–ç•¥æˆåŠŸä¿æŒäº†å‡†ç¡®ç‡ï¼")
        return True
    else:
        print("âŒ æµ‹è¯•å¤±è´¥ï¼šå‡†ç¡®ç‡ä¸‹é™è¿‡å¤š")
        return False


if __name__ == "__main__":
    torch.manual_seed(42)
    np.random.seed(42)
    
    try:
        success = main()
        if success:
            print("\nğŸ‰ ä¿å®ˆè¿›åŒ–ç­–ç•¥æµ‹è¯•æˆåŠŸï¼")
        else:
            print("\nâš ï¸ éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´ç­–ç•¥")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc() 