#!/usr/bin/env python3
"""
NeuroExapt - æ¿€è¿›å¤šç†è®ºé©±åŠ¨çš„è‡ªé€‚åº”æ¶æ„æ¼”åŒ–ç³»ç»Ÿ

ç»“åˆä¿¡æ¯è®ºã€éå‡¸ä¼˜åŒ–ã€éçº¿æ€§è§„åˆ’ã€ç¥ç»æ­£åˆ‡æ ¸ç†è®ºã€æµå½¢å­¦ä¹ ç­‰
å®ç°çœŸæ­£æ™ºèƒ½çš„æ¶æ„è‡ªç”Ÿé•¿ç³»ç»Ÿ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import numpy as np
import networkx as nx
from scipy.optimize import minimize, differential_evolution
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import math
import random
from collections import defaultdict, deque
import sys
import os

# Add neuroexapt to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from neuroexapt.trainer import Trainer, train_with_neuroexapt


class InformationFlowAnalyzer:
    """ä¿¡æ¯æµåŠ¨åˆ†æå™¨ - æ·±åº¦ä¿¡æ¯è®ºåˆ†æ"""
    
    def __init__(self, model, device):
        self.model = model
        self.device = device
        self.activation_cache = {}
        self.gradient_cache = {}
        self.information_graph = nx.DiGraph()
        
    def analyze_information_bottlenecks(self, dataloader, num_batches=5):
        """æ·±åº¦ä¿¡æ¯ç“¶é¢ˆåˆ†æ"""
        self.model.eval()
        
        # æ”¶é›†æ¿€æ´»å’Œæ¢¯åº¦
        hooks = self._register_hooks()
        
        bottlenecks = []
        information_flows = []
        
        with torch.no_grad():
            for batch_idx, (data, target) in enumerate(dataloader):
                if batch_idx >= num_batches:
                    break
                    
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                
                # åˆ†ææ¯å±‚çš„ä¿¡æ¯æµ
                flow_analysis = self._analyze_layer_flows()
                information_flows.append(flow_analysis)
        
        # æ¸…ç†hooks
        for hook in hooks:
            hook.remove()
        
        # è¯†åˆ«å…³é”®ç“¶é¢ˆ
        bottlenecks = self._identify_critical_bottlenecks(information_flows)
        
        return {
            'bottlenecks': bottlenecks,
            'flow_efficiency': self._calculate_flow_efficiency(information_flows),
            'topology_metrics': self._analyze_network_topology(),
            'redundancy_analysis': self._analyze_redundancy()
        }
    
    def _calculate_flow_efficiency(self, information_flows):
        """è®¡ç®—ä¿¡æ¯æµæ•ˆç‡"""
        if not information_flows:
            return 0.0
        
        # è®¡ç®—æ‰€æœ‰å±‚çš„å¹³å‡ä¼ è¾“æ•ˆç‡
        all_efficiencies = []
        for flow in information_flows:
            for layer_name, metrics in flow.items():
                all_efficiencies.append(metrics['transfer_efficiency'])
        
        if all_efficiencies:
            return np.mean(all_efficiencies)
        else:
            return 0.0
    
    def _register_hooks(self):
        """æ³¨å†Œhookæ”¶é›†æ¿€æ´»å’Œæ¢¯åº¦"""
        hooks = []
        
        def forward_hook(name):
            def hook(module, input, output):
                self.activation_cache[name] = output.detach()
            return hook
        
        def backward_hook(name):
            def hook(module, grad_input, grad_output):
                if grad_output[0] is not None:
                    self.gradient_cache[name] = grad_output[0].detach()
            return hook
        
        for name, module in self.model.named_modules():
            if len(list(module.children())) == 0:  # å¶å­èŠ‚ç‚¹
                hooks.append(module.register_forward_hook(forward_hook(name)))
                hooks.append(module.register_backward_hook(backward_hook(name)))
        
        return hooks
    
    def _analyze_layer_flows(self):
        """åˆ†æå±‚é—´ä¿¡æ¯æµåŠ¨"""
        flow_metrics = {}
        
        layer_names = list(self.activation_cache.keys())
        
        for i, layer_name in enumerate(layer_names):
            activation = self.activation_cache[layer_name]
            
            # è®¡ç®—ä¿¡æ¯ç†µ
            entropy = self._calculate_activation_entropy(activation)
            
            # è®¡ç®—ä¿¡æ¯ä¼ é€’æ•ˆç‡
            if i > 0:
                prev_activation = self.activation_cache[layer_names[i-1]]
                transfer_efficiency = self._calculate_transfer_efficiency(
                    prev_activation, activation
                )
            else:
                transfer_efficiency = 1.0
            
            # è®¡ç®—æ¢¯åº¦æµå¼ºåº¦
            gradient_strength = 0.0
            if layer_name in self.gradient_cache:
                grad = self.gradient_cache[layer_name]
                gradient_strength = torch.norm(grad).item()
            
            flow_metrics[layer_name] = {
                'entropy': entropy,
                'transfer_efficiency': transfer_efficiency,
                'gradient_strength': gradient_strength,
                'information_bottleneck_score': entropy / (transfer_efficiency + 1e-8)
            }
        
        return flow_metrics
    
    def _calculate_activation_entropy(self, activation):
        """è®¡ç®—æ¿€æ´»çš„ä¿¡æ¯ç†µ"""
        # å°†æ¿€æ´»å€¼ç¦»æ•£åŒ–
        flat_activation = activation.flatten().cpu().numpy()
        
        # ä½¿ç”¨è‡ªé€‚åº”åˆ†ç®±
        n_bins = min(50, max(10, int(np.sqrt(len(flat_activation)))))
        hist, _ = np.histogram(flat_activation, bins=n_bins)
        
        # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
        prob = hist / hist.sum()
        prob = prob[prob > 0]  # ç§»é™¤é›¶æ¦‚ç‡
        
        # è®¡ç®—ç†µ
        entropy = -np.sum(prob * np.log2(prob + 1e-10))
        
        return entropy
    
    def _calculate_transfer_efficiency(self, prev_activation, current_activation):
        """è®¡ç®—ä¿¡æ¯ä¼ é€’æ•ˆç‡"""
        # ä½¿ç”¨äº’ä¿¡æ¯ä¼°è®¡ä¼ é€’æ•ˆç‡
        prev_flat = prev_activation.flatten().cpu().numpy()
        curr_flat = current_activation.flatten().cpu().numpy()
        
        # ç¡®ä¿ä¸¤ä¸ªå¼ é‡é‡‡æ ·ç›¸åŒæ•°é‡çš„å…ƒç´ 
        sample_size = min(len(prev_flat), len(curr_flat), 10000)
        prev_sampled = prev_flat[:sample_size]
        curr_sampled = curr_flat[:sample_size]
        
        # ç®€åŒ–çš„äº’ä¿¡æ¯ä¼°è®¡
        try:
            correlation = np.corrcoef(prev_sampled, curr_sampled)[0, 1]
            transfer_efficiency = abs(correlation) if not np.isnan(correlation) else 0.0
        except:
            transfer_efficiency = 0.0
        
        return transfer_efficiency
    
    def _identify_critical_bottlenecks(self, information_flows):
        """è¯†åˆ«å…³é”®ä¿¡æ¯ç“¶é¢ˆ"""
        # ç»Ÿè®¡æ‰€æœ‰å±‚çš„ç“¶é¢ˆåˆ†æ•°
        bottleneck_scores = defaultdict(list)
        
        for flow in information_flows:
            for layer_name, metrics in flow.items():
                bottleneck_scores[layer_name].append(metrics['information_bottleneck_score'])
        
        # è®¡ç®—å¹³å‡ç“¶é¢ˆåˆ†æ•°
        avg_bottleneck_scores = {
            layer: np.mean(scores) 
            for layer, scores in bottleneck_scores.items()
        }
        
        # è¯†åˆ«ç“¶é¢ˆï¼ˆåˆ†æ•°é«˜çš„å±‚ï¼‰
        threshold = np.percentile(list(avg_bottleneck_scores.values()), 75)
        
        bottlenecks = [
            {'layer': layer, 'severity': score}
            for layer, score in avg_bottleneck_scores.items()
            if score > threshold
        ]
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
        bottlenecks.sort(key=lambda x: x['severity'], reverse=True)
        
        return bottlenecks
    
    def _analyze_network_topology(self):
        """åˆ†æç½‘ç»œæ‹“æ‰‘ç»“æ„"""
        # æ„å»ºä¿¡æ¯æµå›¾
        self.information_graph.clear()
        
        layer_names = list(self.activation_cache.keys())
        
        # æ·»åŠ èŠ‚ç‚¹
        for layer in layer_names:
            self.information_graph.add_node(layer)
        
        # æ·»åŠ è¾¹ï¼ˆåŸºäºæ¶æ„è¿æ¥ï¼‰
        for i in range(len(layer_names) - 1):
            self.information_graph.add_edge(layer_names[i], layer_names[i+1])
        
        # è®¡ç®—æ‹“æ‰‘æŒ‡æ ‡
        metrics = {
            'centrality': nx.betweenness_centrality(self.information_graph),
            'clustering': nx.clustering(self.information_graph.to_undirected()),
            'path_efficiency': self._calculate_path_efficiency()
        }
        
        return metrics
    
    def _calculate_path_efficiency(self):
        """è®¡ç®—è·¯å¾„æ•ˆç‡"""
        try:
            paths = dict(nx.all_pairs_shortest_path_length(self.information_graph))
            total_efficiency = 0
            count = 0
            
            for source in paths:
                for target in paths[source]:
                    if source != target:
                        distance = paths[source][target]
                        total_efficiency += 1.0 / distance
                        count += 1
            
            return total_efficiency / count if count > 0 else 0
        except:
            return 0
    
    def _analyze_redundancy(self):
        """åˆ†æç½‘ç»œå†—ä½™åº¦"""
        redundancy_scores = {}
        
        layer_names = list(self.activation_cache.keys())
        
        for layer_name in layer_names:
            activation = self.activation_cache[layer_name]
            
            # è®¡ç®—ç‰¹å¾å†—ä½™åº¦
            if len(activation.shape) >= 3:  # å·ç§¯å±‚
                # è®¡ç®—é€šé“é—´ç›¸å…³æ€§
                channels = activation.shape[1]
                if channels > 1:
                    activation_2d = activation.view(activation.shape[0], channels, -1)
                    correlation_matrix = torch.corrcoef(activation_2d.mean(dim=0))
                    
                    # è®¡ç®—å¹³å‡ç›¸å…³æ€§ï¼ˆæ’é™¤å¯¹è§’çº¿ï¼‰
                    mask = ~torch.eye(channels, dtype=torch.bool)
                    avg_correlation = correlation_matrix[mask].abs().mean().item()
                    
                    redundancy_scores[layer_name] = avg_correlation
                else:
                    redundancy_scores[layer_name] = 0
            else:
                redundancy_scores[layer_name] = 0
        
        return redundancy_scores


class NeuralTangentKernelAnalyzer:
    """ç¥ç»æ­£åˆ‡æ ¸åˆ†æå™¨ - åŸºäºNTKç†è®ºçš„æ¶æ„ä¼˜åŒ–"""
    
    def __init__(self, model, device):
        self.model = model
        self.device = device
    
    def analyze_ntk_properties(self, dataloader, num_samples=100):
        """åˆ†æç¥ç»æ­£åˆ‡æ ¸æ€§è´¨"""
        # æ”¶é›†æ ·æœ¬
        samples = []
        labels = []
        
        for data, target in dataloader:
            data, target = data.to(self.device), target.to(self.device)
            samples.append(data)
            labels.append(target)
            
            if len(samples) * data.size(0) >= num_samples:
                break
        
        X = torch.cat(samples, dim=0)[:num_samples]
        y = torch.cat(labels, dim=0)[:num_samples]
        
        # è®¡ç®—NTKçŸ©é˜µ
        ntk_matrix = self._compute_ntk_matrix(X)
        
        # åˆ†æNTKæ€§è´¨
        eigenvalues = torch.linalg.eigvals(ntk_matrix).real
        
        analysis = {
            'condition_number': (eigenvalues.max() / (eigenvalues.min() + 1e-10)).item(),
            'rank': torch.linalg.matrix_rank(ntk_matrix).item(),
            'spectral_bias': self._analyze_spectral_bias(eigenvalues),
            'learning_efficiency': self._estimate_learning_efficiency(ntk_matrix, y),
            'architecture_suggestions': self._generate_ntk_suggestions(eigenvalues)
        }
        
        return analysis
    
    def _compute_ntk_matrix(self, X):
        """è®¡ç®—ç¥ç»æ­£åˆ‡æ ¸çŸ©é˜µ"""
        n = X.size(0)
        ntk_matrix = torch.zeros(n, n, device=self.device)
        
        for i in range(n):
            for j in range(i, n):
                # è®¡ç®—NTK(xi, xj)
                ntk_value = self._compute_ntk_entry(X[i:i+1], X[j:j+1])
                ntk_matrix[i, j] = ntk_value
                ntk_matrix[j, i] = ntk_value
        
        return ntk_matrix
    
    def _compute_ntk_entry(self, x1, x2):
        """è®¡ç®—å•ä¸ªNTKçŸ©é˜µå…ƒç´ """
        # ç®€åŒ–çš„NTKè®¡ç®—ï¼ˆå®é™…åº”è¯¥æ˜¯æ‰€æœ‰å‚æ•°æ¢¯åº¦çš„å†…ç§¯ï¼‰
        self.model.zero_grad()
        
        output1 = self.model(x1)
        output2 = self.model(x2)
        
        # è®¡ç®—æ¢¯åº¦
        grad1 = torch.autograd.grad(
            output1.sum(), self.model.parameters(), 
            create_graph=True, retain_graph=True
        )
        grad2 = torch.autograd.grad(
            output2.sum(), self.model.parameters(), 
            create_graph=True, retain_graph=True
        )
        
        # è®¡ç®—æ¢¯åº¦å†…ç§¯
        ntk_value = sum(
            (g1 * g2).sum() for g1, g2 in zip(grad1, grad2)
        )
        
        return ntk_value.item()
    
    def _analyze_spectral_bias(self, eigenvalues):
        """åˆ†æè°±åå·®"""
        sorted_eigs = torch.sort(eigenvalues, descending=True)[0]
        
        # è®¡ç®—æœ‰æ•ˆç»´åº¦
        total = sorted_eigs.sum()
        cumsum = torch.cumsum(sorted_eigs, dim=0)
        effective_count = torch.sum(cumsum < 0.9 * total)
        # ç¡®ä¿effective_countæ˜¯tensor
        if isinstance(effective_count, torch.Tensor):
            effective_dim = int(effective_count.item()) + 1
        else:
            effective_dim = int(effective_count) + 1
        
        return {
            'effective_dimension': effective_dim,
            'spectral_decay': (sorted_eigs[1] / sorted_eigs[0]).item(),
            'energy_concentration': (sorted_eigs[:10].sum() / total).item()
        }
    
    def _estimate_learning_efficiency(self, ntk_matrix, labels):
        """ä¼°è®¡å­¦ä¹ æ•ˆç‡"""
        # åŸºäºNTKçš„å­¦ä¹ æ•ˆç‡ä¼°è®¡
        try:
            # è®¡ç®—NTKçš„é€†
            ntk_inv = torch.linalg.pinv(ntk_matrix)
            
            # ä¼°è®¡å­¦ä¹ é€Ÿåº¦
            learning_rate_estimate = torch.trace(ntk_inv).item() / len(labels)
            
            return {
                'learning_rate_estimate': learning_rate_estimate,
                'convergence_speed': 1.0 / (torch.norm(ntk_inv).item() + 1e-10)
            }
        except:
            return {'learning_rate_estimate': 0, 'convergence_speed': 0}
    
    def _generate_ntk_suggestions(self, eigenvalues):
        """åŸºäºNTKåˆ†æç”Ÿæˆæ¶æ„å»ºè®®"""
        suggestions = []
        
        condition_number = eigenvalues.max() / (eigenvalues.min() + 1e-10)
        
        if condition_number > 1000:
            suggestions.append({
                'type': 'add_residual_connections',
                'reason': 'High condition number indicates gradient flow issues',
                'priority': 'high'
            })
        
        if len(eigenvalues) < 50:
            suggestions.append({
                'type': 'increase_width',
                'reason': 'Low effective dimension suggests underparameterization',
                'priority': 'medium'
            })
        
        return suggestions


class ManifoldArchitectureOptimizer:
    """æµå½¢å­¦ä¹ æ¶æ„ä¼˜åŒ–å™¨"""
    
    def __init__(self, model, device):
        self.model = model
        self.device = device
    
    def analyze_data_manifold(self, dataloader, num_samples=1000):
        """åˆ†ææ•°æ®æµå½¢ç»“æ„"""
        # æ”¶é›†æ•°æ®è¡¨ç¤º
        representations = []
        labels = []
        
        self.model.eval()
        with torch.no_grad():
            for data, target in dataloader:
                data, target = data.to(self.device), target.to(self.device)
                
                # æå–ä¸­é—´è¡¨ç¤º
                features = self._extract_features(data)
                representations.append(features.cpu())
                labels.append(target.cpu())
                
                if len(representations) * data.size(0) >= num_samples:
                    break
        
        X = torch.cat(representations, dim=0)[:num_samples]
        y = torch.cat(labels, dim=0)[:num_samples]
        
        # æµå½¢åˆ†æ
        manifold_analysis = self._perform_manifold_analysis(X.numpy(), y.numpy())
        
        # ç”Ÿæˆæ¶æ„å»ºè®®
        architecture_suggestions = self._generate_manifold_suggestions(manifold_analysis)
        
        return {
            'manifold_properties': manifold_analysis,
            'architecture_suggestions': architecture_suggestions
        }
    
    def _extract_features(self, x):
        """æå–ä¸­é—´ç‰¹å¾è¡¨ç¤º"""
        # åœ¨å€’æ•°ç¬¬äºŒå±‚æå–ç‰¹å¾
        features = x
        
        if hasattr(self.model, 'features'):
            features = self.model.features(features)
            features = features.view(features.size(0), -1)
        elif hasattr(self.model, 'conv1'):
            # ResNet-likeæ¨¡å‹
            features = self.model.conv1(features)
            # ... æ·»åŠ æ›´å¤šå±‚çš„æå–
        
        return features
    
    def _perform_manifold_analysis(self, X, y):
        """æ‰§è¡Œæµå½¢åˆ†æ"""
        analysis = {}
        
        # t-SNEé™ç»´
        try:
            tsne = TSNE(n_components=2, random_state=42)
            X_tsne = tsne.fit_transform(X)
            
            # è®¡ç®—ç±»åˆ«åˆ†ç¦»åº¦
            class_separation = self._calculate_class_separation(X_tsne, y)
            analysis['class_separation'] = class_separation
            
        except Exception as e:
            analysis['class_separation'] = 0
        
        # PCAåˆ†æ
        try:
            pca = PCA()
            pca.fit(X)
            
            # è®¡ç®—æœ‰æ•ˆç»´åº¦
            explained_var_ratio = pca.explained_variance_ratio_
            cumsum = np.cumsum(explained_var_ratio)
            effective_dim = np.argmax(cumsum > 0.95) + 1
            
            analysis['intrinsic_dimension'] = effective_dim
            analysis['explained_variance_ratio'] = explained_var_ratio[:10].tolist()
            
        except Exception as e:
            analysis['intrinsic_dimension'] = X.shape[1]
            analysis['explained_variance_ratio'] = []
        
        return analysis
    
    def _calculate_class_separation(self, X_embedded, y):
        """è®¡ç®—ç±»åˆ«åˆ†ç¦»åº¦"""
        unique_classes = np.unique(y)
        
        if len(unique_classes) < 2:
            return 0
        
        # è®¡ç®—ç±»å†…è·ç¦»å’Œç±»é—´è·ç¦»
        intra_class_distances = []
        inter_class_distances = []
        
        for class_id in unique_classes:
            class_mask = (y == class_id)
            class_points = X_embedded[class_mask]
            
            if len(class_points) > 1:
                # ç±»å†…è·ç¦»
                intra_dist = np.mean([
                    np.linalg.norm(class_points[i] - class_points[j])
                    for i in range(len(class_points))
                    for j in range(i+1, len(class_points))
                ])
                intra_class_distances.append(intra_dist)
                
                # ç±»é—´è·ç¦»
                other_points = X_embedded[~class_mask]
                if len(other_points) > 0:
                    inter_dist = np.mean([
                        np.linalg.norm(cp - op)
                        for cp in class_points
                        for op in other_points[:100]  # é‡‡æ ·å‡å°‘è®¡ç®—é‡
                    ])
                    inter_class_distances.append(inter_dist)
        
        if len(intra_class_distances) > 0 and len(inter_class_distances) > 0:
            avg_intra = np.mean(intra_class_distances)
            avg_inter = np.mean(inter_class_distances)
            separation = avg_inter / (avg_intra + 1e-10)
        else:
            separation = 0
        
        return separation
    
    def _generate_manifold_suggestions(self, manifold_analysis):
        """åŸºäºæµå½¢åˆ†æç”Ÿæˆæ¶æ„å»ºè®®"""
        suggestions = []
        
        intrinsic_dim = manifold_analysis.get('intrinsic_dimension', 100)
        class_separation = manifold_analysis.get('class_separation', 0)
        
        if intrinsic_dim < 50:
            suggestions.append({
                'type': 'reduce_dimensionality',
                'target_dimension': intrinsic_dim,
                'reason': 'Data has low intrinsic dimension',
                'priority': 'high'
            })
        
        if class_separation < 2.0:
            suggestions.append({
                'type': 'add_nonlinearity',
                'reason': 'Poor class separation indicates need for more complex decision boundary',
                'priority': 'high'
            })
            
            suggestions.append({
                'type': 'increase_depth',
                'reason': 'Deeper network needed for better feature separation',
                'priority': 'medium'
            })
        
        return suggestions


class NonConvexArchitectureOptimizer:
    """éå‡¸ä¼˜åŒ–æ¶æ„æœç´¢å™¨"""
    
    def __init__(self, model, device):
        self.model = model
        self.device = device
        self.search_space = self._define_search_space()
    
    def _define_search_space(self):
        """å®šä¹‰æ¶æ„æœç´¢ç©ºé—´"""
        return {
            'num_layers': (2, 20),
            'layer_widths': (16, 512),
            'kernel_sizes': [1, 3, 5, 7],
            'activation_types': ['relu', 'gelu', 'swish', 'mish'],
            'normalization_types': ['batch', 'layer', 'instance'],
            'connection_types': ['sequential', 'residual', 'dense']
        }
    
    def optimize_architecture(self, train_loader, val_loader, max_iterations=50):
        """ä½¿ç”¨éå‡¸ä¼˜åŒ–æœç´¢æœ€ä¼˜æ¶æ„"""
        print("ğŸ” å¯åŠ¨éå‡¸æ¶æ„ä¼˜åŒ–...")
        
        # å®šä¹‰ç›®æ ‡å‡½æ•°
        def objective_function(architecture_params):
            return self._evaluate_architecture(architecture_params, train_loader, val_loader)
        
        # ä½¿ç”¨å·®åˆ†è¿›åŒ–ç®—æ³•
        bounds = self._get_parameter_bounds()
        
        result = differential_evolution(
            objective_function,
            bounds,
            maxiter=max_iterations,
            popsize=10,
            disp=True
        )
        
        optimal_params = result.x
        optimal_architecture = self._decode_architecture(optimal_params)
        
        return {
            'optimal_architecture': optimal_architecture,
            'optimal_score': result.fun,
            'optimization_result': result
        }
    
    def _get_parameter_bounds(self):
        """è·å–å‚æ•°è¾¹ç•Œ"""
        bounds = []
        
        # å±‚æ•°
        bounds.append(self.search_space['num_layers'])
        
        # æ¯å±‚çš„å®½åº¦ï¼ˆæœ€å¤šæ”¯æŒ10å±‚ï¼‰
        for _ in range(10):
            bounds.append(self.search_space['layer_widths'])
        
        # æ¿€æ´»å‡½æ•°ç±»å‹ï¼ˆç¼–ç ä¸º0-3ï¼‰
        bounds.append((0, 3))
        
        # å½’ä¸€åŒ–ç±»å‹ï¼ˆç¼–ç ä¸º0-2ï¼‰
        bounds.append((0, 2))
        
        # è¿æ¥ç±»å‹ï¼ˆç¼–ç ä¸º0-2ï¼‰
        bounds.append((0, 2))
        
        return bounds
    
    def _decode_architecture(self, params):
        """è§£ç æ¶æ„å‚æ•°"""
        num_layers = int(params[0])
        layer_widths = [int(params[i+1]) for i in range(num_layers)]
        activation_type = self.search_space['activation_types'][int(params[11])]
        norm_type = self.search_space['normalization_types'][int(params[12])]
        connection_type = self.search_space['connection_types'][int(params[13])]
        
        return {
            'num_layers': num_layers,
            'layer_widths': layer_widths,
            'activation_type': activation_type,
            'normalization_type': norm_type,
            'connection_type': connection_type
        }
    
    def _evaluate_architecture(self, params, train_loader, val_loader):
        """è¯„ä¼°æ¶æ„æ€§èƒ½"""
        try:
            # è§£ç æ¶æ„
            architecture = self._decode_architecture(params)
            
            # æ„å»ºæ¨¡å‹
            model = self._build_model_from_architecture(architecture)
            model = model.to(self.device)
            
            # å¿«é€Ÿè®­ç»ƒè¯„ä¼°
            score = self._quick_train_evaluate(model, train_loader, val_loader)
            
            # è¿”å›è´Ÿåˆ†æ•°ï¼ˆå› ä¸ºä¼˜åŒ–å™¨è¦æœ€å°åŒ–ï¼‰
            return -score
            
        except Exception as e:
            print(f"Architecture evaluation failed: {e}")
            return 1000  # æƒ©ç½šæ— æ•ˆæ¶æ„
    
    def _build_model_from_architecture(self, architecture):
        """æ ¹æ®æ¶æ„æè¿°æ„å»ºæ¨¡å‹"""
        layers = []
        
        # è¾“å…¥å±‚
        in_channels = 3
        
        for i, width in enumerate(architecture['layer_widths']):
            # å·ç§¯å±‚
            if i == 0:
                layers.append(nn.Conv2d(in_channels, width, 3, padding=1))
            else:
                prev_width = architecture['layer_widths'][i-1]
                layers.append(nn.Conv2d(prev_width, width, 3, padding=1))
            
            # å½’ä¸€åŒ–å±‚
            if architecture['normalization_type'] == 'batch':
                layers.append(nn.BatchNorm2d(width))
            elif architecture['normalization_type'] == 'layer':
                layers.append(nn.GroupNorm(1, width))
            
            # æ¿€æ´»å‡½æ•°
            if architecture['activation_type'] == 'relu':
                layers.append(nn.ReLU(inplace=True))
            elif architecture['activation_type'] == 'gelu':
                layers.append(nn.GELU())
            elif architecture['activation_type'] == 'swish':
                layers.append(nn.SiLU())
            
            # æ± åŒ–ï¼ˆæ¯éš”ä¸€å±‚ï¼‰
            if i % 2 == 1:
                layers.append(nn.MaxPool2d(2, 2))
        
        # å…¨å±€å¹³å‡æ± åŒ–
        layers.append(nn.AdaptiveAvgPool2d(1))
        
        # åˆ†ç±»å™¨
        final_width = architecture['layer_widths'][-1]
        classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(final_width, 10)
        )
        
        return nn.Sequential(
            nn.Sequential(*layers),
            classifier
        )
    
    def _quick_train_evaluate(self, model, train_loader, val_loader, epochs=3):
        """å¿«é€Ÿè®­ç»ƒè¯„ä¼°"""
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        criterion = nn.CrossEntropyLoss()
        
        model.train()
        
        # å¿«é€Ÿè®­ç»ƒ
        for epoch in range(epochs):
            for batch_idx, (data, target) in enumerate(train_loader):
                if batch_idx > 20:  # åªè®­ç»ƒå°‘é‡batch
                    break
                
                data, target = data.to(self.device), target.to(self.device)
                
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
        
        # è¯„ä¼°
        model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for batch_idx, (data, target) in enumerate(val_loader):
                if batch_idx > 10:  # åªè¯„ä¼°å°‘é‡batch
                    break
                
                data, target = data.to(self.device), target.to(self.device)
                output = model(data)
                pred = output.argmax(dim=1)
                correct += pred.eq(target).sum().item()
                total += target.size(0)
        
        accuracy = correct / total if total > 0 else 0
        return accuracy


class RadicalArchitectureEvolver:
    """æ¿€è¿›æ¶æ„æ¼”åŒ–å™¨ - æ•´åˆæ‰€æœ‰ç†è®º"""
    
    def __init__(self, model, device):
        self.model = model
        self.device = device
        
        # åˆå§‹åŒ–åˆ†æå™¨
        self.info_analyzer = InformationFlowAnalyzer(model, device)
        self.ntk_analyzer = NeuralTangentKernelAnalyzer(model, device)
        self.manifold_optimizer = ManifoldArchitectureOptimizer(model, device)
        self.nonconvex_optimizer = NonConvexArchitectureOptimizer(model, device)
        
        # æ¼”åŒ–å†å²
        self.evolution_history = []
        self.performance_history = []
    
    def radical_evolve(self, train_loader, val_loader, target_accuracy=0.9):
        """æ¿€è¿›æ¼”åŒ–ä¸»å¾ªç¯"""
        print("ğŸ§¬ å¯åŠ¨æ¿€è¿›å¤šç†è®ºæ¶æ„æ¼”åŒ–...")
        
        current_accuracy = self._evaluate_current_performance(val_loader)
        iteration = 0
        
        while current_accuracy < target_accuracy and iteration < 10:
            print(f"\nğŸ”„ æ¼”åŒ–è¿­ä»£ {iteration + 1}")
            print(f"å½“å‰å‡†ç¡®åº¦: {current_accuracy:.3f}")
            
            # 1. ä¿¡æ¯è®ºåˆ†æ
            print("ğŸ“Š æ‰§è¡Œæ·±åº¦ä¿¡æ¯æµåˆ†æ...")
            info_analysis = self.info_analyzer.analyze_information_bottlenecks(train_loader)
            
            # 2. NTKåˆ†æ
            print("ğŸ§® æ‰§è¡Œç¥ç»æ­£åˆ‡æ ¸åˆ†æ...")
            ntk_analysis = self.ntk_analyzer.analyze_ntk_properties(train_loader)
            
            # 3. æµå½¢åˆ†æ
            print("ğŸŒ€ æ‰§è¡Œæ•°æ®æµå½¢åˆ†æ...")
            manifold_analysis = self.manifold_optimizer.analyze_data_manifold(train_loader)
            
            # 4. ç»¼åˆå†³ç­–
            evolution_strategy = self._make_radical_decision(
                info_analysis, ntk_analysis, manifold_analysis
            )
            
            print(f"ğŸš€ æ¼”åŒ–ç­–ç•¥: {evolution_strategy['type']}")
            
            # 5. æ‰§è¡Œæ¼”åŒ–
            if evolution_strategy['type'] == 'nonconvex_search':
                # ä½¿ç”¨éå‡¸ä¼˜åŒ–é‡æ–°è®¾è®¡æ¶æ„
                print("ğŸ” å¯åŠ¨éå‡¸ä¼˜åŒ–æ¶æ„æœç´¢...")
                optimization_result = self.nonconvex_optimizer.optimize_architecture(
                    train_loader, val_loader, max_iterations=20
                )
                self.model = self._rebuild_model_from_optimization(optimization_result)
            else:
                # æ‰§è¡Œå…¶ä»–æ¼”åŒ–ç­–ç•¥
                self.model = self._execute_evolution_strategy(evolution_strategy)
            
            # 6. é‡æ–°è®­ç»ƒå’Œè¯„ä¼°
            print("ğŸ¯ é‡æ–°è®­ç»ƒæ¼”åŒ–åçš„æ¨¡å‹...")
            self._retrain_model(train_loader, val_loader)
            
            new_accuracy = self._evaluate_current_performance(val_loader)
            
            # è®°å½•æ¼”åŒ–å†å²
            self.evolution_history.append({
                'iteration': iteration,
                'strategy': evolution_strategy,
                'accuracy_before': current_accuracy,
                'accuracy_after': new_accuracy,
                'improvement': new_accuracy - current_accuracy
            })
            
            print(f"âœ… æ¼”åŒ–å®Œæˆ: {current_accuracy:.3f} â†’ {new_accuracy:.3f} (+{new_accuracy - current_accuracy:.3f})")
            
            current_accuracy = new_accuracy
            iteration += 1
        
        print(f"\nğŸ‰ æ¿€è¿›æ¼”åŒ–å®Œæˆ!")
        print(f"æœ€ç»ˆå‡†ç¡®åº¦: {current_accuracy:.3f}")
        print(f"æ€»æ¼”åŒ–æ¬¡æ•°: {iteration}")
        
        return {
            'final_model': self.model,
            'final_accuracy': current_accuracy,
            'evolution_history': self.evolution_history
        }
    
    def _make_radical_decision(self, info_analysis, ntk_analysis, manifold_analysis):
        """åŸºäºå¤šç†è®ºåˆ†æåšå‡ºæ¿€è¿›å†³ç­–"""
        suggestions = []
        
        # ä¿¡æ¯è®ºå»ºè®®
        bottlenecks = info_analysis['bottlenecks']
        if len(bottlenecks) > 0:
            worst_bottleneck = bottlenecks[0]
            suggestions.append({
                'type': 'remove_bottleneck',
                'target': worst_bottleneck['layer'],
                'severity': worst_bottleneck['severity'],
                'priority': worst_bottleneck['severity'],
                'source': 'information_theory'
            })
        
        # NTKå»ºè®®
        ntk_suggestions = ntk_analysis.get('architecture_suggestions', [])
        for sugg in ntk_suggestions:
            suggestions.append({
                'type': sugg['type'],
                'priority': 10 if sugg['priority'] == 'high' else 5,
                'source': 'ntk_theory'
            })
        
        # æµå½¢å­¦ä¹ å»ºè®®
        manifold_suggestions = manifold_analysis.get('architecture_suggestions', [])
        for sugg in manifold_suggestions:
            suggestions.append({
                'type': sugg['type'],
                'priority': 8 if sugg['priority'] == 'high' else 3,
                'source': 'manifold_learning'
            })
        
        # å¦‚æœå»ºè®®å¤ªä¿å®ˆï¼Œä½¿ç”¨éå‡¸ä¼˜åŒ–
        if len(suggestions) == 0 or max(s['priority'] for s in suggestions) < 5:
            return {
                'type': 'nonconvex_search',
                'reason': 'Conservative suggestions, using global optimization',
                'priority': 10
            }
        
        # é€‰æ‹©æœ€é«˜ä¼˜å…ˆçº§çš„å»ºè®®
        best_suggestion = max(suggestions, key=lambda x: x['priority'])
        
        return best_suggestion
    
    def _execute_evolution_strategy(self, strategy):
        """æ‰§è¡Œæ¼”åŒ–ç­–ç•¥"""
        strategy_type = strategy['type']
        
        if strategy_type == 'remove_bottleneck':
            return self._remove_information_bottleneck(strategy['target'])
        elif strategy_type == 'add_residual_connections':
            return self._add_residual_connections()
        elif strategy_type == 'increase_width':
            return self._increase_network_width()
        elif strategy_type == 'add_nonlinearity':
            return self._add_nonlinearity()
        elif strategy_type == 'increase_depth':
            return self._increase_network_depth()
        else:
            print(f"Unknown strategy: {strategy_type}")
            return self.model
    
    def _remove_information_bottleneck(self, target_layer):
        """ç§»é™¤ä¿¡æ¯ç“¶é¢ˆ"""
        # è¿™é‡Œå®ç°å…·ä½“çš„ç“¶é¢ˆç§»é™¤é€»è¾‘
        print(f"ç§»é™¤ä¿¡æ¯ç“¶é¢ˆ: {target_layer}")
        return self.model
    
    def _add_residual_connections(self):
        """æ·»åŠ æ®‹å·®è¿æ¥"""
        print("æ·»åŠ æ®‹å·®è¿æ¥")
        return self.model
    
    def _increase_network_width(self):
        """å¢åŠ ç½‘ç»œå®½åº¦"""
        print("å¢åŠ ç½‘ç»œå®½åº¦")
        return self.model
    
    def _add_nonlinearity(self):
        """å¢åŠ éçº¿æ€§"""
        print("å¢åŠ éçº¿æ€§")
        return self.model
    
    def _increase_network_depth(self):
        """å¢åŠ ç½‘ç»œæ·±åº¦"""
        print("å¢åŠ ç½‘ç»œæ·±åº¦")
        return self.model
    
    def _rebuild_model_from_optimization(self, optimization_result):
        """ä»ä¼˜åŒ–ç»“æœé‡å»ºæ¨¡å‹"""
        optimal_arch = optimization_result['optimal_architecture']
        new_model = self.nonconvex_optimizer._build_model_from_architecture(optimal_arch)
        return new_model.to(self.device)
    
    def _retrain_model(self, train_loader, val_loader, epochs=5):
        """é‡æ–°è®­ç»ƒæ¨¡å‹"""
        optimizer = optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=0.01)
        criterion = nn.CrossEntropyLoss()
        
        self.model.train()
        
        for epoch in range(epochs):
            for batch_idx, (data, target) in enumerate(train_loader):
                if batch_idx > 50:  # é™åˆ¶è®­ç»ƒé‡
                    break
                
                data, target = data.to(self.device), target.to(self.device)
                
                optimizer.zero_grad()
                output = self.model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
    
    def _evaluate_current_performance(self, val_loader):
        """è¯„ä¼°å½“å‰æ€§èƒ½"""
        self.model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                pred = output.argmax(dim=1)
                correct += pred.eq(target).sum().item()
                total += target.size(0)
        
        return correct / total if total > 0 else 0


def create_enhanced_dataloaders():
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    
    train_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform_train
    )
    val_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=False, download=True, transform=transform_test
    )
    
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)
    
    return train_loader, val_loader


class SimpleInitialModel(nn.Module):
    """ç®€å•çš„åˆå§‹æ¨¡å‹"""
    def __init__(self, num_classes=10):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
        )
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(64, num_classes)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x


def main():
    print("ğŸ§¬ æ¿€è¿›å¤šç†è®ºé©±åŠ¨æ¶æ„æ¼”åŒ–ç³»ç»Ÿ")
    print("=" * 60)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader = create_enhanced_dataloaders()
    
    # åˆ›å»ºç®€å•çš„åˆå§‹æ¨¡å‹
    model = SimpleInitialModel().to(device)
    
    # åˆ›å»ºæ¿€è¿›æ¼”åŒ–å™¨
    evolver = RadicalArchitectureEvolver(model, device)
    
    # å¼€å§‹æ¿€è¿›æ¼”åŒ–
    result = evolver.radical_evolve(
        train_loader, val_loader, 
        target_accuracy=0.85
    )
    
    # è¾“å‡ºç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ‰ æ¿€è¿›æ¼”åŒ–å®Œæˆ!")
    print("=" * 60)
    
    print(f"æœ€ç»ˆå‡†ç¡®åº¦: {result['final_accuracy']:.3f}")
    print(f"æ¼”åŒ–æ¬¡æ•°: {len(result['evolution_history'])}")
    
    print("\nğŸ“Š æ¼”åŒ–å†å²:")
    for i, evolution in enumerate(result['evolution_history']):
        print(f"  è¿­ä»£ {i+1}: {evolution['strategy']['type']} | "
              f"{evolution['accuracy_before']:.3f} â†’ {evolution['accuracy_after']:.3f} "
              f"(+{evolution['improvement']:.3f})")
    
    print("\nâœ… æ¿€è¿›å¤šç†è®ºæ¶æ„æ¼”åŒ–ç³»ç»Ÿè¿è¡Œå®Œæˆ!")


if __name__ == "__main__":
    main() 