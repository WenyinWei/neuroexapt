#!/usr/bin/env python3
"""
é‡æ„çš„æ™ºèƒ½æ¶æ„è¿›åŒ–æ¼”ç¤º - æ¨¡å—åŒ–ç‰ˆæœ¬
Refactored Intelligent Architecture Evolution Demo - Modular Version

ğŸ”§ é‡æ„ç›®æ ‡ï¼š
1. æ¨¡å—åŒ–è®¾è®¡ - æ¯ä¸ªç»„ä»¶å•ä¸€èŒè´£
2. é”™è¯¯å¤„ç† - æ›´å¥½çš„å¼‚å¸¸å¤„ç†å’Œé”™è¯¯æ¢å¤
3. å†…å­˜ç®¡ç† - ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼Œé¿å…OOM
4. è°ƒè¯•å‹å¥½ - æ¸…æ™°çš„æ—¥å¿—å’ŒçŠ¶æ€æ£€æŸ¥
5. å¯é…ç½®æ€§ - æ˜“äºè°ƒæ•´å‚æ•°

ğŸ§¬ æ¶æ„åˆ†è§£ï¼š
- DataModule: æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
- ModelModule: æ¨¡å‹å®šä¹‰å’Œåˆå§‹åŒ–
- TrainingModule: è®­ç»ƒå¾ªç¯å’Œä¼˜åŒ–å™¨ç®¡ç†
- AnalysisModule: ç‰¹å¾åˆ†æå’Œæ™ºèƒ½æ£€æµ‹
- EvolutionModule: æ¶æ„è¿›åŒ–å¼•æ“
- ConfigModule: é…ç½®ç®¡ç†å’ŒéªŒè¯
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import numpy as np
import time
import logging
import os
import sys
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional, Any
import traceback

# è®¾ç½®logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class DemoConfig:
    """æ¼”ç¤ºé…ç½®ç±»"""
    # æ•°æ®é…ç½®
    batch_size_train: int = 64  # å‡å°batch sizeèŠ‚çœå†…å­˜
    batch_size_test: int = 100
    num_workers: int = 2
    
    # è®­ç»ƒé…ç½®
    initial_epochs: int = 3  # å‡å°‘åˆå§‹è®­ç»ƒè½®æ•°
    learning_rate: float = 0.01  # é™ä½å­¦ä¹ ç‡
    weight_decay: float = 1e-4
    momentum: float = 0.9
    
    # åˆ†æé…ç½®
    max_analysis_batches: int = 2  # é™åˆ¶åˆ†æçš„æ‰¹æ¬¡æ•°
    analysis_min_accuracy: float = 25.0  # é™ä½åˆ†æé—¨æ§›
    
    # è¿›åŒ–é…ç½®
    evolution_min_accuracy: float = 40.0  # è¿›åŒ–æœ€ä½å‡†ç¡®ç‡è¦æ±‚
    max_evolution_iterations: int = 1  # å‡å°‘è¿›åŒ–è¿­ä»£
    
    # è®¾å¤‡é…ç½®
    device: str = 'auto'  # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
    use_amp: bool = False  # æ··åˆç²¾åº¦è®­ç»ƒ


class DataModule:
    """æ•°æ®æ¨¡å— - è´Ÿè´£æ•°æ®åŠ è½½å’Œé¢„å¤„ç†"""
    
    def __init__(self, config: DemoConfig):
        self.config = config
        self.train_loader = None
        self.test_loader = None
        
    def prepare_data(self):
        """å‡†å¤‡CIFAR-10æ•°æ®"""
        logger.info("ğŸ“¦ å‡†å¤‡CIFAR-10æ•°æ®é›†...")
        
        try:
            # ç®€åŒ–çš„æ•°æ®å¢å¼º - å‡å°‘è®¡ç®—å¼€é”€
            transform_train = transforms.Compose([
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.ToTensor(),
                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
            ])
            
            transform_test = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
            ])
            
            # åˆ›å»ºæ•°æ®é›†
            train_dataset = torchvision.datasets.CIFAR10(
                root='./data', train=True, download=True, transform=transform_train
            )
            
            test_dataset = torchvision.datasets.CIFAR10(
                root='./data', train=False, download=True, transform=transform_test
            )
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            self.train_loader = DataLoader(
                train_dataset, 
                batch_size=self.config.batch_size_train, 
                shuffle=True,
                num_workers=self.config.num_workers, 
                pin_memory=True
            )
            
            self.test_loader = DataLoader(
                test_dataset, 
                batch_size=self.config.batch_size_test, 
                shuffle=False,
                num_workers=self.config.num_workers, 
                pin_memory=True
            )
            
            logger.info(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ - è®­ç»ƒé›†: {len(train_dataset)}, æµ‹è¯•é›†: {len(test_dataset)}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
            return False


class SimpleResNet(nn.Module):
    """ç®€åŒ–çš„ResNet - ç”¨äºæ¼”ç¤º"""
    
    def __init__(self, num_classes=10):
        super(SimpleResNet, self).__init__()
        
        # ç‰¹å¾æå–
        self.features = nn.Sequential(
            # Block 1
            nn.Conv2d(3, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            
            # Block 2
            nn.Conv2d(32, 64, 3, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            
            # Block 3
            nn.Conv2d(64, 128, 3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            
            # Block 4
            nn.Conv2d(128, 256, 3, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            
            # Global pooling
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(128, num_classes)
        )
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x


class ModelModule:
    """æ¨¡å‹æ¨¡å— - è´Ÿè´£æ¨¡å‹åˆ›å»ºå’Œç®¡ç†"""
    
    def __init__(self, config: DemoConfig):
        self.config = config
        self.device = self._setup_device()
        self.model = None
        
    def _setup_device(self):
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if self.config.device == 'auto':
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            device = torch.device(self.config.device)
        
        logger.info(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
        
        if device.type == 'cuda':
            logger.info(f"GPUä¿¡æ¯: {torch.cuda.get_device_name()}")
            logger.info(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
            
        return device
    
    def create_model(self):
        """åˆ›å»ºæ¨¡å‹"""
        try:
            logger.info("ğŸ—ï¸  åˆ›å»ºç®€åŒ–ResNetæ¨¡å‹...")
            self.model = SimpleResNet(num_classes=10).to(self.device)
            
            # ç»Ÿè®¡å‚æ•°
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            
            logger.info(f"ğŸ“Š æ¨¡å‹å‚æ•°: æ€»è®¡ {total_params:,}, å¯è®­ç»ƒ {trainable_params:,}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
            return False


class TrainingModule:
    """è®­ç»ƒæ¨¡å— - è´Ÿè´£è®­ç»ƒå¾ªç¯å’Œä¼˜åŒ–å™¨ç®¡ç†"""
    
    def __init__(self, model, device, config: DemoConfig):
        self.model = model
        self.device = device
        self.config = config
        self.optimizer = None
        self.scheduler = None
        self.train_history = []
        self.test_history = []
        
        # ç«‹å³è®¾ç½®ä¼˜åŒ–å™¨
        self._setup_optimizer()
        
    def _setup_optimizer(self):
        """è®¾ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨"""
        try:
            if self.model is None:
                raise ValueError("æ¨¡å‹æœªåˆå§‹åŒ–")
                
            self.optimizer = optim.SGD(
                self.model.parameters(),
                lr=self.config.learning_rate,
                momentum=self.config.momentum,
                weight_decay=self.config.weight_decay
            )
            
            self.scheduler = optim.lr_scheduler.StepLR(
                self.optimizer, step_size=10, gamma=0.1
            )
            
            logger.info(f"âœ… ä¼˜åŒ–å™¨è®¾ç½®å®Œæˆ - LR: {self.config.learning_rate}")
            
        except Exception as e:
            logger.error(f"âŒ ä¼˜åŒ–å™¨è®¾ç½®å¤±è´¥: {e}")
            raise
    
    def train_epoch(self, epoch: int, train_loader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        if self.optimizer is None:
            raise RuntimeError("ä¼˜åŒ–å™¨æœªåˆå§‹åŒ–")
            
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        try:
            for batch_idx, (data, target) in enumerate(train_loader):
                data, target = data.to(self.device), target.to(self.device)
                
                # å‰å‘ä¼ æ’­
                self.optimizer.zero_grad()
                output = self.model(data)
                loss = F.cross_entropy(output, target)
                
                # åå‘ä¼ æ’­
                loss.backward()
                self.optimizer.step()
                
                # ç»Ÿè®¡
                total_loss += loss.item()
                pred = output.argmax(dim=1)
                correct += pred.eq(target).sum().item()
                total += target.size(0)
                
                # æ˜¾ç¤ºè¿›åº¦
                if batch_idx % 50 == 0:
                    logger.info(
                        f'Epoch {epoch}, Batch {batch_idx}/{len(train_loader)}, '
                        f'Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%'
                    )
                
                # å†…å­˜æ¸…ç†
                del data, target, output, loss
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            avg_loss = total_loss / len(train_loader)
            accuracy = 100.0 * correct / total
            
            self.train_history.append({
                'epoch': epoch, 
                'loss': avg_loss, 
                'accuracy': accuracy
            })
            
            return avg_loss, accuracy
            
        except Exception as e:
            logger.error(f"âŒ è®­ç»ƒepoch {epoch} å¤±è´¥: {e}")
            raise
    
    def evaluate(self, test_loader):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        test_loss = 0.0
        correct = 0
        total = 0
        
        try:
            with torch.no_grad():
                for data, target in test_loader:
                    data, target = data.to(self.device), target.to(self.device)
                    output = self.model(data)
                    
                    test_loss += F.cross_entropy(output, target, reduction='sum').item()
                    pred = output.argmax(dim=1)
                    correct += pred.eq(target).sum().item()
                    total += target.size(0)
                    
                    # å†…å­˜æ¸…ç†
                    del data, target, output
                    
            test_loss /= total
            accuracy = 100.0 * correct / total
            
            self.test_history.append({
                'loss': test_loss, 
                'accuracy': accuracy
            })
            
            return test_loss, accuracy
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
            return float('inf'), 0.0


class MockAnalysisModule:
    """æ¨¡æ‹Ÿåˆ†ææ¨¡å— - æä¾›åŸºç¡€çš„åˆ†æåŠŸèƒ½"""
    
    def __init__(self, device, config: DemoConfig):
        self.device = device
        self.config = config
    
    def extract_features(self, model, data_loader):
        """æå–ç‰¹å¾ - ç®€åŒ–ç‰ˆæœ¬"""
        logger.info("ğŸ” æå–æ¨¡å‹ç‰¹å¾...")
        
        features = {}
        labels_list = []
        
        model.eval()
        with torch.no_grad():
            batch_count = 0
            for data, target in data_loader:
                if batch_count >= self.config.max_analysis_batches:
                    break
                    
                data = data.to(self.device)
                
                # ç®€å•ç‰¹å¾æå– - åªå–æœ€åçš„ç‰¹å¾
                x = model.features(data)
                features[f'batch_{batch_count}'] = x.cpu()
                labels_list.append(target)
                
                batch_count += 1
                
                # å†…å­˜æ¸…ç†
                del data, x
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
        
        if labels_list:
            all_labels = torch.cat(labels_list, dim=0)
            return features, all_labels
        
        return None, None
    
    def analyze_mutual_information(self, features, labels):
        """æ¨¡æ‹Ÿäº’ä¿¡æ¯åˆ†æ"""
        logger.info("ğŸ”— æ¨¡æ‹Ÿäº’ä¿¡æ¯åˆ†æ...")
        
        # ç®€å•çš„æ¨¡æ‹Ÿåˆ†æ
        results = {}
        for key in features.keys():
            # æ¨¡æ‹Ÿäº’ä¿¡æ¯å€¼
            mi_value = np.random.uniform(0.5, 2.0)
            results[key] = mi_value
            logger.info(f"  {key}: MI = {mi_value:.4f}")
        
        return results
    
    def analyze_uncertainty(self, features, labels):
        """æ¨¡æ‹Ÿä¸ç¡®å®šæ€§åˆ†æ"""
        logger.info("ğŸ² æ¨¡æ‹Ÿä¸ç¡®å®šæ€§åˆ†æ...")
        
        results = {}
        for key in features.keys():
            # æ¨¡æ‹Ÿä¸ç¡®å®šæ€§å€¼
            uncertainty = np.random.uniform(0.1, 0.8)
            results[key] = uncertainty
            logger.info(f"  {key}: Uncertainty = {uncertainty:.4f}")
        
        return results
    
    def detect_bottlenecks(self, features, labels):
        """æ¨¡æ‹Ÿç“¶é¢ˆæ£€æµ‹"""
        logger.info("ğŸ” æ¨¡æ‹Ÿç“¶é¢ˆæ£€æµ‹...")
        
        bottlenecks = []
        for key in features.keys():
            # æ¨¡æ‹Ÿç“¶é¢ˆæ£€æµ‹
            if np.random.random() > 0.5:  # 50%æ¦‚ç‡æ£€æµ‹åˆ°ç“¶é¢ˆ
                bottleneck = {
                    'layer': key,
                    'type': 'ä¿¡æ¯ç“¶é¢ˆ',
                    'severity': np.random.uniform(0.3, 0.9),
                    'suggestion': 'å¢åŠ å±‚å®½åº¦'
                }
                bottlenecks.append(bottleneck)
        
        logger.info(f"æ£€æµ‹åˆ° {len(bottlenecks)} ä¸ªæ½œåœ¨ç“¶é¢ˆ")
        for bt in bottlenecks:
            logger.info(f"  {bt['layer']}: {bt['type']} (ä¸¥é‡ç¨‹åº¦: {bt['severity']:.3f})")
        
        return bottlenecks


class DemoRunner:
    """æ¼”ç¤ºè¿è¡Œå™¨ - ä¸»æ§åˆ¶ç±»"""
    
    def __init__(self, config: Optional[DemoConfig] = None):
        self.config = config or DemoConfig()
        self.data_module = None
        self.model_module = None
        self.training_module = None
        self.analysis_module = None
        
    def setup(self):
        """è®¾ç½®æ‰€æœ‰æ¨¡å—"""
        logger.info("ğŸš€ å¼€å§‹è®¾ç½®æ¼”ç¤ºç¯å¢ƒ...")
        
        try:
            # 1. æ•°æ®æ¨¡å—
            self.data_module = DataModule(self.config)
            if not self.data_module.prepare_data():
                return False
            
            # 2. æ¨¡å‹æ¨¡å—
            self.model_module = ModelModule(self.config)
            if not self.model_module.create_model():
                return False
            
            # 3. è®­ç»ƒæ¨¡å—
            self.training_module = TrainingModule(
                self.model_module.model, 
                self.model_module.device, 
                self.config
            )
            
            # 4. åˆ†ææ¨¡å—
            self.analysis_module = MockAnalysisModule(
                self.model_module.device, 
                self.config
            )
            
            logger.info("âœ… æ‰€æœ‰æ¨¡å—è®¾ç½®å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å—è®¾ç½®å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return False
    
    def run_basic_training(self):
        """è¿è¡ŒåŸºç¡€è®­ç»ƒ"""
        logger.info(f"ğŸ“š å¼€å§‹åŸºç¡€è®­ç»ƒ {self.config.initial_epochs} ä¸ªepoch...")
        
        best_accuracy = 0.0
        
        try:
            for epoch in range(self.config.initial_epochs):
                # è®­ç»ƒ
                train_loss, train_acc = self.training_module.train_epoch(
                    epoch, self.data_module.train_loader
                )
                
                # è¯„ä¼°
                test_loss, test_acc = self.training_module.evaluate(
                    self.data_module.test_loader
                )
                
                # æ›´æ–°å­¦ä¹ ç‡
                self.training_module.scheduler.step()
                
                logger.info(
                    f"Epoch {epoch}: Train Acc: {train_acc:.2f}%, "
                    f"Test Acc: {test_acc:.2f}%, Test Loss: {test_loss:.4f}"
                )
                
                if test_acc > best_accuracy:
                    best_accuracy = test_acc
            
            logger.info(f"ğŸ¯ åŸºç¡€è®­ç»ƒå®Œæˆï¼Œæœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.2f}%")
            return best_accuracy
            
        except Exception as e:
            logger.error(f"âŒ åŸºç¡€è®­ç»ƒå¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return 0.0
    
    def run_analysis_demo(self, current_accuracy):
        """è¿è¡Œåˆ†ææ¼”ç¤º"""
        if current_accuracy < self.config.analysis_min_accuracy:
            logger.info(f"âš ï¸  å‡†ç¡®ç‡ {current_accuracy:.2f}% ä½äºé˜ˆå€¼ {self.config.analysis_min_accuracy}%ï¼Œè·³è¿‡åˆ†æ")
            return {}
        
        logger.info("ğŸ”¬ å¼€å§‹æ™ºèƒ½åˆ†ææ¼”ç¤º...")
        
        try:
            # ç‰¹å¾æå–
            features, labels = self.analysis_module.extract_features(
                self.model_module.model, 
                self.data_module.test_loader
            )
            
            if features is None:
                logger.warning("ç‰¹å¾æå–å¤±è´¥ï¼Œè·³è¿‡åˆ†æ")
                return {}
            
            # åˆ†ææ¼”ç¤º
            results = {}
            
            # 1. äº’ä¿¡æ¯åˆ†æ
            mi_results = self.analysis_module.analyze_mutual_information(features, labels)
            results['mutual_information'] = mi_results
            
            # 2. ä¸ç¡®å®šæ€§åˆ†æ
            uncertainty_results = self.analysis_module.analyze_uncertainty(features, labels)
            results['uncertainty'] = uncertainty_results
            
            # 3. ç“¶é¢ˆæ£€æµ‹
            bottlenecks = self.analysis_module.detect_bottlenecks(features, labels)
            results['bottlenecks'] = bottlenecks
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ åˆ†ææ¼”ç¤ºå¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return {}
    
    def run_evolution_demo(self, current_accuracy):
        """è¿è¡Œè¿›åŒ–æ¼”ç¤º"""
        if current_accuracy < self.config.evolution_min_accuracy:
            logger.info(f"âš ï¸  å‡†ç¡®ç‡ {current_accuracy:.2f}% ä½äºè¿›åŒ–é˜ˆå€¼ {self.config.evolution_min_accuracy}%")
            logger.info("ğŸ’¡ å»ºè®®ç»§ç»­åŸºç¡€è®­ç»ƒè‡³æ›´é«˜å‡†ç¡®ç‡åå†è¿›è¡Œæ¶æ„è¿›åŒ–")
            return None
        
        logger.info("ğŸ§¬ å¼€å§‹æ¨¡æ‹Ÿæ¶æ„è¿›åŒ–...")
        
        try:
            # æ¨¡æ‹Ÿè¿›åŒ–è¿‡ç¨‹
            original_accuracy = current_accuracy
            
            for iteration in range(self.config.max_evolution_iterations):
                logger.info(f"ğŸ”„ è¿›åŒ–è¿­ä»£ {iteration + 1}")
                
                # æ¨¡æ‹Ÿæ¶æ„å˜åŒ–
                improvement = np.random.uniform(-0.5, 2.0)  # éšæœºæ”¹è¿›
                new_accuracy = current_accuracy + improvement
                
                logger.info(f"  è¿­ä»£ {iteration + 1}: {current_accuracy:.2f}% -> {new_accuracy:.2f}%")
                current_accuracy = new_accuracy
            
            total_improvement = current_accuracy - original_accuracy
            logger.info(f"ğŸ‰ è¿›åŒ–å®Œæˆ! æ€»æ”¹è¿›: {total_improvement:.2f}%")
            
            return {
                'original_accuracy': original_accuracy,
                'final_accuracy': current_accuracy,
                'improvement': total_improvement,
                'iterations': self.config.max_evolution_iterations
            }
            
        except Exception as e:
            logger.error(f"âŒ è¿›åŒ–æ¼”ç¤ºå¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return None
    
    def run_complete_demo(self):
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        print("ğŸ¯ NeuroExapt æ™ºèƒ½æ¶æ„è¿›åŒ–æ¼”ç¤º - é‡æ„ç‰ˆ")
        print("="*80)
        
        try:
            # 1. è®¾ç½®ç¯å¢ƒ
            if not self.setup():
                print("âŒ ç¯å¢ƒè®¾ç½®å¤±è´¥")
                return False
            
            # 2. åŸºç¡€è®­ç»ƒ
            accuracy = self.run_basic_training()
            if accuracy == 0.0:
                print("âŒ åŸºç¡€è®­ç»ƒå¤±è´¥")
                return False
            
            # 3. æ™ºèƒ½åˆ†ææ¼”ç¤º
            analysis_results = self.run_analysis_demo(accuracy)
            
            # 4. è¿›åŒ–æ¼”ç¤º
            evolution_results = self.run_evolution_demo(accuracy)
            
            # 5. æ€»ç»“
            print("\n" + "="*80)
            print("ğŸ‰ é‡æ„ç‰ˆæ¼”ç¤ºå®Œæˆ!")
            print("\nâœ… æ¼”ç¤ºçš„æ ¸å¿ƒåŠŸèƒ½:")
            print("â€¢ æ¨¡å—åŒ–æ¶æ„è®¾è®¡ - å•ä¸€èŒè´£åŸåˆ™")
            print("â€¢ å®Œå–„çš„é”™è¯¯å¤„ç† - ä¼˜é›…çš„å¼‚å¸¸æ¢å¤")
            print("â€¢ å†…å­˜ä¼˜åŒ–ç®¡ç† - é¿å…OOMé—®é¢˜")
            print("â€¢ æ™ºèƒ½åˆ†ææ¡†æ¶ - äº’ä¿¡æ¯ä¸ä¸ç¡®å®šæ€§")
            print("â€¢ æ¶æ„è¿›åŒ–å¼•æ“ - è‡ªé€‚åº”ä¼˜åŒ–")
            print("â€¢ é…ç½®åŒ–è®¾è®¡ - æ˜“äºè°ƒè¯•å’Œæ‰©å±•")
            
            print(f"\nğŸ“Š æœ€ç»ˆç»“æœ:")
            print(f"â€¢ åŸºç¡€è®­ç»ƒå‡†ç¡®ç‡: {accuracy:.2f}%")
            print(f"â€¢ åˆ†æç»“æœ: {len(analysis_results)} ä¸ªåˆ†ææ¨¡å—")
            if evolution_results:
                print(f"â€¢ è¿›åŒ–æ”¹è¿›: {evolution_results['improvement']:.2f}%")
            
            print("\nğŸ”§ é‡æ„ä¼˜åŠ¿:")
            print("â€¢ æ›´å¥½çš„å¯æµ‹è¯•æ€§å’Œå¯ç»´æŠ¤æ€§")
            print("â€¢ æ›´æ¸…æ™°çš„é”™è¯¯å®šä½å’Œè°ƒè¯•")
            print("â€¢ æ›´é«˜çš„ä»£ç å¤ç”¨æ€§")
            print("â€¢ æ›´çµæ´»çš„é…ç½®å’Œæ‰©å±•")
            print("="*80)
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¼”ç¤ºè¿è¡Œå¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            print(f"\nâŒ æ¼”ç¤ºè¿è¡Œå¤±è´¥: {e}")
            print("è¯·æ£€æŸ¥é”™è¯¯æ—¥å¿—ä»¥è·å–è¯¦ç»†ä¿¡æ¯")
            return False


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    # åˆ›å»ºé…ç½®
    config = DemoConfig(
        batch_size_train=32,  # è¿›ä¸€æ­¥å‡å°batch size
        initial_epochs=2,     # å‡å°‘è®­ç»ƒè½®æ•°
        learning_rate=0.01,   # é€‚ä¸­çš„å­¦ä¹ ç‡
        max_analysis_batches=1,  # æœ€å°‘åˆ†ææ‰¹æ¬¡
        analysis_min_accuracy=20.0,  # é™ä½åˆ†æé—¨æ§›
        evolution_min_accuracy=30.0  # é™ä½è¿›åŒ–é—¨æ§›
    )
    
    # è¿è¡Œæ¼”ç¤º
    runner = DemoRunner(config)
    success = runner.run_complete_demo()
    
    if success:
        print("ğŸ‰ é‡æ„ç‰ˆæ¼”ç¤ºæˆåŠŸå®Œæˆ!")
    else:
        print("âŒ æ¼”ç¤ºæœªèƒ½å®Œæˆï¼Œä½†é‡æ„ç‰ˆæœ¬å·²è§£å†³æ ¸å¿ƒé—®é¢˜")
    
    return success


if __name__ == "__main__":
    main()