#!/usr/bin/env python3
"""
å¼ºåŒ–ç‰ˆASO-SEåˆ†ç±»è®­ç»ƒè„šæœ¬ - çœŸåˆ€çœŸæªç‰ˆ

è¿™ä¸ªè„šæœ¬ä¸“é—¨è®¾è®¡æ¥å±•ç¤ºASO-SEæ¡†æ¶ç›¸æ¯”å›ºå®šæ¶æ„çš„å‡†ç¡®ç‡çªç ´èƒ½åŠ›ï¼ŒåŒ…å«ï¼š

ğŸ”¥ æ ¸å¿ƒç‰¹æ€§ï¼š
1. å¼ºå¤§çš„æ–­ç‚¹ç»­ç»ƒç³»ç»Ÿ - åƒä¸‹è½½è½¯ä»¶çš„æ–­ç‚¹ç»­ä¼ 
2. æ™ºèƒ½é”™è¯¯æ¢å¤ - ä»ä»»ä½•å¤±è´¥ä¸­é‡æ–°ç«™èµ·æ¥
3. è®¾å¤‡ä¸€è‡´æ€§ç®¡ç† - å……åˆ†åˆ©ç”¨GPUè®¡ç®—æ½œåŠ›
4. æ¶æ„æ¼”è¿›è¿½è¸ª - è®°å½•æ¯æ¬¡çªå˜å’Œæ”¹è¿›
5. æ€§èƒ½ç›‘æ§ - å®æ—¶è·Ÿè¸ªå‡†ç¡®ç‡çªç ´

ğŸ¯ ç›®æ ‡ï¼šåœ¨CIFAR-10ä¸Šçªç ´ä¼ ç»Ÿå›ºå®šæ¶æ„çš„å‡†ç¡®ç‡ç“¶é¢ˆï¼

ä½¿ç”¨æ–¹æ³•ï¼š
python examples/aso_se_classification_robust.py --epochs 100 --resume_from latest
"""

import argparse
import os
import sys
import time
import logging
from datetime import datetime
import json
import signal
import traceback

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, random_split

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from neuroexapt.core import (
    Network, ASOSEFramework, ASOSEConfig, StabilityMonitor,
    DeviceManager, get_device_manager, CheckpointManager, get_checkpoint_manager
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(f'robust_aso_se_training_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    ]
)
logger = logging.getLogger(__name__)

class RobustASOSETrainer:
    """
    å¼ºåŒ–ç‰ˆASO-SEè®­ç»ƒå™¨
    
    å…·å¤‡å¼ºå¤§çš„é”™è¯¯æ¢å¤å’Œæ–­ç‚¹ç»­ç»ƒèƒ½åŠ›
    """
    
    def __init__(self, config: ASOSEConfig, experiment_name: str = "robust_aso_se_cifar10"):
        """
        Args:
            config: ASO-SEé…ç½®
            experiment_name: å®éªŒåç§°
        """
        self.config = config
        self.experiment_name = experiment_name
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.device_manager = get_device_manager(config.device)
        self.checkpoint_manager = get_checkpoint_manager(
            checkpoint_dir="./robust_checkpoints",
            experiment_name=experiment_name
        )
        
        # æ¨¡å‹å’Œæ¡†æ¶ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self.search_model = None
        self.framework = None
        self.stability_monitor = None
        self.weight_optimizer = None
        self.arch_optimizer = None
        self.weight_scheduler = None
        self.arch_scheduler = None
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_accuracy = 0.0
        self.training_stats = {
            "epoch_stats": [],
            "architecture_evolution": [],
            "performance_breakthroughs": [],
            "error_recoveries": 0,
            "mutation_count": 0
        }
        
        # é”™è¯¯æ¢å¤æœºåˆ¶
        self.max_recovery_attempts = 3
        self.recovery_count = 0
        
        # ä¿¡å·å¤„ç†ï¼ˆç”¨äºä¼˜é›…ä¸­æ–­ï¼‰
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        logger.info(f"ğŸš€ Robust ASO-SE Trainer initialized")
        logger.info(f"ğŸ”§ Device: {self.device_manager.device}")
        logger.info(f"ğŸ“ Checkpoint Manager: {experiment_name}")
    
    def _signal_handler(self, signum, frame):
        """å¤„ç†ä¸­æ–­ä¿¡å·ï¼Œä¼˜é›…ä¿å­˜æ£€æŸ¥ç‚¹"""
        logger.warning(f"âš ï¸ Received signal {signum}, saving checkpoint...")
        if self.current_epoch > 0:
            self._emergency_save_checkpoint()
        logger.info("ğŸ’¾ Emergency checkpoint saved, exiting...")
        sys.exit(0)
    
    def setup_data_loaders(self, batch_size: int = 128, num_workers: int = 2) -> tuple:
        """è®¾ç½®CIFAR-10æ•°æ®åŠ è½½å™¨"""
        logger.info("ğŸ“Š Setting up CIFAR-10 dataset...")
        
        # æ•°æ®å¢å¼º
        transform_train = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])
        
        transform_test = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])
        
        # åŠ è½½æ•°æ®é›†
        train_dataset = torchvision.datasets.CIFAR10(
            root='./data', train=True, download=True, transform=transform_train
        )
        test_dataset = torchvision.datasets.CIFAR10(
            root='./data', train=False, download=False, transform=transform_test
        )
        
        # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
        train_size = int(0.9 * len(train_dataset))  # 90%ç”¨äºè®­ç»ƒ
        valid_size = len(train_dataset) - train_size
        train_subset, valid_subset = random_split(
            train_dataset, [train_size, valid_size],
            generator=torch.Generator().manual_seed(42)
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_subset, batch_size=batch_size, shuffle=True,
            num_workers=num_workers, pin_memory=True, drop_last=True,
            persistent_workers=True if num_workers > 0 else False
        )
        
        valid_loader = DataLoader(
            valid_subset, batch_size=batch_size, shuffle=False,
            num_workers=num_workers, pin_memory=True,
            persistent_workers=True if num_workers > 0 else False
        )
        
        test_loader = DataLoader(
            test_dataset, batch_size=batch_size, shuffle=False,
            num_workers=num_workers, pin_memory=True,
            persistent_workers=True if num_workers > 0 else False
        )
        
        # åŒ…è£…ä¸ºè®¾å¤‡è‡ªåŠ¨è½¬ç§»çš„åŠ è½½å™¨
        train_loader = self.device_manager.create_data_loader_wrapper(train_loader)
        valid_loader = self.device_manager.create_data_loader_wrapper(valid_loader)
        test_loader = self.device_manager.create_data_loader_wrapper(test_loader)
        
        logger.info(f"âœ… Data loaded: {len(train_subset)} train, {len(valid_subset)} valid, {len(test_dataset)} test")
        
        return train_loader, valid_loader, test_loader
    
    def initialize_model_and_framework(self, init_channels: int = 16, layers: int = 8):
        """åˆå§‹åŒ–æ¨¡å‹å’ŒASO-SEæ¡†æ¶"""
        logger.info("ğŸ—ï¸ Initializing model and ASO-SE framework...")
        
        try:
            # åˆ›å»ºæœç´¢æ¨¡å‹
            self.search_model = self.device_manager.safe_model_creation(
                Network, 
                C=init_channels,
                num_classes=10,
                layers=layers,
                quiet=False
            )
            
            # æ³¨å†Œæ¨¡å‹åˆ°è®¾å¤‡ç®¡ç†å™¨
            self.search_model = self.device_manager.register_model("search_model", self.search_model)
            
            # åˆ›å»ºASO-SEæ¡†æ¶
            self.framework = ASOSEFramework(self.search_model, self.config)
            self.framework.initialize_optimizers()
            
            # è·å–ä¼˜åŒ–å™¨å¼•ç”¨
            self.weight_optimizer = self.framework.weight_optimizer
            self.arch_optimizer = self.framework.arch_optimizer
            
            # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
            if self.weight_optimizer:
                self.weight_scheduler = optim.lr_scheduler.CosineAnnealingLR(
                    self.weight_optimizer, T_max=200, eta_min=0.001
                )
            
            if self.arch_optimizer:
                self.arch_scheduler = optim.lr_scheduler.ExponentialLR(
                    self.arch_optimizer, gamma=0.99
                )
            
            # åˆ›å»ºç¨³å®šæ€§ç›‘æ§å™¨
            self.stability_monitor = StabilityMonitor({
                'oscillation_window': 10,
                'oscillation_threshold': 0.1,
                'convergence_patience': 15,
                'degradation_threshold': 0.02
            })
            
            logger.info("âœ… Model and framework initialized successfully")
            
            # è®°å½•æ¨¡å‹ä¿¡æ¯
            total_params = sum(p.numel() for p in self.search_model.parameters())
            arch_params = sum(p.numel() for p in self.search_model.arch_parameters())
            logger.info(f"ğŸ“Š Model info: {total_params:,} total params, {arch_params:,} arch params")
            
        except Exception as e:
            logger.error(f"âŒ Failed to initialize model: {e}")
            raise e
    
    def resume_or_start_training(self, resume_from: str = "latest") -> int:
        """æ¢å¤è®­ç»ƒæˆ–å¼€å§‹æ–°è®­ç»ƒ"""
        if resume_from == "none":
            logger.info("ğŸ†• Starting fresh training (no resume)")
            return 0
        
        try:
            # å‡†å¤‡ä¼˜åŒ–å™¨å­—å…¸
            optimizers = {}
            if self.weight_optimizer:
                optimizers['weight'] = self.weight_optimizer
            if self.arch_optimizer:
                optimizers['arch'] = self.arch_optimizer
            
            # å‡†å¤‡è°ƒåº¦å™¨å­—å…¸
            schedulers = {}
            if self.weight_scheduler:
                schedulers['weight'] = self.weight_scheduler
            if self.arch_scheduler:
                schedulers['arch'] = self.arch_scheduler
            
            # å°è¯•æ¢å¤è®­ç»ƒ
            resume_epoch, training_stats = self.checkpoint_manager.resume_training(
                model=self.search_model,
                optimizers=optimizers,
                schedulers=schedulers,
                framework=self.framework,
                preferred_checkpoint=resume_from
            )
            
            if resume_epoch > 0:
                self.current_epoch = resume_epoch
                self.training_stats.update(training_stats)
                logger.info(f"ğŸ¯ Successfully resumed from epoch {resume_epoch}")
            
            return resume_epoch
            
        except Exception as e:
            logger.warning(f"âš ï¸ Resume failed: {e}, starting fresh training")
            return 0
    
    def train_single_epoch(self, epoch: int, train_loader, valid_loader, criterion) -> dict:
        """è®­ç»ƒå•ä¸ªepoch"""
        epoch_start_time = time.time()
        
        try:
            # æ‰§è¡ŒASO-SEè®­ç»ƒå‘¨æœŸ
            stats = self.framework.train_cycle(train_loader, valid_loader, criterion, epoch)
            
            # æ·»åŠ epochä¿¡æ¯
            stats['epoch'] = epoch
            stats['epoch_time'] = time.time() - epoch_start_time
            
            # æ›´æ–°ç¨³å®šæ€§ç›‘æ§
            if self.stability_monitor:
                stability_metrics = {
                    'loss': stats.get('valid_loss', 0.0),
                    'accuracy': stats.get('valid_accuracy', 0.0)
                }
                self.stability_monitor.update(stability_metrics, epoch)
            
            # æ£€æŸ¥æ€§èƒ½çªç ´
            current_acc = stats.get('valid_accuracy', 0.0)
            if isinstance(current_acc, (int, float)) and current_acc > self.best_accuracy + 1.0:  # 1%ä»¥ä¸Šçš„æå‡æ‰ç®—çªç ´
                breakthrough = {
                    'epoch': epoch,
                    'previous_best': self.best_accuracy,
                    'new_best': current_acc,
                    'improvement': current_acc - self.best_accuracy,
                    'phase': stats.get('phase', 'unknown')
                }
                self.training_stats['performance_breakthroughs'].append(breakthrough)
                logger.info(f"ğŸš€ Performance breakthrough! {self.best_accuracy:.2f}% â†’ {current_acc:.2f}%")
            
            # æ›´æ–°æœ€ä½³æ€§èƒ½
            if isinstance(current_acc, (int, float)) and current_acc > self.best_accuracy:
                self.best_accuracy = current_acc
            
            # è®°å½•æ¶æ„æ¼”è¿›
            if stats.get('phase') == 'mutation':
                mutation_info = {
                    'epoch': epoch,
                    'genotype': str(self.framework.current_genotype) if hasattr(self.framework, 'current_genotype') else None,
                    'performance_before': stats.get('pre_mutation_accuracy', 0.0),
                    'performance_after': stats.get('post_mutation_accuracy', 0.0)
                }
                self.training_stats['architecture_evolution'].append(mutation_info)
                self.training_stats['mutation_count'] += 1
                logger.info(f"ğŸ§¬ Architecture evolution #{self.training_stats['mutation_count']}")
            
            # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
            if self.weight_scheduler and stats.get('phase') in ['warmup', 'weight_retraining']:
                self.weight_scheduler.step()
            
            if self.arch_scheduler and stats.get('phase') == 'arch_training':
                self.arch_scheduler.step()
            
            return stats
            
        except Exception as e:
            logger.error(f"âŒ Training epoch {epoch} failed: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            
            # å°è¯•é”™è¯¯æ¢å¤
            if self.recovery_count < self.max_recovery_attempts:
                self.recovery_count += 1
                self.training_stats['error_recoveries'] += 1
                logger.warning(f"ğŸ”§ Attempting error recovery {self.recovery_count}/{self.max_recovery_attempts}")
                
                # å†…å­˜æ¸…ç†
                self.device_manager.optimize_memory()
                
                # é™ä½æ‰¹å¤§å°ï¼ˆå¦‚æœéœ€è¦ï¼‰
                # è¿™é‡Œå¯ä»¥å®ç°åŠ¨æ€æ‰¹å¤§å°è°ƒæ•´
                
                return {
                    'epoch': epoch,
                    'error': str(e),
                    'phase': 'recovery',
                    'valid_accuracy': 0.0,
                    'train_accuracy': 0.0
                }
            else:
                logger.error(f"âŒ Maximum recovery attempts reached, stopping training")
                raise e
    
    def save_checkpoint_if_needed(self, epoch: int, stats: dict):
        """æ ¹æ®éœ€è¦ä¿å­˜æ£€æŸ¥ç‚¹"""
        should_save = (
            self.checkpoint_manager.should_auto_save(epoch) or
            epoch % 10 == 0 or  # æ¯10ä¸ªepochå¼ºåˆ¶ä¿å­˜
            stats.get('phase') == 'mutation' or  # çªå˜åä¿å­˜
            stats.get('valid_accuracy', 0.0) > self.best_accuracy  # æ€§èƒ½æå‡æ—¶ä¿å­˜
        )
        
        if should_save:
            try:
                # å‡†å¤‡ä¿å­˜æ•°æ®
                model_state = self.search_model.state_dict()
                
                optimizer_states = {}
                if self.weight_optimizer:
                    optimizer_states['weight'] = self.weight_optimizer.state_dict()
                if self.arch_optimizer:
                    optimizer_states['arch'] = self.arch_optimizer.state_dict()
                
                scheduler_states = {}
                if self.weight_scheduler:
                    scheduler_states['weight'] = self.weight_scheduler.state_dict()
                if self.arch_scheduler:
                    scheduler_states['arch'] = self.arch_scheduler.state_dict()
                
                framework_state = {
                    'current_cycle': getattr(self.framework, 'current_cycle', 0),
                    'current_phase': getattr(self.framework, 'current_phase', 'warmup'),
                    'current_genotype': str(getattr(self.framework, 'current_genotype', None)),
                    'best_performance': self.best_accuracy,
                    'training_history': getattr(self.framework, 'training_history', {})
                }
                
                architecture_info = {
                    'mutation_count': self.training_stats['mutation_count'],
                    'current_genotype': str(getattr(self.framework, 'current_genotype', None)),
                    'phase': stats.get('phase', 'unknown')
                }
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                checkpoint_path = self.checkpoint_manager.save_checkpoint(
                    epoch=epoch,
                    model_state=model_state,
                    optimizer_states=optimizer_states,
                    scheduler_states=scheduler_states,
                    training_stats=self.training_stats,
                    framework_state=framework_state,
                    performance_metric=stats.get('valid_accuracy', 0.0),
                    architecture_info=architecture_info
                )
                
                logger.info(f"ğŸ’¾ Checkpoint saved: {checkpoint_path}")
                
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to save checkpoint: {e}")
    
    def _emergency_save_checkpoint(self):
        """ç´§æ€¥ä¿å­˜æ£€æŸ¥ç‚¹"""
        try:
            if self.search_model and self.current_epoch > 0:
                emergency_stats = {
                    'valid_accuracy': self.best_accuracy,
                    'emergency_save': True,
                    'timestamp': datetime.now().isoformat()
                }
                self.save_checkpoint_if_needed(self.current_epoch, emergency_stats)
        except Exception as e:
            logger.error(f"âŒ Emergency checkpoint save failed: {e}")
    
    def evaluate_on_test(self, test_loader, criterion) -> float:
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ç»ˆæ€§èƒ½"""
        logger.info("ğŸ§ª Evaluating on test set...")
        
        model = self.framework.evolvable_model if (
            hasattr(self.framework, 'evolvable_model') and self.framework.evolvable_model
        ) else self.search_model
        
        model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in test_loader:
                # æ•°æ®å·²é€šè¿‡è®¾å¤‡ç®¡ç†å™¨è‡ªåŠ¨è½¬ç§»
                output = model(data)
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)
        
        test_accuracy = 100.0 * correct / total
        logger.info(f"ğŸ¯ Final test accuracy: {test_accuracy:.2f}%")
        
        return test_accuracy
    
    def train(self, epochs: int = 100, batch_size: int = 128, 
              init_channels: int = 16, layers: int = 8, resume_from: str = "latest"):
        """æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        logger.info(f"ğŸš€ Starting robust ASO-SE training for {epochs} epochs")
        logger.info(f"ğŸ“Š Configuration: epochs={epochs}, batch_size={batch_size}, channels={init_channels}, layers={layers}")
        
        start_time = time.time()
        
        try:
            # è®¾ç½®æ•°æ®åŠ è½½å™¨
            train_loader, valid_loader, test_loader = self.setup_data_loaders(batch_size)
            
            # åˆå§‹åŒ–æ¨¡å‹å’Œæ¡†æ¶
            self.initialize_model_and_framework(init_channels, layers)
            
            # æ¢å¤æˆ–å¼€å§‹è®­ç»ƒ
            start_epoch = self.resume_or_start_training(resume_from)
            
            # æŸå¤±å‡½æ•°
            criterion = nn.CrossEntropyLoss().to(self.device_manager.device)
            
            logger.info(f"ğŸ¬ Training started from epoch {start_epoch}")
            
            # è®­ç»ƒä¸»å¾ªç¯
            for epoch in range(start_epoch, epochs):
                self.current_epoch = epoch
                
                logger.info(f"\n{'='*80}")
                logger.info(f"Epoch {epoch}/{epochs-1} - Phase: {getattr(self.framework, 'current_phase', 'unknown')}")
                logger.info(f"{'='*80}")
                
                # è®­ç»ƒå•ä¸ªepoch
                stats = self.train_single_epoch(epoch, train_loader, valid_loader, criterion)
                
                # è®°å½•ç»Ÿè®¡ä¿¡æ¯
                self.training_stats['epoch_stats'].append(stats)
                
                # è¾“å‡ºè¿›åº¦
                self._log_epoch_progress(epoch, stats)
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                self.save_checkpoint_if_needed(epoch, stats)
                
                # å†…å­˜ç®¡ç†
                if epoch % 5 == 0:
                    self.device_manager.optimize_memory()
                
                # é‡ç½®æ¢å¤è®¡æ•°å™¨ï¼ˆæˆåŠŸå®Œæˆepochåï¼‰
                if 'error' not in stats:
                    self.recovery_count = 0
                
                # æ£€æŸ¥æ—©åœ
                if hasattr(self.framework, 'should_early_stop') and self.framework.should_early_stop():
                    logger.info(f"â¹ï¸ Early stopping at epoch {epoch}")
                    break
            
            # è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆè¯„ä¼°
            total_time = time.time() - start_time
            test_accuracy = self.evaluate_on_test(test_loader, criterion)
            
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            self._generate_final_report(total_time, test_accuracy)
            
            logger.info("ğŸ‰ Training completed successfully!")
            
        except KeyboardInterrupt:
            logger.info("â¹ï¸ Training interrupted by user")
            self._emergency_save_checkpoint()
            
        except Exception as e:
            logger.error(f"âŒ Training failed: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            self._emergency_save_checkpoint()
            raise e
    
    def _log_epoch_progress(self, epoch: int, stats: dict):
        """è®°å½•epochè¿›åº¦"""
        phase = stats.get('phase', 'unknown')
        train_acc = stats.get('train_accuracy', 0.0)
        valid_acc = stats.get('valid_accuracy', 0.0)
        train_loss = stats.get('train_loss', 0.0)
        valid_loss = stats.get('valid_loss', 0.0)
        epoch_time = stats.get('epoch_time', 0.0)
        
        # å†…å­˜ä¿¡æ¯
        memory_stats = self.device_manager.get_memory_stats()
        memory_info = ""
        if memory_stats.get('allocated_mb'):
            memory_info = f", GPU: {memory_stats['allocated_mb']:.0f}MB"
        
        # æ€§èƒ½æå‡ä¿¡æ¯
        improvement_info = ""
        if valid_acc > self.best_accuracy:
            improvement_info = f" ğŸš€(+{valid_acc - self.best_accuracy:.2f}%)"
        
        logger.info(f"Epoch {epoch:3d} | {phase:15s} | "
                   f"Train: {train_loss:.4f}/{train_acc:.2f}% | "
                   f"Valid: {valid_loss:.4f}/{valid_acc:.2f}%{improvement_info} | "
                   f"Time: {epoch_time:.1f}s{memory_info}")
        
        # æ¶æ„ä¿¡æ¯
        if phase == 'mutation':
            mutation_count = self.training_stats['mutation_count']
            logger.info(f"ğŸ§¬ Architecture mutation #{mutation_count} completed")
        
        # ç¨³å®šæ€§è­¦å‘Š
        if self.stability_monitor:
            try:
                report = self.stability_monitor.get_comprehensive_report()
                health_score = report.get('current_health_score', 1.0)
                if health_score < 0.7:
                    logger.warning(f"âš ï¸ Training instability detected (health: {health_score:.3f})")
            except:
                pass
    
    def _generate_final_report(self, total_time: float, test_accuracy: float):
        """ç”Ÿæˆæœ€ç»ˆè®­ç»ƒæŠ¥å‘Š"""
        # åˆ›å»ºè®­ç»ƒæŠ¥å‘Š
        report_path = self.checkpoint_manager.create_training_report()
        
        # æ±‡æ€»ä¿¡æ¯
        breakthrough_count = len(self.training_stats['performance_breakthroughs'])
        mutation_count = self.training_stats['mutation_count']
        recovery_count = self.training_stats['error_recoveries']
        
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸ¯ FINAL RESULTS")
        logger.info(f"{'='*80}")
        logger.info(f"   Best Validation Accuracy: {self.best_accuracy:.2f}%")
        logger.info(f"   Final Test Accuracy: {test_accuracy:.2f}%")
        logger.info(f"   Total Training Time: {total_time/3600:.2f} hours")
        logger.info(f"   Performance Breakthroughs: {breakthrough_count}")
        logger.info(f"   Architecture Mutations: {mutation_count}")
        logger.info(f"   Error Recoveries: {recovery_count}")
        logger.info(f"   Training Report: {report_path}")
        logger.info(f"{'='*80}")
        
        # ä¿å­˜è¯¦ç»†ç»Ÿè®¡
        final_stats = {
            'experiment_name': self.experiment_name,
            'best_validation_accuracy': self.best_accuracy,
            'final_test_accuracy': test_accuracy,
            'total_training_time_hours': total_time / 3600,
            'performance_breakthroughs': self.training_stats['performance_breakthroughs'],
            'architecture_evolution': self.training_stats['architecture_evolution'],
            'mutation_count': mutation_count,
            'error_recoveries': recovery_count,
            'final_genotype': str(getattr(self.framework, 'current_genotype', None)),
            'device_info': self.device_manager.get_device_report()
        }
        
        stats_path = f"./robust_checkpoints/{self.experiment_name}/final_results.json"
        os.makedirs(os.path.dirname(stats_path), exist_ok=True)
        with open(stats_path, 'w') as f:
            json.dump(final_stats, f, indent=2, default=str)
        
        logger.info(f"ğŸ“Š Final statistics saved: {stats_path}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Robust ASO-SE Classification Training')
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--epochs', type=int, default=100, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=128, help='æ‰¹å¤§å°')
    parser.add_argument('--init_channels', type=int, default=16, help='åˆå§‹é€šé“æ•°')
    parser.add_argument('--layers', type=int, default=8, help='ç½‘ç»œå±‚æ•°')
    
    # ASO-SEå‚æ•°
    parser.add_argument('--warmup_epochs', type=int, default=10, help='é¢„çƒ­è½®æ•°')
    parser.add_argument('--arch_training_epochs', type=int, default=3, help='æ¶æ„è®­ç»ƒè½®æ•°')
    parser.add_argument('--weight_training_epochs', type=int, default=8, help='æƒé‡è®­ç»ƒè½®æ•°')
    parser.add_argument('--mutation_strength', type=float, default=0.3, help='çªå˜å¼ºåº¦')
    
    # ç»­ç»ƒå‚æ•°
    parser.add_argument('--resume_from', type=str, default='latest', 
                       choices=['latest', 'best', 'none'], help='ä»å“ªä¸ªæ£€æŸ¥ç‚¹æ¢å¤')
    parser.add_argument('--experiment_name', type=str, default='robust_aso_se_cifar10',
                       help='å®éªŒåç§°')
    
    # è®¾å¤‡å‚æ•°
    parser.add_argument('--device', type=str, default=None, help='è®¾å¤‡ (cuda/cpu)')
    parser.add_argument('--num_workers', type=int, default=2, help='æ•°æ®åŠ è½½è¿›ç¨‹æ•°')
    
    args = parser.parse_args()
    
    # åˆ›å»ºASO-SEé…ç½®
    config = ASOSEConfig(
        warmup_epochs=args.warmup_epochs,
        arch_training_epochs=args.arch_training_epochs,
        weight_training_epochs=args.weight_training_epochs,
        mutation_strength=args.mutation_strength,
        device=args.device,
        save_checkpoints=True,
        checkpoint_frequency=5
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = RobustASOSETrainer(config, args.experiment_name)
    
    # å¼€å§‹è®­ç»ƒ
    logger.info("ğŸš€ Starting Robust ASO-SE Training - Ready to breakthrough accuracy limits!")
    
    try:
        trainer.train(
            epochs=args.epochs,
            batch_size=args.batch_size,
            init_channels=args.init_channels,
            layers=args.layers,
            resume_from=args.resume_from
        )
    except Exception as e:
        logger.error(f"âŒ Training failed with error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main() 