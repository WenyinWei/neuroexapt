#!/usr/bin/env python3
"""
å¢å¼ºæ™ºèƒ½æ¶æ„è¿›åŒ–æ¼”ç¤º - 95%å‡†ç¡®ç‡ç›®æ ‡ç‰ˆæœ¬
Enhanced Intelligent Architecture Evolution Demo - 95% Accuracy Target

ğŸ¯ æ ¸å¿ƒç›®æ ‡ï¼š
1. åœ¨CIFAR-10ä¸Šè¾¾åˆ°95%+çš„å‡†ç¡®ç‡
2. ä½¿ç”¨Monte Carlo Dropoutæ›¿ä»£å¤æ‚çš„è´å¶æ–¯å˜åˆ†æ¨æ–­
3. å®ç°å®Œæ•´çš„è´å¶æ–¯å†³ç­–æ¡†æ¶æ¥åˆ¤æ–­å˜å¼‚ä»·å€¼
4. é›†æˆä½ æå‡ºçš„ç†è®ºæ¡†æ¶ï¼šå°†å˜å¼‚æ”¶ç›Šå»ºæ¨¡ä¸ºéšæœºå˜é‡

ğŸ§¬ æ ¸å¿ƒç†è®ºå®ç°ï¼š
Î”I = Î±Â·Î”I_MI + Î²Â·Î”I_cond + Î³Â·Î”I_uncert - Î´Â·Î”I_cost

åŸºäºæœŸæœ›æ•ˆç”¨æœ€å¤§åŒ–çš„å˜å¼‚å†³ç­–ï¼š
E[U(Î”I)] = E[1 - exp(-Î»Â·Î”I)]

ğŸ”§ æŠ€æœ¯æ ˆï¼š
- Enhanced ResNet with SE-attention
- Monte Carlo Dropout uncertainty estimation
- Bayesian mutation decision framework
- Advanced training techniques (Mixup, CutMix, Label Smoothing)
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import numpy as np
import time
import matplotlib.pyplot as plt
from collections import defaultdict
import logging
import os
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥æ–°çš„ç»„ä»¶
from neuroexapt.core import (
    IntelligentArchitectureEvolutionEngine,
    EvolutionConfig,
    MutualInformationEstimator,
    IntelligentBottleneckDetector,
    IntelligentMutationPlanner,
    AdvancedNet2NetTransfer
)

# å¯¼å…¥æ–°çš„ä¸ç¡®å®šæ€§ä¼°è®¡å™¨å’Œå†³ç­–æ¡†æ¶
from neuroexapt.core.monte_carlo_uncertainty_estimator import MonteCarloUncertaintyEstimator
from neuroexapt.core.bayesian_mutation_decision import (
    BayesianMutationDecision, MutationEvidence, MutationPrior, MutationDecision
)

# å¯¼å…¥å¢å¼ºçš„ResNet
from neuroexapt.models.enhanced_resnet import (
    create_enhanced_model, EnhancedTrainingConfig, get_enhanced_transforms,
    LabelSmoothingCrossEntropy, mixup_data, mixup_criterion
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class EnhancedTrainingModule:
    """
    å¢å¼ºçš„è®­ç»ƒæ¨¡å— - ç›®æ ‡95%å‡†ç¡®ç‡
    
    é›†æˆï¼š
    - å¢å¼ºçš„ResNetæ¶æ„
    - é«˜çº§æ•°æ®å¢å¼ºï¼ˆMixup, CutMix, RandomErasingï¼‰
    - æ ‡ç­¾å¹³æ»‘å’Œè‡ªé€‚åº”å­¦ä¹ ç‡
    - æ··åˆç²¾åº¦è®­ç»ƒ
    """
    
    def __init__(self, device, config: EnhancedTrainingConfig = None):
        self.device = device
        self.config = config or EnhancedTrainingConfig()
        
        # åˆ›å»ºå¢å¼ºæ¨¡å‹
        self.model = create_enhanced_model(self.config).to(device)
        
        # è®¾ç½®æ•°æ®åŠ è½½å™¨
        self.train_loader, self.test_loader = self._setup_data_loaders()
        
        # è®¾ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self.optimizer = None
        self.scheduler = None
        self.criterion = None
        self.setup_training_components()
        
        # è®­ç»ƒå†å²
        self.train_history = []
        self.test_history = []
        
        logger.info(f"ğŸš€ Enhanced training module initialized with {self.config.model_type}")
        logger.info(f"ğŸ“Š Model parameters: {sum(p.numel() for p in self.model.parameters()):,}")
        
    def _setup_data_loaders(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        train_transform, test_transform = get_enhanced_transforms()
        
        # CIFAR-10æ•°æ®é›†
        train_dataset = torchvision.datasets.CIFAR10(
            root='./data', train=True, download=True, transform=train_transform
        )
        test_dataset = torchvision.datasets.CIFAR10(
            root='./data', train=False, download=True, transform=test_transform
        )
        
        train_loader = DataLoader(
            train_dataset, batch_size=self.config.batch_size,
            shuffle=True, num_workers=4, pin_memory=True
        )
        test_loader = DataLoader(
            test_dataset, batch_size=self.config.batch_size,
            shuffle=False, num_workers=4, pin_memory=True
        )
        
        return train_loader, test_loader
        
    def setup_training_components(self):
        """è®¾ç½®è®­ç»ƒç»„ä»¶"""
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.SGD(
            self.model.parameters(),
            lr=self.config.initial_lr,
            momentum=self.config.momentum,
            weight_decay=self.config.weight_decay,
            nesterov=True
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.config.lr_schedule == 'cosine_annealing':
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer, T_max=self.config.epochs, eta_min=1e-6
            )
        else:
            self.scheduler = optim.lr_scheduler.MultiStepLR(
                self.optimizer, milestones=self.config.lr_milestones, 
                gamma=self.config.lr_gamma
            )
        
        # æŸå¤±å‡½æ•°
        if self.config.label_smoothing > 0:
            self.criterion = LabelSmoothingCrossEntropy(self.config.label_smoothing)
        else:
            self.criterion = nn.CrossEntropyLoss()
            
        logger.info("âœ… Training components setup complete")
        
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (data, targets) in enumerate(self.train_loader):
            data, targets = data.to(self.device), targets.to(self.device)
            
            # Mixupæ•°æ®å¢å¼º
            if self.config.use_mixup and np.random.rand() < 0.5:
                mixed_data, targets_a, targets_b, lam = mixup_data(
                    data, targets, self.config.mixup_alpha
                )
                
                self.optimizer.zero_grad()
                outputs = self.model(mixed_data)
                loss = mixup_criterion(self.criterion, outputs, targets_a, targets_b, lam)
            else:
                self.optimizer.zero_grad()
                outputs = self.model(data)
                loss = self.criterion(outputs, targets)
            
            loss.backward()
            self.optimizer.step()
            
            # ç»Ÿè®¡
            running_loss += loss.item()
            if not self.config.use_mixup or np.random.rand() >= 0.5:
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
            
            # æ‰“å°è¿›åº¦
            if batch_idx % 100 == 99:
                current_acc = 100. * correct / total if total > 0 else 0
                logger.info(
                    f"Epoch {epoch}, Batch {batch_idx + 1}, "
                    f"Loss: {running_loss/(batch_idx + 1):.6f}, "
                    f"Acc: {current_acc:.2f}%"
                )
        
        train_acc = 100. * correct / total if total > 0 else 0
        avg_loss = running_loss / len(self.train_loader)
        
        return avg_loss, train_acc
        
    def evaluate(self):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        self.model.eval()
        test_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, targets in self.test_loader:
                data, targets = data.to(self.device), targets.to(self.device)
                outputs = self.model(data)
                test_loss += self.criterion(outputs, targets).item()
                
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
        
        test_loss /= len(self.test_loader)
        test_acc = 100. * correct / total
        
        return test_loss, test_acc
        
    def train_epochs(self, num_epochs):
        """è®­ç»ƒå¤šä¸ªepoch"""
        best_acc = 0.0
        
        for epoch in range(num_epochs):
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(epoch + 1)
            
            # è¯„ä¼°
            test_loss, test_acc = self.evaluate()
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()
            
            # è®°å½•å†å²
            self.train_history.append({'loss': train_loss, 'acc': train_acc})
            self.test_history.append({'loss': test_loss, 'acc': test_acc})
            
            # æ›´æ–°æœ€ä½³å‡†ç¡®ç‡
            if test_acc > best_acc:
                best_acc = test_acc
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save(self.model.state_dict(), 'best_model.pth')
            
            logger.info(
                f"Epoch {epoch + 1}: Train Acc: {train_acc:.2f}%, "
                f"Test Acc: {test_acc:.2f}%, Test Loss: {test_loss:.4f}"
            )
            
            # æ—©æœŸåœæ­¢æ£€æŸ¥
            if test_acc >= 95.0:
                logger.info(f"ğŸ‰ Target accuracy of 95% reached! Current: {test_acc:.2f}%")
                break
                
        return best_acc


class EnhancedEvolutionEngine:
    """
    å¢å¼ºçš„æ¶æ„è¿›åŒ–å¼•æ“
    
    é›†æˆï¼š
    - Monte Carloä¸ç¡®å®šæ€§ä¼°è®¡
    - è´å¶æ–¯å˜å¼‚å†³ç­–æ¡†æ¶
    - åŸºäºæœŸæœ›æ•ˆç”¨çš„å˜å¼‚é€‰æ‹©
    """
    
    def __init__(self, device):
        self.device = device
        
        # åˆ›å»ºç»„ä»¶
        self.mi_estimator = MutualInformationEstimator()
        self.uncertainty_estimator = MonteCarloUncertaintyEstimator(
            n_samples=50, dropout_rate=0.1  # å‡å°‘é‡‡æ ·æ¬¡æ•°ä»¥åŠ å¿«é€Ÿåº¦
        )
        self.bottleneck_detector = IntelligentBottleneckDetector(
            mi_estimator=self.mi_estimator,
            uncertainty_estimator=self.uncertainty_estimator
        )
        self.mutation_planner = IntelligentMutationPlanner()
        self.net2net_transfer = AdvancedNet2NetTransfer()
        
        # è´å¶æ–¯å†³ç­–æ¡†æ¶
        self.decision_engine = BayesianMutationDecision(
            alpha=1.5,    # æé«˜äº’ä¿¡æ¯æƒé‡
            beta=1.0,     # æ¡ä»¶äº’ä¿¡æ¯æƒé‡
            gamma=0.8,    # ä¸ç¡®å®šæ€§æƒé‡
            delta=0.3,    # é™ä½æˆæœ¬æƒé‡ä»¥é¼“åŠ±æ¢ç´¢
            risk_aversion=1.5  # ä¸­ç­‰é£é™©åŒæ¶
        )
        
        logger.info("ğŸ§¬ Enhanced evolution engine initialized")
        
    def detect_bottlenecks_with_decision(self, model, data_loader):
        """
        æ£€æµ‹ç“¶é¢ˆå¹¶ä½¿ç”¨è´å¶æ–¯æ¡†æ¶åšå‡ºå˜å¼‚å†³ç­–
        
        Returns:
            List[Tuple[BottleneckInfo, MutationDecision]]: ç“¶é¢ˆä¿¡æ¯å’Œå¯¹åº”çš„å†³ç­–
        """
        logger.info("ğŸ” å¼€å§‹æ£€æµ‹ç“¶é¢ˆ...")
        
        # æ£€æµ‹ç“¶é¢ˆ
        bottlenecks = self.bottleneck_detector.detect_bottlenecks(
            model, data_loader, self.device
        )
        
        logger.info(f"ğŸ¯ æ£€æµ‹åˆ° {len(bottlenecks)} ä¸ªæ½œåœ¨ç“¶é¢ˆ")
        
        # ä¸ºæ¯ä¸ªç“¶é¢ˆåšå‡ºå˜å¼‚å†³ç­–
        bottleneck_decisions = []
        
        for bottleneck in bottlenecks:
            # æ„å»ºå˜å¼‚è¯æ®
            evidence = MutationEvidence(
                mutual_info_gain=bottleneck.mutual_info,
                cond_mutual_info_gain=bottleneck.conditional_mi,
                uncertainty_reduction=bottleneck.uncertainty,
                transfer_cost=0.05,  # ä¼°è®¡çš„è¿ç§»æˆæœ¬
                bottleneck_severity=bottleneck.severity
            )
            
            # æ ¹æ®è®­ç»ƒé˜¶æ®µè°ƒæ•´é£é™©åŒæ¶
            # è¿™é‡Œå‡è®¾æ˜¯ä¸­æœŸè®­ç»ƒ
            self.decision_engine.adjust_risk_aversion('middle')
            
            # åšå‡ºå†³ç­–
            decision = self.decision_engine.make_decision(
                evidence, utility_threshold=0.01
            )
            
            bottleneck_decisions.append((bottleneck, decision))
            
            logger.info(
                f"ğŸ§  å±‚ {bottleneck.layer_name}: {decision.reasoning}"
            )
            
        return bottleneck_decisions
        
    def execute_intelligent_mutations(self, model, bottleneck_decisions):
        """
        æ‰§è¡Œæ™ºèƒ½å˜å¼‚
        
        Args:
            model: åŸå§‹æ¨¡å‹
            bottleneck_decisions: ç“¶é¢ˆå†³ç­–åˆ—è¡¨
            
        Returns:
            å˜å¼‚åçš„æ¨¡å‹
        """
        current_model = model
        mutation_count = 0
        
        for bottleneck, decision in bottleneck_decisions:
            if decision.should_mutate and decision.confidence > 0.3:
                try:
                    # ç”Ÿæˆå˜å¼‚è®¡åˆ’
                    mutation_plans = self.mutation_planner.plan_mutations(
                        [bottleneck], task_type='vision'
                    )
                    
                    if mutation_plans:
                        mutation_plan = mutation_plans[0]
                        
                        # æ‰§è¡Œå˜å¼‚
                        mutated_model = self.net2net_transfer.apply_mutation(
                            current_model, mutation_plan
                        )
                        
                        if mutated_model is not None:
                            current_model = mutated_model
                            mutation_count += 1
                            
                            logger.info(
                                f"âœ… æˆåŠŸå˜å¼‚å±‚ {bottleneck.layer_name}: "
                                f"{mutation_plan.mutation_type}"
                            )
                        else:
                            logger.warning(
                                f"âŒ å˜å¼‚å¤±è´¥: {bottleneck.layer_name}"
                            )
                            
                except Exception as e:
                    logger.error(f"å˜å¼‚é”™è¯¯: {bottleneck.layer_name}: {e}")
                    
        logger.info(f"ğŸ”„ æ€»å…±æ‰§è¡Œäº† {mutation_count} ä¸ªå˜å¼‚")
        return current_model


def enhanced_intelligent_evolution_demo():
    """
    å¢å¼ºæ™ºèƒ½æ¶æ„è¿›åŒ–æ¼”ç¤ºä¸»å‡½æ•°
    
    ç›®æ ‡ï¼šåœ¨CIFAR-10ä¸Šè¾¾åˆ°95%å‡†ç¡®ç‡
    """
    print("="*60)
    print("ğŸš€ å¢å¼ºæ™ºèƒ½æ¶æ„è¿›åŒ–æ¼”ç¤º")
    print("ğŸ¯ ç›®æ ‡ï¼šCIFAR-10ä¸Š95%å‡†ç¡®ç‡")
    print("ğŸ§¬ ç†è®ºï¼šè´å¶æ–¯å˜å¼‚å†³ç­– + Monte Carloä¸ç¡®å®šæ€§")
    print("="*60)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºå¢å¼ºè®­ç»ƒé…ç½®
    config = EnhancedTrainingConfig()
    config.epochs = 50  # å‡å°‘epochæ•°ç”¨äºæ¼”ç¤º
    config.batch_size = 128
    
    # åˆå§‹åŒ–è®­ç»ƒæ¨¡å—
    print("\nğŸ“š åˆå§‹åŒ–å¢å¼ºè®­ç»ƒæ¨¡å—...")
    trainer = EnhancedTrainingModule(device, config)
    
    # åŸºç¡€è®­ç»ƒ
    print("\nğŸ“š å¼€å§‹åŸºç¡€è®­ç»ƒ...")
    initial_epochs = 15
    best_acc = trainer.train_epochs(initial_epochs)
    
    print(f"\nğŸ¯ åŸºç¡€è®­ç»ƒå®Œæˆï¼Œæœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}%")
    
    # å¦‚æœå·²ç»è¾¾åˆ°95%ï¼Œç›´æ¥è¿”å›
    if best_acc >= 95.0:
        print("\nğŸ‰ å·²è¾¾åˆ°95%å‡†ç¡®ç‡ç›®æ ‡ï¼")
        return trainer.model
        
    # æ™ºèƒ½æ¶æ„è¿›åŒ–
    print("\nğŸ§¬ å¼€å§‹æ™ºèƒ½æ¶æ„è¿›åŒ–...")
    
    evolution_engine = EnhancedEvolutionEngine(device)
    
    max_evolution_rounds = 3
    current_model = trainer.model
    
    for round_idx in range(max_evolution_rounds):
        print(f"\nğŸ”„ è¿›åŒ–è½®æ¬¡ {round_idx + 1}/{max_evolution_rounds}")
        
        # æ£€æµ‹ç“¶é¢ˆå¹¶åšå‡ºå†³ç­–
        bottleneck_decisions = evolution_engine.detect_bottlenecks_with_decision(
            current_model, trainer.test_loader
        )
        
        # è¿‡æ»¤å‡ºå€¼å¾—å˜å¼‚çš„ç“¶é¢ˆ
        valuable_mutations = [
            (b, d) for b, d in bottleneck_decisions 
            if d.should_mutate and d.confidence > 0.3
        ]
        
        if not valuable_mutations:
            print("\nğŸ›‘ æ²¡æœ‰å‘ç°å€¼å¾—æ‰§è¡Œçš„å˜å¼‚ï¼Œåœæ­¢è¿›åŒ–")
            break
            
        print(f"\nğŸ“‹ å‘ç° {len(valuable_mutations)} ä¸ªå€¼å¾—æ‰§è¡Œçš„å˜å¼‚")
        
        # æ‰§è¡Œå˜å¼‚
        mutated_model = evolution_engine.execute_intelligent_mutations(
            current_model, valuable_mutations
        )
        
        # æ›´æ–°è®­ç»ƒå™¨çš„æ¨¡å‹
        trainer.model = mutated_model.to(device)
        trainer.setup_training_components()  # é‡æ–°è®¾ç½®ä¼˜åŒ–å™¨
        
        # ç»§ç»­è®­ç»ƒå˜å¼‚åçš„æ¨¡å‹
        print(f"\nğŸ“š è®­ç»ƒå˜å¼‚åçš„æ¨¡å‹...")
        additional_epochs = 10
        new_best_acc = trainer.train_epochs(additional_epochs)
        
        print(f"\nğŸ“Š å˜å¼‚åæœ€ä½³å‡†ç¡®ç‡: {new_best_acc:.2f}%")
        
        # è®°å½•å˜å¼‚ç»“æœ
        for bottleneck, decision in valuable_mutations:
            actual_gain = (new_best_acc - best_acc) / 100.0
            success = actual_gain > 0
            
            evolution_engine.decision_engine.record_mutation_outcome(
                MutationEvidence(
                    mutual_info_gain=bottleneck.mutual_info,
                    cond_mutual_info_gain=bottleneck.conditional_mi,
                    uncertainty_reduction=bottleneck.uncertainty,
                    transfer_cost=0.05,
                    bottleneck_severity=bottleneck.severity
                ),
                actual_gain,
                success
            )
            
        best_acc = max(best_acc, new_best_acc)
        current_model = trainer.model
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
        if best_acc >= 95.0:
            print(f"\nğŸ‰ è¾¾åˆ°95%å‡†ç¡®ç‡ç›®æ ‡ï¼æœ€ç»ˆå‡†ç¡®ç‡: {best_acc:.2f}%")
            break
    
    print(f"\nğŸ è¿›åŒ–å®Œæˆï¼Œæœ€ç»ˆæœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}%")
    
    # æ˜¾ç¤ºè®­ç»ƒå†å²
    if trainer.test_history:
        final_acc = trainer.test_history[-1]['acc']
        print(f"\nğŸ“ˆ æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {final_acc:.2f}%")
        
        if final_acc >= 95.0:
            print("\nğŸŠ æˆåŠŸè¾¾æˆ95%å‡†ç¡®ç‡ç›®æ ‡ï¼")
        else:
            print(f"\nğŸ“Š è·ç¦»95%ç›®æ ‡è¿˜å·®: {95.0 - final_acc:.2f}%")
            print("ğŸ’¡ å»ºè®®ï¼šå¢åŠ è®­ç»ƒè½®æ•°æˆ–è°ƒæ•´è¶…å‚æ•°")
    
    return current_model


if __name__ == "__main__":
    try:
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(42)
        np.random.seed(42)
        
        # è¿è¡Œæ¼”ç¤º
        final_model = enhanced_intelligent_evolution_demo()
        
        print("\nâœ¨ æ¼”ç¤ºå®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"æ¼”ç¤ºè¿‡ç¨‹ä¸­é‡åˆ°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        
        print("\nâŒ æ¼”ç¤ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")