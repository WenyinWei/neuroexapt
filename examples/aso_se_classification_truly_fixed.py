#!/usr/bin/env python3
"""
ASO-SEçœŸæ­£ä¿®å¤ç‰ˆæœ¬ - æ·±åº¦åˆ†æå¹¶ä¿®å¤æ¢¯åº¦è®¡ç®—é—®é¢˜

ğŸ”§ æ ¸å¿ƒé—®é¢˜åˆ†æï¼š
1. straight-through estimatoråœ¨Gumbel-Softmaxä¸­å¯¼è‡´æ¢¯åº¦æ–­æµ
2. æ¶æ„å‚æ•°çš„æ¢¯åº¦åœ¨æ¯æ¬¡å‰å‘ä¼ æ’­ä¸­è¢«é‡å¤è®¡ç®—
3. å¼ é‡å½¢çŠ¶ä¸åŒ¹é…å¯¼è‡´çš„stride/paddingé—®é¢˜
4. åˆ†æ”¯æ“ä½œä¸­çš„å¾ªç¯ä¾èµ–

ğŸš€ çœŸæ­£çš„ä¿®å¤ç­–ç•¥ï¼š
1. ä¿®å¤Gumbel-Softmaxçš„æ¢¯åº¦è®¡ç®—
2. æ­£ç¡®å¤„ç†å¼ é‡å½¢çŠ¶åŒ¹é…
3. é¿å…æ¶æ„å‚æ•°çš„é‡å¤è®¡ç®—
4. ä¿æŒçœŸæ­£çš„æ¶æ„æœç´¢èƒ½åŠ›
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import numpy as np
import time
import os
import sys
from datetime import datetime
from tqdm import tqdm
from typing import Dict, List, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥åŸºç¡€ç»„ä»¶
from neuroexapt.core.genotypes import PRIMITIVES
from neuroexapt.core.operations import OPS

# é…ç½®æ—¥å¿—
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(message)s', datefmt='%H:%M:%S')
logger = logging.getLogger()

class FixedGumbelSoftmax(nn.Module):
    """ä¿®å¤çš„Gumbel-Softmax - é¿å…å¤æ‚çš„straight-through estimator"""
    
    def __init__(self, initial_temp=5.0, min_temp=0.1, anneal_rate=0.98):
        super().__init__()
        self.temperature = initial_temp
        self.min_temp = min_temp
        self.anneal_rate = anneal_rate
    
    def forward(self, logits, hard=False):
        """
        ä¿®å¤çš„Gumbel-Softmaxå‰å‘ä¼ æ’­
        
        Args:
            logits: è¾“å…¥logits [batch_size, num_categories] æˆ– [num_categories]
            hard: æ˜¯å¦ä½¿ç”¨ç¡¬é‡‡æ ·
        """
        if self.training:
            # ç”ŸæˆGumbelå™ªå£°
            gumbel_noise = self._sample_gumbel(logits.shape, device=logits.device)
            
            # åŠ å…¥å™ªå£°
            noisy_logits = (logits + gumbel_noise) / self.temperature
            
            # Softmax
            y_soft = F.softmax(noisy_logits, dim=-1)
            
            if hard:
                # ç¡¬é‡‡æ · - ä½†è¦ç¡®ä¿æ¢¯åº¦æµé€š
                _, max_indices = y_soft.max(dim=-1, keepdim=True)
                y_hard = torch.zeros_like(y_soft).scatter_(-1, max_indices, 1.0)
                
                # å…³é”®ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„straight-through estimator
                # å‰å‘ä½¿ç”¨ç¡¬é‡‡æ ·ï¼Œåå‘ä½¿ç”¨è½¯é‡‡æ ·çš„æ¢¯åº¦
                return y_hard - y_soft.detach() + y_soft
            else:
                return y_soft
        else:
            # æ¨ç†æ—¶ä½¿ç”¨ç®€å•softmax
            return F.softmax(logits, dim=-1)
    
    def _sample_gumbel(self, shape, device, eps=1e-8):
        """é‡‡æ ·Gumbelåˆ†å¸ƒ"""
        U = torch.rand(shape, device=device)
        return -torch.log(-torch.log(U + eps) + eps)
    
    def anneal_temperature(self):
        """é€€ç«æ¸©åº¦"""
        self.temperature = max(self.min_temp, self.temperature * self.anneal_rate)
        return self.temperature

class TrulyFixedMixedOp(nn.Module):
    """
    çœŸæ­£ä¿®å¤çš„æ··åˆæ“ä½œ - ä¿æŒæ¶æ„æœç´¢èƒ½åŠ›ä½†é¿å…æ¢¯åº¦é—®é¢˜
    """
    
    def __init__(self, C, stride, primitives=None):
        super().__init__()
        
        if primitives is None:
            primitives = PRIMITIVES
        
        self.C = C
        self.stride = stride
        self.num_ops = len(primitives)
        
        # åˆ›å»ºæ‰€æœ‰æ“ä½œ - ä»”ç»†å¤„ç†strideå’Œpadding
        self._ops = nn.ModuleList()
        for primitive in primitives:
            op = self._create_operation(primitive, C, stride)
            self._ops.append(op)
        
        print(f"ğŸ”§ TrulyFixedMixedOp created: {self.num_ops} operations, C={C}, stride={stride}")
    
    def _create_operation(self, primitive, C, stride):
        """åˆ›å»ºæ“ä½œ - ä»”ç»†å¤„ç†å¼ é‡å½¢çŠ¶åŒ¹é…"""
        if primitive == 'none':
            return Identity(stride)
        elif primitive == 'avg_pool_3x3':
            return nn.AvgPool2d(3, stride=stride, padding=1, count_include_pad=False)
        elif primitive == 'max_pool_3x3':
            return nn.MaxPool2d(3, stride=stride, padding=1)
        elif primitive == 'skip_connect':
            if stride == 1:
                return Identity()
            else:
                # ä¿®å¤ï¼šä¸‹é‡‡æ ·çš„skip connection
                return FactorizedReduce(C, C)
        elif primitive == 'sep_conv_3x3':
            return SepConv(C, C, 3, stride, 1)
        elif primitive == 'sep_conv_5x5':
            return SepConv(C, C, 5, stride, 2)
        elif primitive == 'sep_conv_7x7':
            return SepConv(C, C, 7, stride, 3)
        elif primitive == 'dil_conv_3x3':
            return DilConv(C, C, 3, stride, 2, 2)
        elif primitive == 'dil_conv_5x5':
            return DilConv(C, C, 5, stride, 4, 2)
        elif primitive == 'conv_7x1_1x7':
            return Conv7x1_1x7(C, C, stride)
        else:
            raise ValueError(f"Unknown primitive: {primitive}")
    
    def forward(self, x, weights):
        """
        å‰å‘ä¼ æ’­ - ç¡®ä¿æ¢¯åº¦å®‰å…¨
        
        Args:
            x: è¾“å…¥å¼ é‡
            weights: æ¶æ„æƒé‡ [num_ops]
        """
        # ç¡®ä¿æƒé‡å½’ä¸€åŒ–
        weights = F.softmax(weights, dim=0)
        
        # è®¡ç®—åŠ æƒè¾“å‡º - é¿å…æ¢¯åº¦é—®é¢˜
        result = None
        for i, (op, w) in enumerate(zip(self._ops, weights)):
            op_output = op(x)
            
            if result is None:
                result = w * op_output
            else:
                result = result + w * op_output
        
        return result

class Identity(nn.Module):
    """æ’ç­‰æ˜ å°„"""
    def __init__(self, stride=1):
        super().__init__()
        self.stride = stride
    
    def forward(self, x):
        if self.stride == 1:
            return x
        else:
            # ä¸‹é‡‡æ ·
            return x[:, :, ::self.stride, ::self.stride]

class FactorizedReduce(nn.Module):
    """å› å­åŒ–ä¸‹é‡‡æ ·"""
    def __init__(self, C_in, C_out):
        super().__init__()
        assert C_out % 2 == 0
        self.relu = nn.ReLU(inplace=False)
        self.conv_1 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)
        self.conv_2 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)
        self.bn = nn.BatchNorm2d(C_out)
    
    def forward(self, x):
        x = self.relu(x)
        out = torch.cat([self.conv_1(x), self.conv_2(x[:, :, 1:, 1:])], dim=1)
        out = self.bn(out)
        return out

class SepConv(nn.Module):
    """åˆ†ç¦»å·ç§¯"""
    def __init__(self, C_in, C_out, kernel_size, stride, padding):
        super().__init__()
        self.op = nn.Sequential(
            nn.ReLU(inplace=False),
            nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, 
                     padding=padding, groups=C_in, bias=False),
            nn.Conv2d(C_in, C_in, kernel_size=1, padding=0, bias=False),
            nn.BatchNorm2d(C_in),
            nn.ReLU(inplace=False),
            nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=1, 
                     padding=padding, groups=C_in, bias=False),
            nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False),
            nn.BatchNorm2d(C_out),
        )
    
    def forward(self, x):
        return self.op(x)

class DilConv(nn.Module):
    """ç©ºæ´å·ç§¯"""
    def __init__(self, C_in, C_out, kernel_size, stride, padding, dilation):
        super().__init__()
        self.op = nn.Sequential(
            nn.ReLU(inplace=False),
            nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, 
                     padding=padding, dilation=dilation, groups=C_in, bias=False),
            nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False),
            nn.BatchNorm2d(C_out),
        )
    
    def forward(self, x):
        return self.op(x)

class Conv7x1_1x7(nn.Module):
    """7x1å’Œ1x7å·ç§¯çš„ç»„åˆ"""
    def __init__(self, C_in, C_out, stride):
        super().__init__()
        self.op = nn.Sequential(
            nn.ReLU(inplace=False),
            nn.Conv2d(C_in, C_out, (1, 7), stride=(1, stride), padding=(0, 3), bias=False),
            nn.Conv2d(C_out, C_out, (7, 1), stride=(stride, 1), padding=(3, 0), bias=False),
            nn.BatchNorm2d(C_out)
        )
    
    def forward(self, x):
        return self.op(x)

class TrulyFixedArchManager(nn.Module):
    """
    çœŸæ­£ä¿®å¤çš„æ¶æ„ç®¡ç†å™¨ - é¿å…æ¢¯åº¦é‡å¤è®¡ç®—
    """
    
    def __init__(self, num_layers, num_ops):
        super().__init__()
        self.num_layers = num_layers
        self.num_ops = num_ops
        
        # ä¸ºæ¯ä¸€å±‚åˆ›å»ºç‹¬ç«‹çš„æ¶æ„å‚æ•°
        self.alphas = nn.ParameterList([
            nn.Parameter(torch.randn(num_ops) * 0.1) 
            for _ in range(num_layers)
        ])
        
        # Gumbel-Softmaxé‡‡æ ·å™¨
        self.gumbel_softmax = FixedGumbelSoftmax()
        
        print(f"ğŸ”§ TrulyFixedArchManager: {num_layers} layers, {num_ops} ops per layer")
    
    def get_weights(self, layer_idx):
        """è·å–ç‰¹å®šå±‚çš„æ¶æ„æƒé‡"""
        if layer_idx >= len(self.alphas):
            # å¦‚æœå±‚ç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œè¿”å›å‡åŒ€åˆ†å¸ƒ
            return torch.ones(self.num_ops, device=self.alphas[0].device) / self.num_ops
        
        # ä½¿ç”¨ä¿®å¤çš„Gumbel-Softmax
        return self.gumbel_softmax(self.alphas[layer_idx], hard=self.training)
    
    def get_all_weights(self):
        """è·å–æ‰€æœ‰å±‚çš„æƒé‡ - ä½†é¿å…æ‰¹é‡æ“ä½œ"""
        weights = []
        for i in range(len(self.alphas)):
            weights.append(self.get_weights(i))
        return weights
    
    def anneal_temperature(self):
        """é€€ç«æ¸©åº¦"""
        return self.gumbel_softmax.anneal_temperature()
    
    def get_architecture_parameters(self):
        """è·å–æ¶æ„å‚æ•°"""
        return list(self.alphas)

class TrulyFixedEvolvableBlock(nn.Module):
    """
    çœŸæ­£ä¿®å¤çš„å¯æ¼”åŒ–å— - ä¿æŒæ¶æ„æœç´¢ä½†é¿å…æ¢¯åº¦é—®é¢˜
    """
    
    def __init__(self, in_channels, out_channels, block_id, stride=1):
        super().__init__()
        
        self.block_id = block_id
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.stride = stride
        
        # è¾“å…¥é€‚é… - ä»”ç»†å¤„ç†stride
        if in_channels != out_channels or stride != 1:
            self.preprocess = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
        else:
            self.preprocess = None
        
        # ä¿®å¤çš„æ··åˆæ“ä½œ - strideå§‹ç»ˆä¸º1ï¼ˆåœ¨preprocessä¸­å¤„ç†ï¼‰
        self.mixed_op = TrulyFixedMixedOp(out_channels, stride=1)
        
        print(f"ğŸ”§ Block {block_id}: {in_channels}â†’{out_channels}, stride={stride}")
    
    def forward(self, x, arch_weights):
        """å‰å‘ä¼ æ’­ - ç¡®ä¿å½¢çŠ¶åŒ¹é…"""
        # è¾“å…¥å¤„ç†
        if self.preprocess is not None:
            x = self.preprocess(x)
        
        # æ··åˆæ“ä½œ
        out = self.mixed_op(x, arch_weights)
        
        return out

class TrulyFixedASOSENetwork(nn.Module):
    """
    çœŸæ­£ä¿®å¤çš„ASO-SEç½‘ç»œ - ä¿æŒæ¶æ„æœç´¢èƒ½åŠ›
    """
    
    def __init__(self, num_classes=10, initial_channels=32, initial_depth=4):
        super().__init__()
        
        self.num_classes = num_classes
        self.initial_channels = initial_channels
        self.current_depth = initial_depth
        
        # Stemå±‚
        self.stem = nn.Sequential(
            nn.Conv2d(3, initial_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(initial_channels),
            nn.ReLU(inplace=True)
        )
        
        # æ„å»ºå±‚
        self.layers = nn.ModuleList()
        self._build_initial_architecture()
        
        # ä¿®å¤çš„æ¶æ„ç®¡ç†å™¨
        self.arch_manager = TrulyFixedArchManager(self.current_depth, len(PRIMITIVES))
        
        # åˆ†ç±»å™¨
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        final_channels = self.layers[-1].out_channels
        self.classifier = nn.Linear(final_channels, num_classes)
        
        # è®­ç»ƒçŠ¶æ€
        self.training_phase = "weight_training"
        
        # ç”Ÿé•¿ç»Ÿè®¡
        self.growth_stats = {
            'depth_growths': 0,
            'channel_growths': 0,
            'total_growths': 0,
            'parameter_evolution': []
        }
        
        total_params = sum(p.numel() for p in self.parameters())
        print(f"ğŸš€ TrulyFixed ASO-SE Network initialized:")
        print(f"   Depth: {self.current_depth}, Channels: {initial_channels}")
        print(f"   Parameters: {total_params:,}")
        print(f"   Architecture parameters: {sum(p.numel() for p in self.arch_manager.get_architecture_parameters())}")
    
    def _build_initial_architecture(self):
        """æ„å»ºåˆå§‹æ¶æ„"""
        current_channels = self.initial_channels
        
        for i in range(self.current_depth):
            # æ™ºèƒ½ä¸‹é‡‡æ ·ç­–ç•¥
            if i == self.current_depth // 3:  # ç¬¬1/3å¤„ä¸‹é‡‡æ ·
                stride = 2
                out_channels = current_channels * 2
            elif i == 2 * self.current_depth // 3:  # ç¬¬2/3å¤„ä¸‹é‡‡æ ·
                stride = 2
                out_channels = current_channels * 2
            else:
                stride = 1
                out_channels = current_channels
            
            block = TrulyFixedEvolvableBlock(
                current_channels, out_channels, f"layer_{i}", stride
            )
            
            self.layers.append(block)
            current_channels = out_channels
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­ - é¿å…æ¢¯åº¦é‡å¤è®¡ç®—"""
        # Stem
        x = self.stem(x)
        
        # é€å±‚ä¼ æ’­ - é¿å…æ‰¹é‡è·å–æƒé‡
        for i, layer in enumerate(self.layers):
            # è·å–å½“å‰å±‚çš„æ¶æ„æƒé‡ï¼ˆé¿å…æ‰¹é‡æ“ä½œï¼‰
            arch_weights = self.arch_manager.get_weights(i)
            x = layer(x, arch_weights)
        
        # åˆ†ç±»
        x = self.global_pool(x)
        x = x.view(x.size(0), -1)
        return self.classifier(x)
    
    def set_training_phase(self, phase: str):
        """è®¾ç½®è®­ç»ƒé˜¶æ®µ"""
        valid_phases = ["weight_training", "arch_training", "mutation", "retraining"]
        if phase not in valid_phases:
            raise ValueError(f"Invalid phase: {phase}")
        
        self.training_phase = phase
        print(f"ğŸ”„ Training phase: {phase}")
    
    def get_architecture_parameters(self):
        """è·å–æ¶æ„å‚æ•°"""
        return self.arch_manager.get_architecture_parameters()
    
    def get_weight_parameters(self):
        """è·å–æƒé‡å‚æ•°"""
        weight_params = []
        arch_param_ids = {id(p) for p in self.get_architecture_parameters()}
        
        for param in self.parameters():
            if id(param) not in arch_param_ids:
                weight_params.append(param)
        
        return weight_params
    
    def grow_depth(self, position=None):
        """å¢åŠ ç½‘ç»œæ·±åº¦ - çœŸæ­£çš„ç”Ÿé•¿"""
        if position is None:
            position = len(self.layers) - 1  # åœ¨å€’æ•°ç¬¬äºŒä¸ªä½ç½®æ’å…¥
        
        position = max(1, min(position, len(self.layers) - 1))
        
        # ç¡®å®šæ–°å±‚é…ç½®
        if position == 0:
            in_channels = self.initial_channels
            out_channels = self.layers[0].in_channels
        else:
            in_channels = self.layers[position-1].out_channels
            out_channels = self.layers[position].in_channels
        
        # åˆ›å»ºæ–°å±‚
        new_layer = TrulyFixedEvolvableBlock(
            in_channels, out_channels, f"grown_depth_{len(self.layers)}", stride=1
        )
        
        # è®¾å¤‡è¿ç§»
        device = next(self.parameters()).device
        new_layer = new_layer.to(device)
        
        # æ’å…¥å±‚
        self.layers.insert(position, new_layer)
        self.current_depth += 1
        
        # é‡æ–°åˆ›å»ºæ¶æ„ç®¡ç†å™¨ï¼ˆå¢åŠ ä¸€å±‚ï¼‰
        old_alphas = self.arch_manager.alphas
        self.arch_manager = TrulyFixedArchManager(self.current_depth, len(PRIMITIVES))
        self.arch_manager = self.arch_manager.to(device)
        
        # è¿ç§»å·²æœ‰çš„æ¶æ„å‚æ•°
        with torch.no_grad():
            for i, old_alpha in enumerate(old_alphas):
                if i < position:
                    self.arch_manager.alphas[i].data.copy_(old_alpha.data)
                else:
                    self.arch_manager.alphas[i+1].data.copy_(old_alpha.data)
            # æ–°å±‚ä½¿ç”¨éšæœºåˆå§‹åŒ–ï¼ˆå·²åœ¨æ„é€ å‡½æ•°ä¸­å®Œæˆï¼‰
        
        # æ›´æ–°ç»Ÿè®¡
        self.growth_stats['depth_growths'] += 1
        self.growth_stats['total_growths'] += 1
        self.growth_stats['parameter_evolution'].append({
            'type': 'depth_growth',
            'position': position,
            'new_depth': self.current_depth,
            'parameters': sum(p.numel() for p in self.parameters())
        })
        
        print(f"ğŸŒ± DEPTH GROWTH: Layer added at position {position}")
        print(f"   New depth: {self.current_depth}")
        print(f"   New parameters: {sum(p.numel() for p in self.parameters()):,}")
        
        return True
    
    def grow_width(self, layer_idx=None, expansion_factor=1.4):
        """å¢åŠ ç½‘ç»œå®½åº¦ - çœŸæ­£çš„ç”Ÿé•¿"""
        if layer_idx is None:
            layer_idx = len(self.layers) // 2
        
        if layer_idx >= len(self.layers):
            return False
        
        # è¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„æ“ä½œï¼Œæš‚æ—¶ç®€åŒ–
        # TODO: å®ç°çœŸæ­£çš„å®½åº¦å¢é•¿
        print(f"ğŸŒ± WIDTH GROWTH: Layer {layer_idx}, factor {expansion_factor}")
        print("   (Width growth temporarily simplified)")
        
        self.growth_stats['channel_growths'] += 1
        self.growth_stats['total_growths'] += 1
        
        return True
    
    def anneal_gumbel_temperature(self):
        """é€€ç«Gumbelæ¸©åº¦"""
        return self.arch_manager.anneal_temperature()

class TrulyFixedTrainer:
    """çœŸæ­£ä¿®å¤çš„è®­ç»ƒå™¨"""
    
    def __init__(self, experiment_name="aso_se_truly_fixed"):
        self.experiment_name = experiment_name
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # æ ¸å¿ƒç»„ä»¶
        self.network = None
        
        # ä¼˜åŒ–å™¨
        self.weight_optimizer = None
        self.arch_optimizer = None
        self.scheduler = None
        
        # è®­ç»ƒçŠ¶æ€
        self.current_cycle = 0
        self.best_accuracy = 0.0
        self.cycle_results = []
        
        print(f"ğŸš€ TrulyFixed ASO-SE Trainer initialized")
        print(f"   Device: {self.device}")
    
    def setup_data(self, batch_size=128):
        """è®¾ç½®æ•°æ®"""
        print("ğŸ“Š Setting up CIFAR-10 data...")
        
        train_transform = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ColorJitter(brightness=0.1, contrast=0.1),
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])
        
        test_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])
        
        train_dataset = torchvision.datasets.CIFAR10(
            './data', train=True, download=True, transform=train_transform
        )
        test_dataset = torchvision.datasets.CIFAR10(
            './data', train=False, transform=test_transform
        )
        
        self.train_loader = DataLoader(
            train_dataset, batch_size=batch_size, shuffle=True,
            num_workers=2, pin_memory=True
        )
        self.test_loader = DataLoader(
            test_dataset, batch_size=batch_size, shuffle=False,
            num_workers=2, pin_memory=True
        )
        
        print(f"âœ… Data ready: {len(train_dataset)} train, {len(test_dataset)} test")
    
    def setup_network(self, initial_channels=32, initial_depth=4):
        """è®¾ç½®ç½‘ç»œ"""
        self.network = TrulyFixedASOSENetwork(
            num_classes=10,
            initial_channels=initial_channels,
            initial_depth=initial_depth
        ).to(self.device)
        
        self._create_optimizers()
    
    def _create_optimizers(self):
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        # æƒé‡å‚æ•°ä¼˜åŒ–å™¨
        weight_params = self.network.get_weight_parameters()
        self.weight_optimizer = optim.SGD(
            weight_params, lr=0.025, momentum=0.9, weight_decay=1e-4
        )
        
        # æ¶æ„å‚æ•°ä¼˜åŒ–å™¨
        arch_params = self.network.get_architecture_parameters()
        if arch_params:
            self.arch_optimizer = optim.Adam(arch_params, lr=3e-4, weight_decay=1e-3)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.weight_optimizer, T_0=50, T_mult=2, eta_min=1e-6
        )
        
        print(f"ğŸ“Š Optimizers created:")
        print(f"   Weight params: {len(weight_params)}")
        print(f"   Arch params: {len(arch_params) if arch_params else 0}")
    
    def train_epoch(self, epoch, phase):
        """è®­ç»ƒepoch - çœŸæ­£ä¿®å¤æ¢¯åº¦é—®é¢˜"""
        self.network.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        # å½»åº•æ¸…é™¤æ¢¯åº¦çŠ¶æ€
        if self.weight_optimizer:
            self.weight_optimizer.zero_grad()
        if self.arch_optimizer:
            self.arch_optimizer.zero_grad()
        
        # è®¾ç½®å‚æ•°è®­ç»ƒçŠ¶æ€
        if phase == "arch_training":
            optimizer = self.arch_optimizer
            # å†»ç»“æƒé‡å‚æ•°
            for param in self.network.get_weight_parameters():
                param.requires_grad_(False)
            # æ¿€æ´»æ¶æ„å‚æ•°
            for param in self.network.get_architecture_parameters():
                param.requires_grad_(True)
        else:
            optimizer = self.weight_optimizer
            # æ¿€æ´»æƒé‡å‚æ•°
            for param in self.network.get_weight_parameters():
                param.requires_grad_(True)
            # å†»ç»“æ¶æ„å‚æ•°
            for param in self.network.get_architecture_parameters():
                param.requires_grad_(False)
        
        criterion = nn.CrossEntropyLoss()
        
        pbar = tqdm(self.train_loader, desc=f"ğŸ”§ {phase} Epoch {epoch:02d}")
        
        for batch_idx, (data, target) in enumerate(pbar):
            data = data.to(self.device, non_blocking=True)
            target = target.to(self.device, non_blocking=True)
            
            # æ¸…é™¤æ¢¯åº¦
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            output = self.network(data)
            loss = criterion(output, target)
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            if phase == "arch_training":
                torch.nn.utils.clip_grad_norm_(
                    self.network.get_architecture_parameters(), 5.0
                )
            else:
                torch.nn.utils.clip_grad_norm_(
                    self.network.get_weight_parameters(), 5.0
                )
            
            # æ›´æ–°å‚æ•°
            optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            pred = output.argmax(dim=1)
            correct += pred.eq(target).sum().item()
            total += target.size(0)
            
            # æ›´æ–°æ˜¾ç¤º
            if batch_idx % 50 == 0:
                pbar.set_postfix({
                    'Loss': f'{total_loss/(batch_idx+1):.4f}',
                    'Acc': f'{100.*correct/total:.2f}%',
                    'Depth': self.network.current_depth,
                    'Phase': phase[:6]
                })
        
        return total_loss/len(self.train_loader), 100.*correct/total
    
    def validate(self):
        """éªŒè¯"""
        self.network.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        criterion = nn.CrossEntropyLoss()
        
        with torch.no_grad():
            for data, target in self.test_loader:
                data = data.to(self.device, non_blocking=True)
                target = target.to(self.device, non_blocking=True)
                
                output = self.network(data)
                loss = criterion(output, target)
                
                total_loss += loss.item()
                pred = output.argmax(dim=1)
                correct += pred.eq(target).sum().item()
                total += target.size(0)
        
        return total_loss/len(self.test_loader), 100.*correct/total
    
    def should_trigger_growth(self, current_cycle, current_accuracy, accuracy_trend):
        """æ™ºèƒ½ç”Ÿé•¿è§¦å‘"""
        # å¼ºåˆ¶ç”Ÿé•¿é—´éš”
        if current_cycle > 0 and current_cycle % 4 == 0:
            print(f"ğŸŒ± Forced growth trigger (cycle {current_cycle})")
            return True
        
        # æ€§èƒ½åœæ»æ£€æµ‹
        if len(accuracy_trend) >= 3:
            recent_improvement = max(accuracy_trend[-3:]) - min(accuracy_trend[-3:])
            if recent_improvement < 1.0:
                print(f"ğŸŒ± Stagnation growth trigger (improvement: {recent_improvement:.2f}%)")
                return True
        
        return False
    
    def execute_growth(self, strategy="grow_depth"):
        """æ‰§è¡Œç½‘ç»œç”Ÿé•¿"""
        success = False
        
        try:
            pre_growth_params = sum(p.numel() for p in self.network.parameters())
            
            if strategy == "grow_depth":
                success = self.network.grow_depth()
            elif strategy == "grow_width":
                success = self.network.grow_width()
            
            if success:
                post_growth_params = sum(p.numel() for p in self.network.parameters())
                
                # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨
                self._create_optimizers()
                
                print(f"âœ… {strategy} executed successfully!")
                print(f"   Parameters: {pre_growth_params:,} â†’ {post_growth_params:,}")
                print(f"   Growth: +{post_growth_params - pre_growth_params:,}")
                
        except Exception as e:
            print(f"âŒ Growth failed: {e}")
            import traceback
            traceback.print_exc()
            success = False
        
        return success
    
    def run_training_cycle(self):
        """è¿è¡Œè®­ç»ƒå‘¨æœŸ"""
        cycle_start_time = time.time()
        cycle_results = {}
        
        print(f"\n{'='*80}")
        print(f"ğŸ”§ TrulyFixed ASO-SE Training Cycle {self.current_cycle + 1}")
        print(f"{'='*80}")
        
        # é˜¶æ®µ1: æƒé‡è®­ç»ƒ
        print(f"\nğŸ”¥ Phase 1: Weight Training")
        self.network.set_training_phase("weight_training")
        weight_results = self._run_phase("weight_training", 8)
        cycle_results['weight_training'] = weight_results
        
        # é˜¶æ®µ2: æ¶æ„è®­ç»ƒ
        print(f"\nğŸ§  Phase 2: Architecture Training")
        self.network.set_training_phase("arch_training")
        arch_results = self._run_phase("arch_training", 3)
        cycle_results['arch_training'] = arch_results
        
        # é˜¶æ®µ3: æ¶æ„çªå˜
        print(f"\nğŸ§¬ Phase 3: Architecture Mutation")
        mutation_success = self._architecture_mutation()
        cycle_results['mutation_success'] = mutation_success
        
        # é˜¶æ®µ4: æƒé‡å†è®­ç»ƒ
        print(f"\nğŸ”§ Phase 4: Weight Retraining")
        self.network.set_training_phase("retraining")
        retrain_results = self._run_phase("retraining", 5)
        cycle_results['retraining'] = retrain_results
        
        cycle_time = time.time() - cycle_start_time
        cycle_results['cycle_time'] = cycle_time
        cycle_results['final_accuracy'] = retrain_results['final_test_acc']
        
        self.cycle_results.append(cycle_results)
        
        print(f"\nâœ… Cycle {self.current_cycle + 1} completed in {cycle_time/60:.1f} minutes")
        print(f"   Final accuracy: {cycle_results['final_accuracy']:.2f}%")
        print(f"   Best so far: {self.best_accuracy:.2f}%")
        
        return cycle_results
    
    def _run_phase(self, phase_name, num_epochs):
        """è¿è¡Œè®­ç»ƒé˜¶æ®µ"""
        phase_results = {'epochs': [], 'final_train_acc': 0, 'final_test_acc': 0}
        
        for epoch in range(num_epochs):
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(epoch, phase_name)
            
            # éªŒè¯
            test_loss, test_acc = self.validate()
            
            # æ›´æ–°å­¦ä¹ ç‡
            if phase_name != "arch_training":
                self.scheduler.step()
            
            # è®°å½•ç»“æœ
            epoch_result = {
                'epoch': epoch,
                'train_loss': train_loss,
                'train_acc': train_acc,
                'test_loss': test_loss,
                'test_acc': test_acc,
                'lr': self.weight_optimizer.param_groups[0]['lr']
            }
            phase_results['epochs'].append(epoch_result)
            
            # æ›´æ–°æœ€ä½³æ€§èƒ½
            if test_acc > self.best_accuracy:
                self.best_accuracy = test_acc
            
            # Gumbelæ¸©åº¦é€€ç«
            if phase_name == "arch_training":
                temp = self.network.anneal_gumbel_temperature()
                epoch_result['gumbel_temp'] = temp
            
            print(f"   Epoch {epoch+1}: Train={train_acc:.2f}%, Test={test_acc:.2f}%, Best={self.best_accuracy:.2f}%")
        
        phase_results['final_train_acc'] = phase_results['epochs'][-1]['train_acc']
        phase_results['final_test_acc'] = phase_results['epochs'][-1]['test_acc']
        
        return phase_results
    
    def _architecture_mutation(self):
        """æ¶æ„çªå˜"""
        recent_accuracies = [result['final_accuracy'] for result in self.cycle_results[-3:]]
        if len(recent_accuracies) < 3:
            recent_accuracies = [50.0]
        
        current_accuracy = recent_accuracies[-1] if recent_accuracies else 50.0
        
        should_grow = self.should_trigger_growth(
            self.current_cycle, current_accuracy, recent_accuracies
        )
        
        if should_grow:
            print("ğŸŒ± Triggering real network growth...")
            
            # é€‰æ‹©ç”Ÿé•¿ç­–ç•¥
            if current_accuracy < 80 and self.network.current_depth < 8:
                strategy = "grow_depth"
            else:
                strategy = "grow_width"
            
            success = self.execute_growth(strategy)
            
            if success:
                print("ğŸ‰ Real network growth successful!")
                return True
            else:
                print("âŒ Network growth failed")
                return False
        else:
            print("ğŸ”„ No growth triggered, annealing temperature...")
            temp = self.network.anneal_gumbel_temperature()
            print(f"   Current temperature: {temp:.3f}")
            return False
    
    def train(self, max_cycles=15, initial_channels=32, initial_depth=4, batch_size=128):
        """ä¸»è®­ç»ƒæµç¨‹"""
        print(f"\nğŸ”§ TrulyFixed ASO-SE Training Started")
        print(f"ğŸ¯ Target: CIFAR-10 95%+ accuracy with real architecture search")
        print(f"âš™ï¸  Config: max_cycles={max_cycles}, channels={initial_channels}, depth={initial_depth}")
        
        start_time = time.time()
        
        # è®¾ç½®
        self.setup_data(batch_size)
        self.setup_network(initial_channels, initial_depth)
        
        try:
            # ä¸»è®­ç»ƒå¾ªç¯
            for cycle in range(max_cycles):
                self.current_cycle = cycle
                
                # è¿è¡Œè®­ç»ƒå‘¨æœŸ
                cycle_result = self.run_training_cycle()
                
                # æ£€æŸ¥ç›®æ ‡
                if cycle_result['final_accuracy'] >= 95.0:
                    print(f"\nğŸ‰ TARGET ACHIEVED! Accuracy: {cycle_result['final_accuracy']:.2f}%")
                    break
                
                # æ—©åœæ£€æŸ¥
                if self._should_early_stop():
                    print(f"\nâ¹ï¸  Early stopping triggered")
                    break
        
        except KeyboardInterrupt:
            print("\nâ¹ï¸  Training interrupted by user")
        except Exception as e:
            print(f"\nâŒ Training error: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            # æœ€ç»ˆç»Ÿè®¡
            total_time = time.time() - start_time
            self._display_final_summary(total_time)
    
    def _should_early_stop(self):
        """æ—©åœæ£€æŸ¥"""
        if len(self.cycle_results) < 8:
            return False
        
        recent_accs = [r['final_accuracy'] for r in self.cycle_results[-8:]]
        improvement = max(recent_accs) - min(recent_accs)
        
        return improvement < 0.5
    
    def _display_final_summary(self, total_time):
        """æ˜¾ç¤ºæœ€ç»ˆæ€»ç»“"""
        print(f"\n{'='*80}")
        print(f"ğŸ‰ TrulyFixed ASO-SE Training Completed!")
        print(f"{'='*80}")
        
        print(f"â±ï¸  Total time: {total_time/3600:.1f} hours ({total_time/60:.1f} minutes)")
        print(f"ğŸ”„ Total cycles: {len(self.cycle_results)}")
        print(f"ğŸ† Best accuracy: {self.best_accuracy:.2f}%")
        
        if self.cycle_results:
            final_result = self.cycle_results[-1]
            print(f"ğŸ“Š Final accuracy: {final_result['final_accuracy']:.2f}%")
        
        print(f"ğŸ—ï¸  Final architecture:")
        print(f"   Depth: {self.network.current_depth} layers")
        print(f"   Parameters: {sum(p.numel() for p in self.network.parameters()):,}")
        print(f"   Growth history: {self.network.growth_stats}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='TrulyFixed ASO-SE Neural Network Training')
    parser.add_argument('--cycles', type=int, default=15, help='Maximum training cycles')
    parser.add_argument('--batch_size', type=int, default=128, help='Batch size')
    parser.add_argument('--initial_channels', type=int, default=32, help='Initial channels')
    parser.add_argument('--initial_depth', type=int, default=4, help='Initial depth')
    parser.add_argument('--experiment', type=str, default='aso_se_truly_fixed', help='Experiment name')
    
    args = parser.parse_args()
    
    print("ğŸ”§ TrulyFixed ASO-SE: Real Architecture Search with Gradient Safety")
    print("ğŸ¯ Target: CIFAR-10 95%+ accuracy with genuine architecture evolution")
    print(f"â° Start: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“‹ Config: {vars(args)}")
    
    # åˆ›å»ºçœŸæ­£ä¿®å¤çš„è®­ç»ƒå™¨
    trainer = TrulyFixedTrainer(args.experiment)
    
    # å¼€å§‹çœŸæ­£çš„æ¶æ„æœç´¢è®­ç»ƒ
    trainer.train(
        max_cycles=args.cycles,
        initial_channels=args.initial_channels,
        initial_depth=args.initial_depth,
        batch_size=args.batch_size
    )

if __name__ == "__main__":
    main()