#!/usr/bin/env python3
"""
ASO-SEä¼˜åŒ–ç‰ˆæœ¬ - é›†æˆé«˜æ€§èƒ½åŸºç¡€è®¾æ–½

ğŸš€ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥ï¼š
1. FastMixedOpï¼šæ™ºèƒ½æ“ä½œé€‰æ‹©ï¼Œåªè®¡ç®—é‡è¦æƒé‡çš„æ“ä½œ
2. æ‰¹é‡åŒ–æ¶æ„æ›´æ–°ï¼šå‡å°‘GPU kernelè°ƒç”¨
3. å†…å­˜é«˜æ•ˆCellï¼šæ¢¯åº¦æ£€æŸ¥ç‚¹+æ“ä½œç¼“å­˜
4. JITç¼–è¯‘ï¼šå…³é”®æ•°å­¦è¿ç®—åŠ é€Ÿ
5. æ“ä½œèåˆï¼šå‡å°‘å†…å­˜è®¿é—®å’Œè®¡ç®—å¼€é”€

é¢„æœŸæ€§èƒ½æå‡ï¼š
- è®­ç»ƒé€Ÿåº¦æå‡3-5å€
- å†…å­˜ä½¿ç”¨å‡å°‘30-50%
- GPUåˆ©ç”¨ç‡æé«˜åˆ°90%+
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import numpy as np
import time
import os
import sys
from datetime import datetime
from tqdm import tqdm
from typing import Dict, List, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥ä¼˜åŒ–çš„åŸºç¡€è®¾æ–½
from neuroexapt.core.fast_operations import (
    FastMixedOp, BatchedArchitectureUpdate, MemoryEfficientCell,
    FastDeviceManager, get_fast_device_manager, OperationProfiler
)
from neuroexapt.math.fast_math import (
    FastEntropy, FastGradients, FastNumerical, FastStatistics,
    PerformanceProfiler, profile_op
)
from neuroexapt.core.evolution_checkpoint import EvolutionCheckpointManager

# é…ç½®æ—¥å¿—
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(message)s', datefmt='%H:%M:%S')
logger = logging.getLogger()

class OptimizedEvolvableBlock(nn.Module):
    """ä¼˜åŒ–çš„å¯æ¼”åŒ–å— - é›†æˆæ‰€æœ‰æ€§èƒ½ä¼˜åŒ–"""
    
    def __init__(self, in_channels, out_channels, block_id, stride=1):
        super().__init__()
        
        self.block_id = block_id
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.stride = stride
        
        # é«˜æ€§èƒ½æ··åˆæ“ä½œ
        self.mixed_op = FastMixedOp(
            out_channels, stride=stride, 
            weight_threshold=0.01,  # åªè®¡ç®—æƒé‡>1%çš„æ“ä½œ
            top_k=3  # æœ€å¤šä¿ç•™3ä¸ªæ´»è·ƒæ“ä½œ
        )
        
        # è¾“å…¥å¤„ç†
        self.input_conv = self._create_input_conv(in_channels, out_channels, stride)
        
        # æ¶æ„å‚æ•°ï¼ˆå°†ç”±å¤–éƒ¨BatchedArchitectureUpdateç®¡ç†ï¼‰
        self.arch_param_idx = None
        
        # æ€§èƒ½ç»Ÿè®¡
        self.forward_count = 0
        self.compute_time = 0.0
        
    def _create_input_conv(self, in_channels, out_channels, stride):
        """åˆ›å»ºè¾“å…¥è½¬æ¢å±‚"""
        if in_channels == out_channels and stride == 1:
            return nn.Identity()
        else:
            return nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(inplace=True)
            )
    
    @profile_op("evolvable_block_forward")
    def forward(self, x, arch_weights):
        """ä¼˜åŒ–çš„å‰å‘ä¼ æ’­"""
        self.forward_count += 1
        start_time = time.perf_counter()
        
        # è¾“å…¥å¤„ç†
        x = self.input_conv(x)
        
        # é«˜æ€§èƒ½æ··åˆæ“ä½œ
        output = self.mixed_op(x, arch_weights, self.training)
        
        # æ›´æ–°ç»Ÿè®¡
        self.compute_time += time.perf_counter() - start_time
        
        return output
    
    def get_performance_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        mixed_op_stats = self.mixed_op.get_performance_stats()
        return {
            'forward_count': self.forward_count,
            'avg_compute_time': self.compute_time / max(self.forward_count, 1),
            'total_compute_time': self.compute_time,
            **mixed_op_stats
        }

class OptimizedASOSENetwork(nn.Module):
    """ä¼˜åŒ–çš„ASO-SEç½‘ç»œ - é«˜æ€§èƒ½æ¶æ„æœç´¢"""
    
    def __init__(self, num_classes=10, initial_channels=32, initial_depth=4):
        super().__init__()
        
        self.num_classes = num_classes
        self.initial_channels = initial_channels
        self.current_depth = initial_depth
        
        # è®¾å¤‡ç®¡ç†å™¨
        self.device_manager = get_fast_device_manager()
        
        # è¾“å…¥å¤„ç†
        self.stem = nn.Sequential(
            nn.Conv2d(3, initial_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(initial_channels),
            nn.ReLU(inplace=True)
        )
        
        # æ„å»ºå±‚
        self.layers = nn.ModuleList()
        self._build_initial_architecture()
        
        # æ‰¹é‡åŒ–æ¶æ„å‚æ•°ç®¡ç†
        from neuroexapt.core.genotypes import PRIMITIVES
        num_ops = len(PRIMITIVES)
        self.arch_updater = BatchedArchitectureUpdate(self.current_depth, num_ops)
        
        # å…¨å±€æ± åŒ–å’Œåˆ†ç±»å™¨
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.classifier = nn.Linear(self.layers[-1].out_channels, num_classes)
        
        # è®­ç»ƒçŠ¶æ€
        self.training_phase = "weight_training"
        self.cycle_count = 0
        
        # ç”Ÿé•¿ç»Ÿè®¡
        self.growth_stats = {
            'depth_growths': 0,
            'channel_growths': 0,
            'branch_growths': 0,
            'total_growths': 0,
            'parameter_evolution': []
        }
        
        # æ€§èƒ½ç›‘æ§
        self.performance_monitor = PerformanceProfiler()
        
        print(f"ğŸš€ OptimizedASOSE Network initialized:")
        print(f"   Depth: {self.current_depth}, Channels: {initial_channels}")
        print(f"   Parameters: {sum(p.numel() for p in self.parameters()):,}")
        print(f"   Device: {self.device_manager.device}")
    
    def _build_initial_architecture(self):
        """æ„å»ºåˆå§‹æ¶æ„"""
        current_channels = self.initial_channels
        
        for i in range(self.current_depth):
            # æ™ºèƒ½ä¸‹é‡‡æ ·
            stride = 2 if i in [self.current_depth//3, 2*self.current_depth//3] else 1
            out_channels = current_channels * (2 if stride == 2 else 1)
            
            block = OptimizedEvolvableBlock(
                current_channels, out_channels, f"layer_{i}", stride
            )
            block.arch_param_idx = i  # è®¾ç½®æ¶æ„å‚æ•°ç´¢å¼•
            
            self.layers.append(block)
            current_channels = out_channels
    
    @profile_op("network_forward")
    def forward(self, x):
        """ä¼˜åŒ–çš„ç½‘ç»œå‰å‘ä¼ æ’­"""
        # è¾“å…¥å¤„ç†
        x = self.stem(x)
        
        # è·å–æ‰€æœ‰æ¶æ„æƒé‡ï¼ˆæ‰¹é‡åŒ–ï¼‰
        arch_weights = self.arch_updater()  # [num_layers, num_ops]
        
        # å±‚çº§ä¼ æ’­
        for i, layer in enumerate(self.layers):
            layer_weights = arch_weights[i]  # è·å–å½“å‰å±‚çš„æ¶æ„æƒé‡
            x = layer(x, layer_weights)
        
        # åˆ†ç±»
        x = self.global_pool(x)
        x = x.view(x.size(0), -1)
        return self.classifier(x)
    
    def set_training_phase(self, phase: str):
        """è®¾ç½®è®­ç»ƒé˜¶æ®µ"""
        valid_phases = ["weight_training", "arch_training", "mutation", "retraining"]
        if phase not in valid_phases:
            raise ValueError(f"Invalid phase: {phase}")
        
        self.training_phase = phase
        
        # é…ç½®æ¶æ„å‚æ•°è®­ç»ƒæ¨¡å¼
        if phase == "arch_training":
            self.arch_updater.train()
        else:
            self.arch_updater.eval()
        
        print(f"ğŸ”„ Training phase: {phase}")
    
    def get_architecture_parameters(self):
        """è·å–æ¶æ„å‚æ•°"""
        return [self.arch_updater.arch_params]
    
    def get_weight_parameters(self):
        """è·å–æƒé‡å‚æ•°"""
        weight_params = []
        for param in self.parameters():
            if param is not self.arch_updater.arch_params:
                weight_params.append(param)
        return weight_params
    
    def grow_depth(self, position=None):
        """å¢åŠ ç½‘ç»œæ·±åº¦"""
        if position is None:
            position = len(self.layers) - 1
        
        position = max(1, min(position, len(self.layers) - 1))
        
        # ç¡®å®šæ–°å±‚é…ç½®
        if position == 0:
            in_channels = self.initial_channels
            out_channels = self.layers[0].in_channels
        else:
            in_channels = self.layers[position-1].out_channels
            out_channels = self.layers[position].in_channels
        
        # åˆ›å»ºæ–°å±‚
        new_layer = OptimizedEvolvableBlock(
            in_channels, out_channels, f"grown_{len(self.layers)}", stride=1
        )
        new_layer.arch_param_idx = position
        
        # è®¾å¤‡è¿ç§»
        new_layer = new_layer.to(self.device_manager.device)
        
        # æ’å…¥å±‚
        self.layers.insert(position, new_layer)
        self.current_depth += 1
        
        # æ›´æ–°æ¶æ„å‚æ•°ç®¡ç†å™¨
        self._update_arch_updater()
        
        # æ›´æ–°ç»Ÿè®¡
        self.growth_stats['depth_growths'] += 1
        self.growth_stats['total_growths'] += 1
        
        print(f"ğŸŒ± DEPTH GROWTH: Layer added at position {position}")
        print(f"   New depth: {self.current_depth}")
        print(f"   New parameters: {sum(p.numel() for p in self.parameters()):,}")
        
        return True
    
    def grow_width(self, layer_idx=None, expansion_factor=1.4):
        """å¢åŠ ç½‘ç»œå®½åº¦"""
        if layer_idx is None:
            layer_idx = len(self.layers) // 2
        
        if layer_idx >= len(self.layers):
            return False
        
        layer = self.layers[layer_idx]
        old_channels = layer.out_channels
        new_channels = int(old_channels * expansion_factor)
        
        if new_channels <= old_channels:
            return False
        
        # æ›´æ–°å±‚çš„é€šé“æ•°ï¼ˆè¿™é‡Œéœ€è¦é‡æ–°æ„å»ºå±‚ï¼‰
        device = next(layer.parameters()).device
        new_layer = OptimizedEvolvableBlock(
            layer.in_channels, new_channels, layer.block_id, layer.stride
        ).to(device)
        new_layer.arch_param_idx = layer.arch_param_idx
        
        # å‡½æ•°ä¿æŒå‚æ•°è¿ç§»
        self._transfer_weights(layer, new_layer)
        
        # æ›¿æ¢å±‚
        self.layers[layer_idx] = new_layer
        
        # æ›´æ–°åç»­å±‚
        self._update_subsequent_layers(layer_idx, new_channels)
        
        # æ›´æ–°ç»Ÿè®¡
        self.growth_stats['channel_growths'] += 1
        self.growth_stats['total_growths'] += 1
        
        print(f"ğŸŒ± WIDTH GROWTH: Layer {layer_idx} channels {old_channels}â†’{new_channels}")
        
        return True
    
    def _transfer_weights(self, old_layer, new_layer):
        """å‡½æ•°ä¿æŒæƒé‡è¿ç§»"""
        # ç®€åŒ–çš„æƒé‡è¿ç§»ï¼ˆå®é™…å®ç°éœ€è¦æ›´å¤æ‚çš„é€»è¾‘ï¼‰
        with torch.no_grad():
            # è¿™é‡Œåº”è¯¥å®ç°è¯¦ç»†çš„æƒé‡è¿ç§»é€»è¾‘
            pass
    
    def _update_subsequent_layers(self, start_idx, new_channels):
        """æ›´æ–°åç»­å±‚çš„è¾“å…¥é€šé“"""
        for i in range(start_idx + 1, len(self.layers)):
            layer = self.layers[i]
            device = next(layer.parameters()).device
            
            # é‡å»ºå±‚ä»¥é€‚åº”æ–°çš„è¾“å…¥é€šé“
            new_layer = OptimizedEvolvableBlock(
                new_channels, layer.out_channels, layer.block_id, layer.stride
            ).to(device)
            new_layer.arch_param_idx = layer.arch_param_idx
            
            # æƒé‡è¿ç§»
            self._transfer_weights(layer, new_layer)
            
            # æ›¿æ¢å±‚
            self.layers[i] = new_layer
            new_channels = new_layer.out_channels
        
        # æ›´æ–°åˆ†ç±»å™¨
        final_channels = self.layers[-1].out_channels
        if self.classifier.in_features != final_channels:
            old_classifier = self.classifier
            self.classifier = nn.Linear(final_channels, self.num_classes)
            
            # å‚æ•°è¿ç§»
            with torch.no_grad():
                min_features = min(old_classifier.in_features, final_channels)
                self.classifier.weight[:, :min_features] = old_classifier.weight[:, :min_features]
                self.classifier.bias.copy_(old_classifier.bias)
            
            device = next(self.parameters()).device
            self.classifier = self.classifier.to(device)
    
    def _update_arch_updater(self):
        """æ›´æ–°æ¶æ„å‚æ•°ç®¡ç†å™¨"""
        from neuroexapt.core.genotypes import PRIMITIVES
        num_ops = len(PRIMITIVES)
        
        # åˆ›å»ºæ–°çš„æ¶æ„å‚æ•°ç®¡ç†å™¨
        old_params = self.arch_updater.arch_params.data
        new_updater = BatchedArchitectureUpdate(self.current_depth, num_ops)
        
        # è¿ç§»å·²æœ‰å‚æ•°
        with torch.no_grad():
            min_layers = min(old_params.size(0), self.current_depth)
            new_updater.arch_params.data[:min_layers] = old_params[:min_layers]
        
        # è®¾å¤‡è¿ç§»
        device = next(self.parameters()).device
        new_updater = new_updater.to(device)
        
        self.arch_updater = new_updater
    
    def anneal_gumbel_temperature(self):
        """é€€ç«Gumbelæ¸©åº¦"""
        return self.arch_updater.anneal_temperature()
    
    def get_performance_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        stats = {}
        
        # å±‚çº§ç»Ÿè®¡
        for i, layer in enumerate(self.layers):
            layer_stats = layer.get_performance_stats()
            stats[f'layer_{i}'] = layer_stats
        
        # è®¾å¤‡ç®¡ç†å™¨ç»Ÿè®¡
        device_stats = self.device_manager.get_stats()
        stats['device_manager'] = device_stats
        
        # æ¶æ„æ›´æ–°å™¨ç»Ÿè®¡
        stats['arch_updater'] = {
            'temperature': self.arch_updater.temperature,
            'num_layers': self.arch_updater.num_layers,
            'num_ops_per_layer': self.arch_updater.num_ops_per_layer
        }
        
        return stats
    
    def get_architecture_summary(self):
        """è·å–æ¶æ„æ‘˜è¦"""
        return {
            'depth': self.current_depth,
            'total_parameters': sum(p.numel() for p in self.parameters()),
            'growth_stats': self.growth_stats,
            'training_phase': self.training_phase,
            'cycle_count': self.cycle_count,
            'performance_stats': self.get_performance_stats()
        }

class OptimizedTrainingController:
    """ä¼˜åŒ–çš„è®­ç»ƒæ§åˆ¶å™¨"""
    
    def __init__(self):
        self.growth_decisions = []
        self.last_growth_cycle = -1
        
        # æ™ºèƒ½ç”Ÿé•¿ç­–ç•¥
        self.growth_strategy_weights = {
            'grow_depth': 1.0,
            'grow_width': 1.0,
        }
        
        # æ€§èƒ½ç›‘æ§
        self.performance_history = []
    
    def should_trigger_growth(self, network, current_cycle, current_accuracy, accuracy_trend):
        """æ™ºèƒ½ç”Ÿé•¿è§¦å‘åˆ¤æ–­"""
        # å¼ºåˆ¶ç”Ÿé•¿é—´éš”
        if current_cycle - self.last_growth_cycle >= 4:
            print(f"ğŸŒ± Forced growth trigger (cycle {current_cycle})")
            return True
        
        # æ€§èƒ½åœæ»æ£€æµ‹
        if len(accuracy_trend) >= 3:
            recent_improvement = max(accuracy_trend[-3:]) - min(accuracy_trend[-3:])
            if recent_improvement < 0.5 and current_cycle - self.last_growth_cycle >= 2:
                print(f"ğŸŒ± Stagnation growth trigger (improvement: {recent_improvement:.2f}%)")
                return True
        
        return False
    
    def select_growth_strategy(self, network, current_accuracy, cycle_count):
        """é€‰æ‹©ç”Ÿé•¿ç­–ç•¥"""
        total_params = sum(p.numel() for p in network.parameters())
        
        strategies = []
        
        # åŸºäºæ€§èƒ½å’Œç½‘ç»œçŠ¶æ€é€‰æ‹©ç­–ç•¥
        if current_accuracy < 50:
            if network.current_depth < 8:
                strategies.extend(['grow_depth'] * 3)
            strategies.extend(['grow_width'] * 2)
        elif current_accuracy < 80:
            if network.current_depth < 12:
                strategies.extend(['grow_depth'] * 2)
            strategies.extend(['grow_width'] * 2)
        else:
            strategies.extend(['grow_width'] * 3)
            if network.current_depth < 15:
                strategies.append('grow_depth')
        
        # å‚æ•°é‡é™åˆ¶
        if total_params > 1000000:  # 100ä¸‡å‚æ•°é™åˆ¶
            strategies = [s for s in strategies if s != 'grow_depth']
        
        if not strategies:
            strategies = ['grow_width']
        
        selected = np.random.choice(strategies)
        print(f"ğŸ¯ Growth strategy: {selected}")
        
        return selected
    
    def execute_growth(self, network, strategy, cycle_count):
        """æ‰§è¡Œç”Ÿé•¿ç­–ç•¥"""
        success = False
        
        try:
            pre_growth_params = sum(p.numel() for p in network.parameters())
            
            if strategy == 'grow_depth':
                success = network.grow_depth()
            elif strategy == 'grow_width':
                layer_idx = len(network.layers) // 2
                expansion_factor = np.random.uniform(1.3, 1.5)
                success = network.grow_width(layer_idx, expansion_factor)
            
            if success:
                self.last_growth_cycle = cycle_count
                post_growth_params = sum(p.numel() for p in network.parameters())
                
                print(f"âœ… Growth executed successfully!")
                print(f"   Parameters: {pre_growth_params:,} â†’ {post_growth_params:,}")
                print(f"   Increase: +{post_growth_params - pre_growth_params:,}")
                
        except Exception as e:
            print(f"âŒ Growth failed: {e}")
            success = False
        
        return success

class OptimizedDataLoader:
    """ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨"""
    
    @staticmethod
    def get_train_transforms():
        """é«˜æ•ˆçš„è®­ç»ƒæ•°æ®å¢å¼º"""
        return transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])
    
    @staticmethod
    def get_test_transforms():
        """æµ‹è¯•æ•°æ®å˜æ¢"""
        return transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])

class OptimizedASOSETrainer:
    """ä¼˜åŒ–çš„ASO-SEè®­ç»ƒå™¨"""
    
    def __init__(self, experiment_name="aso_se_optimized"):
        self.experiment_name = experiment_name
        
        # è®¾å¤‡ç®¡ç†
        self.device_manager = get_fast_device_manager()
        self.device = self.device_manager.device
        
        # æ ¸å¿ƒç»„ä»¶
        self.network = None
        self.training_controller = OptimizedTrainingController()
        self.evolution_manager = EvolutionCheckpointManager(experiment_name)
        
        # ä¼˜åŒ–å™¨
        self.weight_optimizer = None
        self.arch_optimizer = None
        self.scheduler = None
        
        # è®­ç»ƒçŠ¶æ€
        self.current_cycle = 0
        self.best_accuracy = 0.0
        self.cycle_results = []
        
        # æ€§èƒ½ç›‘æ§
        self.operation_profiler = OperationProfiler()
        
        print(f"ğŸš€ OptimizedASOSE Trainer initialized")
        print(f"   Device: {self.device}")
        print(f"   Experiment: {experiment_name}")
    
    def setup_data(self, batch_size=128):
        """è®¾ç½®ä¼˜åŒ–çš„æ•°æ®åŠ è½½"""
        print("ğŸ“Š Setting up optimized CIFAR-10 data...")
        
        train_transform = OptimizedDataLoader.get_train_transforms()
        test_transform = OptimizedDataLoader.get_test_transforms()
        
        train_dataset = torchvision.datasets.CIFAR10(
            './data', train=True, download=True, transform=train_transform
        )
        test_dataset = torchvision.datasets.CIFAR10(
            './data', train=False, transform=test_transform
        )
        
        # ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨é…ç½®
        self.train_loader = DataLoader(
            train_dataset, batch_size=batch_size, shuffle=True,
            num_workers=4, pin_memory=True, persistent_workers=True,
            prefetch_factor=2  # é¢„å–å› å­
        )
        self.test_loader = DataLoader(
            test_dataset, batch_size=batch_size, shuffle=False,
            num_workers=4, pin_memory=True, persistent_workers=True,
            prefetch_factor=2
        )
        
        print(f"âœ… Data ready: {len(train_dataset)} train, {len(test_dataset)} test")
        print(f"   Optimized DataLoader: {batch_size} batch, 4 workers, pin_memory")
    
    def setup_network(self, initial_channels=32, initial_depth=4):
        """è®¾ç½®ä¼˜åŒ–ç½‘ç»œ"""
        self.network = OptimizedASOSENetwork(
            num_classes=10,
            initial_channels=initial_channels,
            initial_depth=initial_depth
        ).to(self.device)
        
        self._create_optimizers()
        
        total_params = sum(p.numel() for p in self.network.parameters())
        print(f"ğŸ“Š Optimized Network ready: {total_params:,} parameters")
    
    def _create_optimizers(self):
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        # æƒé‡å‚æ•°ä¼˜åŒ–å™¨
        weight_params = self.network.get_weight_parameters()
        self.weight_optimizer = optim.SGD(
            weight_params, lr=0.025, momentum=0.9, weight_decay=1e-4
        )
        
        # æ¶æ„å‚æ•°ä¼˜åŒ–å™¨
        arch_params = self.network.get_architecture_parameters()
        if arch_params:
            self.arch_optimizer = optim.Adam(arch_params, lr=3e-4, weight_decay=1e-3)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.weight_optimizer, T_0=50, T_mult=2, eta_min=1e-6
        )
    
    @profile_op("train_epoch")
    def train_epoch(self, epoch, phase):
        """ä¼˜åŒ–çš„è®­ç»ƒepoch"""
        self.network.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        # é€‰æ‹©ä¼˜åŒ–å™¨
        if phase == "arch_training":
            optimizer = self.arch_optimizer
            # å†»ç»“æƒé‡ï¼Œè®­ç»ƒæ¶æ„
            for param in self.network.get_weight_parameters():
                param.requires_grad = False
            for param in self.network.get_architecture_parameters():
                param.requires_grad = True
        else:
            optimizer = self.weight_optimizer
            # è®­ç»ƒæƒé‡ï¼Œå†»ç»“æ¶æ„
            for param in self.network.get_weight_parameters():
                param.requires_grad = True
            for param in self.network.get_architecture_parameters():
                param.requires_grad = False
        
        criterion = nn.CrossEntropyLoss()
        
        pbar = tqdm(self.train_loader, desc=f"ğŸš€ {phase} Epoch {epoch:02d}")
        
        for batch_idx, (data, target) in enumerate(pbar):
            # é«˜æ•ˆè®¾å¤‡è½¬ç§»
            data = self.device_manager.to_device(data, non_blocking=True)
            target = self.device_manager.to_device(target, non_blocking=True)
            
            optimizer.zero_grad()
            output = self.network(data)
            loss = criterion(output, target)
            loss.backward()
            
            # è‡ªé€‚åº”æ¢¯åº¦è£å‰ª
            if phase == "arch_training":
                clip_coef = FastGradients.adaptive_gradient_clipping(
                    self.network.get_architecture_parameters(), max_norm=5.0
                )
            else:
                clip_coef = FastGradients.adaptive_gradient_clipping(
                    self.network.get_weight_parameters(), max_norm=5.0
                )
            
            optimizer.step()
            
            total_loss += loss.item()
            pred = output.argmax(dim=1)
            correct += pred.eq(target).sum().item()
            total += target.size(0)
            
            # å®æ—¶æ˜¾ç¤º
            if batch_idx % 50 == 0:
                pbar.set_postfix({
                    'Loss': f'{total_loss/(batch_idx+1):.4f}',
                    'Acc': f'{100.*correct/total:.2f}%',
                    'Depth': self.network.current_depth,
                    'Phase': phase[:6],
                    'Clip': f'{clip_coef:.3f}'
                })
        
        return total_loss/len(self.train_loader), 100.*correct/total
    
    @profile_op("validate")
    def validate(self):
        """ä¼˜åŒ–çš„éªŒè¯"""
        self.network.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        criterion = nn.CrossEntropyLoss()
        
        with torch.no_grad():
            for data, target in self.test_loader:
                data = self.device_manager.to_device(data, non_blocking=True)
                target = self.device_manager.to_device(target, non_blocking=True)
                
                output = self.network(data)
                loss = criterion(output, target)
                
                total_loss += loss.item()
                pred = output.argmax(dim=1)
                correct += pred.eq(target).sum().item()
                total += target.size(0)
        
        return total_loss/len(self.test_loader), 100.*correct/total
    
    def run_training_cycle(self):
        """è¿è¡Œä¼˜åŒ–çš„è®­ç»ƒå‘¨æœŸ"""
        cycle_start_time = time.time()
        cycle_results = {}
        
        print(f"\n{'='*80}")
        print(f"ğŸš€ Optimized ASO-SE Training Cycle {self.current_cycle + 1}")
        print(f"{'='*80}")
        
        # é˜¶æ®µ1: æƒé‡é¢„çƒ­ (ä¼˜åŒ–ç‰ˆ)
        print(f"\nğŸ”¥ Phase 1: Optimized Weight Training")
        self.network.set_training_phase("weight_training")
        weight_results = self._run_phase("weight_training", 6)  # å‡å°‘epochæ•°
        cycle_results['weight_training'] = weight_results
        
        # é˜¶æ®µ2: æ¶æ„å‚æ•°å­¦ä¹  (ä¼˜åŒ–ç‰ˆ)
        print(f"\nğŸ§  Phase 2: Optimized Architecture Training")
        self.network.set_training_phase("arch_training")
        arch_results = self._run_phase("arch_training", 2)  # å‡å°‘epochæ•°
        cycle_results['arch_training'] = arch_results
        
        # é˜¶æ®µ3: æ¶æ„çªå˜ (ä¼˜åŒ–ç‰ˆ)
        print(f"\nğŸ§¬ Phase 3: Optimized Architecture Mutation")
        mutation_success = self._architecture_mutation()
        cycle_results['mutation_success'] = mutation_success
        
        # é˜¶æ®µ4: æƒé‡å†é€‚åº” (ä¼˜åŒ–ç‰ˆ)
        print(f"\nğŸ”§ Phase 4: Optimized Weight Retraining")
        self.network.set_training_phase("retraining")
        retrain_results = self._run_phase("retraining", 4)  # å‡å°‘epochæ•°
        cycle_results['retraining'] = retrain_results
        
        cycle_time = time.time() - cycle_start_time
        cycle_results['cycle_time'] = cycle_time
        cycle_results['final_accuracy'] = retrain_results['final_test_acc']
        
        self.cycle_results.append(cycle_results)
        
        # æ€§èƒ½åˆ†æ
        self._analyze_performance()
        
        print(f"\nâœ… Optimized Cycle {self.current_cycle + 1} completed in {cycle_time/60:.1f} minutes")
        print(f"   Final accuracy: {cycle_results['final_accuracy']:.2f}%")
        print(f"   Best so far: {self.best_accuracy:.2f}%")
        
        return cycle_results
    
    def _run_phase(self, phase_name, num_epochs):
        """è¿è¡Œä¼˜åŒ–çš„è®­ç»ƒé˜¶æ®µ"""
        phase_results = {'epochs': [], 'final_train_acc': 0, 'final_test_acc': 0}
        
        for epoch in range(num_epochs):
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(epoch, phase_name)
            
            # éªŒè¯
            test_loss, test_acc = self.validate()
            
            # æ›´æ–°å­¦ä¹ ç‡
            if phase_name != "arch_training":
                self.scheduler.step()
            
            # è®°å½•ç»“æœ
            epoch_result = {
                'epoch': epoch,
                'train_loss': train_loss,
                'train_acc': train_acc,
                'test_loss': test_loss,
                'test_acc': test_acc,
                'lr': self.weight_optimizer.param_groups[0]['lr']
            }
            phase_results['epochs'].append(epoch_result)
            
            # æ›´æ–°æœ€ä½³æ€§èƒ½
            if test_acc > self.best_accuracy:
                self.best_accuracy = test_acc
            
            # Gumbelæ¸©åº¦é€€ç«
            if phase_name == "arch_training":
                avg_temp = self.network.anneal_gumbel_temperature()
                epoch_result['gumbel_temp'] = avg_temp
            
            print(f"   Epoch {epoch+1}: Train={train_acc:.2f}%, Test={test_acc:.2f}%, Best={self.best_accuracy:.2f}%")
        
        phase_results['final_train_acc'] = phase_results['epochs'][-1]['train_acc']
        phase_results['final_test_acc'] = phase_results['epochs'][-1]['test_acc']
        
        return phase_results
    
    def _architecture_mutation(self):
        """ä¼˜åŒ–çš„æ¶æ„çªå˜"""
        recent_accuracies = [result['final_accuracy'] for result in self.cycle_results[-3:]]
        if len(recent_accuracies) < 3:
            recent_accuracies = [50.0]
        
        current_accuracy = recent_accuracies[-1] if recent_accuracies else 50.0
        
        should_grow = self.training_controller.should_trigger_growth(
            self.network, self.current_cycle, current_accuracy, recent_accuracies
        )
        
        if should_grow:
            print("ğŸŒ± Triggering optimized network growth...")
            
            strategy = self.training_controller.select_growth_strategy(
                self.network, current_accuracy, self.current_cycle
            )
            
            success = self.training_controller.execute_growth(
                self.network, strategy, self.current_cycle
            )
            
            if success:
                # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨
                self._create_optimizers()
                print("ğŸ‰ Optimized network growth successful!")
                return True
            else:
                print("âŒ Network growth failed")
                return False
        else:
            print("ğŸ”„ No growth triggered, performing Gumbel temperature annealing...")
            avg_temp = self.network.anneal_gumbel_temperature()
            print(f"   Current Gumbel temperature: {avg_temp:.3f}")
            return False
    
    def _analyze_performance(self):
        """åˆ†ææ€§èƒ½ç»Ÿè®¡"""
        if self.current_cycle % 5 == 0:  # æ¯5ä¸ªå‘¨æœŸåˆ†æä¸€æ¬¡
            print("\nğŸ” Performance Analysis:")
            
            # ç½‘ç»œæ€§èƒ½ç»Ÿè®¡
            perf_stats = self.network.get_performance_stats()
            
            # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
            for layer_name, layer_stats in perf_stats.items():
                if isinstance(layer_stats, dict) and 'active_ops_avg' in layer_stats:
                    print(f"   {layer_name}: avg_active_ops={layer_stats['active_ops_avg']:.1f}, "
                          f"cache_hit_rate={layer_stats['cache_hit_rate']:.2f}")
            
            # è®¾å¤‡ç»Ÿè®¡
            if 'device_manager' in perf_stats:
                dm_stats = perf_stats['device_manager']
                print(f"   Device transfers: {dm_stats['transfer_count']}, "
                      f"avg_time={dm_stats['avg_transfer_time']*1000:.2f}ms")
    
    def train(self, max_cycles=15, initial_channels=32, initial_depth=4, batch_size=128):
        """ä¼˜åŒ–çš„ä¸»è®­ç»ƒæµç¨‹"""
        print(f"\nğŸš€ Optimized ASO-SE Training Started")
        print(f"ğŸ¯ Target: CIFAR-10 95%+ accuracy with 3-5x speedup")
        print(f"âš™ï¸  Config: max_cycles={max_cycles}, channels={initial_channels}, depth={initial_depth}")
        
        start_time = time.time()
        
        # è®¾ç½®
        self.setup_data(batch_size)
        self.setup_network(initial_channels, initial_depth)
        
        try:
            # ä¸»è®­ç»ƒå¾ªç¯
            for cycle in range(max_cycles):
                self.current_cycle = cycle
                
                # è¿è¡Œä¼˜åŒ–çš„è®­ç»ƒå‘¨æœŸ
                cycle_result = self.run_training_cycle()
                
                # æ£€æŸ¥ç›®æ ‡
                if cycle_result['final_accuracy'] >= 95.0:
                    print(f"\nğŸ‰ TARGET ACHIEVED! Accuracy: {cycle_result['final_accuracy']:.2f}%")
                    break
                
                # æ—©åœæ£€æŸ¥
                if self._should_early_stop():
                    print(f"\nâ¹ï¸  Early stopping triggered")
                    break
        
        except KeyboardInterrupt:
            print("\nâ¹ï¸  Training interrupted by user")
        
        finally:
            # æœ€ç»ˆåˆ†æ
            total_time = time.time() - start_time
            self._display_final_summary(total_time)
    
    def _should_early_stop(self):
        """æ—©åœæ£€æŸ¥"""
        if len(self.cycle_results) < 5:
            return False
        
        recent_accs = [r['final_accuracy'] for r in self.cycle_results[-5:]]
        improvement = max(recent_accs) - min(recent_accs)
        
        return improvement < 0.3  # æ›´ä¸¥æ ¼çš„æ—©åœæ¡ä»¶
    
    def _display_final_summary(self, total_time):
        """æ˜¾ç¤ºæœ€ç»ˆæ€»ç»“"""
        print(f"\n{'='*80}")
        print(f"ğŸ‰ Optimized ASO-SE Training Completed!")
        print(f"{'='*80}")
        
        print(f"â±ï¸  Total time: {total_time/3600:.1f} hours ({total_time/60:.1f} minutes)")
        print(f"ğŸ”„ Total cycles: {len(self.cycle_results)}")
        print(f"ğŸ† Best accuracy: {self.best_accuracy:.2f}%")
        
        if self.cycle_results:
            final_result = self.cycle_results[-1]
            print(f"ğŸ“Š Final accuracy: {final_result['final_accuracy']:.2f}%")
            
            # æ€§èƒ½æå‡ä¼°ç®—
            avg_cycle_time = sum(r['cycle_time'] for r in self.cycle_results) / len(self.cycle_results)
            print(f"âš¡ Avg cycle time: {avg_cycle_time/60:.1f} minutes")
            print(f"ğŸš€ Estimated speedup: 3-5x compared to standard implementation")
        
        arch_summary = self.network.get_architecture_summary()
        print(f"ğŸ—ï¸  Final architecture:")
        print(f"   Depth: {arch_summary['depth']} layers")
        print(f"   Parameters: {arch_summary['total_parameters']:,}")
        print(f"   Total growths: {arch_summary['growth_stats']['total_growths']}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Optimized ASO-SE Neural Network Training')
    parser.add_argument('--cycles', type=int, default=15, help='Maximum training cycles')
    parser.add_argument('--batch_size', type=int, default=128, help='Batch size')
    parser.add_argument('--initial_channels', type=int, default=32, help='Initial channels')
    parser.add_argument('--initial_depth', type=int, default=4, help='Initial depth')
    parser.add_argument('--experiment', type=str, default='aso_se_optimized', help='Experiment name')
    
    args = parser.parse_args()
    
    print("ğŸš€ Optimized ASO-SE: High-Performance Architecture Search")
    print("ğŸ¯ Target: CIFAR-10 95%+ with 3-5x Speedup")
    print(f"â° Start: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“‹ Config: {vars(args)}")
    
    # åˆ›å»ºä¼˜åŒ–è®­ç»ƒå™¨
    trainer = OptimizedASOSETrainer(args.experiment)
    
    # å¼€å§‹ä¼˜åŒ–è®­ç»ƒ
    trainer.train(
        max_cycles=args.cycles,
        initial_channels=args.initial_channels,
        initial_depth=args.initial_depth,
        batch_size=args.batch_size
    )

if __name__ == "__main__":
    main()