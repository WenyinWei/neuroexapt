#!/usr/bin/env python3
"""
æ™ºèƒ½æ¶æ„è¿›åŒ–æ¼”ç¤º - CIFAR-10å®æˆ˜ç‰ˆ
Intelligent Architecture Evolution Demo - CIFAR-10 Edition

ğŸ”¬ åŸºäºäº’ä¿¡æ¯å’Œè´å¶æ–¯æ¨æ–­çš„ç¥ç»ç½‘ç»œæ¶æ„è‡ªé€‚åº”å˜å¼‚ç³»ç»Ÿ

ğŸ§¬ æ¼”ç¤ºå†…å®¹ï¼š
1. åŸºäºMINEçš„äº’ä¿¡æ¯ä¼°è®¡ - é‡åŒ–ç‰¹å¾ä¸ç›®æ ‡çš„ä¿¡æ¯ä¾èµ–
2. è´å¶æ–¯ä¸ç¡®å®šæ€§é‡åŒ– - è¯„ä¼°ç‰¹å¾è¡¨å¾çš„ç¨³å®šæ€§
3. æ™ºèƒ½ç“¶é¢ˆæ£€æµ‹ - ç²¾ç¡®å®šä½ç½‘ç»œæ€§èƒ½é™åˆ¶ç‚¹
4. åŸºäºç“¶é¢ˆçš„æ™ºèƒ½å˜å¼‚è§„åˆ’ - 15ç§å˜å¼‚ç­–ç•¥ç²¾ç¡®åŒ¹é…
5. å…ˆè¿›Net2Netå‚æ•°è¿ç§» - ä¿è¯åŠŸèƒ½ç­‰ä»·æ€§çš„å¹³æ»‘è¿ç§»
6. å®Œæ•´æ¶æ„è¿›åŒ–æµç¨‹ - æ£€æµ‹â†’è§„åˆ’â†’è¿ç§»â†’è¯„ä¼°â†’è¿­ä»£

ğŸ¯ ç›®æ ‡ï¼šåœ¨CIFAR-10ä¸Šå±•ç¤ºæ™ºèƒ½æ¶æ„è¿›åŒ–çš„å®Œæ•´æµç¨‹
ğŸ”¬ ç†è®ºåŸºç¡€ï¼šå°†æŠ½è±¡çš„"å¤©èµ‹ä¸Šé™"è½¬åŒ–ä¸ºå¯è®¡ç®—çš„äº’ä¿¡æ¯å’Œä¸ç¡®å®šæ€§æŒ‡æ ‡
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import numpy as np
import time
import matplotlib.pyplot as plt
from collections import defaultdict
import logging
import os
import sys

# å¯¼å…¥æ–°çš„æ™ºèƒ½æ¶æ„è¿›åŒ–ç»„ä»¶
from neuroexapt.core import (
    IntelligentArchitectureEvolutionEngine,
    EvolutionConfig,
    MutualInformationEstimator,
    BayesianUncertaintyEstimator,
    IntelligentBottleneckDetector,
    IntelligentMutationPlanner,
    AdvancedNet2NetTransfer
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class EvolvableResNet(nn.Module):
    """å¯è¿›åŒ–çš„ResNetæ¶æ„ - ç”¨äºæ™ºèƒ½è¿›åŒ–æ¼”ç¤º"""
    
    def __init__(self, num_classes=10):
        super(EvolvableResNet, self).__init__()
        
        # åˆå§‹ç‰¹å¾æå–
        self.stem = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )
        
        # å¯è¿›åŒ–çš„ç‰¹å¾æå–å±‚
        self.layer1 = self._make_layer(64, 64, 2, stride=1)
        self.layer2 = self._make_layer(64, 128, 2, stride=2)
        self.layer3 = self._make_layer(128, 256, 2, stride=2)
        self.layer4 = self._make_layer(256, 512, 2, stride=2)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
        
    def _make_layer(self, in_channels, out_channels, blocks, stride=1):
        """åˆ›å»ºResNetå±‚"""
        layers = []
        
        # ç¬¬ä¸€ä¸ªå—å¯èƒ½æœ‰é™é‡‡æ ·
        layers.append(BasicBlock(in_channels, out_channels, stride))
        
        # åç»­å—
        for _ in range(1, blocks):
            layers.append(BasicBlock(out_channels, out_channels, 1))
            
        return nn.Sequential(*layers)
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        x = self.stem(x)
        
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        
        x = self.avgpool(x)
        x = self.classifier(x)
        
        return x


class BasicBlock(nn.Module):
    """åŸºç¡€æ®‹å·®å—"""
    
    def __init__(self, in_channels, out_channels, stride=1):
        super(BasicBlock, self).__init__()
        
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, 
                              stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,
                              stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        # è·³è·ƒè¿æ¥
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1,
                         stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
    
    def forward(self, x):
        residual = self.shortcut(x)
        
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        
        out += residual
        out = self.relu(out)
        
        return out


class IntelligentEvolutionTrainer:
    """æ™ºèƒ½è¿›åŒ–è®­ç»ƒå™¨"""
    
    def __init__(self, model, device, train_loader, test_loader):
        self.model = model.to(device)
        self.device = device
        self.train_loader = train_loader
        self.test_loader = test_loader
        
        # è®­ç»ƒå†å²
        self.train_history = []
        self.test_history = []
        self.evolution_history = []
        
        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self.optimizer = None
        self.scheduler = None
        
    def setup_optimizer(self, learning_rate=0.1):
        """è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        self.optimizer = optim.SGD(
            self.model.parameters(),
            lr=learning_rate,
            momentum=0.9,
            weight_decay=5e-4,
            nesterov=True
        )
        
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=200, eta_min=1e-6
        )
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(self.train_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            self.optimizer.zero_grad()
            output = self.model(data)
            loss = F.cross_entropy(output, target)
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
            
            if batch_idx % 100 == 0:
                logger.info(f'Epoch {epoch}, Batch {batch_idx}, '
                           f'Loss: {loss.item():.6f}, '
                           f'Acc: {100.*correct/total:.2f}%')
        
        avg_loss = total_loss / len(self.train_loader)
        accuracy = 100. * correct / total
        
        self.train_history.append({'epoch': epoch, 'loss': avg_loss, 'accuracy': accuracy})
        return avg_loss, accuracy
    
    def test(self):
        """æµ‹è¯•æ¨¡å‹æ€§èƒ½"""
        self.model.eval()
        test_loss = 0
        correct = 0
        
        with torch.no_grad():
            for data, target in self.test_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                test_loss += F.cross_entropy(output, target, reduction='sum').item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
        
        test_loss /= len(self.test_loader.dataset)
        accuracy = 100. * correct / len(self.test_loader.dataset)
        
        return test_loss, accuracy
    
    def extract_features_and_labels(self, model, data_loader, max_batches=3, save_to_disk=False):
        """æå–æ¨¡å‹ç‰¹å¾å’Œæ ‡ç­¾ç”¨äºæ™ºèƒ½åˆ†æ
        
        Args:
            save_to_disk: æ˜¯å¦å°†ç‰¹å¾ä¿å­˜åˆ°ç£ç›˜ä»¥èŠ‚çœå†…å­˜
        """
        feature_dict = {}
        all_labels = []
        
        # å¦‚æœéœ€è¦ä¿å­˜åˆ°ç£ç›˜ï¼Œåˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = None
        if save_to_disk:
            import tempfile
            temp_dir = tempfile.mkdtemp()
            logger.info(f"åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºç‰¹å¾å­˜å‚¨: {temp_dir}")
        
        # æ³¨å†Œhookæ”¶é›†ç‰¹å¾
        def get_hook(name):
            def hook(module, input, output):
                if isinstance(output, torch.Tensor):
                    if save_to_disk:
                        # ä¿å­˜åˆ°ç£ç›˜
                        if name not in feature_dict:
                            feature_dict[name] = []
                        # ä¿å­˜æ–‡ä»¶è·¯å¾„è€Œä¸æ˜¯å¼ é‡
                        filepath = f"{temp_dir}/{name}_{len(feature_dict[name])}.pt"
                        torch.save(output.detach().cpu(), filepath)
                        feature_dict[name].append(filepath)
                    else:
                        # ä¿å­˜åˆ°å†…å­˜ï¼ˆåŸå§‹æ–¹å¼ï¼‰
                        if name not in feature_dict:
                            feature_dict[name] = []
                        feature_dict[name].append(output.detach().cpu())
            return hook
        
        # ä¸ºä¸»è¦å±‚æ³¨å†Œhook
        hooks = []
        for name, module in model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)) and any(
                keyword in name for keyword in ['layer', 'classifier', 'stem']
            ):
                hook = module.register_forward_hook(get_hook(name))
                hooks.append(hook)
        
        model.eval()
        with torch.no_grad():
            for batch_idx, (data, target) in enumerate(data_loader):
                if batch_idx >= max_batches:
                    break
                    
                data = data.to(self.device)
                _ = model(data)
                all_labels.append(target)
                
                # åŠæ—¶æ¸…ç†GPUå†…å­˜
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        # æ¸…ç†hooks
        for hook in hooks:
            hook.remove()
        
        # æ‹¼æ¥æ ‡ç­¾
        if all_labels:
            all_labels = torch.cat(all_labels, dim=0)
        else:
            return None
        
        # å¤„ç†ç‰¹å¾æ•°æ®
        if save_to_disk:
            # è¿”å›ç£ç›˜æ–‡ä»¶ä¿¡æ¯å’ŒåŠ è½½å‡½æ•°
            def load_feature(name):
                """æŒ‰éœ€åŠ è½½ç‰¹å¾"""
                if name in feature_dict:
                    features = []
                    for filepath in feature_dict[name]:
                        features.append(torch.load(filepath))
                    result = torch.cat(features, dim=0)
                    # åŠ è½½åç«‹å³åˆ é™¤æ–‡ä»¶ä»¥èŠ‚çœç£ç›˜ç©ºé—´
                    for filepath in feature_dict[name]:
                        import os
                        if os.path.exists(filepath):
                            os.remove(filepath)
                    return result
                return None
            
            layer_names = list(feature_dict.keys())
            return (feature_dict, all_labels, layer_names, load_feature, temp_dir)
        else:
            # åŸå§‹æ–¹å¼ï¼šç›´æ¥æ‹¼æ¥ç‰¹å¾
            final_features = {}
            for name, feature_list in feature_dict.items():
                if feature_list:
                    final_features[name] = torch.cat(feature_list, dim=0)
            
            layer_names = list(final_features.keys())
            return (final_features, all_labels, layer_names)
        
    def cleanup_temp_features(self, temp_dir):
        """æ¸…ç†ä¸´æ—¶ç‰¹å¾æ–‡ä»¶"""
        if temp_dir:
            import shutil
            try:
                shutil.rmtree(temp_dir)
                logger.info(f"æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir}")
            except Exception as e:
                logger.warning(f"æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")


def prepare_cifar10_data(batch_size_train=128, batch_size_test=100):
    """å‡†å¤‡CIFAR-10æ•°æ®"""
    logger.info("å‡†å¤‡CIFAR-10æ•°æ®é›†...")
    
    # è®­ç»ƒæ—¶æ•°æ®å¢å¼º
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(degrees=10),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        transforms.RandomErasing(p=0.1, scale=(0.02, 0.33))
    ])
    
    # æµ‹è¯•æ—¶æ ‡å‡†åŒ–
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform_train
    )
    
    test_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=False, download=True, transform=transform_test
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset, batch_size=batch_size_train, shuffle=True, 
        num_workers=2, pin_memory=True
    )
    
    test_loader = DataLoader(
        test_dataset, batch_size=batch_size_test, shuffle=False,
        num_workers=2, pin_memory=True
    )
    
    logger.info(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}, æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")
    return train_loader, test_loader


def demo_mutual_information_analysis(trainer):
    """æ¼”ç¤ºï¼šäº’ä¿¡æ¯åˆ†æ"""
    print("="*60)
    print("ğŸ”— æ¼”ç¤ºï¼šäº’ä¿¡æ¯åˆ†æ")
    print("="*60)
    
    # æ£€æŸ¥è®­ç»ƒè¿›åº¦ - åªæœ‰åœ¨æ¨¡å‹æœ‰ä¸€å®šæ€§èƒ½æ—¶æ‰è¿›è¡Œåˆ†æ
    model = trainer.model
    device = trainer.device
    
    # è¯„ä¼°å½“å‰æ€§èƒ½
    _, current_accuracy = trainer.evaluate()
    print(f"ğŸ“Š å½“å‰æ¨¡å‹å‡†ç¡®ç‡: {current_accuracy:.2f}%")
    
    # å¦‚æœå‡†ç¡®ç‡å¤ªä½ï¼Œè·³è¿‡å¤æ‚åˆ†æ
    if current_accuracy < 30.0:
        print("âš ï¸  æ¨¡å‹å‡†ç¡®ç‡è¿‡ä½ï¼Œè·³è¿‡äº’ä¿¡æ¯åˆ†æä»¥èŠ‚çœè®¡ç®—èµ„æº")
        print("ğŸ’¡ å»ºè®®ï¼šåœ¨æ¨¡å‹æ”¶æ•›åˆ°30%ä»¥ä¸Šå‡†ç¡®ç‡åå†è¿›è¡Œæ™ºèƒ½åˆ†æ")
        return {}
    
    print("ğŸ¯ æ¨¡å‹æ€§èƒ½è¶³å¤Ÿï¼Œå¼€å§‹äº’ä¿¡æ¯åˆ†æ...")
    
    # æå–ç‰¹å¾ï¼ˆä½¿ç”¨æ›´å°çš„æ‰¹æ¬¡ä»¥èŠ‚çœå†…å­˜ï¼‰
    features_and_labels = trainer.extract_features_and_labels(
        model, trainer.test_loader, max_batches=2  # å‡å°‘æ‰¹æ¬¡æ•°
    )
    
    if not features_and_labels:
        print("âŒ ç‰¹å¾æå–å¤±è´¥")
        return {}
        
    features, labels, layer_names = features_and_labels
    print(f"âœ… æå–åˆ° {len(layer_names)} ä¸ªç‰¹å¾å±‚: {layer_names}")
    
    # åˆ›å»ºäº’ä¿¡æ¯ä¼°è®¡å™¨
    mi_estimator = MutualInformationEstimator(device=device)
    
    print("ğŸ”— å¼€å§‹è®¡ç®—äº’ä¿¡æ¯...")
    mi_results = {}
    
    # åªåˆ†æå…³é”®å±‚ä»¥èŠ‚çœè®¡ç®—èµ„æº
    key_layers = [name for name in layer_names if any(
        keyword in name for keyword in ['layer1.0', 'layer2.0', 'layer3.0', 'layer4.0', 'classifier']
    )]
    
    print(f"ğŸ¯ åˆ†æ {len(key_layers)} ä¸ªå…³é”®å±‚: {key_layers}")
    
    for layer_name in key_layers:
        if layer_name in features:
            try:
                layer_features = features[layer_name]
                layer_labels = labels  # ä¿æŒåŸå§‹labelsï¼Œä¸åšé™åˆ¶
                
                # è®¡ç®—äº’ä¿¡æ¯ï¼ˆä½¿ç”¨åŸå§‹batch_sizeï¼‰
                mi_value = mi_estimator.estimate_layerwise_mi(
                    layer_features, layer_labels, layer_name, 
                    num_classes=10, num_epochs=30  # å‡å°‘è®­ç»ƒè½®æ•°
                )
                mi_results[layer_name] = mi_value
                
                # æ¸…ç†å½“å‰å±‚çš„ç‰¹å¾ä»¥é‡Šæ”¾å†…å­˜
                del layer_features
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                
            except Exception as e:
                print(f"âš ï¸  å±‚ {layer_name} äº’ä¿¡æ¯è®¡ç®—å¤±è´¥: {e}")
                mi_results[layer_name] = 0.0
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nğŸ”— äº’ä¿¡æ¯ç»“æœ I(H; Y):")
    for layer_name, mi_value in mi_results.items():
        status = "âœ“ æ­£å¸¸" if mi_value > 1.0 else "âš ï¸  åä½"
        print(f"  {layer_name}: {mi_value:.4f} ({status})")
    
    return mi_results


def demo_uncertainty_analysis(trainer):
    """æ¼”ç¤ºï¼šè´å¶æ–¯ä¸ç¡®å®šæ€§åˆ†æ"""
    print("\n" + "="*60)
    print("ğŸ² æ¼”ç¤ºï¼šè´å¶æ–¯ä¸ç¡®å®šæ€§åˆ†æ")
    print("="*60)
    
         # æ£€æŸ¥è®­ç»ƒè¿›åº¦
     _, current_accuracy = trainer.evaluate()
     if current_accuracy < 30.0:
        print("âš ï¸  æ¨¡å‹å‡†ç¡®ç‡è¿‡ä½ï¼Œè·³è¿‡ä¸ç¡®å®šæ€§åˆ†æ")
        return {}
    
    # æå–ç‰¹å¾
    features_and_labels = trainer.extract_features_and_labels(
        trainer.model, trainer.test_loader, max_batches=2
    )
    
    if not features_and_labels:
        print("âŒ ç‰¹å¾æå–å¤±è´¥")
        return {}
        
    features, labels, layer_names = features_and_labels
    print(f"âœ… æå–åˆ° {len(layer_names)} ä¸ªç‰¹å¾å±‚: {layer_names}")
    
    # åˆ›å»ºä¸ç¡®å®šæ€§ä¼°è®¡å™¨
    uncertainty_estimator = BayesianUncertaintyEstimator(device=trainer.device)
    
    print(f"ğŸ“Š å¼€å§‹è®¡ç®— {len(layer_names)} ä¸ªå±‚çš„ä¸ç¡®å®šæ€§...")
    uncertainty_results = {}
    
    # åªåˆ†æå…³é”®å±‚
    key_layers = [name for name in layer_names if any(
        keyword in name for keyword in ['layer2.0', 'layer3.0', 'layer4.0', 'classifier']
    )]
    
    for layer_name in key_layers:
        if layer_name in features:
            try:
                layer_features = features[layer_name]
                layer_labels = labels  # ä¿æŒåŸå§‹labelså’Œbatch_size
                
                uncertainty = uncertainty_estimator.estimate_uncertainty(
                    layer_features, layer_labels, layer_name, num_classes=10
                )
                uncertainty_results[layer_name] = uncertainty
                
                # æ¸…ç†å½“å‰å±‚ç‰¹å¾
                del layer_features
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                
            except Exception as e:
                print(f"âš ï¸  å±‚ {layer_name} ä¸ç¡®å®šæ€§è®¡ç®—å¤±è´¥: {e}")
                uncertainty_results[layer_name] = 0.0
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nğŸ² ç‰¹å¾ä¸ç¡®å®šæ€§ç»“æœ U(H_k):")
    for layer_name, uncertainty in uncertainty_results.items():
        status = "âœ“ æ­£å¸¸" if uncertainty < 0.5 else "âš ï¸  è¿‡é«˜"
        print(f"  {layer_name}: {uncertainty:.4f} ({status})")
    
    return uncertainty_results


def demo_intelligent_bottleneck_detection(trainer):
    """æ¼”ç¤ºï¼šæ™ºèƒ½ç“¶é¢ˆæ£€æµ‹"""
    print("\n" + "="*60)
    print("ğŸ” æ¼”ç¤ºï¼šæ™ºèƒ½ç“¶é¢ˆæ£€æµ‹")
    print("="*60)
    
         # æ£€æŸ¥è®­ç»ƒè¿›åº¦
     _, current_accuracy = trainer.evaluate()
     if current_accuracy < 40.0:  # ç“¶é¢ˆæ£€æµ‹éœ€è¦æ›´é«˜çš„å‡†ç¡®ç‡
        print("âš ï¸  æ¨¡å‹å‡†ç¡®ç‡è¿‡ä½ï¼Œè·³è¿‡ç“¶é¢ˆæ£€æµ‹")
        print("ğŸ’¡ å»ºè®®ï¼šåœ¨æ¨¡å‹æ”¶æ•›åˆ°40%ä»¥ä¸Šå‡†ç¡®ç‡åå†è¿›è¡Œç“¶é¢ˆæ£€æµ‹")
        return []
    
    # æå–ç‰¹å¾
    features_and_labels = trainer.extract_features_and_labels(
        trainer.model, trainer.test_loader, max_batches=2
    )
    
    if not features_and_labels:
        print("âŒ ç‰¹å¾æå–å¤±è´¥")
        return []
        
    features, labels, layer_names = features_and_labels
    print(f"âœ… æå–åˆ° {len(layer_names)} ä¸ªç‰¹å¾å±‚: {layer_names}")
    
    # åˆ›å»ºç“¶é¢ˆæ£€æµ‹å™¨
    detector = IntelligentBottleneckDetector(
        device=trainer.device,
        confidence_threshold=0.7  # æé«˜ç½®ä¿¡åº¦é˜ˆå€¼
    )
    
    print("ğŸ” å¼€å§‹æ™ºèƒ½ç“¶é¢ˆæ£€æµ‹...")
    
    # æ£€æµ‹ç“¶é¢ˆ
    try:
        bottleneck_reports = detector.detect_bottlenecks(
            features=features,
            labels=labels,
            layer_names=layer_names,
            num_classes=10
        )
        
        print(f"ğŸ“Š æ£€æµ‹ç»“æœ: å‘ç° {len(bottleneck_reports)} ä¸ªæ½œåœ¨ç“¶é¢ˆ")
        
        # æ˜¾ç¤ºå‰5ä¸ªæœ€ä¸¥é‡çš„ç“¶é¢ˆ
        print("ğŸ” ç“¶é¢ˆæ£€æµ‹æŠ¥å‘Š")
        print("="*50)
        
        for i, report in enumerate(bottleneck_reports[:5]):
            print(f"\nğŸ”´ #{i+1} å±‚: {report.layer_name}")
            print(f"   ç±»å‹: {report.bottleneck_type.value}")
            print(f"   ä¸¥é‡ç¨‹åº¦: {report.severity:.3f} | ç½®ä¿¡åº¦: {report.confidence:.3f}")
            print(f"   äº’ä¿¡æ¯: {report.mutual_info:.4f} | æ¡ä»¶äº’ä¿¡æ¯: {report.conditional_mutual_info:.4f}")
            print(f"   ä¸ç¡®å®šæ€§: {report.uncertainty:.4f}")
            print(f"   åŸå› : {report.explanation}")
            print(f"   å»ºè®®: {', '.join(report.suggested_mutations)}")
        
        if len(bottleneck_reports) > 5:
            print(f"\n... è¿˜æœ‰ {len(bottleneck_reports) - 5} ä¸ªç“¶é¢ˆæœªæ˜¾ç¤º")
        
        # ç»Ÿè®¡ä¿¡æ¯
        bottleneck_types = {}
        total_severity = 0
        for report in bottleneck_reports:
            bottleneck_types[report.bottleneck_type.value] = bottleneck_types.get(report.bottleneck_type.value, 0) + 1
            total_severity += report.severity
        
        avg_severity = total_severity / len(bottleneck_reports) if bottleneck_reports else 0
        
        print(f"\nğŸ“Š æ€»è®¡: {len(bottleneck_reports)} ä¸ªç“¶é¢ˆ | å¹³å‡ä¸¥é‡ç¨‹åº¦: {avg_severity:.3f}")
        print(f"ğŸ“ˆ ç“¶é¢ˆç±»å‹åˆ†å¸ƒ: {bottleneck_types}")
        
        # æ¨èä¼˜å…ˆå¤„ç†çš„å±‚
        priority_layers = [report.layer_name for report in bottleneck_reports[:3]]
        print(f"ğŸ¯ å»ºè®®ä¼˜å…ˆå¤„ç†: {priority_layers}")
        
        return bottleneck_reports
        
    except Exception as e:
        print(f"âŒ ç“¶é¢ˆæ£€æµ‹å¤±è´¥: {e}")
        return []


def demo_intelligent_evolution(trainer, initial_epochs=5):
    """æ¼”ç¤ºï¼šå®Œæ•´æ™ºèƒ½æ¶æ„è¿›åŒ–æµç¨‹"""
    print("\n" + "="*60)
    print("ğŸš€ æ¼”ç¤ºï¼šå®Œæ•´æ™ºèƒ½æ¶æ„è¿›åŒ–æµç¨‹")
    print("="*60)
    
    model = trainer.model
    device = trainer.device
    
    # åŸºç¡€è®­ç»ƒ
    print(f"ğŸ“š å¼€å§‹åŸºç¡€è®­ç»ƒ {initial_epochs} ä¸ªepoch...")
    best_accuracy = 0
    
    for epoch in range(initial_epochs):
        train_acc = trainer.train_epoch(epoch)
                 test_loss, test_acc = trainer.evaluate()
        
        print(f"Epoch {epoch}: Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%, Test Loss: {test_loss:.4f}")
        
        if test_acc > best_accuracy:
            best_accuracy = test_acc
    
    print(f"ğŸ¯ åŸºç¡€è®­ç»ƒå®Œæˆï¼Œæœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_accuracy:.2f}%")
    
    # æ£€æŸ¥æ˜¯å¦å‡†å¤‡å¥½è¿›è¡Œè¿›åŒ–
    if best_accuracy < 50.0:
        print("âš ï¸  æ¨¡å‹æ€§èƒ½ä¸è¶³ï¼Œå»ºè®®ç»§ç»­åŸºç¡€è®­ç»ƒ")
        print("ğŸ’¡ æ™ºèƒ½è¿›åŒ–åœ¨æ¨¡å‹è¾¾åˆ°50%ä»¥ä¸Šå‡†ç¡®ç‡æ—¶æ•ˆæœæœ€ä½³")
        return model
    
    # é…ç½®è¿›åŒ–å¼•æ“
    evolution_config = EvolutionConfig(
        confidence_threshold=0.8,  # æé«˜ç½®ä¿¡åº¦é˜ˆå€¼
        max_mutations_per_iteration=2,  # å‡å°‘çªå˜æ•°é‡
        risk_tolerance=0.8,
        max_iterations=2,  # å‡å°‘è¿­ä»£æ¬¡æ•°ä»¥èŠ‚çœè®¡ç®—
        patience=2,
        min_improvement=0.02,
        task_type='vision',
        evaluation_samples=500  # å‡å°‘è¯„ä¼°æ ·æœ¬
    )
    
    print(f"ğŸ§¬ é…ç½®æ™ºèƒ½è¿›åŒ–å¼•æ“: {evolution_config.max_iterations} è½®è¿­ä»£")
    
    # åˆ›å»ºè¿›åŒ–å¼•æ“
    evolution_engine = IntelligentArchitectureEvolutionEngine(config=evolution_config)
    
    print("ğŸš€ å¼€å§‹æ™ºèƒ½æ¶æ„è¿›åŒ–...")
    
    def evaluation_function(model):
        """è¯„ä¼°å‡½æ•°"""
        trainer.model = model
                 _, accuracy = trainer.evaluate()
        return accuracy / 100.0  # è½¬æ¢ä¸º0-1èŒƒå›´
    
    try:
        # å¼€å§‹è¿›åŒ–
        final_model, evolution_history = evolution_engine.evolve(
            model=model,
            evaluation_fn=evaluation_function,
            data_loader=trainer.test_loader,
            device=device
        )
        
        # æ˜¾ç¤ºè¿›åŒ–ç»“æœ
        print(f"\nğŸ‰ è¿›åŒ–å®Œæˆ!")
        print(f"åˆå§‹æ€§èƒ½: {evolution_history[0].performance_before:.4f}")
        print(f"æœ€ç»ˆæ€§èƒ½: {evolution_history[-1].performance_after:.4f}")
        print(f"æ€»æ”¹è¿›: {evolution_history[-1].performance_after - evolution_history[0].performance_before:.4f}")
        
        return final_model
        
    except Exception as e:
        print(f"âŒ è¿›åŒ–è¿‡ç¨‹ä¸­é‡åˆ°é”™è¯¯: {e}")
        print("è¿™å¯èƒ½æ˜¯ç”±äºæ¼”ç¤ºç¯å¢ƒçš„é™åˆ¶ï¼Œå®é™…ä½¿ç”¨ä¸­è¯·ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®å’Œè®¡ç®—èµ„æº")
        return model


def run_complete_demo():
    """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
    print("ğŸ¯ NeuroExapt æ™ºèƒ½æ¶æ„è¿›åŒ–æ¼”ç¤º - CIFAR-10ç‰ˆ")
    print("åŸºäºäº’ä¿¡æ¯å’Œè´å¶æ–¯æ¨æ–­çš„ç¥ç»ç½‘ç»œæ¶æ„è‡ªé€‚åº”å˜å¼‚ç³»ç»Ÿ")
    print("="*80)
    
    # è®¾å¤‡é…ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # å‡†å¤‡æ•°æ®
    train_loader, test_loader = prepare_cifar10_data()
    
    # åˆ›å»ºæ¨¡å‹
    print("ğŸ—ï¸  åˆ›å»ºå¯è¿›åŒ–çš„ResNetæ¨¡å‹...")
    model = EvolvableResNet(num_classes=10)
    
    # ç»Ÿè®¡æ¨¡å‹å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°: æ€»è®¡ {total_params:,}, å¯è®­ç»ƒ {trainable_params:,}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = IntelligentEvolutionTrainer(model, device, train_loader, test_loader)
    
    # è¿è¡Œå„ä¸ªæ¼”ç¤ºç»„ä»¶
    try:
        # 1. äº’ä¿¡æ¯åˆ†ææ¼”ç¤º
        mi_results = demo_mutual_information_analysis(trainer)
        
        # 2. ä¸ç¡®å®šæ€§åˆ†ææ¼”ç¤º
        uncertainty_results = demo_uncertainty_analysis(trainer)
        
        # 3. ç“¶é¢ˆæ£€æµ‹æ¼”ç¤º
        bottleneck_reports = demo_intelligent_bottleneck_detection(trainer)
        
        # 4. å®Œæ•´è¿›åŒ–æ¼”ç¤º
        evolved_model = demo_intelligent_evolution(trainer, initial_epochs=5)
        
        print("\n" + "="*80)
        print("ğŸ‰ æ™ºèƒ½æ¶æ„è¿›åŒ–æ¼”ç¤ºå®Œæˆ!")
        print("\næ–°æ¡†æ¶æˆåŠŸå±•ç¤ºäº†ä»¥ä¸‹æ ¸å¿ƒèƒ½åŠ›:")
        print("âœ… åŸºäºMINEçš„äº’ä¿¡æ¯ä¼°è®¡ - é‡åŒ–ç‰¹å¾ä¿¡æ¯å«é‡")
        print("âœ… è´å¶æ–¯ä¸ç¡®å®šæ€§é‡åŒ– - è¯„ä¼°ç‰¹å¾ç¨³å®šæ€§")
        print("âœ… æ™ºèƒ½ç“¶é¢ˆæ£€æµ‹ - ç²¾ç¡®å®šä½æ€§èƒ½é™åˆ¶ç‚¹")
        print("âœ… åŸºäºç“¶é¢ˆçš„å˜å¼‚è§„åˆ’ - 15ç§ç­–ç•¥ç²¾ç¡®åŒ¹é…")
        print("âœ… å…ˆè¿›Net2Netå‚æ•°è¿ç§» - åŠŸèƒ½ç­‰ä»·æ€§ä¿è¯")
        print("âœ… å®Œæ•´æ¶æ„è¿›åŒ–æµç¨‹ - è‡ªé€‚åº”ä¼˜åŒ–ç­–ç•¥")
        
        print("\nğŸ”¬ ç†è®ºåˆ›æ–°:")
        print("â€¢ å°†æŠ½è±¡çš„'ç¥ç»ç½‘ç»œå¤©èµ‹ä¸Šé™'è½¬åŒ–ä¸ºå¯è®¡ç®—çš„æ•°å­¦æŒ‡æ ‡")
        print("â€¢ åŸºäºä¿¡æ¯è®ºå’Œè´å¶æ–¯æ¨æ–­çš„ç§‘å­¦å˜å¼‚æŒ‡å¯¼")
        print("â€¢ ä»å¯å‘å¼'æ— è¶£å˜å¼‚'å‡çº§ä¸ºç†è®ºæŒ‡å¯¼çš„'æ™ºèƒ½å˜å¼‚'")
        print("="*80)
        
    except Exception as e:
        logger.error(f"æ¼”ç¤ºè¿‡ç¨‹ä¸­é‡åˆ°é”™è¯¯: {e}")
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­é‡åˆ°é”™è¯¯: {e}")
        print("è¿™å¯èƒ½æ˜¯ç”±äº:")
        print("1. ç¼ºå°‘å¿…è¦çš„ä¾èµ–åŒ…")
        print("2. è®¡ç®—èµ„æºä¸è¶³")
        print("3. æ•°æ®åŠ è½½é—®é¢˜")
        print("\nè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®å¹¶é‡è¯•")


if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    # è¿è¡Œå®Œæ•´æ¼”ç¤º
    run_complete_demo()