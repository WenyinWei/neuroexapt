#!/usr/bin/env python3
"""
é«˜çº§DNMå½¢æ€å‘ç”Ÿæ¼”ç¤º
Advanced DNM Morphogenesis Demo

ğŸ§¬ æ¼”ç¤ºå†…å®¹ï¼š
1. ä¸²è¡Œåˆ†è£‚ (Serial Division) - å¢åŠ ç½‘ç»œæ·±åº¦ï¼Œæå‡è¡¨è¾¾èƒ½åŠ›
2. å¹¶è¡Œåˆ†è£‚ (Parallel Division) - åˆ›å»ºå¤šåˆ†æ”¯ç»“æ„ï¼Œå¢å¼ºç‰¹å¾å¤šæ ·æ€§  
3. æ··åˆåˆ†è£‚ (Hybrid Division) - ç»„åˆä¸åŒå±‚ç±»å‹ï¼Œæ¢ç´¢å¤æ‚æ¶æ„
4. æ™ºèƒ½ç“¶é¢ˆè¯†åˆ«å’Œå†³ç­–åˆ¶å®š
5. æ€§èƒ½å¯¹æ¯”åˆ†æ

ğŸ¯ ç›®æ ‡ï¼šåœ¨CIFAR-10ä¸Šå®ç°90%+å‡†ç¡®ç‡
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import numpy as np
import time
import matplotlib.pyplot as plt
from collections import defaultdict
import logging

# å¯¼å…¥å¢å¼ºçš„DNMç»„ä»¶
from neuroexapt.core import (
    EnhancedDNMFramework,
    AdvancedBottleneckAnalyzer,
    AdvancedMorphogenesisExecutor,
    IntelligentMorphogenesisDecisionMaker,
    MorphogenesisType,
    MorphogenesisDecision
)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)

class AdaptiveResNet(nn.Module):
    """å¢å¼ºçš„è‡ªé€‚åº”ResNet - å†²åˆº95%å‡†ç¡®ç‡"""
    
    def __init__(self, num_classes=10):
        super(AdaptiveResNet, self).__init__()
        
        # ğŸš€ å¢å¼ºçš„åˆå§‹ç‰¹å¾æå–
        self.conv1 = nn.Conv2d(3, 64, 3, padding=1, bias=False)  # å¢åŠ åˆå§‹é€šé“æ•°
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        
        # ğŸš€ æ›´æ·±çš„ç‰¹å¾æå–ç½‘ç»œ
        self.feature_block1 = self._make_resnet_block(64, 128, 2, 2)    # 2ä¸ªæ®‹å·®å—
        self.feature_block2 = self._make_resnet_block(128, 256, 2, 2)   # 2ä¸ªæ®‹å·®å—  
        self.feature_block3 = self._make_resnet_block(256, 512, 2, 2)   # 2ä¸ªæ®‹å·®å—
        self.feature_block4 = self._make_resnet_block(512, 512, 1, 2)   # 2ä¸ªæ®‹å·®å—ï¼Œä¸é™é‡‡æ ·
        
        # ğŸš€ å¢å¼ºçš„å…¨å±€ç‰¹å¾èšåˆ
        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.global_max_pool = nn.AdaptiveMaxPool2d((1, 1))
        
        # ğŸš€ æ›´å¼ºçš„åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(512 * 2, 1024),  # ç»“åˆavgå’Œmax pooling
            nn.BatchNorm1d(1024),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            
            nn.Linear(1024, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            
            nn.Linear(256, num_classes)
        )
        
    def _make_resnet_block(self, in_channels, out_channels, stride, num_blocks):
        """åˆ›å»ºæ®‹å·®å—ç»„"""
        layers = []
        
        # ç¬¬ä¸€ä¸ªå—å¯èƒ½æœ‰é™é‡‡æ ·
        layers.append(ResidualBlock(in_channels, out_channels, stride))
        
        # åç»­å—ä¿æŒç›¸åŒå°ºå¯¸
        for _ in range(1, num_blocks):
            layers.append(ResidualBlock(out_channels, out_channels, 1))
            
        return nn.Sequential(*layers)
    
    def forward(self, x):
        # ğŸš€ å¢å¼ºçš„åˆå§‹ç‰¹å¾æå–
        x = self.relu(self.bn1(self.conv1(x)))
        
        # ğŸš€ æ·±åº¦æ®‹å·®ç‰¹å¾æå–
        x = self.feature_block1(x)
        x = self.feature_block2(x)
        x = self.feature_block3(x)
        x = self.feature_block4(x)
        
        # ğŸš€ åŒé‡å…¨å±€æ± åŒ–ç‰¹å¾èšåˆ
        avg_pool = self.global_pool(x)
        max_pool = self.global_max_pool(x)
        x = torch.cat([avg_pool, max_pool], dim=1)  # ç‰¹å¾èåˆ
        
        # åˆ†ç±»
        x = self.classifier(x)
        
        return x

class ResidualBlock(nn.Module):
    """æ®‹å·®å—å®ç°"""
    
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()
        
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        
        # æ®‹å·®è¿æ¥
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
    
    def forward(self, x):
        residual = self.shortcut(x)
        
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        
        out += residual
        out = self.relu(out)
        
        return out

class AdvancedDNMTrainer:
    """é«˜çº§DNMè®­ç»ƒå™¨"""
    
    def __init__(self, model, device, train_loader, test_loader):
        self.model = model.to(device)
        self.device = device
        self.train_loader = train_loader
        self.test_loader = test_loader
        
        # ğŸš€ å¢å¼ºçš„DNMæ¡†æ¶é…ç½® - å†²åˆº95%å‡†ç¡®ç‡
        self.dnm_config = {
            'trigger_interval': 8,  # æ¯8ä¸ªepochæ£€æŸ¥ä¸€æ¬¡ï¼Œæ›´ç¨³å®š
            'complexity_threshold': 0.5,  # é™ä½é˜ˆå€¼ï¼Œæ›´å®¹æ˜“è§¦å‘
            'enable_serial_division': True,
            'enable_parallel_division': True,
            'enable_hybrid_division': True,
            'max_parameter_growth_ratio': 3.0  # å…è®¸æ›´å¤šå‚æ•°å¢é•¿
        }
        
        self.dnm_framework = EnhancedDNMFramework(self.dnm_config)
        
        # è®­ç»ƒå†å²
        self.train_history = []
        self.test_history = []
        self.morphogenesis_history = []
        self.parameter_history = []
        
    def capture_network_state(self):
        """æ•è·ç½‘ç»œçŠ¶æ€ï¼ˆæ¿€æ´»å€¼å’Œæ¢¯åº¦ï¼‰"""
        print("      ğŸ” å¼€å§‹è¯¦ç»†çš„ç½‘ç»œçŠ¶æ€æ•è·...")
        activations = {}
        gradients = {}
        
        # æ³¨å†Œé’©å­å‡½æ•°
        def forward_hook(name):
            def hook(module, input, output):
                try:
                    if isinstance(output, torch.Tensor):
                        activations[name] = output.detach().cpu()
                        print(f"        ğŸ“ˆ å‰å‘é’©å­æ•è·: {name} - å½¢çŠ¶ {output.shape}")
                except Exception as e:
                    print(f"        âŒ å‰å‘é’©å­é”™è¯¯ {name}: {e}")
            return hook
        
        def backward_hook(name):
            def hook(module, grad_input, grad_output):
                try:
                    if grad_output[0] is not None:
                        gradients[name] = grad_output[0].detach().cpu()
                        print(f"        ğŸ“‰ åå‘é’©å­æ•è·: {name} - å½¢çŠ¶ {grad_output[0].shape}")
                except Exception as e:
                    print(f"        âŒ åå‘é’©å­é”™è¯¯ {name}: {e}")
            return hook
        
        # æ³¨å†Œé’©å­
        print("      ğŸ“ æ³¨å†Œç½‘ç»œé’©å­...")
        hooks = []
        hook_count = 0
        
        for name, module in self.model.named_modules():
            if isinstance(module, (nn.Linear, nn.Conv2d)):
                try:
                    hooks.append(module.register_forward_hook(forward_hook(name)))
                    hooks.append(module.register_backward_hook(backward_hook(name)))
                    hook_count += 2
                    print(f"        âœ… é’©å­æ³¨å†ŒæˆåŠŸ: {name} ({type(module).__name__})")
                except Exception as e:
                    print(f"        âŒ é’©å­æ³¨å†Œå¤±è´¥: {name} - {e}")
        
        print(f"      ğŸ“Š æ€»å…±æ³¨å†Œäº† {hook_count} ä¸ªé’©å­")
        
        # æ‰§è¡Œä¸€æ¬¡å‰å‘å’Œåå‘ä¼ æ’­
        print("      ğŸš€ æ‰§è¡Œå‰å‘å’Œåå‘ä¼ æ’­...")
        try:
            self.model.train()
            data, target = next(iter(self.train_loader))
            data, target = data.to(self.device), target.to(self.device)
            print(f"        ğŸ“Š è¾“å…¥æ•°æ®å½¢çŠ¶: {data.shape}")
            
            output = self.model(data)
            print(f"        ğŸ“Š è¾“å‡ºå½¢çŠ¶: {output.shape}")
            
            loss = F.cross_entropy(output, target)
            print(f"        ğŸ“Š æŸå¤±å€¼: {loss.item():.6f}")
            
            # æ¸…ç©ºä¹‹å‰çš„æ¢¯åº¦
            self.model.zero_grad()
            loss.backward()
            print("        âœ… åå‘ä¼ æ’­å®Œæˆ")
            
        except Exception as e:
            print(f"        âŒ å‰å‘/åå‘ä¼ æ’­å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        # ç§»é™¤é’©å­
        print("      ğŸ§¹ æ¸…ç†é’©å­...")
        removed_count = 0
        for hook in hooks:
            try:
                hook.remove()
                removed_count += 1
            except Exception as e:
                print(f"        âŒ é’©å­ç§»é™¤å¤±è´¥: {e}")
        
        print(f"      âœ… ç§»é™¤äº† {removed_count} ä¸ªé’©å­")
        print(f"      ğŸ“Š æ•è·çš„æ¿€æ´»: {len(activations)} ä¸ª")
        print(f"      ğŸ“Š æ•è·çš„æ¢¯åº¦: {len(gradients)} ä¸ª")
        
        return activations, gradients
    
    def train_epoch(self, optimizer, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(self.train_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            optimizer.zero_grad()
            output = self.model(data)
            loss = F.cross_entropy(output, target)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
            
            if batch_idx % 100 == 0:
                print(f'    Train Batch: {batch_idx:3d}/{len(self.train_loader)}, '
                      f'Loss: {loss.item():.6f}, Acc: {100.*correct/total:.2f}%')
        
        avg_loss = total_loss / len(self.train_loader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def test_epoch(self):
        """æµ‹è¯•ä¸€ä¸ªepoch"""
        self.model.eval()
        test_loss = 0
        correct = 0
        
        with torch.no_grad():
            for data, target in self.test_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                test_loss += F.cross_entropy(output, target, reduction='sum').item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
        
        test_loss /= len(self.test_loader.dataset)
        accuracy = 100. * correct / len(self.test_loader.dataset)
        
        return test_loss, accuracy
    
    def train_with_morphogenesis(self, epochs=80):  # ğŸš€ å¢åŠ åˆ°80ä¸ªepoch
        """å¸¦å½¢æ€å‘ç”Ÿçš„è®­ç»ƒ - å†²åˆº95%å‡†ç¡®ç‡"""
        print("ğŸ§¬ å¼€å§‹é«˜çº§DNMè®­ç»ƒ - å†²åˆº95%å‡†ç¡®ç‡...")
        print("=" * 60)
        
        # ğŸš€ å¢å¼ºçš„ä¼˜åŒ–å™¨é…ç½®
        # ä½¿ç”¨SGD + Momentumï¼Œå¯¹CIFAR-10æ›´æœ‰æ•ˆ
        optimizer = optim.SGD(
            self.model.parameters(), 
            lr=0.1,              # è¾ƒé«˜çš„åˆå§‹å­¦ä¹ ç‡
            momentum=0.9,        # å¼ºåŠ¨é‡
            weight_decay=5e-4,   # é€‚ä¸­çš„æƒé‡è¡°å‡
            nesterov=True        # NesterovåŠ¨é‡
        )
        
        # ğŸš€ å¤šé˜¶æ®µå­¦ä¹ ç‡è°ƒåº¦
        scheduler = optim.lr_scheduler.MultiStepLR(
            optimizer, 
            milestones=[30, 60, 75],  # åœ¨30, 60, 75 epoché™ä½å­¦ä¹ ç‡
            gamma=0.1                 # æ¯æ¬¡é™ä½10å€
        )
        
        # è®°å½•åˆå§‹å‚æ•°æ•°é‡
        initial_params = sum(p.numel() for p in self.model.parameters())
        print(f"ğŸ“Š åˆå§‹å‚æ•°æ•°é‡: {initial_params:,}")
        self.parameter_history.append(initial_params)
        
        best_test_acc = 0.0
        patience_counter = 0
        
        # ğŸš€ æ·»åŠ å­¦ä¹ ç‡é¢„çƒ­
        warmup_epochs = 5
        warmup_scheduler = optim.lr_scheduler.LinearLR(
            optimizer, 
            start_factor=0.1, 
            end_factor=1.0, 
            total_iters=warmup_epochs
        )
        
        for epoch in range(epochs):
            print(f"\nğŸ§¬ Epoch {epoch+1}/{epochs}")
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(optimizer, epoch)
            
            # æµ‹è¯•
            test_loss, test_acc = self.test_epoch()
            
            # æ›´æ–°å†å²
            self.train_history.append((train_loss, train_acc))
            self.test_history.append((test_loss, test_acc))
            self.dnm_framework.update_performance_history(test_acc / 100.0)
            
            print(f"  ğŸ“Š Train: {train_acc:.2f}% (Loss: {train_loss:.4f}) | "
                  f"Test: {test_acc:.2f}% (Loss: {test_loss:.4f})")
            
            # ğŸš€ æ™ºèƒ½å­¦ä¹ ç‡è°ƒåº¦
            if epoch < warmup_epochs:
                warmup_scheduler.step()
                current_lr = optimizer.param_groups[0]['lr']
                print(f"  ğŸ”¥ é¢„çƒ­é˜¶æ®µ: LR={current_lr:.6f}")
            else:
                scheduler.step()
                current_lr = optimizer.param_groups[0]['lr']
                print(f"  ğŸ“ˆ å½“å‰å­¦ä¹ ç‡: {current_lr:.6f}")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å½¢æ€å‘ç”Ÿ
            if epoch >= 10:  # è®©ç½‘ç»œç¨³å®šè®­ç»ƒæ›´é•¿æ—¶é—´
                print(f"  ğŸ”¬ å½¢æ€å‘ç”Ÿæ£€æŸ¥ - Epoch {epoch}")
                print(f"    ğŸ“Š å½“å‰æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
                print(f"    ğŸ“‹ æ¨¡å‹ç»“æ„å±‚æ•°: {len(list(self.model.modules()))}")
                
                print("  ğŸ“ˆ å¼€å§‹æ•è·ç½‘ç»œçŠ¶æ€...")
                try:
                    activations, gradients = self.capture_network_state()
                    print(f"    âœ… æ¿€æ´»ç»Ÿè®¡å®Œæˆ: {len(activations)} ä¸ªæ¨¡å—")
                    print(f"    âœ… æ¢¯åº¦ç»Ÿè®¡å®Œæˆ: {len(gradients)} ä¸ªæ¨¡å—")
                except Exception as e:
                    print(f"    âŒ ç½‘ç»œçŠ¶æ€æ•è·å¤±è´¥: {e}")
                    activations, gradients = {}, {}
                
                print("  ğŸ§  æ„å»ºåˆ†æä¸Šä¸‹æ–‡...")
                context = {
                    'epoch': epoch,
                    'activations': activations,
                    'gradients': gradients,
                    'performance_history': self.dnm_framework.performance_history
                }
                print(f"    âœ… æ€§èƒ½å†å²é•¿åº¦: {len(self.dnm_framework.performance_history)}")
                print(f"    âœ… ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆ")
                
                print("  ğŸš€ å¼€å§‹æ‰§è¡Œå½¢æ€å‘ç”Ÿåˆ†æ...")
                try:
                    # æ‰§è¡Œå½¢æ€å‘ç”Ÿ
                    results = self.dnm_framework.execute_morphogenesis(self.model, context)
                    print(f"    âœ… å½¢æ€å‘ç”Ÿåˆ†æå®Œæˆ")
                    print(f"    ğŸ“‹ è¿”å›ç»“æœé”®: {list(results.keys())}")
                    print(f"    ğŸ”§ æ¨¡å‹æ˜¯å¦ä¿®æ”¹: {results.get('model_modified', False)}")
                except Exception as e:
                    print(f"    âŒ å½¢æ€å‘ç”Ÿæ‰§è¡Œå¤±è´¥: {e}")
                    import traceback
                    print("    ğŸ“‹ è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
                    traceback.print_exc()
                    results = {'model_modified': False}
                
                if results['model_modified']:
                    print(f"  ğŸ‰ å½¢æ€å‘ç”ŸæˆåŠŸ!")
                    print(f"    ç±»å‹: {results['morphogenesis_type']}")
                    print(f"    æ–°å¢å‚æ•°: {results['parameters_added']:,}")
                    print(f"    ç½®ä¿¡åº¦: {results.get('decision_confidence', 0):.3f}")
                    
                    print("  ğŸ”„ å¼€å§‹æ›´æ–°æ¨¡å‹...")
                    old_param_count = sum(p.numel() for p in self.model.parameters())
                    print(f"    ğŸ“Š åŸå§‹æ¨¡å‹å‚æ•°: {old_param_count:,}")
                    
                    # æ›´æ–°æ¨¡å‹
                    try:
                        self.model = results['new_model']
                        new_param_count = sum(p.numel() for p in self.model.parameters())
                        print(f"    ğŸ“Š æ–°æ¨¡å‹å‚æ•°: {new_param_count:,}")
                        print(f"    ğŸ“ˆ å‚æ•°å¢é•¿: {new_param_count - old_param_count:,}")
                        print(f"    âœ… æ¨¡å‹æ›´æ–°æˆåŠŸ")
                    except Exception as e:
                        print(f"    âŒ æ¨¡å‹æ›´æ–°å¤±è´¥: {e}")
                        import traceback
                        traceback.print_exc()
                        continue
                    
                    # ğŸš€ é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨ä»¥åŒ…å«æ–°å‚æ•°ï¼Œä¿æŒå½“å‰å­¦ä¹ ç‡
                    print("  âš™ï¸ é‡å»ºä¼˜åŒ–å™¨...")
                    current_lr = optimizer.param_groups[0]['lr']
                    print(f"    ğŸ“ˆ ä¿æŒå­¦ä¹ ç‡: {current_lr:.6f}")
                    
                    try:
                        optimizer = optim.SGD(
                            self.model.parameters(), 
                            lr=current_lr,
                            momentum=0.9,
                            weight_decay=5e-4,
                            nesterov=True
                        )
                        print(f"    âœ… ä¼˜åŒ–å™¨é‡å»ºæˆåŠŸ")
                        print(f"    ğŸ“Š ä¼˜åŒ–å™¨å‚æ•°ç»„æ•°: {len(optimizer.param_groups)}")
                    except Exception as e:
                        print(f"    âŒ ä¼˜åŒ–å™¨é‡å»ºå¤±è´¥: {e}")
                        import traceback
                        traceback.print_exc()
                        continue
                    
                    # é‡æ–°åˆ›å»ºè°ƒåº¦å™¨
                    print("  ğŸ“… é‡å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨...")
                    remaining_epochs = epochs - epoch
                    print(f"    ğŸ“Š å‰©ä½™è®­ç»ƒè½®æ•°: {remaining_epochs}")
                    
                    if remaining_epochs > 0:
                        milestones = [m - epoch for m in [30, 60, 75] if m > epoch]
                        print(f"    ğŸ“ˆ è°ƒæ•´åçš„é‡Œç¨‹ç¢‘: {milestones}")
                        
                        if milestones:
                            try:
                                scheduler = optim.lr_scheduler.MultiStepLR(
                                    optimizer, milestones=milestones, gamma=0.1
                                )
                                print(f"    âœ… è°ƒåº¦å™¨é‡å»ºæˆåŠŸ")
                            except Exception as e:
                                print(f"    âŒ è°ƒåº¦å™¨é‡å»ºå¤±è´¥: {e}")
                        else:
                            print(f"    â„¹ï¸ æ— éœ€é‡å»ºè°ƒåº¦å™¨(æ— å‰©ä½™é‡Œç¨‹ç¢‘)")
                    else:
                        print(f"    â„¹ï¸ æ— éœ€é‡å»ºè°ƒåº¦å™¨(æ— å‰©ä½™è½®æ•°)")
                    
                    # è®°å½•å½¢æ€å‘ç”Ÿäº‹ä»¶
                    print("  ğŸ“ è®°å½•å½¢æ€å‘ç”Ÿå†å²...")
                    current_params = sum(p.numel() for p in self.model.parameters())
                    self.parameter_history.append(current_params)
                    
                    morphogenesis_event = {
                        'epoch': epoch,
                        'type': results['morphogenesis_type'],
                        'parameters_added': results['parameters_added'],
                        'test_acc_before': test_acc,
                        'total_params': current_params
                    }
                    
                    self.morphogenesis_history.append(morphogenesis_event)
                    print(f"    âœ… å†å²è®°å½•å®Œæˆ")
                    print(f"    ğŸ“Š æ€»å‚æ•°: {current_params:,}")
                    print(f"    ğŸ“ˆ å‚æ•°å¢é•¿ç‡: {((current_params-initial_params)/initial_params*100):.1f}%")
                    print(f"    ğŸ“‹ å½¢æ€å‘ç”Ÿäº‹ä»¶æ€»æ•°: {len(self.morphogenesis_history)}")
                    
                    print("  ğŸ§¹ æ‰§è¡Œå†…å­˜æ¸…ç†...")
                    import gc
                    gc.collect()
                    print("    âœ… å†…å­˜æ¸…ç†å®Œæˆ")
                else:
                    # æ²¡æœ‰å½¢æ€å‘ç”Ÿæ—¶ä¹Ÿè®°å½•å‚æ•°æ•°é‡
                    current_params = sum(p.numel() for p in self.model.parameters())
                    self.parameter_history.append(current_params)
            else:
                # å‰å‡ ä¸ªepochä¹Ÿè®°å½•å‚æ•°æ•°é‡
                current_params = sum(p.numel() for p in self.model.parameters())
                self.parameter_history.append(current_params)
            
            # ğŸš€ æ€§èƒ½ç›‘æ§å’Œæ—©åœæ£€æŸ¥
            if test_acc > best_test_acc:
                best_test_acc = test_acc
                patience_counter = 0
                print(f"  ğŸ¯ æ–°çš„æœ€ä½³å‡†ç¡®ç‡: {best_test_acc:.2f}%!")
                
                # ğŸ† é‡Œç¨‹ç¢‘æç¤º
                if best_test_acc >= 95.0:
                    print("  ğŸ† æ­å–œï¼è¾¾åˆ°95%+å‡†ç¡®ç‡ç›®æ ‡!")
                elif best_test_acc >= 90.0:
                    print("  ğŸŒŸ å¾ˆå¥½ï¼è¾¾åˆ°90%+å‡†ç¡®ç‡!")
                elif best_test_acc >= 85.0:
                    print("  âœ¨ ä¸é”™ï¼è¾¾åˆ°85%+å‡†ç¡®ç‡!")
            else:
                patience_counter += 1
                
            # å¢åŠ è€å¿ƒå€¼ï¼Œç»™æ›´å¤šæ—¶é—´è®­ç»ƒ
            if patience_counter >= 15:
                print(f"  ğŸ›‘ Early stopping triggered at epoch {epoch+1}")
                break
                
            # ğŸš€ è¿›åº¦æç¤º
            progress = (epoch + 1) / epochs * 100
            if progress % 25 == 0:
                print(f"  ğŸ“Š è®­ç»ƒè¿›åº¦: {progress:.0f}% å®Œæˆ")
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“Š æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_test_acc:.2f}%")
        
        return best_test_acc
    
    def analyze_morphogenesis_effects(self):
        """åˆ†æå½¢æ€å‘ç”Ÿæ•ˆæœ"""
        print("\nğŸ”¬ å½¢æ€å‘ç”Ÿæ•ˆæœåˆ†æ")
        print("=" * 50)
        
        summary = self.dnm_framework.get_morphogenesis_summary()
        
        print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"  å½¢æ€å‘ç”Ÿäº‹ä»¶: {summary['total_events']}")
        print(f"  æ–°å¢å‚æ•°: {summary['total_parameters_added']:,}")
        print(f"  å½¢æ€å‘ç”Ÿç±»å‹åˆ†å¸ƒ: {summary['morphogenesis_types']}")
        
        if self.morphogenesis_history:
            print(f"\nğŸ“ˆ æ€§èƒ½æ”¹è¿›åˆ†æ:")
            
            for i, event in enumerate(self.morphogenesis_history):
                # è®¡ç®—å½¢æ€å‘ç”Ÿåçš„æ€§èƒ½å˜åŒ–
                epoch = event['epoch']
                if epoch + 5 < len(self.test_history):
                    acc_before = event['test_acc_before']
                    acc_after = self.test_history[epoch + 5][1]  # 5ä¸ªepochåçš„å‡†ç¡®ç‡
                    improvement = acc_after - acc_before
                    
                    print(f"  äº‹ä»¶ {i+1} (Epoch {epoch}):")
                    print(f"    ç±»å‹: {event['type']}")
                    print(f"    æ–°å¢å‚æ•°: {event['parameters_added']:,}")
                    print(f"    æ€§èƒ½å˜åŒ–: {acc_before:.2f}% â†’ {acc_after:.2f}% "
                          f"({improvement:+.2f}%)")
        
        return summary
    
    def plot_training_progress(self):
        """ç»˜åˆ¶è®­ç»ƒè¿›åº¦å›¾"""
        if len(self.train_history) == 0:
            return
            
        epochs = range(1, len(self.train_history) + 1)
        train_accs = [acc for _, acc in self.train_history]
        test_accs = [acc for _, acc in self.test_history]
        
        # ç¡®ä¿å‚æ•°å†å²é•¿åº¦ä¸epochåŒ¹é…
        param_history_aligned = self.parameter_history[:len(self.train_history)]
        if len(param_history_aligned) < len(self.train_history):
            # å¦‚æœå‚æ•°å†å²ä¸å¤Ÿé•¿ï¼Œç”¨æœ€åä¸€ä¸ªå€¼å¡«å……
            last_param = param_history_aligned[-1] if param_history_aligned else 0
            param_history_aligned.extend([last_param] * (len(self.train_history) - len(param_history_aligned)))
        
        plt.figure(figsize=(15, 5))
        
        # å‡†ç¡®ç‡æ›²çº¿
        plt.subplot(1, 3, 1)
        plt.plot(epochs, train_accs, label='Train Accuracy', color='blue')
        plt.plot(epochs, test_accs, label='Test Accuracy', color='red')
        
        # æ ‡è®°å½¢æ€å‘ç”Ÿäº‹ä»¶
        for event in self.morphogenesis_history:
            if event['epoch'] <= len(self.train_history):
                plt.axvline(x=event['epoch'], color='green', linestyle='--', alpha=0.7)
                plt.text(event['epoch'], max(test_accs) * 0.9, 
                        event['type'].split('_')[0], rotation=90, fontsize=8)
        
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy (%)')
        plt.title('Training Progress')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # å‚æ•°å¢é•¿æ›²çº¿
        plt.subplot(1, 3, 2)
        param_growth = [(p - param_history_aligned[0]) / param_history_aligned[0] * 100 
                       for p in param_history_aligned]
        plt.plot(epochs, param_growth, color='purple', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('Parameter Growth (%)')
        plt.title('Parameter Growth')
        plt.grid(True, alpha=0.3)
        
        # å½¢æ€å‘ç”Ÿç±»å‹åˆ†å¸ƒ
        plt.subplot(1, 3, 3)
        if self.morphogenesis_history:
            types = [event['type'] for event in self.morphogenesis_history]
            type_counts = {t: types.count(t) for t in set(types)}
            
            plt.pie(type_counts.values(), labels=type_counts.keys(), autopct='%1.1f%%')
            plt.title('Morphogenesis Types')
        else:
            plt.text(0.5, 0.5, 'No Morphogenesis\nEvents', 
                    ha='center', va='center', transform=plt.gca().transAxes)
            plt.title('Morphogenesis Types')
        
        plt.tight_layout()
        plt.savefig('advanced_dnm_training_progress.png', dpi=150, bbox_inches='tight')
        plt.show()

def prepare_data():
    """å‡†å¤‡CIFAR-10æ•°æ® - å¢å¼ºç‰ˆæ•°æ®å¢å¼ºç­–ç•¥"""
    # ğŸš€ å¼ºåŒ–æ•°æ®å¢å¼ºç­–ç•¥ - å†²åˆº95%å‡†ç¡®ç‡
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(degrees=15),  # éšæœºæ—‹è½¬
        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # éšæœºå¹³ç§»
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # é¢œè‰²æŠ–åŠ¨
        transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.1),  # éšæœºé«˜æ–¯æ¨¡ç³Š
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        transforms.RandomErasing(p=0.1, scale=(0.02, 0.33), ratio=(0.3, 3.3))  # éšæœºæ“¦é™¤
    ])
    
    # æµ‹è¯•æ—¶ä½¿ç”¨æ ‡å‡†é¢„å¤„ç†
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])
    
    train_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform_train
    )
    
    test_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=False, download=True, transform=transform_test
    )
    
    # ğŸš€ å¹³è¡¡æ€§èƒ½å’Œç¨³å®šæ€§çš„é…ç½®
    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2, pin_memory=False)
    test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2, pin_memory=False)
    
    return train_loader, test_loader

def compare_with_fixed_architecture():
    """ä¸å›ºå®šæ¶æ„è¿›è¡Œå¯¹æ¯”"""
    print("\nâš–ï¸ å¯¹æ¯”å›ºå®šæ¶æ„ vs è‡ªé€‚åº”æ¶æ„")
    print("=" * 60)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    train_loader, test_loader = prepare_data()
    
    # 1. è®­ç»ƒå›ºå®šæ¶æ„
    print("ğŸ”§ è®­ç»ƒå›ºå®šæ¶æ„...")
    fixed_model = AdaptiveResNet()
    fixed_trainer = AdvancedDNMTrainer(fixed_model, device, train_loader, test_loader)
    
    # ç¦ç”¨å½¢æ€å‘ç”Ÿ
    fixed_trainer.dnm_framework.config['trigger_interval'] = 999  # æ°¸ä¸è§¦å‘
    
    fixed_acc = fixed_trainer.train_with_morphogenesis(epochs=30)
    
    # 2. è®­ç»ƒè‡ªé€‚åº”æ¶æ„
    print("\nğŸ§¬ è®­ç»ƒè‡ªé€‚åº”æ¶æ„...")
    adaptive_model = AdaptiveResNet()
    adaptive_trainer = AdvancedDNMTrainer(adaptive_model, device, train_loader, test_loader)
    
    adaptive_acc = adaptive_trainer.train_with_morphogenesis(epochs=30)
    
    # 3. åˆ†æç»“æœ
    print("\nğŸ“Š å¯¹æ¯”ç»“æœ:")
    print(f"  å›ºå®šæ¶æ„æœ€ä½³å‡†ç¡®ç‡: {fixed_acc:.2f}%")
    print(f"  è‡ªé€‚åº”æ¶æ„æœ€ä½³å‡†ç¡®ç‡: {adaptive_acc:.2f}%")
    print(f"  æ€§èƒ½æå‡: {adaptive_acc - fixed_acc:+.2f}%")
    
    # åˆ†æå½¢æ€å‘ç”Ÿæ•ˆæœ
    adaptive_summary = adaptive_trainer.analyze_morphogenesis_effects()
    
    return fixed_acc, adaptive_acc, adaptive_summary

def demonstrate_morphogenesis_types():
    """æ¼”ç¤ºä¸åŒå½¢æ€å‘ç”Ÿç±»å‹"""
    print("\nğŸ­ æ¼”ç¤ºä¸åŒå½¢æ€å‘ç”Ÿç±»å‹")
    print("=" * 50)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # æµ‹è¯•æ¯ç§å½¢æ€å‘ç”Ÿç±»å‹
    morphogenesis_types = [
        MorphogenesisType.SERIAL_DIVISION,
        MorphogenesisType.PARALLEL_DIVISION,
        MorphogenesisType.HYBRID_DIVISION
    ]
    
    results = {}
    
    for morph_type in morphogenesis_types:
        print(f"\nğŸ”¬ æµ‹è¯• {morph_type.value}...")
        
        model = AdaptiveResNet().to(device)
        original_params = sum(p.numel() for p in model.parameters())
        
        # åˆ›å»ºå†³ç­–
        decision = MorphogenesisDecision(
            morphogenesis_type=morph_type,
            target_location='classifier.1',
            confidence=0.8,
            expected_improvement=0.05,
            complexity_cost=0.3,
            parameters_added=5000,
            reasoning=f"æ¼”ç¤º{morph_type.value}"
        )
        
        # æ‰§è¡Œå½¢æ€å‘ç”Ÿ
        executor = AdvancedMorphogenesisExecutor()
        try:
            new_model, params_added = executor.execute_morphogenesis(model, decision)
            new_params = sum(p.numel() for p in new_model.parameters())
            
            # æµ‹è¯•åŠŸèƒ½
            test_input = torch.randn(4, 3, 32, 32).to(device)
            with torch.no_grad():
                output = new_model(test_input)
            
            results[morph_type.value] = {
                'success': True,
                'original_params': original_params,
                'new_params': new_params,
                'params_added': params_added,
                'growth_ratio': (new_params - original_params) / original_params,
                'output_shape': output.shape
            }
            
            print(f"  âœ… æˆåŠŸ")
            print(f"    åŸå§‹å‚æ•°: {original_params:,}")
            print(f"    æ–°å¢å‚æ•°: {params_added:,}")
            print(f"    æ€»å‚æ•°: {new_params:,}")
            print(f"    å¢é•¿ç‡: {results[morph_type.value]['growth_ratio']:.1%}")
            
        except Exception as e:
            results[morph_type.value] = {
                'success': False,
                'error': str(e)
            }
            print(f"  âŒ å¤±è´¥: {e}")
    
    return results

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸ§¬ é«˜çº§DNMå½¢æ€å‘ç”Ÿæ¼”ç¤º")
    print("=" * 60)
    
    try:
        # 1. æ¼”ç¤ºä¸åŒå½¢æ€å‘ç”Ÿç±»å‹
        morphogenesis_results = demonstrate_morphogenesis_types()
        
        # 2. å®Œæ•´è®­ç»ƒæ¼”ç¤º
        print(f"\nğŸš€ å®Œæ•´è®­ç»ƒæ¼”ç¤º")
        print("=" * 50)
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
        
        # å‡†å¤‡æ•°æ®
        train_loader, test_loader = prepare_data()
        
        # åˆ›å»ºæ¨¡å‹å’Œè®­ç»ƒå™¨
        model = AdaptiveResNet()
        trainer = AdvancedDNMTrainer(model, device, train_loader, test_loader)
        
        # ğŸš€ å†²åˆº95%å‡†ç¡®ç‡ - å¢åŠ è®­ç»ƒè½®æ•°
        best_acc = trainer.train_with_morphogenesis(epochs=80)
        
        # åˆ†æç»“æœ
        summary = trainer.analyze_morphogenesis_effects()
        
        # ç»˜åˆ¶å›¾è¡¨
        trainer.plot_training_progress()
        
        print(f"\nğŸ‰ æ¼”ç¤ºå®Œæˆ!")
        print("=" * 60)
        
        print(f"\nğŸ“Š æœ€ç»ˆç»“æœ:")
        print(f"  æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_acc:.2f}%")
        print(f"  å½¢æ€å‘ç”Ÿäº‹ä»¶: {summary['total_events']}")
        print(f"  æ–°å¢å‚æ•°: {summary['total_parameters_added']:,}")
        print(f"  æ”¯æŒçš„å½¢æ€å‘ç”Ÿç±»å‹: {len([r for r in morphogenesis_results.values() if r['success']])}/3")
        
        if best_acc >= 95.0:
            print("  ğŸ† æ­å–œï¼æˆåŠŸè¾¾åˆ°95%+å‡†ç¡®ç‡ç›®æ ‡!")
        elif best_acc >= 90.0:
            print("  ğŸŒŸ å¾ˆå¥½ï¼è¾¾åˆ°90%+å‡†ç¡®ç‡ï¼Œæ¥è¿‘ç›®æ ‡!")
        elif best_acc >= 85.0:
            print("  âœ¨ ä¸é”™ï¼è¾¾åˆ°85%+å‡†ç¡®ç‡ï¼Œç»§ç»­ä¼˜åŒ–ä¸­...")
        elif summary['total_events'] > 0:
            print("  ğŸ”§ å½¢æ€å‘ç”ŸåŠŸèƒ½æ­£å¸¸ï¼Œéœ€è¦æ›´å¤šè®­ç»ƒæ—¶é—´")
        else:
            print("  âš ï¸ å»ºè®®è°ƒæ•´è§¦å‘é˜ˆå€¼ä»¥æ¿€æ´»æ›´å¤šå½¢æ€å‘ç”Ÿ")
            
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()