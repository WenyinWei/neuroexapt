#!/usr/bin/env python3
"""
ğŸ”§ ç®€åŒ–çš„BatchNormåŒæ­¥è°ƒè¯•è„šæœ¬
ç›´æ¥æµ‹è¯•DNMæ¡†æ¶ä¸­çš„BatchNormåŒæ­¥é—®é¢˜
"""

import torch
import torch.nn as nn
import logging
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# é…ç½®è¯¦ç»†æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

# ç›´æ¥å¯¼å…¥DNMæ¨¡å—ï¼Œé¿å…æ•´ä¸ªneuroexaptåŒ…çš„ä¾èµ–é—®é¢˜
try:
    # ç›´æ¥ä»æ–‡ä»¶å¯¼å…¥ï¼Œç»•è¿‡__init__.py
    import importlib.util
    spec = importlib.util.spec_from_file_location(
        "dnm_neuron_division", 
        "/workspace/neuroexapt/core/dnm_neuron_division.py"
    )
    dnm_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(dnm_module)
    DNMNeuronDivision = dnm_module.DNMNeuronDivision
    logger.info("âœ… æˆåŠŸå¯¼å…¥DNMæ¨¡å—")
except Exception as e:
    logger.error(f"âŒ æ— æ³•å¯¼å…¥DNMæ¨¡å—: {e}")
    sys.exit(1)

class SimpleTestNet(nn.Module):
    """ç®€å•çš„æµ‹è¯•ç½‘ç»œï¼Œæ¨¡æ‹ŸResNetç»“æ„"""
    
    def __init__(self):
        super().__init__()
        
        # ç®€å•çš„stemç»“æ„ (ç±»ä¼¼ResNet)
        self.stem = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),  # stem.0
            nn.BatchNorm2d(64),  # stem.1
            nn.ReLU(inplace=True),  # stem.2
        )
        
        # ç®€å•çš„layer1ç»“æ„
        self.layer1 = nn.Sequential(
            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False),  # layer1.0
            nn.BatchNorm2d(64),  # layer1.1
            nn.ReLU(inplace=True),  # layer1.2
        )
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.stem(x)
        x = self.layer1(x)
        x = nn.functional.adaptive_avg_pool2d(x, (1, 1))
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

def test_batchnorm_sync():
    """æµ‹è¯•BatchNormåŒæ­¥åŠŸèƒ½"""
    
    logger.info("ğŸ§ª å¼€å§‹BatchNormåŒæ­¥æµ‹è¯•")
    
    # åˆ›å»ºæµ‹è¯•ç½‘ç»œ
    model = SimpleTestNet()
    logger.info(f"åŸå§‹æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters())}")
    
    # æ‰“å°æ¨¡å‹ç»“æ„
    logger.info("ğŸ“‹ æ¨¡å‹ç»“æ„:")
    for name, module in model.named_modules():
        if isinstance(module, (nn.Conv2d, nn.BatchNorm2d)):
            if isinstance(module, nn.Conv2d):
                logger.info(f"  {name}: Conv2d(in={module.in_channels}, out={module.out_channels})")
            else:
                logger.info(f"  {name}: BatchNorm2d(features={module.num_features})")
    
    # åˆ›å»ºDNMç®¡ç†å™¨é…ç½®
    config = {
        'splitter': {
            'entropy_threshold': 0.3,  # é™ä½é˜ˆå€¼ï¼Œç¡®ä¿ä¼šè§¦å‘åˆ†è£‚
            'overload_threshold': 0.3,
            'split_probability': 0.8,  # æé«˜åˆ†è£‚æ¦‚ç‡
            'max_splits_per_layer': 5,
            'inheritance_noise': 0.1
        },
        'monitoring': {
            'target_layers': ['conv', 'linear'],
            'analysis_frequency': 1,  # æ¯æ¬¡éƒ½åˆ†æ
            'min_epoch_before_split': 0  # ç«‹å³å¼€å§‹åˆ†è£‚
        }
    }
    
    dnm_manager = DNMNeuronDivision(config)
    
    # æ³¨å†Œhooks
    dnm_manager.register_model_hooks(model)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 4
    x = torch.randn(batch_size, 3, 32, 32)
    
    logger.info("ğŸ” æ‰§è¡Œå‰å‘ä¼ æ’­ä»¥æ”¶é›†ç»Ÿè®¡ä¿¡æ¯")
    with torch.no_grad():
        y = model(x)
        logger.info(f"è¾“å‡ºå½¢çŠ¶: {y.shape}")
    
    # æ‰§è¡Œç¥ç»å…ƒåˆ†è£‚
    logger.info("âš¡ æ‰§è¡ŒDNMç¥ç»å…ƒåˆ†è£‚")
    try:
        result = dnm_manager.analyze_and_split(model, epoch=0)
        splits_made = result.get('splits_executed', 0)
        logger.info(f"âœ… åˆ†è£‚å®Œæˆï¼Œæ‰§è¡Œäº† {splits_made} æ¬¡åˆ†è£‚")
    except Exception as e:
        logger.error(f"âŒ åˆ†è£‚è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        import traceback
        traceback.print_exc()
        return False
    
    # æ£€æŸ¥æ¨¡å‹å‚æ•°å˜åŒ–
    new_param_count = sum(p.numel() for p in model.parameters())
    logger.info(f"åˆ†è£‚åæ¨¡å‹å‚æ•°: {new_param_count}")
    
    # æ‰“å°åˆ†è£‚åçš„æ¨¡å‹ç»“æ„
    logger.info("ğŸ“‹ åˆ†è£‚åæ¨¡å‹ç»“æ„:")
    for name, module in model.named_modules():
        if isinstance(module, (nn.Conv2d, nn.BatchNorm2d)):
            if isinstance(module, nn.Conv2d):
                logger.info(f"  {name}: Conv2d(in={module.in_channels}, out={module.out_channels})")
            else:
                logger.info(f"  {name}: BatchNorm2d(features={module.num_features})")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­æ˜¯å¦æ­£å¸¸
    logger.info("ğŸ” æµ‹è¯•åˆ†è£‚åçš„å‰å‘ä¼ æ’­")
    try:
        with torch.no_grad():
            y_new = model(x)
            logger.info(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {y_new.shape}")
            return True
    except Exception as e:
        logger.error(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    logger.info("ğŸš€ å¯åŠ¨BatchNormåŒæ­¥è°ƒè¯•")
    
    success = test_batchnorm_sync()
    
    if success:
        logger.info("ğŸ‰ BatchNormåŒæ­¥æµ‹è¯•æˆåŠŸï¼")
    else:
        logger.error("ğŸ’¥ BatchNormåŒæ­¥æµ‹è¯•å¤±è´¥ï¼")