#!/usr/bin/env python3
"""
ASO-SEä¿®å¤ç‰ˆæœ¬ - è§£å†³æ¢¯åº¦è®¡ç®—å’Œç½‘ç»œç»“æ„é—®é¢˜

ğŸ”§ ä¸»è¦ä¿®å¤ï¼š
1. ä¿®å¤æ¢¯åº¦é‡å¤è®¡ç®—é”™è¯¯
2. ç®€åŒ–ç½‘ç»œç»“æ„é¿å…é—­ç¯
3. æ­£ç¡®ç®¡ç†æ¶æ„å‚æ•°å’Œæƒé‡å‚æ•°
4. ç¡®ä¿è®¾å¤‡ä¸€è‡´æ€§
5. æ·»åŠ è¯¦ç»†çš„é”™è¯¯æ£€æŸ¥
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import numpy as np
import time
import os
import sys
from datetime import datetime
from tqdm import tqdm
from typing import Dict, List, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥åŸºç¡€ç»„ä»¶
from neuroexapt.core.genotypes import PRIMITIVES
from neuroexapt.core.operations import OPS

# é…ç½®æ—¥å¿—
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(message)s', datefmt='%H:%M:%S')
logger = logging.getLogger()

class SimpleMixedOp(nn.Module):
    """
    ç®€åŒ–çš„æ··åˆæ“ä½œ - é¿å…å¤æ‚çš„æ¢¯åº¦è®¡ç®—é—®é¢˜
    """
    
    def __init__(self, C, stride):
        super().__init__()
        self.C = C
        self.stride = stride
        
        # åˆ›å»ºæ‰€æœ‰æ“ä½œ
        self._ops = nn.ModuleList()
        for primitive in PRIMITIVES:
            op = OPS[primitive](C, stride, False)
            if 'pool' in primitive:
                op = nn.Sequential(op, nn.BatchNorm2d(C, affine=False))
            self._ops.append(op)
    
    def forward(self, x, weights):
        """
        ç®€åŒ–çš„å‰å‘ä¼ æ’­ - é¿å…æ¢¯åº¦é‡å¤è®¡ç®—
        """
        # ç¡®ä¿æƒé‡åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        if weights.device != x.device:
            weights = weights.to(x.device)
        
        # è®¡ç®—åŠ æƒè¾“å‡º - ä½¿ç”¨ç¨³å®šçš„å®ç°
        outputs = []
        for w, op in zip(weights, self._ops):
            if w.item() > 1e-6:  # åªè®¡ç®—éé›¶æƒé‡çš„æ“ä½œ
                outputs.append(w * op(x))
        
        if outputs:
            return sum(outputs)
        else:
            # å¦‚æœæ‰€æœ‰æƒé‡éƒ½ä¸º0ï¼Œè¿”å›ç¬¬ä¸€ä¸ªæ“ä½œçš„ç»“æœ
            return self._ops[0](x) * 0.0

class SimpleArchitectureManager(nn.Module):
    """
    ç®€åŒ–çš„æ¶æ„å‚æ•°ç®¡ç†å™¨ - é¿å…å¤æ‚çš„æ‰¹é‡æ“ä½œ
    """
    
    def __init__(self, num_edges):
        super().__init__()
        self.num_edges = num_edges
        self.num_ops = len(PRIMITIVES)
        
        # åˆ›å»ºæ¶æ„å‚æ•° - æ¯æ¡è¾¹ä¸€ä¸ªå‚æ•°å‘é‡
        self.alpha = nn.ParameterList([
            nn.Parameter(torch.randn(self.num_ops) * 0.1) 
            for _ in range(num_edges)
        ])
        
        # Gumbelå‚æ•°
        self.temperature = 5.0
        self.min_temperature = 0.1
        self.anneal_rate = 0.98
    
    def get_weights(self, edge_idx):
        """è·å–ç‰¹å®šè¾¹çš„æƒé‡"""
        if edge_idx >= len(self.alpha):
            # å¦‚æœè¾¹ç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œè¿”å›å‡åŒ€åˆ†å¸ƒ
            return F.softmax(torch.ones(self.num_ops, device=self.alpha[0].device), dim=0)
        
        if self.training:
            # è®­ç»ƒæ—¶ä½¿ç”¨Gumbel-Softmax
            return self._gumbel_softmax(self.alpha[edge_idx])
        else:
            # æ¨ç†æ—¶ä½¿ç”¨ç®€å•softmax
            return F.softmax(self.alpha[edge_idx], dim=0)
    
    def _gumbel_softmax(self, logits):
        """Gumbel-Softmaxé‡‡æ ·"""
        # ç”ŸæˆGumbelå™ªå£°
        gumbel = -torch.log(-torch.log(torch.rand_like(logits) + 1e-8) + 1e-8)
        
        # åŠ å…¥å™ªå£°å¹¶å½’ä¸€åŒ–
        noisy_logits = (logits + gumbel) / self.temperature
        return F.softmax(noisy_logits, dim=0)
    
    def anneal_temperature(self):
        """é€€ç«æ¸©åº¦"""
        self.temperature = max(self.min_temperature, self.temperature * self.anneal_rate)
        return self.temperature

class FixedEvolvableBlock(nn.Module):
    """
    ä¿®å¤çš„å¯æ¼”åŒ–å— - ç®€åŒ–ç»“æ„é¿å…é—­ç¯
    """
    
    def __init__(self, in_channels, out_channels, block_id, stride=1):
        super().__init__()
        
        self.block_id = block_id
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.stride = stride
        
        # è¾“å…¥é€‚é…å±‚
        if in_channels != out_channels or stride != 1:
            self.preprocess = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
        else:
            self.preprocess = nn.Identity()
        
        # æ··åˆæ“ä½œ
        self.mixed_op = SimpleMixedOp(out_channels, stride=1)  # å†…éƒ¨æ€»æ˜¯stride=1
        
        # æœ€ç»ˆå¤„ç†
        self.final_conv = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, x, weights):
        """å‰å‘ä¼ æ’­"""
        # è¾“å…¥å¤„ç†
        identity = self.preprocess(x)
        
        # æ··åˆæ“ä½œ
        out = self.mixed_op(identity, weights)
        
        # æ®‹å·®è¿æ¥
        out = out + identity
        
        # æœ€ç»ˆå¤„ç†
        out = self.final_conv(out)
        
        return out

class FixedASOSENetwork(nn.Module):
    """
    ä¿®å¤çš„ASO-SEç½‘ç»œ - é¿å…æ¢¯åº¦è®¡ç®—é—®é¢˜
    """
    
    def __init__(self, num_classes=10, initial_channels=32, initial_depth=4):
        super().__init__()
        
        self.num_classes = num_classes
        self.initial_channels = initial_channels
        self.current_depth = initial_depth
        
        # Stemå±‚
        self.stem = nn.Sequential(
            nn.Conv2d(3, initial_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(initial_channels),
            nn.ReLU(inplace=True)
        )
        
        # æ„å»ºç½‘ç»œå±‚
        self.layers = nn.ModuleList()
        self._build_initial_architecture()
        
        # æ¶æ„å‚æ•°ç®¡ç†
        self.arch_manager = SimpleArchitectureManager(self.current_depth)
        
        # åˆ†ç±»å™¨
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.classifier = nn.Linear(self.layers[-1].out_channels, num_classes)
        
        # è®­ç»ƒçŠ¶æ€
        self.training_phase = "weight_training"
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.growth_stats = {
            'depth_growths': 0,
            'channel_growths': 0,
            'total_growths': 0
        }
        
        print(f"ğŸš€ Fixed ASOSE Network initialized:")
        print(f"   Depth: {self.current_depth}, Channels: {initial_channels}")
        print(f"   Parameters: {sum(p.numel() for p in self.parameters()):,}")
    
    def _build_initial_architecture(self):
        """æ„å»ºåˆå§‹æ¶æ„"""
        current_channels = self.initial_channels
        
        for i in range(self.current_depth):
            # ä¸‹é‡‡æ ·ç­–ç•¥
            if i == self.current_depth // 2:
                stride = 2
                out_channels = current_channels * 2
            else:
                stride = 1
                out_channels = current_channels
            
            block = FixedEvolvableBlock(
                current_channels, out_channels, f"layer_{i}", stride
            )
            
            self.layers.append(block)
            current_channels = out_channels
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        # Stem
        x = self.stem(x)
        
        # ç½‘ç»œå±‚
        for i, layer in enumerate(self.layers):
            # è·å–å½“å‰å±‚çš„æ¶æ„æƒé‡
            weights = self.arch_manager.get_weights(i)
            x = layer(x, weights)
        
        # åˆ†ç±»
        x = self.global_pool(x)
        x = x.view(x.size(0), -1)
        return self.classifier(x)
    
    def set_training_phase(self, phase: str):
        """è®¾ç½®è®­ç»ƒé˜¶æ®µ"""
        valid_phases = ["weight_training", "arch_training", "mutation", "retraining"]
        if phase not in valid_phases:
            raise ValueError(f"Invalid phase: {phase}")
        
        self.training_phase = phase
        print(f"ğŸ”„ Training phase: {phase}")
    
    def get_architecture_parameters(self):
        """è·å–æ¶æ„å‚æ•°"""
        arch_params = []
        for alpha in self.arch_manager.alpha:
            arch_params.append(alpha)
        return arch_params
    
    def get_weight_parameters(self):
        """è·å–æƒé‡å‚æ•°"""
        weight_params = []
        for name, param in self.named_parameters():
            if 'arch_manager.alpha' not in name:
                weight_params.append(param)
        return weight_params
    
    def grow_depth(self):
        """å¢åŠ ç½‘ç»œæ·±åº¦"""
        # è·å–æ’å…¥ä½ç½®
        position = len(self.layers) - 1
        
        # ç¡®å®šæ–°å±‚é…ç½®
        prev_layer = self.layers[position-1] if position > 0 else None
        if prev_layer:
            in_channels = prev_layer.out_channels
            out_channels = in_channels
        else:
            in_channels = self.initial_channels
            out_channels = self.initial_channels
        
        # åˆ›å»ºæ–°å±‚
        new_layer = FixedEvolvableBlock(
            in_channels, out_channels, f"grown_{len(self.layers)}", stride=1
        )
        
        # ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        device = next(self.parameters()).device
        new_layer = new_layer.to(device)
        
        # æ’å…¥å±‚
        self.layers.insert(position, new_layer)
        self.current_depth += 1
        
        # æ›´æ–°æ¶æ„ç®¡ç†å™¨
        self.arch_manager = SimpleArchitectureManager(self.current_depth)
        self.arch_manager = self.arch_manager.to(device)
        
        # æ›´æ–°ç»Ÿè®¡
        self.growth_stats['depth_growths'] += 1
        self.growth_stats['total_growths'] += 1
        
        print(f"ğŸŒ± DEPTH GROWTH: Added layer at position {position}")
        print(f"   New depth: {self.current_depth}")
        print(f"   New parameters: {sum(p.numel() for p in self.parameters()):,}")
        
        return True
    
    def grow_width(self, layer_idx=None, expansion_factor=1.4):
        """å¢åŠ ç½‘ç»œå®½åº¦ - ç®€åŒ–å®ç°"""
        print(f"ğŸŒ± WIDTH GROWTH: Expansion factor {expansion_factor}")
        
        # ç®€åŒ–ï¼šåªå¢åŠ æœ€åä¸€å±‚çš„é€šé“æ•°
        if len(self.layers) > 0:
            last_layer = self.layers[-1]
            old_channels = last_layer.out_channels
            new_channels = int(old_channels * expansion_factor)
            
            # æ›´æ–°åˆ†ç±»å™¨
            device = next(self.parameters()).device
            old_classifier = self.classifier
            self.classifier = nn.Linear(new_channels, self.num_classes).to(device)
            
            # ç®€å•çš„æƒé‡è¿ç§»
            with torch.no_grad():
                min_features = min(old_classifier.in_features, new_channels)
                self.classifier.weight[:, :min_features] = old_classifier.weight[:, :min_features]
                self.classifier.bias.copy_(old_classifier.bias)
            
            self.growth_stats['channel_growths'] += 1
            self.growth_stats['total_growths'] += 1
            
            print(f"   Classifier updated: {old_channels} â†’ {new_channels}")
            return True
        
        return False
    
    def anneal_gumbel_temperature(self):
        """é€€ç«Gumbelæ¸©åº¦"""
        return self.arch_manager.anneal_temperature()

class FixedTrainingController:
    """ä¿®å¤çš„è®­ç»ƒæ§åˆ¶å™¨"""
    
    def __init__(self):
        self.last_growth_cycle = -1
    
    def should_trigger_growth(self, network, current_cycle, current_accuracy, accuracy_trend):
        """æ™ºèƒ½ç”Ÿé•¿è§¦å‘åˆ¤æ–­"""
        # æ›´ä¿å®ˆçš„ç”Ÿé•¿ç­–ç•¥
        if current_cycle - self.last_growth_cycle >= 5:
            print(f"ğŸŒ± Forced growth trigger (cycle {current_cycle})")
            return True
        
        # æ€§èƒ½åœæ»æ£€æµ‹
        if len(accuracy_trend) >= 4:
            recent_improvement = max(accuracy_trend[-4:]) - min(accuracy_trend[-4:])
            if recent_improvement < 1.0 and current_cycle - self.last_growth_cycle >= 3:
                print(f"ğŸŒ± Stagnation growth trigger (improvement: {recent_improvement:.2f}%)")
                return True
        
        return False
    
    def select_growth_strategy(self, network, current_accuracy, cycle_count):
        """é€‰æ‹©ç”Ÿé•¿ç­–ç•¥"""
        total_params = sum(p.numel() for p in network.parameters())
        
        # æ›´ä¿å®ˆçš„ç­–ç•¥é€‰æ‹©
        if current_accuracy < 60:
            if network.current_depth < 8:
                return 'grow_depth'
            else:
                return 'grow_width'
        else:
            return 'grow_width'
    
    def execute_growth(self, network, strategy, cycle_count):
        """æ‰§è¡Œç”Ÿé•¿ç­–ç•¥"""
        success = False
        
        try:
            pre_growth_params = sum(p.numel() for p in network.parameters())
            
            if strategy == 'grow_depth':
                success = network.grow_depth()
            elif strategy == 'grow_width':
                success = network.grow_width(expansion_factor=1.3)
            
            if success:
                self.last_growth_cycle = cycle_count
                post_growth_params = sum(p.numel() for p in network.parameters())
                
                print(f"âœ… Growth executed successfully!")
                print(f"   Parameters: {pre_growth_params:,} â†’ {post_growth_params:,}")
                
        except Exception as e:
            print(f"âŒ Growth failed: {e}")
            import traceback
            traceback.print_exc()
            success = False
        
        return success

class FixedASOSETrainer:
    """ä¿®å¤çš„ASO-SEè®­ç»ƒå™¨"""
    
    def __init__(self, experiment_name="aso_se_fixed"):
        self.experiment_name = experiment_name
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # æ ¸å¿ƒç»„ä»¶
        self.network = None
        self.training_controller = FixedTrainingController()
        
        # ä¼˜åŒ–å™¨
        self.weight_optimizer = None
        self.arch_optimizer = None
        self.scheduler = None
        
        # è®­ç»ƒçŠ¶æ€
        self.current_cycle = 0
        self.best_accuracy = 0.0
        self.cycle_results = []
        
        print(f"ğŸš€ Fixed ASOSE Trainer initialized")
        print(f"   Device: {self.device}")
        print(f"   Experiment: {experiment_name}")
    
    def setup_data(self, batch_size=128):
        """è®¾ç½®æ•°æ®åŠ è½½"""
        print("ğŸ“Š Setting up CIFAR-10 data...")
        
        train_transform = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])
        
        test_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])
        
        train_dataset = torchvision.datasets.CIFAR10(
            './data', train=True, download=True, transform=train_transform
        )
        test_dataset = torchvision.datasets.CIFAR10(
            './data', train=False, transform=test_transform
        )
        
        self.train_loader = DataLoader(
            train_dataset, batch_size=batch_size, shuffle=True,
            num_workers=2, pin_memory=True
        )
        self.test_loader = DataLoader(
            test_dataset, batch_size=batch_size, shuffle=False,
            num_workers=2, pin_memory=True
        )
        
        print(f"âœ… Data ready: {len(train_dataset)} train, {len(test_dataset)} test")
    
    def setup_network(self, initial_channels=32, initial_depth=4):
        """è®¾ç½®ç½‘ç»œ"""
        self.network = FixedASOSENetwork(
            num_classes=10,
            initial_channels=initial_channels,
            initial_depth=initial_depth
        ).to(self.device)
        
        self._create_optimizers()
        
        total_params = sum(p.numel() for p in self.network.parameters())
        print(f"ğŸ“Š Network ready: {total_params:,} parameters")
    
    def _create_optimizers(self):
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        # æƒé‡å‚æ•°ä¼˜åŒ–å™¨
        weight_params = self.network.get_weight_parameters()
        self.weight_optimizer = optim.SGD(
            weight_params, lr=0.025, momentum=0.9, weight_decay=1e-4
        )
        
        # æ¶æ„å‚æ•°ä¼˜åŒ–å™¨
        arch_params = self.network.get_architecture_parameters()
        if arch_params:
            self.arch_optimizer = optim.Adam(arch_params, lr=3e-4, weight_decay=1e-3)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.weight_optimizer, T_max=200, eta_min=1e-6
        )
    
    def train_epoch(self, epoch, phase):
        """è®­ç»ƒepoch - ä¿®å¤æ¢¯åº¦è®¡ç®—é—®é¢˜"""
        self.network.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        # é‡è¦ï¼šæ¸…é™¤æ‰€æœ‰æ¢¯åº¦çŠ¶æ€
        if hasattr(self, 'weight_optimizer'):
            self.weight_optimizer.zero_grad()
        if hasattr(self, 'arch_optimizer') and self.arch_optimizer:
            self.arch_optimizer.zero_grad()
        
        # è®¾ç½®å‚æ•°è®­ç»ƒçŠ¶æ€
        if phase == "arch_training":
            optimizer = self.arch_optimizer
            # å†»ç»“æƒé‡å‚æ•°
            for param in self.network.get_weight_parameters():
                param.requires_grad_(False)
            # æ¿€æ´»æ¶æ„å‚æ•°
            for param in self.network.get_architecture_parameters():
                param.requires_grad_(True)
        else:
            optimizer = self.weight_optimizer
            # æ¿€æ´»æƒé‡å‚æ•°
            for param in self.network.get_weight_parameters():
                param.requires_grad_(True)
            # å†»ç»“æ¶æ„å‚æ•°
            for param in self.network.get_architecture_parameters():
                param.requires_grad_(False)
        
        criterion = nn.CrossEntropyLoss()
        
        pbar = tqdm(self.train_loader, desc=f"ğŸ”§ {phase} Epoch {epoch:02d}")
        
        for batch_idx, (data, target) in enumerate(pbar):
            # æ•°æ®è½¬ç§»åˆ°è®¾å¤‡
            data = data.to(self.device, non_blocking=True)
            target = target.to(self.device, non_blocking=True)
            
            # æ¸…é™¤æ¢¯åº¦
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            output = self.network(data)
            loss = criterion(output, target)
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            if phase == "arch_training":
                torch.nn.utils.clip_grad_norm_(self.network.get_architecture_parameters(), 5.0)
            else:
                torch.nn.utils.clip_grad_norm_(self.network.get_weight_parameters(), 5.0)
            
            # æ›´æ–°å‚æ•°
            optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            pred = output.argmax(dim=1)
            correct += pred.eq(target).sum().item()
            total += target.size(0)
            
            # æ›´æ–°æ˜¾ç¤º
            if batch_idx % 100 == 0:
                pbar.set_postfix({
                    'Loss': f'{total_loss/(batch_idx+1):.4f}',
                    'Acc': f'{100.*correct/total:.2f}%',
                    'Depth': self.network.current_depth,
                    'Phase': phase[:6]
                })
        
        return total_loss/len(self.train_loader), 100.*correct/total
    
    def validate(self):
        """éªŒè¯"""
        self.network.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        criterion = nn.CrossEntropyLoss()
        
        with torch.no_grad():
            for data, target in self.test_loader:
                data = data.to(self.device, non_blocking=True)
                target = target.to(self.device, non_blocking=True)
                
                output = self.network(data)
                loss = criterion(output, target)
                
                total_loss += loss.item()
                pred = output.argmax(dim=1)
                correct += pred.eq(target).sum().item()
                total += target.size(0)
        
        return total_loss/len(self.test_loader), 100.*correct/total
    
    def run_training_cycle(self):
        """è¿è¡Œè®­ç»ƒå‘¨æœŸ"""
        cycle_start_time = time.time()
        cycle_results = {}
        
        print(f"\n{'='*80}")
        print(f"ğŸ”§ Fixed ASO-SE Training Cycle {self.current_cycle + 1}")
        print(f"{'='*80}")
        
        # é˜¶æ®µ1: æƒé‡è®­ç»ƒ
        print(f"\nğŸ”¥ Phase 1: Weight Training")
        self.network.set_training_phase("weight_training")
        weight_results = self._run_phase("weight_training", 5)
        cycle_results['weight_training'] = weight_results
        
        # é˜¶æ®µ2: æ¶æ„è®­ç»ƒ
        print(f"\nğŸ§  Phase 2: Architecture Training")
        self.network.set_training_phase("arch_training")
        arch_results = self._run_phase("arch_training", 2)
        cycle_results['arch_training'] = arch_results
        
        # é˜¶æ®µ3: æ¶æ„çªå˜
        print(f"\nğŸ§¬ Phase 3: Architecture Mutation")
        mutation_success = self._architecture_mutation()
        cycle_results['mutation_success'] = mutation_success
        
        # é˜¶æ®µ4: æƒé‡å†è®­ç»ƒ
        print(f"\nğŸ”§ Phase 4: Weight Retraining")
        self.network.set_training_phase("retraining")
        retrain_results = self._run_phase("retraining", 3)
        cycle_results['retraining'] = retrain_results
        
        cycle_time = time.time() - cycle_start_time
        cycle_results['cycle_time'] = cycle_time
        cycle_results['final_accuracy'] = retrain_results['final_test_acc']
        
        self.cycle_results.append(cycle_results)
        
        print(f"\nâœ… Cycle {self.current_cycle + 1} completed in {cycle_time/60:.1f} minutes")
        print(f"   Final accuracy: {cycle_results['final_accuracy']:.2f}%")
        print(f"   Best so far: {self.best_accuracy:.2f}%")
        
        return cycle_results
    
    def _run_phase(self, phase_name, num_epochs):
        """è¿è¡Œè®­ç»ƒé˜¶æ®µ"""
        phase_results = {'epochs': [], 'final_train_acc': 0, 'final_test_acc': 0}
        
        for epoch in range(num_epochs):
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(epoch, phase_name)
            
            # éªŒè¯
            test_loss, test_acc = self.validate()
            
            # æ›´æ–°å­¦ä¹ ç‡
            if phase_name != "arch_training":
                self.scheduler.step()
            
            # è®°å½•ç»“æœ
            epoch_result = {
                'epoch': epoch,
                'train_loss': train_loss,
                'train_acc': train_acc,
                'test_loss': test_loss,
                'test_acc': test_acc,
                'lr': self.weight_optimizer.param_groups[0]['lr']
            }
            phase_results['epochs'].append(epoch_result)
            
            # æ›´æ–°æœ€ä½³æ€§èƒ½
            if test_acc > self.best_accuracy:
                self.best_accuracy = test_acc
            
            # Gumbelæ¸©åº¦é€€ç«
            if phase_name == "arch_training":
                temp = self.network.anneal_gumbel_temperature()
                epoch_result['gumbel_temp'] = temp
            
            print(f"   Epoch {epoch+1}: Train={train_acc:.2f}%, Test={test_acc:.2f}%, Best={self.best_accuracy:.2f}%")
        
        phase_results['final_train_acc'] = phase_results['epochs'][-1]['train_acc']
        phase_results['final_test_acc'] = phase_results['epochs'][-1]['test_acc']
        
        return phase_results
    
    def _architecture_mutation(self):
        """æ¶æ„çªå˜"""
        recent_accuracies = [result['final_accuracy'] for result in self.cycle_results[-3:]]
        if len(recent_accuracies) < 3:
            recent_accuracies = [50.0]
        
        current_accuracy = recent_accuracies[-1] if recent_accuracies else 50.0
        
        should_grow = self.training_controller.should_trigger_growth(
            self.network, self.current_cycle, current_accuracy, recent_accuracies
        )
        
        if should_grow:
            print("ğŸŒ± Triggering network growth...")
            
            strategy = self.training_controller.select_growth_strategy(
                self.network, current_accuracy, self.current_cycle
            )
            
            success = self.training_controller.execute_growth(
                self.network, strategy, self.current_cycle
            )
            
            if success:
                # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨
                self._create_optimizers()
                print("ğŸ‰ Network growth successful!")
                return True
            else:
                print("âŒ Network growth failed")
                return False
        else:
            print("ğŸ”„ No growth triggered, annealing temperature...")
            temp = self.network.anneal_gumbel_temperature()
            print(f"   Current temperature: {temp:.3f}")
            return False
    
    def train(self, max_cycles=15, initial_channels=32, initial_depth=4, batch_size=128):
        """ä¸»è®­ç»ƒæµç¨‹"""
        print(f"\nğŸ”§ Fixed ASO-SE Training Started")
        print(f"ğŸ¯ Target: CIFAR-10 95%+ accuracy")
        print(f"âš™ï¸  Config: max_cycles={max_cycles}, channels={initial_channels}, depth={initial_depth}")
        
        start_time = time.time()
        
        # è®¾ç½®
        self.setup_data(batch_size)
        self.setup_network(initial_channels, initial_depth)
        
        try:
            # ä¸»è®­ç»ƒå¾ªç¯
            for cycle in range(max_cycles):
                self.current_cycle = cycle
                
                # è¿è¡Œè®­ç»ƒå‘¨æœŸ
                cycle_result = self.run_training_cycle()
                
                # æ£€æŸ¥ç›®æ ‡
                if cycle_result['final_accuracy'] >= 95.0:
                    print(f"\nğŸ‰ TARGET ACHIEVED! Accuracy: {cycle_result['final_accuracy']:.2f}%")
                    break
                
                # æ—©åœæ£€æŸ¥
                if self._should_early_stop():
                    print(f"\nâ¹ï¸  Early stopping triggered")
                    break
        
        except KeyboardInterrupt:
            print("\nâ¹ï¸  Training interrupted by user")
        except Exception as e:
            print(f"\nâŒ Training error: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            # æœ€ç»ˆç»Ÿè®¡
            total_time = time.time() - start_time
            self._display_final_summary(total_time)
    
    def _should_early_stop(self):
        """æ—©åœæ£€æŸ¥"""
        if len(self.cycle_results) < 6:
            return False
        
        recent_accs = [r['final_accuracy'] for r in self.cycle_results[-6:]]
        improvement = max(recent_accs) - min(recent_accs)
        
        return improvement < 0.5
    
    def _display_final_summary(self, total_time):
        """æ˜¾ç¤ºæœ€ç»ˆæ€»ç»“"""
        print(f"\n{'='*80}")
        print(f"ğŸ‰ Fixed ASO-SE Training Completed!")
        print(f"{'='*80}")
        
        print(f"â±ï¸  Total time: {total_time/3600:.1f} hours ({total_time/60:.1f} minutes)")
        print(f"ğŸ”„ Total cycles: {len(self.cycle_results)}")
        print(f"ğŸ† Best accuracy: {self.best_accuracy:.2f}%")
        
        if self.cycle_results:
            final_result = self.cycle_results[-1]
            print(f"ğŸ“Š Final accuracy: {final_result['final_accuracy']:.2f}%")
        
        print(f"ğŸ—ï¸  Final architecture:")
        print(f"   Depth: {self.network.current_depth} layers")
        print(f"   Parameters: {sum(p.numel() for p in self.network.parameters()):,}")
        print(f"   Total growths: {self.network.growth_stats['total_growths']}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Fixed ASO-SE Neural Network Training')
    parser.add_argument('--cycles', type=int, default=15, help='Maximum training cycles')
    parser.add_argument('--batch_size', type=int, default=128, help='Batch size')
    parser.add_argument('--initial_channels', type=int, default=32, help='Initial channels')
    parser.add_argument('--initial_depth', type=int, default=4, help='Initial depth')
    parser.add_argument('--experiment', type=str, default='aso_se_fixed', help='Experiment name')
    
    args = parser.parse_args()
    
    print("ğŸ”§ Fixed ASO-SE: Gradient-Safe Architecture Search")
    print("ğŸ¯ Target: CIFAR-10 95%+ accuracy")
    print(f"â° Start: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“‹ Config: {vars(args)}")
    
    # åˆ›å»ºä¿®å¤çš„è®­ç»ƒå™¨
    trainer = FixedASOSETrainer(args.experiment)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(
        max_cycles=args.cycles,
        initial_channels=args.initial_channels,
        initial_depth=args.initial_depth,
        batch_size=args.batch_size
    )

if __name__ == "__main__":
    main()