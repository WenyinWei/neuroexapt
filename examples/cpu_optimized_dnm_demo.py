#!/usr/bin/env python3
"""
CPUä¼˜åŒ–çš„é«˜çº§DNMæ¼”ç¤º - è§£å†³å†…å­˜é—®é¢˜
ä¿®å¤è¢«killedçš„é—®é¢˜ï¼Œä¼˜åŒ–èµ„æºä½¿ç”¨
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
import logging
import gc
import sys
import os

# è®¾ç½®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from neuroexapt.core import (
    AdvancedBottleneckAnalyzer,
    AdvancedMorphogenesisExecutor, 
    IntelligentMorphogenesisDecisionMaker,
    EnhancedDNMFramework
)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)

class CPUOptimizedResNet(nn.Module):
    """CPUä¼˜åŒ–çš„ResNet - å‡å°‘å‚æ•°é‡å’Œè®¡ç®—å¤æ‚åº¦"""
    
    def __init__(self, num_classes=10):
        super(CPUOptimizedResNet, self).__init__()
        
        # ğŸš€ CPUå‹å¥½çš„åˆå§‹ç‰¹å¾æå– - å‡å°‘é€šé“æ•°
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1, bias=False)  # 64 â†’ 32
        self.bn1 = nn.BatchNorm2d(32)
        self.relu = nn.ReLU(inplace=True)
        
        # ğŸš€ è½»é‡çº§æ®‹å·®å—ç»„
        self.feature_block1 = self._make_resnet_block(32, 64, 2, 1)    # å‡å°‘å—æ•°
        self.feature_block2 = self._make_resnet_block(64, 128, 2, 1)   
        self.feature_block3 = self._make_resnet_block(128, 256, 2, 1)   
        
        # ğŸš€ ç®€åŒ–çš„å…¨å±€ç‰¹å¾èšåˆ
        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))
        
        # ğŸš€ CPUå‹å¥½çš„åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(256, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            
            nn.Linear(512, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            
            nn.Linear(128, num_classes)
        )
    
    def _make_resnet_block(self, in_channels, out_channels, stride, num_blocks):
        """åˆ›å»ºè½»é‡çº§æ®‹å·®å—ç»„"""
        layers = []
        
        # ç¬¬ä¸€ä¸ªå—å¯èƒ½æœ‰é™é‡‡æ ·
        layers.append(LightResidualBlock(in_channels, out_channels, stride))
        
        # åç»­å—ä¿æŒç›¸åŒå°ºå¯¸
        for _ in range(1, num_blocks):
            layers.append(LightResidualBlock(out_channels, out_channels, 1))
            
        return nn.Sequential(*layers)
    
    def forward(self, x):
        # ğŸš€ CPUä¼˜åŒ–çš„å‰å‘ä¼ æ’­
        x = self.relu(self.bn1(self.conv1(x)))
        
        # æ®‹å·®ç‰¹å¾æå–
        x = self.feature_block1(x)
        x = self.feature_block2(x)
        x = self.feature_block3(x)
        
        # å…¨å±€æ± åŒ–å’Œåˆ†ç±»
        x = self.global_pool(x)
        x = self.classifier(x)
        
        return x

class LightResidualBlock(nn.Module):
    """è½»é‡çº§æ®‹å·®å—"""
    
    def __init__(self, in_channels, out_channels, stride=1):
        super(LightResidualBlock, self).__init__()
        
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        
        # æ®‹å·®è¿æ¥
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
    
    def forward(self, x):
        residual = self.shortcut(x)
        
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        
        out += residual
        out = self.relu(out)
        
        return out

class CPUOptimizedDNMTrainer:
    """CPUä¼˜åŒ–çš„DNMè®­ç»ƒå™¨"""
    
    def __init__(self, model, device, train_loader, test_loader):
        self.model = model.to(device)
        self.device = device
        self.train_loader = train_loader
        self.test_loader = test_loader
        
        # ğŸš€ CPUä¼˜åŒ–çš„DNMæ¡†æ¶é…ç½®
        self.dnm_config = {
            'trigger_interval': 10,  # å¢åŠ é—´éš”ï¼Œå‡å°‘è§¦å‘é¢‘ç‡
            'complexity_threshold': 0.6,  # æé«˜é˜ˆå€¼ï¼Œå‡å°‘ä¸å¿…è¦çš„å˜å¼‚
            'enable_serial_division': True,
            'enable_parallel_division': False,  # æš‚æ—¶ç¦ç”¨å¹¶è¡Œåˆ†è£‚
            'enable_hybrid_division': False,    # æš‚æ—¶ç¦ç”¨æ··åˆåˆ†è£‚
            'max_parameter_growth_ratio': 1.5   # é™åˆ¶å‚æ•°å¢é•¿
        }
        
        self.dnm_framework = EnhancedDNMFramework(self.dnm_config)
        
        # è®­ç»ƒå†å²
        self.train_history = []
        self.test_history = []
        self.parameter_history = []
        self.morphogenesis_history = []
    
    def train_epoch(self, optimizer):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        running_loss = 0.0
        running_acc = 0.0
        total_samples = 0
        
        for batch_idx, (data, target) in enumerate(self.train_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            optimizer.zero_grad()
            output = self.model(data)
            loss = F.cross_entropy(output, target)
            loss.backward()
            optimizer.step()
            
            # ç»Ÿè®¡
            running_loss += loss.item() * data.size(0)
            pred = output.argmax(dim=1, keepdim=True)
            running_acc += pred.eq(target.view_as(pred)).sum().item()
            total_samples += data.size(0)
            
            # ğŸš€ æ˜¾ç¤ºè¿›åº¦ï¼Œå‡å°‘å†…å­˜å ç”¨
            if batch_idx % 50 == 0:
                current_acc = 100. * running_acc / total_samples
                print(f"    Train Batch: {batch_idx:3d}/{len(self.train_loader)}, "
                      f"Loss: {loss.item():.6f}, Acc: {current_acc:.2f}%")
                
                # ğŸš€ å¼ºåˆ¶åƒåœ¾å›æ”¶
                if batch_idx % 100 == 0:
                    gc.collect()
        
        avg_loss = running_loss / total_samples
        avg_acc = 100. * running_acc / total_samples
        
        return avg_loss, avg_acc
    
    def test_epoch(self):
        """æµ‹è¯•ä¸€ä¸ªepoch"""
        self.model.eval()
        test_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in self.test_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                test_loss += F.cross_entropy(output, target, reduction='sum').item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)
        
        test_loss /= total
        accuracy = 100. * correct / total
        
        return test_loss, accuracy
    
    def capture_network_state(self):
        """æ•è·ç½‘ç»œçŠ¶æ€ - ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        activations = {}
        gradients = {}
        
        # ğŸš€ åªæ•è·å…³é”®å±‚çš„çŠ¶æ€ï¼Œå‡å°‘å†…å­˜å ç”¨
        key_modules = ['feature_block3', 'classifier']
        
        for name, module in self.model.named_modules():
            if any(key in name for key in key_modules):
                # ç®€åŒ–çš„æ¿€æ´»ç»Ÿè®¡
                if hasattr(module, 'weight') and module.weight is not None:
                    activations[name] = {
                        'mean': float(module.weight.data.mean()),
                        'std': float(module.weight.data.std())
                    }
                    
                    if module.weight.grad is not None:
                        gradients[name] = {
                            'mean': float(module.weight.grad.mean()),
                            'std': float(module.weight.grad.std())
                        }
        
        return activations, gradients
    
    def train_with_morphogenesis(self, epochs=50):  # å‡å°‘é»˜è®¤è½®æ•°
        """CPUä¼˜åŒ–çš„å½¢æ€å‘ç”Ÿè®­ç»ƒ"""
        print("ğŸ§¬ å¼€å§‹CPUä¼˜åŒ–çš„DNMè®­ç»ƒ...")
        print("=" * 60)
        
        # ğŸš€ CPUä¼˜åŒ–çš„è®­ç»ƒé…ç½®
        optimizer = optim.SGD(
            self.model.parameters(), 
            lr=0.05,             # ç¨å¾®é™ä½å­¦ä¹ ç‡
            momentum=0.9,
            weight_decay=1e-4,   # å‡å°‘æƒé‡è¡°å‡
            nesterov=True
        )
        
        # ğŸš€ ç®€åŒ–çš„å­¦ä¹ ç‡è°ƒåº¦
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)
        
        # è®°å½•åˆå§‹å‚æ•°æ•°é‡
        initial_params = sum(p.numel() for p in self.model.parameters())
        print(f"ğŸ“Š åˆå§‹å‚æ•°æ•°é‡: {initial_params:,}")
        self.parameter_history.append(initial_params)
        
        best_test_acc = 0.0
        patience_counter = 0
        
        for epoch in range(epochs):
            print(f"\nğŸ§¬ Epoch {epoch+1}/{epochs}")
            
            # è®­ç»ƒå’Œæµ‹è¯•
            train_loss, train_acc = self.train_epoch(optimizer)
            test_loss, test_acc = self.test_epoch()
            
            # è®°å½•å†å²
            self.train_history.append((train_loss, train_acc))
            self.test_history.append((test_loss, test_acc))
            
            print(f"  ğŸ“Š Train: {train_acc:.2f}% (Loss: {train_loss:.4f}) | "
                  f"Test: {test_acc:.2f}% (Loss: {test_loss:.4f})")
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step()
            current_lr = optimizer.param_groups[0]['lr']
            print(f"  ğŸ“ˆ å½“å‰å­¦ä¹ ç‡: {current_lr:.6f}")
            
            # ğŸš€ å‡å°‘å½¢æ€å‘ç”Ÿé¢‘ç‡ï¼ŒèŠ‚çœèµ„æº
            if epoch >= 15 and epoch % self.dnm_config['trigger_interval'] == 0:
                print("  ğŸ”¬ æ£€æŸ¥å½¢æ€å‘ç”Ÿéœ€æ±‚...")
                
                activations, gradients = self.capture_network_state()
                
                context = {
                    'epoch': epoch,
                    'activations': activations,
                    'gradients': gradients,
                    'performance_history': self.dnm_framework.performance_history
                }
                
                # æ‰§è¡Œå½¢æ€å‘ç”Ÿ
                results = self.dnm_framework.execute_morphogenesis(self.model, context)
                
                if results['model_modified']:
                    print(f"  ğŸ‰ å½¢æ€å‘ç”ŸæˆåŠŸ!")
                    print(f"    ç±»å‹: {results['morphogenesis_type']}")
                    print(f"    æ–°å¢å‚æ•°: {results['parameters_added']:,}")
                    
                    # æ›´æ–°æ¨¡å‹
                    self.model = results['new_model']
                    
                    # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨
                    current_lr = optimizer.param_groups[0]['lr']
                    optimizer = optim.SGD(
                        self.model.parameters(), 
                        lr=current_lr,
                        momentum=0.9,
                        weight_decay=1e-4,
                        nesterov=True
                    )
                    
                    # è®°å½•å½¢æ€å‘ç”Ÿäº‹ä»¶
                    current_params = sum(p.numel() for p in self.model.parameters())
                    self.parameter_history.append(current_params)
                    
                    self.morphogenesis_history.append({
                        'epoch': epoch,
                        'type': results['morphogenesis_type'],
                        'parameters_added': results['parameters_added'],
                        'test_acc_before': test_acc,
                        'total_params': current_params
                    })
                    
                    print(f"    æ€»å‚æ•°: {current_params:,} "
                          f"(+{((current_params-initial_params)/initial_params*100):.1f}%)")
                    
                    # ğŸš€ å¼ºåˆ¶åƒåœ¾å›æ”¶
                    gc.collect()
                else:
                    current_params = sum(p.numel() for p in self.model.parameters())
                    self.parameter_history.append(current_params)
            else:
                current_params = sum(p.numel() for p in self.model.parameters())
                self.parameter_history.append(current_params)
            
            # æ€§èƒ½ç›‘æ§
            if test_acc > best_test_acc:
                best_test_acc = test_acc
                patience_counter = 0
                print(f"  ğŸ¯ æ–°çš„æœ€ä½³å‡†ç¡®ç‡: {best_test_acc:.2f}%!")
                
                if best_test_acc >= 95.0:
                    print("  ğŸ† æ­å–œï¼è¾¾åˆ°95%+å‡†ç¡®ç‡ç›®æ ‡!")
                elif best_test_acc >= 90.0:
                    print("  ğŸŒŸ å¾ˆå¥½ï¼è¾¾åˆ°90%+å‡†ç¡®ç‡!")
                elif best_test_acc >= 85.0:
                    print("  âœ¨ ä¸é”™ï¼è¾¾åˆ°85%+å‡†ç¡®ç‡!")
            else:
                patience_counter += 1
            
            # æ—©åœæ£€æŸ¥
            if patience_counter >= 20:  # å¢åŠ è€å¿ƒå€¼
                print(f"  ğŸ›‘ Early stopping triggered at epoch {epoch+1}")
                break
            
            # ğŸš€ å®šæœŸåƒåœ¾å›æ”¶
            if epoch % 5 == 0:
                gc.collect()
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“Š æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_test_acc:.2f}%")
        
        return best_test_acc
    
    def analyze_morphogenesis_effects(self):
        """åˆ†æå½¢æ€å‘ç”Ÿæ•ˆæœ"""
        print("\nğŸ”¬ å½¢æ€å‘ç”Ÿæ•ˆæœåˆ†æ")
        print("=" * 50)
        
        total_events = len(self.morphogenesis_history)
        total_params_added = sum(event['parameters_added'] for event in self.morphogenesis_history)
        
        print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"  å½¢æ€å‘ç”Ÿäº‹ä»¶: {total_events}")
        print(f"  æ–°å¢å‚æ•°: {total_params_added:,}")
        
        if total_events > 0:
            types = [event['type'] for event in self.morphogenesis_history]
            type_counts = {t: types.count(t) for t in set(types)}
            print(f"  å½¢æ€å‘ç”Ÿç±»å‹åˆ†å¸ƒ: {type_counts}")
            
            print(f"\nğŸ“ˆ æ€§èƒ½æ”¹è¿›åˆ†æ:")
            for i, event in enumerate(self.morphogenesis_history):
                if i < len(self.morphogenesis_history) - 1:
                    next_event = self.morphogenesis_history[i + 1]
                    acc_improvement = next_event['test_acc_before'] - event['test_acc_before']
                    print(f"  äº‹ä»¶ {i+1} (Epoch {event['epoch']}):")
                    print(f"    ç±»å‹: {event['type']}")
                    print(f"    æ–°å¢å‚æ•°: {event['parameters_added']:,}")
                    print(f"    æ€§èƒ½å˜åŒ–: {event['test_acc_before']:.2f}% â†’ "
                          f"{next_event['test_acc_before']:.2f}% ({acc_improvement:+.2f}%)")
        
        return {
            'total_events': total_events,
            'total_parameters_added': total_params_added,
            'morphogenesis_types': type_counts if total_events > 0 else {}
        }

def prepare_cpu_optimized_data():
    """å‡†å¤‡CPUä¼˜åŒ–çš„CIFAR-10æ•°æ®"""
    # ğŸš€ é€‚åº¦çš„æ•°æ®å¢å¼º - é¿å…è¿‡åº¦è®¡ç®—
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])
    
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])
    
    train_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform_train
    )
    
    test_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=False, download=True, transform=transform_test
    )
    
    # ğŸš€ CPUå‹å¥½çš„æ•°æ®åŠ è½½é…ç½®
    train_loader = DataLoader(
        train_dataset, 
        batch_size=128,      # å‡å°‘æ‰¹æ¬¡å¤§å°
        shuffle=True, 
        num_workers=2,       # å‡å°‘workeræ•°é‡
        pin_memory=False     # CPUç¯å¢ƒä¸‹å…³é—­
    )
    
    test_loader = DataLoader(
        test_dataset, 
        batch_size=100, 
        shuffle=False, 
        num_workers=2,
        pin_memory=False
    )
    
    return train_loader, test_loader

def main():
    """ä¸»å‡½æ•°"""
    try:
        print("ğŸš€ CPUä¼˜åŒ–çš„é«˜çº§DNMæ¼”ç¤º")
        print("=" * 60)
        
        # è®¾å¤‡é…ç½®
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
        
        # ğŸš€ è®¾ç½®CPUä¼˜åŒ–
        if device.type == 'cpu':
            torch.set_num_threads(4)  # é™åˆ¶çº¿ç¨‹æ•°
            print("ğŸ”§ CPUä¼˜åŒ–: è®¾ç½®çº¿ç¨‹æ•°ä¸º4")
        
        # å‡†å¤‡æ•°æ®
        print("ğŸ“Š å‡†å¤‡æ•°æ®...")
        train_loader, test_loader = prepare_cpu_optimized_data()
        
        # åˆ›å»ºæ¨¡å‹å’Œè®­ç»ƒå™¨
        print("ğŸ—ï¸ åˆ›å»ºCPUä¼˜åŒ–æ¨¡å‹...")
        model = CPUOptimizedResNet()
        trainer = CPUOptimizedDNMTrainer(model, device, train_loader, test_loader)
        
        # ğŸš€ CPUå‹å¥½çš„è®­ç»ƒè®¾ç½®
        print("ğŸƒ å¼€å§‹è®­ç»ƒ...")
        best_acc = trainer.train_with_morphogenesis(epochs=40)  # å‡å°‘è½®æ•°
        
        # åˆ†æç»“æœ
        summary = trainer.analyze_morphogenesis_effects()
        
        print(f"\nğŸ¯ æœ€ç»ˆç»“æœ:")
        print(f"  æœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}%")
        print(f"  å½¢æ€å‘ç”Ÿäº‹ä»¶: {summary['total_events']}")
        print(f"  æ–°å¢å‚æ•°: {summary['total_parameters_added']:,}")
        
        if best_acc >= 90.0:
            print("  ğŸ† ä¼˜ç§€ï¼CPUç¯å¢ƒä¸‹è¾¾åˆ°90%+å‡†ç¡®ç‡!")
        elif best_acc >= 85.0:
            print("  ğŸŒŸ å¾ˆå¥½ï¼CPUç¯å¢ƒä¸‹è¾¾åˆ°85%+å‡†ç¡®ç‡!")
        elif summary['total_events'] > 0:
            print("  ğŸ”§ å½¢æ€å‘ç”ŸåŠŸèƒ½æ­£å¸¸ï¼Œç»§ç»­ä¼˜åŒ–ä¸­...")
        else:
            print("  âš ï¸ å»ºè®®è°ƒæ•´é…ç½®ä»¥æ¿€æ´»æ›´å¤šå½¢æ€å‘ç”Ÿ")
            
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # ğŸš€ ç¡®ä¿æ¸…ç†èµ„æº
        gc.collect()
        print("ğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")

if __name__ == "__main__":
    main()