#!/usr/bin/env python3
"""
ğŸš€ NeuroExapt é«˜æ€§èƒ½è®­ç»ƒè„šæœ¬ - å†²å‡»CIFAR-10 95%å‡†ç¡®ç‡

ä¸“ä¸ºè¾¾åˆ°æœ€é«˜æ€§èƒ½è€Œè®¾è®¡çš„è®­ç»ƒé…ç½®ï¼š
1. ğŸ¯ ç›®æ ‡å‡†ç¡®ç‡ï¼š95%+
2. â° è®­ç»ƒæ—¶é•¿ï¼š100+ epochs
3. ğŸ§  æ™ºèƒ½å­¦ä¹ ç‡è°ƒåº¦
4. ğŸ“ˆ é«˜çº§æ•°æ®å¢å¼º
5. ğŸ”§ æ¨¡å‹å®¹é‡ä¼˜åŒ–
6. ğŸ’« åˆ†ç¦»è®­ç»ƒç­–ç•¥
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import torchvision
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torch.backends.cudnn as cudnn
import numpy as np
import sys
import os
import time
import argparse
from typing import Optional
from tqdm import tqdm
from pathlib import Path
import math

# Add the parent directory to the path to import neuroexapt
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import NeuroExapt core components
from neuroexapt.core.model import Network
from neuroexapt.core.separated_training import SeparatedTrainingStrategy, SeparatedOptimizer, SeparatedTrainer
from neuroexapt.utils.utils import AvgrageMeter, accuracy

class AdvancedCIFAR10Transforms:
    """é«˜çº§CIFAR-10æ•°æ®å¢å¼ºç­–ç•¥"""
    
    @staticmethod
    def get_train_transform():
        """è®­ç»ƒæ—¶çš„é«˜çº§æ•°æ®å¢å¼º"""
        return transforms.Compose([
            # éšæœºè£å‰ªå’Œå¡«å……
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(p=0.5),
            
            # é¢œè‰²å¢å¼º
            transforms.ColorJitter(
                brightness=0.2,
                contrast=0.2, 
                saturation=0.2,
                hue=0.1
            ),
            
            # éšæœºæ—‹è½¬
            transforms.RandomRotation(degrees=15),
            
            # éšæœºæ“¦é™¤
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
            
            # éšæœºæ“¦é™¤ï¼ˆåœ¨tensorä¸Šï¼‰
            transforms.RandomErasing(
                p=0.25,
                scale=(0.02, 0.33),
                ratio=(0.3, 3.3),
                value=0
            ),
        ])
    
    @staticmethod
    def get_test_transform():
        """æµ‹è¯•æ—¶çš„æ ‡å‡†åŒ–"""
        return transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        ])

class CosineAnnealingWarmUpRestarts(optim.lr_scheduler._LRScheduler):
    """å¸¦çƒ­èº«çš„ä½™å¼¦é€€ç«é‡å¯å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    
    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1):
        if T_0 <= 0 or not isinstance(T_0, int):
            raise ValueError("Expected positive integer T_0, but got {}".format(T_0))
        if T_mult < 1 or not isinstance(T_mult, int):
            raise ValueError("Expected integer T_mult >= 1, but got {}".format(T_mult))
        if T_up < 0 or not isinstance(T_up, int):
            raise ValueError("Expected non-negative integer T_up, but got {}".format(T_up))
        
        self.T_0 = T_0
        self.T_mult = T_mult
        self.base_eta_max = eta_max
        self.eta_max = eta_max
        self.T_up = T_up
        self.T_i = T_0
        self.gamma = gamma
        self.cycle = 0
        self.T_cur = last_epoch
        
        super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch)
    
    def get_lr(self):
        if self.T_cur == -1:
            return self.base_lrs
        elif self.T_cur < self.T_up:
            return [(self.eta_max - base_lr)*self.T_cur / self.T_up + base_lr for base_lr in self.base_lrs]
        else:
            return [base_lr + (self.eta_max - base_lr) * (1 + math.cos(math.pi * (self.T_cur-self.T_up) / (self.T_i - self.T_up))) / 2
                    for base_lr in self.base_lrs]

    def step(self, epoch=None):
        if epoch is None:
            epoch = self.last_epoch + 1
            self.T_cur = self.T_cur + 1
            if self.T_cur >= self.T_i:
                self.cycle += 1
                self.T_cur = self.T_cur - self.T_i
                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up
        else:
            if epoch >= self.T_0:
                if self.T_mult == 1:
                    self.T_cur = epoch % self.T_0
                    self.cycle = epoch // self.T_0
                else:
                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))
                    self.cycle = n
                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)
                    self.T_i = self.T_0 * self.T_mult ** (n)
            else:
                self.T_i = self.T_0
                self.T_cur = epoch
                
        self.eta_max = self.base_eta_max * (self.gamma**self.cycle)
        self.last_epoch = math.floor(epoch)
        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):
            param_group['lr'] = lr

def setup_high_performance_args():
    """é«˜æ€§èƒ½è®­ç»ƒå‚æ•°è®¾ç½®"""
    parser = argparse.ArgumentParser(description='NeuroExapt High Performance Training - å†²å‡»95%å‡†ç¡®ç‡')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--data', type=str, default='./data', help='dataset location')
    parser.add_argument('--train_portion', type=float, default=0.8, help='training data portion (å¢åŠ è®­ç»ƒæ•°æ®)')
    
    # é«˜æ€§èƒ½è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=120, help='è®­ç»ƒè½®æ•° (æ›´é•¿è®­ç»ƒ)')
    parser.add_argument('--batch_size', type=int, default=96, help='batch size (ä¼˜åŒ–å)')
    parser.add_argument('--learning_rate', type=float, default=0.05, help='åˆå§‹å­¦ä¹ ç‡ (æ›´é«˜)')
    parser.add_argument('--weight_decay', type=float, default=5e-4, help='weight decay (æ›´å¼ºæ­£åˆ™åŒ–)')
    parser.add_argument('--momentum', type=float, default=0.9, help='momentum')
    parser.add_argument('--grad_clip', type=float, default=5.0, help='gradient clipping')
    
    # æ¨¡å‹æ¶æ„å‚æ•°
    parser.add_argument('--init_channels', type=int, default=36, help='åˆå§‹é€šé“æ•° (å¢åŠ å®¹é‡)')
    parser.add_argument('--layers', type=int, default=20, help='ç½‘ç»œå±‚æ•° (æ›´æ·±ç½‘ç»œ)')
    parser.add_argument('--potential_layers', type=int, default=8, help='æ½œåœ¨å±‚æ•° (å¢åŠ æœç´¢ç©ºé—´)')
    
    # åˆ†ç¦»è®­ç»ƒå‚æ•° 
    parser.add_argument('--weight_epochs', type=int, default=5, help='è¿ç»­æƒé‡è®­ç»ƒè½®æ•°')
    parser.add_argument('--arch_epochs', type=int, default=1, help='æ¶æ„è®­ç»ƒè½®æ•°')
    parser.add_argument('--warmup_epochs', type=int, default=10, help='é¢„çƒ­è½®æ•° (æ›´é•¿é¢„çƒ­)')
    parser.add_argument('--arch_learning_rate', type=float, default=6e-4, help='æ¶æ„å­¦ä¹ ç‡')
    
    # å­¦ä¹ ç‡è°ƒåº¦
    parser.add_argument('--lr_scheduler', type=str, default='cosine_restart', 
                       choices=['cosine', 'cosine_restart', 'step'], help='å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥')
    parser.add_argument('--T_0', type=int, default=20, help='ä½™å¼¦é‡å¯å‘¨æœŸ')
    parser.add_argument('--T_mult', type=int, default=2, help='é‡å¯å‘¨æœŸå€æ•°')
    parser.add_argument('--eta_min', type=float, default=1e-6, help='æœ€å°å­¦ä¹ ç‡')
    
    # é«˜çº§ä¼˜åŒ–é€‰é¡¹
    parser.add_argument('--label_smoothing', type=float, default=0.1, help='æ ‡ç­¾å¹³æ»‘')
    parser.add_argument('--mixup_alpha', type=float, default=0.2, help='Mixupå‚æ•°')
    parser.add_argument('--cutmix_prob', type=float, default=0.25, help='CutMixæ¦‚ç‡')
    parser.add_argument('--drop_path_prob', type=float, default=0.2, help='DropPathæ¦‚ç‡')
    
    # æ€§èƒ½é€‰é¡¹
    parser.add_argument('--num_workers', type=int, default=8, help='æ•°æ®åŠ è½½å™¨è¿›ç¨‹æ•°')
    parser.add_argument('--pin_memory', action='store_true', default=True, help='pin memory')
    parser.add_argument('--use_amp', action='store_true', help='ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦')
    
    # ä¿å­˜å’Œæ—¥å¿—
    parser.add_argument('--save_dir', type=str, default='./checkpoints_high_perf', help='æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•')
    parser.add_argument('--save_freq', type=int, default=10, help='ä¿å­˜é¢‘ç‡')
    parser.add_argument('--log_freq', type=int, default=5, help='æ—¥å¿—é¢‘ç‡')
    
    # å…¶ä»–
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--resume', type=str, default='', help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹')
    parser.add_argument('--evaluate_only', action='store_true', help='ä»…è¯„ä¼°æ¨¡å¼')
    
    return parser.parse_args()

def setup_environment(args):
    """è®¾ç½®é«˜æ€§èƒ½è®­ç»ƒç¯å¢ƒ"""
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)
    
    # CUDAä¼˜åŒ–è®¾ç½®
    if torch.cuda.is_available():
        cudnn.benchmark = True
        cudnn.enabled = True
        cudnn.deterministic = False  # ä¸ºäº†æ€§èƒ½ï¼Œå…³é—­ç¡®å®šæ€§
        
        # å¯ç”¨é«˜æ€§èƒ½æ¨¡å¼
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        
        print(f"ğŸš€ CUDAè®¾å¤‡: {torch.cuda.get_device_name()}")
        print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(args.save_dir, exist_ok=True)
    
    print(f"âœ… é«˜æ€§èƒ½è®­ç»ƒç¯å¢ƒè®¾ç½®å®Œæˆ")

def create_high_performance_data_loaders(args):
    """åˆ›å»ºé«˜æ€§èƒ½æ•°æ®åŠ è½½å™¨"""
    print("ğŸ“Š åˆ›å»ºé«˜æ€§èƒ½æ•°æ®åŠ è½½å™¨...")
    
    # ä½¿ç”¨é«˜çº§æ•°æ®å¢å¼º
    train_transform = AdvancedCIFAR10Transforms.get_train_transform()
    test_transform = AdvancedCIFAR10Transforms.get_test_transform()
    
    # åŠ è½½æ•°æ®é›†
    train_data = dset.CIFAR10(root=args.data, train=True, download=True, transform=train_transform)
    test_data = dset.CIFAR10(root=args.data, train=False, transform=test_transform)
    
    # è®­ç»ƒéªŒè¯åˆ’åˆ†
    num_train = len(train_data)
    indices = list(range(num_train))
    split = int(np.floor(args.train_portion * num_train))
    
    print(f"   æ•°æ®åˆ’åˆ†: {split}è®­ç»ƒ + {num_train - split}éªŒè¯ + {len(test_data)}æµ‹è¯•")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_queue = data.DataLoader(
        train_data, 
        batch_size=args.batch_size,
        sampler=data.SubsetRandomSampler(indices[:split]),
        num_workers=args.num_workers,
        pin_memory=args.pin_memory,
        drop_last=True,
        persistent_workers=True  # é«˜æ€§èƒ½é€‰é¡¹
    )
    
    valid_queue = data.DataLoader(
        train_data,
        batch_size=args.batch_size,
        sampler=data.SubsetRandomSampler(indices[split:num_train]),
        num_workers=args.num_workers,
        pin_memory=args.pin_memory,
        drop_last=False,
        persistent_workers=True
    )
    
    test_queue = data.DataLoader(
        test_data,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=args.pin_memory,
        persistent_workers=True
    )
    
    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ: {len(train_queue)}è®­ç»ƒ + {len(valid_queue)}éªŒè¯ + {len(test_queue)}æµ‹è¯•")
    
    return train_queue, valid_queue, test_queue

def create_high_performance_model(args):
    """åˆ›å»ºé«˜æ€§èƒ½æ¨¡å‹"""
    print(f"ğŸ§  åˆ›å»ºé«˜æ€§èƒ½æ¨¡å‹...")
    
    model = Network(
        C=args.init_channels,
        num_classes=10,
        layers=args.layers,
        potential_layers=args.potential_layers,
        use_compile=True,  # å¯ç”¨torch.compile
        progress_tracking=False,
        quiet=False
    )
    
    if torch.cuda.is_available():
        model = model.cuda()
    
    # å‚æ•°ç»Ÿè®¡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"   æ€»å‚æ•°: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"   æ¨¡å‹å¤§å°: {total_params * 4 / 1e6:.1f}MB")
    
    return model

def create_advanced_criterion(args):
    """åˆ›å»ºé«˜çº§æŸå¤±å‡½æ•°"""
    if args.label_smoothing > 0:
        # æ ‡ç­¾å¹³æ»‘äº¤å‰ç†µ
        criterion = nn.CrossEntropyLoss(label_smoothing=args.label_smoothing)
        print(f"   ä½¿ç”¨æ ‡ç­¾å¹³æ»‘: {args.label_smoothing}")
    else:
        criterion = nn.CrossEntropyLoss()
    
    return criterion.cuda()

def create_advanced_scheduler(optimizer, args):
    """åˆ›å»ºé«˜çº§å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    if args.lr_scheduler == 'cosine_restart':
        scheduler = CosineAnnealingWarmUpRestarts(
            optimizer, 
            T_0=args.T_0, 
            T_mult=args.T_mult,
            eta_max=args.learning_rate,
            T_up=5,  # çƒ­èº«æ­¥æ•°
            gamma=0.5  # é‡å¯åè¡°å‡
        )
        print(f"   ä½¿ç”¨ä½™å¼¦é‡å¯è°ƒåº¦å™¨: T_0={args.T_0}, T_mult={args.T_mult}")
    elif args.lr_scheduler == 'cosine':
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=args.epochs, 
            eta_min=args.eta_min
        )
        print(f"   ä½¿ç”¨ä½™å¼¦é€€ç«è°ƒåº¦å™¨")
    else:
        scheduler = optim.lr_scheduler.StepLR(
            optimizer, 
            step_size=30, 
            gamma=0.1
        )
        print(f"   ä½¿ç”¨é˜¶æ¢¯è°ƒåº¦å™¨")
    
    return scheduler

def save_checkpoint(state, filename):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    torch.save(state, filename)
    print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {filename}")

def load_checkpoint(filename, model, optimizer=None, scheduler=None):
    """åŠ è½½æ£€æŸ¥ç‚¹"""
    print(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: {filename}")
    checkpoint = torch.load(filename)
    
    model.load_state_dict(checkpoint['state_dict'])
    start_epoch = checkpoint['epoch']
    best_acc = checkpoint['best_acc']
    
    if optimizer is not None and 'optimizer' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer'])
    
    if scheduler is not None and 'scheduler' in checkpoint:
        scheduler.load_state_dict(checkpoint['scheduler'])
    
    print(f"   æ¢å¤åˆ°epoch {start_epoch}, æœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}%")
    return start_epoch, best_acc

def train_epoch_separated(separated_trainer, train_queue, valid_queue, epoch, args, scaler=None):
    """åˆ†ç¦»è®­ç»ƒä¸€ä¸ªepoch"""
    start_time = time.time()
    
    # ä½¿ç”¨åˆ†ç¦»è®­ç»ƒå™¨
    stats = separated_trainer.train_epoch(train_queue, valid_queue, epoch)
    
    train_time = time.time() - start_time
    
    # è·å–è®­ç»ƒç»Ÿè®¡
    if 'accuracy' in stats:
        train_acc = stats['accuracy']
        train_loss = stats['loss'] 
    else:
        # æ¶æ„è®­ç»ƒepochï¼Œå‡†ç¡®ç‡éœ€è¦é€šè¿‡éªŒè¯è·å¾—
        train_acc = 0.0
        train_loss = stats['loss']
    
    return train_acc, train_loss, train_time

def validate_epoch(valid_queue, model, criterion):
    """éªŒè¯ä¸€ä¸ªepoch"""
    model.eval()
    
    objs = AvgrageMeter()
    top1 = AvgrageMeter()
    top5 = AvgrageMeter()
    
    with torch.no_grad():
        for input, target in tqdm(valid_queue, desc="éªŒè¯", leave=False):
            input = input.cuda(non_blocking=True)
            target = target.cuda(non_blocking=True)
            
            logits = model(input)
            loss = criterion(logits, target)
            
            prec1, prec5 = accuracy(logits, target, topk=(1, 5))
            n = input.size(0)
            objs.update(loss.item(), n)
            top1.update(prec1.item(), n)
            top5.update(prec5.item(), n)
    
    return top1.avg, top5.avg, objs.avg

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    args = setup_high_performance_args()
    setup_environment(args)
    
    print("ğŸš€ NeuroExapt é«˜æ€§èƒ½è®­ç»ƒ - å†²å‡»CIFAR-10 95%å‡†ç¡®ç‡")
    print("=" * 80)
    print(f"ğŸ¯ ç›®æ ‡å‡†ç¡®ç‡: 95%+")
    print(f"â° è®­ç»ƒè½®æ•°: {args.epochs}")
    print(f"ğŸ§  æ¨¡å‹å®¹é‡: C={args.init_channels}, L={args.layers}")
    print(f"ğŸ“Š æ‰¹å¤§å°: {args.batch_size}")
    print(f"ğŸ“ˆ å­¦ä¹ ç‡è°ƒåº¦: {args.lr_scheduler}")
    print("=" * 80)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_queue, valid_queue, test_queue = create_high_performance_data_loaders(args)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_high_performance_model(args)
    
    # åˆ›å»ºæŸå¤±å‡½æ•°
    criterion = create_advanced_criterion(args)
    
    # åˆ›å»ºåˆ†ç¦»è®­ç»ƒç»„ä»¶
    separated_strategy = SeparatedTrainingStrategy(
        weight_training_epochs=args.weight_epochs,
        arch_training_epochs=args.arch_epochs,
        total_epochs=args.epochs,
        warmup_epochs=args.warmup_epochs
    )
    
    separated_optimizer = SeparatedOptimizer(
        model,
        weight_lr=args.learning_rate,
        arch_lr=args.arch_learning_rate,
        weight_momentum=args.momentum,
        weight_decay=args.weight_decay
    )
    
    separated_trainer = SeparatedTrainer(
        model, separated_strategy, separated_optimizer, criterion
    )
    
    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    weight_scheduler = create_advanced_scheduler(separated_optimizer.weight_optimizer, args)
    arch_scheduler = create_advanced_scheduler(separated_optimizer.arch_optimizer, args)
    
    # è‡ªåŠ¨æ··åˆç²¾åº¦
    scaler = torch.cuda.amp.GradScaler() if args.use_amp else None
    if args.use_amp:
        print("âš¡ å¯ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ")
    
    # æ¢å¤è®­ç»ƒ
    start_epoch = 0
    best_acc = 0.0
    best_test_acc = 0.0
    
    if args.resume:
        start_epoch, best_acc = load_checkpoint(
            args.resume, model, separated_optimizer.weight_optimizer, weight_scheduler
        )
    
    # ä»…è¯„ä¼°æ¨¡å¼
    if args.evaluate_only:
        print("ğŸ“Š ä»…è¯„ä¼°æ¨¡å¼")
        val_acc, val_acc5, val_loss = validate_epoch(valid_queue, model, criterion)
        test_acc, test_acc5, test_loss = validate_epoch(test_queue, model, criterion)
        print(f"éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}% (Top-5: {val_acc5:.2f}%)")
        print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}% (Top-5: {test_acc5:.2f}%)")
        return
    
    print(f"\nğŸš€ å¼€å§‹é«˜æ€§èƒ½è®­ç»ƒ!")
    print(f"è®­ç»ƒè®¡åˆ’: {separated_strategy.get_schedule_summary()}")
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(start_epoch, args.epochs):
        print(f"\n{'='*80}")
        print(f"ğŸ“Š EPOCH {epoch+1}/{args.epochs}")
        print(f"{'='*80}")
        
        training_mode = separated_strategy.get_training_mode(epoch)
        print(f"ğŸ”„ è®­ç»ƒæ¨¡å¼: {'ğŸ‹ï¸ æƒé‡è®­ç»ƒ' if training_mode == 'weight' else 'ğŸ§¬ æ¶æ„è®­ç»ƒ'}")
        
        # è®­ç»ƒ
        train_acc, train_loss, train_time = train_epoch_separated(
            separated_trainer, train_queue, valid_queue, epoch, args, scaler
        )
        
        # éªŒè¯
        val_acc, val_acc5, val_loss = validate_epoch(valid_queue, model, criterion)
        
        # æµ‹è¯•ï¼ˆæ¯5ä¸ªepochæµ‹è¯•ä¸€æ¬¡ï¼‰
        if (epoch + 1) % 5 == 0:
            test_acc, test_acc5, test_loss = validate_epoch(test_queue, model, criterion)
            if test_acc > best_test_acc:
                best_test_acc = test_acc
        else:
            test_acc = 0.0
        
        # æ›´æ–°å­¦ä¹ ç‡
        if training_mode == 'weight':
            weight_scheduler.step()
        else:
            arch_scheduler.step()
        
        # è®°å½•æœ€ä½³å‡†ç¡®ç‡
        is_best = val_acc > best_acc
        if is_best:
            best_acc = val_acc
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        current_lr = separated_optimizer.weight_optimizer.param_groups[0]['lr']
        print(f"ğŸ“ˆ è®­ç»ƒ: å‡†ç¡®ç‡={train_acc:.2f}% æŸå¤±={train_loss:.4f} æ—¶é—´={train_time:.0f}s")
        print(f"ğŸ“Š éªŒè¯: å‡†ç¡®ç‡={val_acc:.2f}% (Top-5: {val_acc5:.2f}%) æŸå¤±={val_loss:.4f}")
        if test_acc > 0:
            print(f"ğŸ¯ æµ‹è¯•: å‡†ç¡®ç‡={test_acc:.2f}% (Top-5: {test_acc5:.2f}%)")
        print(f"â­ æœ€ä½³: éªŒè¯={best_acc:.2f}% æµ‹è¯•={best_test_acc:.2f}%")
        print(f"ğŸ”§ å­¦ä¹ ç‡: {current_lr:.6f}")
        
        # 95%ç›®æ ‡æ£€æŸ¥
        if val_acc >= 95.0:
            print(f"ğŸ‰ è¾¾æˆç›®æ ‡! éªŒè¯å‡†ç¡®ç‡è¾¾åˆ° {val_acc:.2f}% >= 95%!")
        if best_test_acc >= 95.0:
            print(f"ğŸ† æµ‹è¯•å‡†ç¡®ç‡ä¹Ÿè¾¾åˆ° {best_test_acc:.2f}% >= 95%!")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % args.save_freq == 0 or is_best:
            checkpoint_path = os.path.join(args.save_dir, f'checkpoint_epoch_{epoch+1}.pth')
            save_checkpoint({
                'epoch': epoch + 1,
                'state_dict': model.state_dict(),
                'best_acc': best_acc,
                'optimizer': separated_optimizer.weight_optimizer.state_dict(),
                'scheduler': weight_scheduler.state_dict(),
                'args': args,
            }, checkpoint_path)
            
            if is_best:
                best_path = os.path.join(args.save_dir, 'best_model.pth')
                save_checkpoint({
                    'epoch': epoch + 1,
                    'state_dict': model.state_dict(),
                    'best_acc': best_acc,
                    'test_acc': best_test_acc,
                    'args': args,
                }, best_path)
    
    # æœ€ç»ˆæµ‹è¯•
    print(f"\n{'='*80}")
    print(f"ğŸŠ è®­ç»ƒå®Œæˆ!")
    print(f"{'='*80}")
    
    final_test_acc, final_test_acc5, _ = validate_epoch(test_queue, model, criterion)
    
    print(f"ğŸ“Š æœ€ç»ˆç»“æœ:")
    print(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.2f}%")
    print(f"   æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_test_acc:.2f}%")
    print(f"   æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {final_test_acc:.2f}% (Top-5: {final_test_acc5:.2f}%)")
    
    if final_test_acc >= 95.0:
        print(f"ğŸ† æ­å–œ! æˆåŠŸè¾¾åˆ°95%ç›®æ ‡å‡†ç¡®ç‡!")
        print(f"ğŸ¯ æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {final_test_acc:.2f}%")
    elif final_test_acc >= 93.0:
        print(f"ğŸŒŸ å¾ˆæ¥è¿‘äº†! æµ‹è¯•å‡†ç¡®ç‡: {final_test_acc:.2f}%")
        print(f"ğŸ’¡ å»ºè®®ç»§ç»­è®­ç»ƒæˆ–è°ƒæ•´è¶…å‚æ•°")
    else:
        print(f"ğŸ“ˆ å½“å‰æµ‹è¯•å‡†ç¡®ç‡: {final_test_acc:.2f}%")
        print(f"ğŸ’¡ å»ºè®®å¢åŠ è®­ç»ƒè½®æ•°æˆ–è°ƒæ•´æ¨¡å‹å®¹é‡")
    
    # æ˜¾ç¤ºæœ€ç»ˆæ¶æ„
    try:
        genotype = model.genotype()
        print(f"\nğŸ—ï¸ æœ€ç»ˆå‘ç°çš„æ¶æ„:")
        print(f"   Normal: {genotype.normal}")
        print(f"   Reduce: {genotype.reduce}")
    except:
        print(f"âš ï¸ æ— æ³•è·å–æœ€ç»ˆæ¶æ„")

if __name__ == "__main__":
    main() 