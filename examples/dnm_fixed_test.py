#!/usr/bin/env python3
"""
DNM (Dynamic Neural Morphogenesis) é‡æ„æµ‹è¯•

ğŸ§¬ æ–°ç‰¹æ€§ï¼š
1. å¤šç†è®ºæ”¯æ’‘çš„è§¦å‘æœºåˆ¶
2. æ›´æ™ºèƒ½çš„ç¥ç»å…ƒåˆ†è£‚ç­–ç•¥
3. è‡ªé€‚åº”çš„å½¢æ€å‘ç”Ÿåˆ¤æ–­
4. æ›´æ¿€è¿›çš„æ—©æœŸå¹²é¢„
5. çªç ´90%å‡†ç¡®ç‡çš„è®¾è®¡

ğŸ¯ ç›®æ ‡ï¼šéªŒè¯é‡æ„åçš„DNMæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆçªç ´æ€§èƒ½ç“¶é¢ˆ
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import numpy as np
import time
import logging
from tqdm import tqdm
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from neuroexapt.core.dnm_framework import DNMFramework
from neuroexapt.core.dnm_neuron_division import AdaptiveNeuronDivision

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(name)s:%(message)s')
logger = logging.getLogger('neuroexapt.core.dnm_framework')

class AdvancedCNNModel(nn.Module):
    """å¢å¼ºçš„CNNæ¨¡å‹ï¼Œä¸ºDNMä¼˜åŒ–"""
    
    def __init__(self, num_classes=10):
        super(AdvancedCNNModel, self).__init__()
        
        # ç‰¹å¾æå–éƒ¨åˆ†
        self.features = nn.Sequential(
            # ç¬¬ä¸€ä¸ªå·ç§¯å—
            nn.Conv2d(3, 32, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout2d(0.25),
            
            # ç¬¬äºŒä¸ªå·ç§¯å—
            nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout2d(0.25),
            
            # ç¬¬ä¸‰ä¸ªå·ç§¯å—  
            nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout2d(0.25),
        )
        
        # åˆ†ç±»å™¨éƒ¨åˆ†
        self.classifier = nn.Sequential(
            nn.Linear(128 * 4 * 4, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
        
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

class ActivationHook:
    """æ¿€æ´»å€¼æ•è·é’©å­"""
    
    def __init__(self):
        self.activations = {}
        self.hooks = []
    
    def hook_fn(self, name):
        def fn(module, input, output):
            self.activations[name] = output.detach()
        return fn
    
    def register_hooks(self, model):
        for name, module in model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)):
                # åªä¸ºä¸»è¦å±‚æ³¨å†Œé’©å­ï¼Œé¿å…è¿‡å¤šçš„æ¿€æ´»å€¼
                if ('classifier' in name and isinstance(module, nn.Linear)) or \
                   ('features' in name and isinstance(module, nn.Conv2d) and 'features.17' in name):
                    hook = module.register_forward_hook(self.hook_fn(name))
                    self.hooks.append(hook)
    
    def remove_hooks(self):
        for hook in self.hooks:
            hook.remove()
        self.hooks.clear()

class GradientHook:
    """æ¢¯åº¦æ•è·é’©å­"""
    
    def __init__(self):
        self.gradients = {}
    
    def capture_gradients(self, model):
        self.gradients.clear()
        for name, param in model.named_parameters():
            if param.grad is not None:
                self.gradients[name] = param.grad.detach().clone()

def load_cifar10_data(batch_size=128):
    """åŠ è½½CIFAR-10æ•°æ®é›†"""
    
    # æ•°æ®å¢å¼ºå’Œæ ‡å‡†åŒ–
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    
    # åŠ è½½æ•°æ®é›†
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)
    
    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)
    
    return trainloader, testloader

def train_epoch(model, train_loader, optimizer, criterion, device, activation_hook, gradient_hook):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    pbar = tqdm(train_loader, desc="Training")
    for batch_idx, (data, target) in enumerate(pbar):
        data, target = data.to(device), target.to(device)
        
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        
        # æ•è·æ¢¯åº¦
        gradient_hook.capture_gradients(model)
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        running_loss += loss.item()
        _, predicted = output.max(1)
        total += target.size(0)
        correct += predicted.eq(target).sum().item()
        
        if batch_idx % 50 == 0:
            pbar.set_postfix({
                'Loss': f'{running_loss/(batch_idx+1):.4f}',
                'Acc': f'{100.*correct/total:.2f}%'
            })
    
    return running_loss / len(train_loader), 100. * correct / total

def validate_epoch(model, val_loader, criterion, device):
    """éªŒè¯ä¸€ä¸ªepoch"""
    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, target in val_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            val_loss += criterion(output, target).item()
            
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()
    
    return val_loss / len(val_loader), 100. * correct / total

def count_parameters(model):
    """è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

def main():
    """ä¸»è®­ç»ƒå¾ªç¯"""
    print("ğŸ§¬ DNM (Dynamic Neural Morphogenesis) é‡æ„æµ‹è¯•")
    print("="*60)
    
    # è®¾å¤‡é…ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ•°æ®åŠ è½½
    print("ğŸ“ åŠ è½½CIFAR-10æ•°æ®é›†...")
    train_loader, val_loader = load_cifar10_data(batch_size=128)
    
    # æ¨¡å‹åˆå§‹åŒ–
    print("ğŸ—ï¸ åˆå§‹åŒ–æ¨¡å‹...")
    model = AdvancedCNNModel(num_classes=10).to(device)
    initial_params = count_parameters(model)
    print(f"åˆå§‹å‚æ•°æ•°é‡: {initial_params:,}")
    
    # DNMæ¡†æ¶é…ç½®
    dnm_config = {
        'morphogenesis_interval': 3,  # æ¯3ä¸ªepochæ£€æŸ¥ä¸€æ¬¡
        'max_morphogenesis_per_epoch': 1,  # æ¯æ¬¡æœ€å¤š1æ¬¡å½¢æ€å‘ç”Ÿ
        'performance_improvement_threshold': 0.01,  # æ€§èƒ½æ”¹å–„é˜ˆå€¼
    }
    
    # åˆå§‹åŒ–DNMæ¡†æ¶
    print("ğŸ§¬ åˆå§‹åŒ–DNMæ¡†æ¶...")
    dnm = DNMFramework(model, dnm_config)
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)
    criterion = nn.CrossEntropyLoss()
    
    # é’©å­è®¾ç½®
    activation_hook = ActivationHook()
    activation_hook.register_hooks(model)
    gradient_hook = GradientHook()
    
    # è®­ç»ƒé…ç½®
    epochs = 100
    best_acc = 0.0
    patience = 25
    patience_counter = 0
    morphogenesis_events = 0
    
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    print("="*60)
    
    start_time = time.time()
    
    for epoch in range(epochs):
        current_lr = optimizer.param_groups[0]['lr']
        
        print(f"\nğŸ§¬ Epoch {epoch+1}/{epochs} - Dynamic Morphogenesis")
        
        # è®­ç»ƒ
        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, 
                                          device, activation_hook, gradient_hook)
        
        # éªŒè¯
        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)
        
        # æ›´æ–°DNMæ¡†æ¶çŠ¶æ€
        dnm.update_caches(activation_hook.activations, gradient_hook.gradients)
        dnm.record_performance(val_acc)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å½¢æ€å‘ç”Ÿ
        train_metrics = {'accuracy': train_acc, 'loss': train_loss, 'learning_rate': current_lr}
        val_metrics = {'accuracy': val_acc, 'loss': val_loss}
        
        should_trigger, reasons = dnm.should_trigger_morphogenesis(epoch+1, train_metrics, val_metrics)
        
        if should_trigger:
            print(f"  ğŸ”„ Triggering morphogenesis analysis...")
            for reason in reasons:
                print(f"    - {reason}")
                
            # æ‰§è¡Œå½¢æ€å‘ç”Ÿ
            results = dnm.execute_morphogenesis(epoch+1)
            if results['neuron_divisions'] > 0:
                model = dnm.model  # æ›´æ–°æ¨¡å‹å¼•ç”¨
                morphogenesis_events += 1
                
                # é‡æ–°è®¾ç½®ä¼˜åŒ–å™¨ï¼ˆå› ä¸ºæ¨¡å‹å‚æ•°å˜äº†ï¼‰
                optimizer = optim.SGD(model.parameters(), lr=current_lr, momentum=0.9, weight_decay=5e-4)
                # åˆ›å»ºæ–°çš„è°ƒåº¦å™¨ï¼Œä½†è¦ä¿æŒå½“å‰çš„æ­¥æ•°
                scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(1, 100-epoch))
                # å¿«è¿›è°ƒåº¦å™¨åˆ°å½“å‰epoch
                for _ in range(epoch + 1):
                    scheduler.step()
                
                # é‡æ–°æ³¨å†Œé’©å­
                activation_hook.remove_hooks()
                activation_hook.register_hooks(model)
                
                print(f"    âœ… å½¢æ€å‘ç”Ÿå®Œæˆ: {results['neuron_divisions']} æ¬¡ç¥ç»å…ƒåˆ†è£‚")
                print(f"    ğŸ“Š æ–°å¢å‚æ•°: {results['parameters_added']:,}")
        
        current_params = count_parameters(model)
        param_growth = ((current_params - initial_params) / initial_params) * 100
        
        print(f"  ğŸ“Š Train: {train_acc:.2f}% | Val: {val_acc:.2f}% | Params: {current_params:,} | ")
        
        # è¯¦ç»†æ—¥å¿—è¾“å‡ºï¼ˆæ¯3ä¸ªepochï¼‰
        if (epoch + 1) % 3 == 0:
            print(f"ğŸ“ˆ Epoch {epoch+1:3d}: Train Acc={train_acc:.2f}% Loss={train_loss:.4f} | "
                  f"Val Acc={val_acc:.2f}% Loss={val_loss:.4f} | "
                  f"Params={current_params:,} (+{param_growth:.1f}%) | LR={current_lr:.6f}")
        
        # æ›´æ–°æœ€ä½³å‡†ç¡®ç‡
        if val_acc > best_acc:
            best_acc = val_acc
            patience_counter = 0
        else:
            patience_counter += 1
        
        # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆåœ¨æ—©åœæ£€æŸ¥ä¹‹å‰ï¼‰
        scheduler.step()
        
        # æ—©åœæ£€æŸ¥
        if patience_counter >= patience:
            print(f"  ğŸ›‘ Early stopping triggered (patience: {patience})")
            break
    
    # è®­ç»ƒå®Œæˆ
    print("âœ… DNM training completed")
    
    training_time = time.time() - start_time
    final_params = count_parameters(model)
    param_growth = ((final_params - initial_params) / initial_params) * 100
    
    print(f"   Final accuracy: {val_acc:.2f}% | Best: {best_acc:.2f}%")
    print(f"   Morphogenesis events: {morphogenesis_events}")
    
    # è·å–DNMæ‘˜è¦
    summary = dnm.get_morphogenesis_summary()
    
    print(f"\nğŸ‰ DNMè®­ç»ƒå®Œæˆ!")
    print("="*60)
    print(f"ğŸ“Š æœ€ç»ˆç»“æœ:")
    print(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.2f}%")
    print(f"   æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
    print(f"   å‚æ•°å¢é•¿: +{param_growth:.1f}% ({initial_params:,} â†’ {final_params:,})")
    print(f"   è®­ç»ƒæ—¶é—´: {training_time/60:.1f}åˆ†é’Ÿ")
    print(f"   å½¢æ€å‘ç”Ÿäº‹ä»¶: {summary['total_events']}")
    print(f"   æ€»ç¥ç»å…ƒåˆ†è£‚: {summary['total_neuron_divisions']}")
    print(f"   æ€»è¿æ¥ç”Ÿé•¿: 0")  # æœªå®ç°
    print(f"   æ€»ä¼˜åŒ–æ¬¡æ•°: 0")  # æœªå®ç°
    
    # å½¢æ€å‘ç”Ÿäº‹ä»¶åˆ†æ
    if summary['events_detail']:
        print(f"\nğŸ§¬ å½¢æ€å‘ç”Ÿäº‹ä»¶åˆ†æ:")
        for i, event in enumerate(summary['events_detail']):
            print(f"   äº‹ä»¶ {i+1} (Epoch {event['epoch']}):")
            print(f"     ç¥ç»å…ƒåˆ†è£‚: {event['params_added']}")
            print(f"     è¿æ¥ç”Ÿé•¿: 0")
            print(f"     ä¼˜åŒ–è§¦å‘: False")
            print(f"     è§¦å‘å‰æ€§èƒ½: {event.get('performance_before', 0):.2f}%")
    
    # æ€§èƒ½è¯„ä¼°
    if best_acc >= 90.0:
        print(f"\nğŸ† BREAKTHROUGH: æˆåŠŸçªç ´90%å‡†ç¡®ç‡å¤§å…³! ({best_acc:.2f}%)")
    elif best_acc >= 85.0:
        print(f"\nğŸ”„ IMPROVING: éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ– ({best_acc:.2f}%)")
    else:
        print(f"\nâš ï¸ NEEDS WORK: éœ€è¦é‡å¤§æ”¹è¿› ({best_acc:.2f}%)")
    
    print(f"\nâœ… æµ‹è¯•å®Œæˆ!")
    print(f"   DNMæ¡†æ¶è¿è¡Œæ­£å¸¸")
    print(f"   æœ€ç»ˆå‡†ç¡®ç‡: {val_acc:.2f}%")
    
    # æ¸…ç†é’©å­
    activation_hook.remove_hooks()

if __name__ == "__main__":
    main()