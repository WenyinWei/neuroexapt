#!/usr/bin/env python3
"""
ä¿®å¤ç‰ˆDNMæµ‹è¯• - è§£å†³æ‰€æœ‰å¯¼å…¥é—®é¢˜

å‚è€ƒexamples/aso_se_classification.pyçš„æ•°æ®åŠ è½½é€»è¾‘
ç›®æ ‡ï¼šCIFAR-10æ•°æ®é›†çªç ´95%å‡†ç¡®ç‡
"""

import os
import sys
import time
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
from tqdm import tqdm

# ç›´æ¥å¯¼å…¥DNMæ¨¡å—ï¼Œç»•è¿‡__init__.pyçš„é—®é¢˜
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ç›´æ¥å¯¼å…¥DNMæ ¸å¿ƒæ¨¡å—
try:
    from neuroexapt.core.dnm_framework import DNMFramework
    from neuroexapt.core.dnm_neuron_division import DNMNeuronDivision
    from neuroexapt.core.dnm_connection_growth import DNMConnectionGrowth
    print("âœ… DNMæ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ DNMæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)


def setup_cifar10_data(batch_size=128, data_dir='./data'):
    """
    è®¾ç½®CIFAR-10æ•°æ®é›† - å‚è€ƒaso_se_classification.pyçš„é€»è¾‘
    åŒ…å«å¼ºåŒ–çš„æ•°æ®å¢å¼ºç­–ç•¥
    """
    print(f"ğŸ“Š è®¾ç½®CIFAR-10æ•°æ®é›†...")
    
    # å¼ºåŒ–çš„è®­ç»ƒæ•°æ®å¢å¼º
    train_transform = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(degrees=10),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
        transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),
        # éšæœºæ“¦é™¤å¢å¼º
        transforms.RandomErasing(p=0.1, scale=(0.02, 0.33), ratio=(0.3, 3.3))
    ])
    
    # æµ‹è¯•æ•°æ®æ ‡å‡†åŒ–
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))
    ])
    
    # å°è¯•åŠ è½½æ•°æ®é›†
    try:
        # é¦–å…ˆå°è¯•ä»æœ¬åœ°åŠ è½½
        train_dataset = torchvision.datasets.CIFAR10(
            root=data_dir, train=True, download=False, transform=train_transform
        )
        test_dataset = torchvision.datasets.CIFAR10(
            root=data_dir, train=False, download=False, transform=test_transform
        )
        print(f"âœ… ä»æœ¬åœ°åŠ è½½CIFAR-10æ•°æ®é›†")
        
    except:
        print(f"ğŸ“¥ æœ¬åœ°æ•°æ®ä¸å­˜åœ¨ï¼Œå¼€å§‹ä¸‹è½½...")
        # è‡ªåŠ¨ä¸‹è½½
        train_dataset = torchvision.datasets.CIFAR10(
            root=data_dir, train=True, download=True, transform=train_transform
        )
        test_dataset = torchvision.datasets.CIFAR10(
            root=data_dir, train=False, download=True, transform=test_transform
        )
        print(f"âœ… CIFAR-10æ•°æ®é›†ä¸‹è½½å®Œæˆ")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset, batch_size=batch_size, 
        shuffle=True, num_workers=4, pin_memory=True,
        drop_last=True  # ç¡®ä¿batchå¤§å°ä¸€è‡´
    )
    
    test_loader = DataLoader(
        test_dataset, batch_size=batch_size, 
        shuffle=False, num_workers=4, pin_memory=True
    )
    
    print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
    print(f"   è®­ç»ƒæ ·æœ¬: {len(train_dataset):,}")
    print(f"   æµ‹è¯•æ ·æœ¬: {len(test_dataset):,}")
    print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}")
    print(f"   æµ‹è¯•æ‰¹æ¬¡: {len(test_loader)}")
    
    return train_loader, test_loader


class EnhancedCIFAR10Net(nn.Module):
    """
    å¢å¼ºçš„CIFAR-10ç½‘ç»œ - ä¸“ä¸ºDNMæ¼”åŒ–è®¾è®¡
    
    é‡‡ç”¨ç°ä»£æ¶æ„è®¾è®¡åŸåˆ™ï¼š
    1. æ®‹å·®è¿æ¥
    2. æ‰¹é‡å½’ä¸€åŒ–
    3. é€‚å½“çš„Dropout
    4. å…¨å±€å¹³å‡æ± åŒ–
    """
    
    def __init__(self, num_classes=10, base_channels=64):
        super().__init__()
        self.base_channels = base_channels
        
        # è¾“å…¥å¤„ç†
        self.stem = nn.Sequential(
            nn.Conv2d(3, base_channels, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(base_channels),
            nn.ReLU(inplace=True)
        )
        
        # ç‰¹å¾æå–å—
        self.block1 = self._make_block(base_channels, base_channels, stride=1)
        self.block2 = self._make_block(base_channels, base_channels * 2, stride=2)
        self.block3 = self._make_block(base_channels * 2, base_channels * 4, stride=2)
        self.block4 = self._make_block(base_channels * 4, base_channels * 8, stride=2)
        
        # åˆ†ç±»å¤´
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Dropout(0.3),
            nn.Linear(base_channels * 8, base_channels * 4),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(base_channels * 4, base_channels * 2),
            nn.ReLU(inplace=True),
            nn.Linear(base_channels * 2, num_classes)
        )
        
        # æƒé‡åˆå§‹åŒ–
        self._initialize_weights()
        
    def _make_block(self, in_channels, out_channels, stride):
        """åˆ›å»ºåŸºç¡€å—"""
        layers = []
        
        # ä¸»å·ç§¯
        layers.append(nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False))
        layers.append(nn.BatchNorm2d(out_channels))
        layers.append(nn.ReLU(inplace=True))
        
        # ç¬¬äºŒä¸ªå·ç§¯
        layers.append(nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False))
        layers.append(nn.BatchNorm2d(out_channels))
        
        # æ®‹å·®è¿æ¥
        if stride != 1 or in_channels != out_channels:
            shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
        else:
            shortcut = nn.Identity()
        
        return ResidualBlock(nn.Sequential(*layers), shortcut)
    
    def _initialize_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        x = self.stem(x)
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        x = self.block4(x)
        x = self.global_pool(x)
        x = self.classifier(x)
        return x


class ResidualBlock(nn.Module):
    """æ®‹å·®å—"""
    
    def __init__(self, main_path, shortcut):
        super().__init__()
        self.main_path = main_path
        self.shortcut = shortcut
        self.relu = nn.ReLU(inplace=True)
    
    def forward(self, x):
        identity = self.shortcut(x)
        out = self.main_path(x)
        out += identity
        out = self.relu(out)
        return out


def create_optimized_dnm_config():
    """åˆ›å»ºä¼˜åŒ–çš„DNMé…ç½® - é’ˆå¯¹CIFAR-10 95%ç›®æ ‡"""
    
    return {
        'neuron_division': {
            'splitter': {
                'entropy_threshold': 0.5,        # è¾ƒä½é˜ˆå€¼ï¼Œä¿ƒè¿›åˆ†è£‚
                'overload_threshold': 0.4,       # è¾ƒä½è¿‡è½½é˜ˆå€¼
                'split_probability': 0.7,        # è¾ƒé«˜åˆ†è£‚æ¦‚ç‡
                'max_splits_per_layer': 3,       # å…è®¸é€‚é‡åˆ†è£‚
                'inheritance_noise': 0.08        # é€‚ä¸­çš„ç»§æ‰¿å™ªå£°
            },
            'monitoring': {
                'target_layers': ['conv', 'linear'],
                'analysis_frequency': 4,         # æ›´é¢‘ç¹çš„åˆ†æ
                'min_epoch_before_split': 8      # é€‚ä¸­çš„å¼€å§‹æ—¶æœº
            }
        },
        'connection_growth': {
            'analyzer': {
                'correlation_threshold': 0.12,   # é€‚ä¸­çš„ç›¸å…³æ€§é˜ˆå€¼
                'history_length': 8              # é€‚ä¸­çš„å†å²é•¿åº¦
            },
            'growth': {
                'max_new_connections': 3,        # é€‚é‡æ–°è¿æ¥
                'min_correlation_threshold': 0.08,
                'growth_frequency': 6,           # é€‚ä¸­çš„ç”Ÿé•¿é¢‘ç‡
                'connection_types': ['skip_connection', 'attention_connection']
            },
            'filtering': {
                'min_layer_distance': 1,         # å…è®¸ç›¸é‚»å±‚è¿æ¥
                'max_layer_distance': 6,         # é€‚ä¸­çš„è¿æ¥èŒƒå›´
                'avoid_redundant_connections': True
            }
        },
        'multi_objective': {
            'evolution': {
                'population_size': 8,            # é€‚ä¸­çš„ç§ç¾¤å¤§å°
                'max_generations': 10,           # é€‚ä¸­çš„ä»£æ•°
                'mutation_rate': 0.4,
                'crossover_rate': 0.7,
                'elitism_ratio': 0.2
            },
            'optimization': {
                'trigger_frequency': 15,         # é€‚ä¸­çš„è§¦å‘é¢‘ç‡
                'performance_plateau_threshold': 0.005,
                'min_improvement_epochs': 3
            }
        },
        'framework': {
            'morphogenesis_frequency': 4,       # è¾ƒé¢‘ç¹çš„å½¢æ€å‘ç”Ÿ
            'performance_tracking_window': 8,
            'early_stopping_patience': 25,
            'target_accuracy_threshold': 95.0, # ç›®æ ‡95%
            'enable_architecture_snapshots': True,
            'adaptive_morphogenesis': True
        }
    }


def run_dnm_cifar10_test():
    """è¿è¡ŒCIFAR-10 DNMæµ‹è¯•"""
    
    print("ğŸ§¬ DNM CIFAR-10 Test - Target: 95% Accuracy")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(42)
    
    # è®¾å¤‡é…ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ Device: {device}")
    
    # æ•°æ®å‡†å¤‡
    train_loader, test_loader = setup_cifar10_data(batch_size=128)
    
    # æ¨¡å‹åˆ›å»º
    model = EnhancedCIFAR10Net(num_classes=10, base_channels=64)
    initial_params = sum(p.numel() for p in model.parameters())
    print(f"\nğŸ—ï¸ æ¨¡å‹ä¿¡æ¯:")
    print(f"   åˆå§‹å‚æ•°: {initial_params:,}")
    print(f"   åˆå§‹æ¨¡å‹å¤§å°: {initial_params * 4 / 1024 / 1024:.2f} MB")
    
    # DNMé…ç½®
    dnm_config = create_optimized_dnm_config()
    
    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = optim.SGD(
        model.parameters(), 
        lr=0.1, momentum=0.9, weight_decay=5e-4, nesterov=True
    )
    
    # ä½™å¼¦é€€ç«å­¦ä¹ ç‡
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)
    
    # æ ‡ç­¾å¹³æ»‘çš„äº¤å‰ç†µ
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    
    print(f"\nğŸš€ å¼€å§‹DNMè®­ç»ƒ")
    print("=" * 60)
    
    # è®­ç»ƒå›è°ƒ
    def training_callback(dnm_framework, model, epoch_record):
        epoch = epoch_record['epoch']
        train_acc = epoch_record['train_acc']
        val_acc = epoch_record['val_acc']
        train_loss = epoch_record['train_loss']
        val_loss = epoch_record['val_loss']
        params = epoch_record['model_params']
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        current_lr = scheduler.get_last_lr()[0]
        
        # è®¡ç®—å‚æ•°å¢é•¿
        param_growth = (params - initial_params) / initial_params * 100
        
        # å®šæœŸè¾“å‡ºè¯¦ç»†ä¿¡æ¯
        if epoch % 3 == 0 or epoch < 10:
            print(f"ğŸ“ˆ Epoch {epoch:3d}: "
                  f"Train Acc={train_acc:5.2f}% Loss={train_loss:.4f} | "
                  f"Val Acc={val_acc:5.2f}% Loss={val_loss:.4f} | "
                  f"Params={params:,} (+{param_growth:4.1f}%) | "
                  f"LR={current_lr:.6f}")
    
    # å¼€å§‹è®­ç»ƒ
    start_time = time.time()
    
    try:
        result = DNMFramework(dnm_config).train_with_morphogenesis(
            model=model,
            train_loader=train_loader,
            val_loader=test_loader,
            epochs=100,
            optimizer=optimizer,
            criterion=criterion,
            callbacks=[training_callback]
        )
        
        # ç»“æœåˆ†æ
        training_time = time.time() - start_time
        final_params = sum(p.numel() for p in result['model'].parameters())
        param_growth = (final_params - initial_params) / initial_params * 100
        
        print(f"\nğŸ‰ DNMè®­ç»ƒå®Œæˆ!")
        print("=" * 60)
        print(f"ğŸ“Š æœ€ç»ˆç»“æœ:")
        print(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {result['best_val_accuracy']:.2f}%")
        print(f"   æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {result['final_val_accuracy']:.2f}%")
        print(f"   å‚æ•°å¢é•¿: +{param_growth:.1f}% ({initial_params:,} â†’ {final_params:,})")
        print(f"   è®­ç»ƒæ—¶é—´: {training_time/60:.1f}åˆ†é’Ÿ")
        print(f"   å½¢æ€å‘ç”Ÿäº‹ä»¶: {len(result['morphogenesis_events'])}")
        print(f"   æ€»ç¥ç»å…ƒåˆ†è£‚: {result['statistics']['total_neuron_splits']}")
        print(f"   æ€»è¿æ¥ç”Ÿé•¿: {result['statistics']['total_connections_grown']}")
        print(f"   æ€»ä¼˜åŒ–æ¬¡æ•°: {result['statistics']['total_optimizations']}")
        
        # å½¢æ€å‘ç”Ÿåˆ†æ
        if result['morphogenesis_events']:
            print(f"\nğŸ§¬ å½¢æ€å‘ç”Ÿäº‹ä»¶åˆ†æ:")
            for i, event in enumerate(result['morphogenesis_events'][-5:]):
                print(f"   äº‹ä»¶ {i+1} (Epoch {event['epoch']}):")
                print(f"     ç¥ç»å…ƒåˆ†è£‚: {event['neuron_splits']}")
                print(f"     è¿æ¥ç”Ÿé•¿: {event['connections_grown']}")
                print(f"     ä¼˜åŒ–è§¦å‘: {event['optimization_triggered']}")
                print(f"     è§¦å‘å‰æ€§èƒ½: {event['performance_before']:.2f}%")
        
        # æˆåŠŸè¯„ä¼°
        if result['best_val_accuracy'] >= 95.0:
            print(f"\nğŸ† SUCCESS: è¾¾åˆ°ç›®æ ‡95%å‡†ç¡®ç‡! ({result['best_val_accuracy']:.2f}%)")
        elif result['best_val_accuracy'] >= 90.0:
            print(f"\nğŸ¯ GOOD: æ¥è¿‘ç›®æ ‡! ({result['best_val_accuracy']:.2f}%)")
            print(f"   å»ºè®®: è°ƒæ•´é…ç½®å‚æ•°æˆ–å¢åŠ è®­ç»ƒè½®æ•°")
        else:
            print(f"\nğŸ”„ IMPROVING: éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ– ({result['best_val_accuracy']:.2f}%)")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if result['best_val_accuracy'] >= 85.0:
            model_path = f"cifar10_dnm_{result['best_val_accuracy']:.1f}percent.pth"
            torch.save({
                'model_state_dict': result['model'].state_dict(),
                'config': dnm_config,
                'accuracy': result['best_val_accuracy'],
                'morphogenesis_events': result['morphogenesis_events']
            }, model_path)
            print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        return result
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨DNM CIFAR-10æµ‹è¯•")
    
    result = run_dnm_cifar10_test()
    
    if result:
        print(f"\nâœ… æµ‹è¯•å®Œæˆ!")
        print(f"   DNMæ¡†æ¶è¿è¡Œæ­£å¸¸")
        print(f"   æœ€ç»ˆå‡†ç¡®ç‡: {result['final_val_accuracy']:.2f}%")
        if result['best_val_accuracy'] >= 95.0:
            print(f"   ğŸ¯ æˆåŠŸè¾¾åˆ°95%ç›®æ ‡!")
    else:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥")
        print(f"   æ£€æŸ¥é”™è¯¯ä¿¡æ¯è¿›è¡Œè°ƒè¯•")