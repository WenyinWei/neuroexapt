#!/usr/bin/env python3
"""
æœ€å°å¯å·¥ä½œæ¼”ç¤º - ä¿®å¤optimizeré”™è¯¯
Minimal Working Demo - Fix Optimizer Error

ğŸ¯ ç›®æ ‡ï¼šæä¾›ä¸€ä¸ªç¡®ä¿å¯ä»¥è¿è¡Œçš„æœ€å°æ¼”ç¤ºï¼Œè§£å†³åŸå§‹çš„optimizer.zero_grad()é”™è¯¯

ğŸ”§ å…³é”®ä¿®å¤ï¼š
1. ç¡®ä¿optimizeråœ¨ä½¿ç”¨å‰æ­£ç¡®åˆå§‹åŒ–
2. æ·»åŠ å……åˆ†çš„é”™è¯¯æ£€æŸ¥å’Œå¼‚å¸¸å¤„ç†
3. æä¾›æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯å’Œè¯Šæ–­
4. ä½¿ç”¨æœ€å°‘çš„ä¾èµ–æ¥é¿å…å¤æ‚çš„å¯¼å…¥é—®é¢˜
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import logging
import traceback

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class MinimalModel(nn.Module):
    """æœ€å°æ¨¡å‹ - ç¡®ä¿å¯ä»¥æ­£å¸¸å·¥ä½œ"""
    
    def __init__(self, input_size=32*32*3, hidden_size=128, num_classes=10):
        super(MinimalModel, self).__init__()
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(hidden_size, num_classes)
        
    def forward(self, x):
        x = self.flatten(x)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return x


class SafeTrainer:
    """å®‰å…¨è®­ç»ƒå™¨ - ä¸“æ³¨äºè§£å†³optimizeré”™è¯¯"""
    
    def __init__(self, model, device='cpu', lr=0.01):
        self.model = model
        self.device = device
        self.model.to(self.device)
        
        # å…³é”®ä¿®å¤ï¼šç«‹å³åˆå§‹åŒ–optimizer
        self.optimizer = None
        self.setup_optimizer(lr)
        
        # éªŒè¯optimizeråˆå§‹åŒ–
        if self.optimizer is None:
            raise RuntimeError("âŒ ä¼˜åŒ–å™¨åˆå§‹åŒ–å¤±è´¥ - è¿™æ˜¯åŸå§‹é”™è¯¯çš„æ ¹æº!")
        
        logger.info("âœ… SafeTraineråˆå§‹åŒ–æˆåŠŸï¼Œoptimizerå·²æ­£ç¡®è®¾ç½®")
    
    def setup_optimizer(self, lr):
        """å®‰å…¨çš„ä¼˜åŒ–å™¨è®¾ç½®"""
        try:
            if self.model is None:
                raise ValueError("æ¨¡å‹ä¸ºNoneï¼Œæ— æ³•åˆ›å»ºä¼˜åŒ–å™¨")
            
            # æ£€æŸ¥æ¨¡å‹å‚æ•°
            params = list(self.model.parameters())
            if not params:
                raise ValueError("æ¨¡å‹æ²¡æœ‰å¯è®­ç»ƒå‚æ•°")
            
            # åˆ›å»ºä¼˜åŒ–å™¨
            self.optimizer = optim.SGD(
                self.model.parameters(),
                lr=lr,
                momentum=0.9
            )
            
            logger.info(f"âœ… ä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸ - ç±»å‹: {type(self.optimizer).__name__}, LR: {lr}")
            
        except Exception as e:
            logger.error(f"âŒ ä¼˜åŒ–å™¨è®¾ç½®å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            raise
    
    def safe_train_step(self, data, target):
        """å®‰å…¨çš„è®­ç»ƒæ­¥éª¤ - åŒ…å«å®Œæ•´çš„é”™è¯¯æ£€æŸ¥"""
        try:
            # æ£€æŸ¥optimizerçŠ¶æ€
            if self.optimizer is None:
                raise RuntimeError("ä¼˜åŒ–å™¨ä¸ºNone - è¿™æ­£æ˜¯åŸå§‹é”™è¯¯çš„åŸå› !")
            
            # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
            if self.model is None:
                raise RuntimeError("æ¨¡å‹ä¸ºNone")
            
            # æ£€æŸ¥æ•°æ®
            if data is None or target is None:
                raise ValueError("æ•°æ®æˆ–æ ‡ç­¾ä¸ºNone")
            
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            data = data.to(self.device)
            target = target.to(self.device)
            
            # è®­ç»ƒæ­¥éª¤
            self.model.train()
            
            # å…³é”®æ­¥éª¤ï¼šæ¸…é›¶æ¢¯åº¦
            self.optimizer.zero_grad()  # è¿™é‡Œæ˜¯åŸå§‹é”™è¯¯å‘ç”Ÿçš„åœ°æ–¹
            
            # å‰å‘ä¼ æ’­
            output = self.model(data)
            
            # è®¡ç®—æŸå¤±
            loss = F.cross_entropy(output, target)
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ›´æ–°å‚æ•°
            self.optimizer.step()
            
            # è®¡ç®—å‡†ç¡®ç‡
            pred = output.argmax(dim=1)
            correct = pred.eq(target).sum().item()
            accuracy = correct / target.size(0)
            
            return loss.item(), accuracy
            
        except Exception as e:
            logger.error(f"âŒ è®­ç»ƒæ­¥éª¤å¤±è´¥: {e}")
            logger.error(f"optimizerçŠ¶æ€: {self.optimizer}")
            logger.error(f"modelçŠ¶æ€: {self.model}")
            raise
    
    def evaluate(self, data_loader):
        """å®‰å…¨çš„æ¨¡å‹è¯„ä¼°"""
        try:
            self.model.eval()
            total_loss = 0.0
            correct = 0
            total = 0
            
            with torch.no_grad():
                for data, target in data_loader:
                    data, target = data.to(self.device), target.to(self.device)
                    output = self.model(data)
                    
                    loss = F.cross_entropy(output, target, reduction='sum')
                    total_loss += loss.item()
                    
                    pred = output.argmax(dim=1)
                    correct += pred.eq(target).sum().item()
                    total += target.size(0)
            
            avg_loss = total_loss / total
            accuracy = correct / total
            
            return avg_loss, accuracy
            
        except Exception as e:
            logger.error(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
            return float('inf'), 0.0


def create_mock_data(num_samples=1000, batch_size=32):
    """åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›†"""
    try:
        logger.info("ğŸ“¦ åˆ›å»ºæ¨¡æ‹ŸCIFAR-10æ•°æ®é›†...")
        
        # åˆ›å»ºéšæœºæ•°æ®
        data = torch.randn(num_samples, 3, 32, 32)
        labels = torch.randint(0, 10, (num_samples,))
        
        # åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨
        dataset = TensorDataset(data, labels)
        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        logger.info(f"âœ… æ¨¡æ‹Ÿæ•°æ®é›†åˆ›å»ºæˆåŠŸ - æ ·æœ¬æ•°: {num_samples}, batch_size: {batch_size}")
        return loader
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥: {e}")
        raise


def run_minimal_demo():
    """è¿è¡Œæœ€å°æ¼”ç¤º"""
    print("ğŸ”§ æœ€å°å¯å·¥ä½œæ¼”ç¤º - ä¿®å¤optimizer.zero_grad()é”™è¯¯")
    print("="*60)
    
    try:
        # 1. è®¾ç½®è®¾å¤‡
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
        
        # 2. åˆ›å»ºæ¨¡å‹
        logger.info("ğŸ—ï¸  åˆ›å»ºæœ€å°æ¨¡å‹...")
        model = MinimalModel()
        logger.info(f"ğŸ“Š æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # 3. åˆ›å»ºè®­ç»ƒå™¨ - è¿™é‡Œä¼šæ­£ç¡®åˆå§‹åŒ–optimizer
        logger.info("âš™ï¸  åˆå§‹åŒ–å®‰å…¨è®­ç»ƒå™¨...")
        trainer = SafeTrainer(model, device, lr=0.01)
        
        # 4. åˆ›å»ºæ•°æ®
        train_loader = create_mock_data(num_samples=500, batch_size=16)
        test_loader = create_mock_data(num_samples=100, batch_size=16)
        
        # 5. è¿›è¡Œå‡ ä¸ªè®­ç»ƒæ­¥éª¤
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒæ¼”ç¤º...")
        
        num_epochs = 2
        for epoch in range(num_epochs):
            epoch_loss = 0.0
            epoch_acc = 0.0
            num_batches = 0
            
            for batch_idx, (data, target) in enumerate(train_loader):
                # æ‰§è¡Œå®‰å…¨çš„è®­ç»ƒæ­¥éª¤
                loss, acc = trainer.safe_train_step(data, target)
                
                epoch_loss += loss
                epoch_acc += acc
                num_batches += 1
                
                # æ¯5ä¸ªbatchæŠ¥å‘Šä¸€æ¬¡
                if batch_idx % 5 == 0:
                    logger.info(f"Epoch {epoch}, Batch {batch_idx}: Loss={loss:.4f}, Acc={acc:.2%}")
            
            # Epochæ€»ç»“
            avg_loss = epoch_loss / num_batches
            avg_acc = epoch_acc / num_batches
            
            # è¯„ä¼°
            test_loss, test_acc = trainer.evaluate(test_loader)
            
            logger.info(f"Epoch {epoch} å®Œæˆ - è®­ç»ƒ: Loss={avg_loss:.4f}, Acc={avg_acc:.2%} | æµ‹è¯•: Loss={test_loss:.4f}, Acc={test_acc:.2%}")
        
        print("\n" + "="*60)
        print("ğŸ‰ æœ€å°æ¼”ç¤ºæˆåŠŸå®Œæˆ!")
        print("\nâœ… å…³é”®ä¿®å¤è¯´æ˜:")
        print("â€¢ optimizeråœ¨SafeTrainer.__init__()ä¸­ç«‹å³åˆå§‹åŒ–")
        print("â€¢ æ·»åŠ äº†optimizerçŠ¶æ€æ£€æŸ¥")
        print("â€¢ æä¾›äº†è¯¦ç»†çš„é”™è¯¯è¯Šæ–­ä¿¡æ¯")
        print("â€¢ ä½¿ç”¨äº†æœ€å°‘çš„ä¾èµ–é¿å…å¯¼å…¥é—®é¢˜")
        
        print(f"\nğŸ“Š æœ€ç»ˆç»“æœ:")
        print(f"â€¢ å®Œæˆè®­ç»ƒ: {num_epochs} epochs")
        print(f"â€¢ æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2%}")
        print(f"â€¢ æµ‹è¯•æŸå¤±: {test_loss:.4f}")
        
        print(f"\nğŸ”§ åŸå§‹é”™è¯¯åŸå› :")
        print("â€¢ åŸä»£ç ä¸­optimizeræ²¡æœ‰åœ¨train_epochè°ƒç”¨å‰åˆå§‹åŒ–")
        print("â€¢ IntelligentEvolutionTrainer.setup_optimizer()ä»æœªè¢«è°ƒç”¨")
        print("â€¢ å½“trainer.train_epoch()æ‰§è¡Œself.optimizer.zero_grad()æ—¶ï¼Œself.optimizerä¸ºNone")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        
        print(f"\nâŒ æ¼”ç¤ºå¤±è´¥: {e}")
        print("\nğŸ” é”™è¯¯è¯Šæ–­:")
        print("â€¢ å¦‚æœæ˜¯optimizerç›¸å…³é”™è¯¯ï¼Œæ£€æŸ¥æ¨¡å‹åˆå§‹åŒ–")
        print("â€¢ å¦‚æœæ˜¯å¯¼å…¥é”™è¯¯ï¼Œæ£€æŸ¥PyTorchå®‰è£…")
        print("â€¢ å¦‚æœæ˜¯CUDAé”™è¯¯ï¼Œå°è¯•ä½¿ç”¨CPU: device='cpu'")
        
        return False


def demonstrate_original_error():
    """æ¼”ç¤ºåŸå§‹é”™è¯¯çš„äº§ç”Ÿè¿‡ç¨‹"""
    print("\nğŸš¨ æ¼”ç¤ºåŸå§‹é”™è¯¯çš„äº§ç”Ÿè¿‡ç¨‹")
    print("="*50)
    
    try:
        # æ¨¡æ‹ŸåŸå§‹ä»£ç çš„é”™è¯¯æƒ…å†µ
        class BuggyTrainer:
            def __init__(self, model, device):
                self.model = model
                self.device = device
                self.optimizer = None  # ğŸ‘ˆ è¿™é‡Œæ˜¯é—®é¢˜æ‰€åœ¨ - optimizerä¸ºNone
                # æ³¨æ„ï¼šsetup_optimizerä»æœªè¢«è°ƒç”¨
                
            def setup_optimizer(self, lr):
                """è¿™ä¸ªæ–¹æ³•å®šä¹‰äº†ä½†ä»æœªè¢«è°ƒç”¨"""
                self.optimizer = optim.SGD(self.model.parameters(), lr=lr)
                
            def train_epoch(self):
                """è¿™é‡Œä¼šè§¦å‘é”™è¯¯"""
                self.optimizer.zero_grad()  # âŒ AttributeError: 'NoneType' object has no attribute 'zero_grad'
        
        model = MinimalModel()
        buggy_trainer = BuggyTrainer(model, 'cpu')
        
        print(f"optimizerçŠ¶æ€: {buggy_trainer.optimizer}")  # None
        print("å°è¯•è°ƒç”¨train_epoch()...")
        
        # è¿™é‡Œä¼šè§¦å‘åŸå§‹é”™è¯¯
        buggy_trainer.train_epoch()
        
    except AttributeError as e:
        print(f"âœ… æˆåŠŸé‡ç°åŸå§‹é”™è¯¯: {e}")
        print("ğŸ”§ è¿™æ­£æ˜¯åŸå§‹ä»£ç çš„é—®é¢˜ï¼šoptimizerä»æœªè¢«åˆå§‹åŒ–!")
        
    except Exception as e:
        print(f"âŒ æ„å¤–é”™è¯¯: {e}")


if __name__ == "__main__":
    print("ğŸ¯ NeuroExapt - æœ€å°å¯å·¥ä½œæ¼”ç¤º")
    print("ä¸“æ³¨è§£å†³: 'NoneType' object has no attribute 'zero_grad' é”™è¯¯")
    print("="*80)
    
    # é¦–å…ˆæ¼”ç¤ºåŸå§‹é”™è¯¯
    demonstrate_original_error()
    
    # ç„¶åå±•ç¤ºä¿®å¤åçš„ç‰ˆæœ¬
    success = run_minimal_demo()
    
    if success:
        print("\nğŸ‰ é—®é¢˜å·²è§£å†³! é‡æ„ç‰ˆæœ¬å¯ä»¥æ­£å¸¸å·¥ä½œ")
        print("\nğŸ’¡ è¦è¿è¡Œå®Œæ•´åŠŸèƒ½ï¼Œè¯·ä½¿ç”¨:")
        print("   python examples/intelligent_evolution_demo_refactored.py")
    else:
        print("\nâŒ ä»æœ‰é—®é¢˜éœ€è¦è§£å†³")
        print("\nğŸ’¡ å»ºè®®è¿è¡Œè¯Šæ–­è„šæœ¬:")
        print("   python examples/test_core_modules.py")