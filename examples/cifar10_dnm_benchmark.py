#!/usr/bin/env python3
"""
CIFAR-10 DNM Benchmark - ç›®æ ‡å‡†ç¡®ç‡ 95%

ä½¿ç”¨DNMæ¡†æ¶åœ¨CIFAR-10æ•°æ®é›†ä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•
åˆå§‹æ¶æ„é‡‡ç”¨ResNet-18å˜ä½“ï¼Œé€šè¿‡DNMæ¼”åŒ–å®ç°95%å‡†ç¡®ç‡çªç ´
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import time
import os
import sys
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥DNMæ¡†æ¶
from neuroexapt.core.dnm_framework import train_with_dnm
from neuroexapt.core.dnm_neuron_division import DNMNeuronDivision
from neuroexapt.core.dnm_connection_growth import DNMConnectionGrowth
from neuroexapt.math.pareto_optimization import DNMMultiObjectiveOptimization


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,  # æ”¹ä¸ºINFOçº§åˆ«ä»¥æ˜¾ç¤ºBatchNormåŒæ­¥æ—¥å¿—
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('dnm_benchmark.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)


class EvolvableResNet18(nn.Module):
    """å¯æ¼”åŒ–çš„ResNet-18æ¶æ„ - ä¸“é—¨è®¾è®¡ç”¨äºDNMæ¼”åŒ–"""
    
    def __init__(self, num_classes=10, initial_channels=64):
        super().__init__()
        self.initial_channels = initial_channels
        
        # åˆå§‹å·ç§¯å±‚
        self.conv1 = nn.Conv2d(3, initial_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(initial_channels)
        self.relu = nn.ReLU(inplace=True)
        
        # å¯æ¼”åŒ–çš„æ®‹å·®å—
        self.layer1 = self._make_layer(initial_channels, initial_channels, 2, stride=1)
        self.layer2 = self._make_layer(initial_channels, initial_channels * 2, 2, stride=2)
        self.layer3 = self._make_layer(initial_channels * 2, initial_channels * 4, 2, stride=2)
        self.layer4 = self._make_layer(initial_channels * 4, initial_channels * 8, 2, stride=2)
        
        # å¯æ¼”åŒ–çš„åˆ†ç±»å™¨
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc_layers = nn.Sequential(
            nn.Flatten(),
            nn.Linear(initial_channels * 8, initial_channels * 4),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(initial_channels * 4, initial_channels * 2),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(initial_channels * 2, num_classes)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
    
    def _make_layer(self, in_channels, out_channels, blocks, stride=1):
        layers = []
        
        # ç¬¬ä¸€ä¸ªå—å¯èƒ½éœ€è¦é™é‡‡æ ·
        layers.append(EvolvableBasicBlock(in_channels, out_channels, stride))
        
        # å…¶ä½™å—
        for _ in range(1, blocks):
            layers.append(EvolvableBasicBlock(out_channels, out_channels, 1))
        
        return nn.Sequential(*layers)
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        # åˆå§‹ç‰¹å¾æå–
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        
        # æ®‹å·®å±‚
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        
        # åˆ†ç±»
        x = self.avgpool(x)
        x = self.fc_layers(x)
        
        return x


class EvolvableBasicBlock(nn.Module):
    """å¯æ¼”åŒ–çš„åŸºç¡€æ®‹å·®å—"""
    
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, 
                              stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,
                              stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        
        # è·³è·ƒè¿æ¥
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, 
                         stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
        else:
            self.shortcut = nn.Identity()
    
    def forward(self, x):
        identity = self.shortcut(x)
        
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        
        out = self.conv2(out)
        out = self.bn2(out)
        
        out += identity
        out = self.relu(out)
        
        return out


def prepare_cifar10_data(batch_size=128, num_workers=4):
    """å‡†å¤‡CIFAR-10æ•°æ®é›†"""
    
    # æ•°æ®å¢å¼ºç­–ç•¥
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(15),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))
    ])
    
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))
    ])
    
    # åŠ è½½æ•°æ®é›†
    trainset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform_train
    )
    testset = torchvision.datasets.CIFAR10(
        root='./data', train=False, download=True, transform=transform_test
    )
    
    train_loader = DataLoader(
        trainset, batch_size=batch_size, shuffle=True, 
        num_workers=num_workers, pin_memory=True
    )
    test_loader = DataLoader(
        testset, batch_size=batch_size, shuffle=False,
        num_workers=num_workers, pin_memory=True
    )
    
    print(f"ğŸ“Š CIFAR-10 Dataset loaded:")
    print(f"   Training samples: {len(trainset)}")
    print(f"   Test samples: {len(testset)}")
    print(f"   Batch size: {batch_size}")
    
    return train_loader, test_loader


def create_optimized_dnm_config():
    """åˆ›å»ºä¼˜åŒ–çš„DNMé…ç½®ï¼Œç›®æ ‡95%å‡†ç¡®ç‡"""
    
    return {
        'neuron_division': {
            'splitter': {
                'entropy_threshold': 0.6,        # é€‚ä¸­çš„åˆ†è£‚é˜ˆå€¼
                'overload_threshold': 0.5,       # è¾ƒä½çš„è¿‡è½½é˜ˆå€¼ï¼Œæ›´ç§¯æåˆ†è£‚
                'split_probability': 0.6,        # æé«˜åˆ†è£‚æ¦‚ç‡
                'max_splits_per_layer': 4,       # å…è®¸æ›´å¤šåˆ†è£‚
                'inheritance_noise': 0.05        # è¾ƒå°çš„ç»§æ‰¿å™ªå£°
            },
            'monitoring': {
                'target_layers': ['conv', 'linear'],
                'analysis_frequency': 4,         # æ›´é¢‘ç¹çš„åˆ†æ
                'min_epoch_before_split': 8      # æ›´æ—©å¼€å§‹åˆ†è£‚
            }
        },
        'connection_growth': {
            'analyzer': {
                'correlation_threshold': 0.12,   # è¾ƒä½çš„ç›¸å…³æ€§é˜ˆå€¼
                'history_length': 10             # æ›´é•¿çš„å†å²è®°å½•
            },
            'growth': {
                'max_new_connections': 4,        # æ›´å¤šè¿æ¥
                'min_correlation_threshold': 0.08,
                'growth_frequency': 6,           # æ›´é¢‘ç¹çš„è¿æ¥ç”Ÿé•¿
                'connection_types': ['skip_connection', 'attention_connection']
            },
            'filtering': {
                'min_layer_distance': 1,         # å…è®¸ç›¸é‚»å±‚è¿æ¥
                'max_layer_distance': 8,         # æ›´å¤§çš„è¿æ¥èŒƒå›´
                'avoid_redundant_connections': True
            }
        },
        'multi_objective': {
            'evolution': {
                'population_size': 10,           # é€‚ä¸­çš„ç§ç¾¤å¤§å°
                'max_generations': 12,           # é€‚ä¸­çš„ä»£æ•°
                'mutation_rate': 0.4,
                'crossover_rate': 0.8,
                'elitism_ratio': 0.2
            },
            'optimization': {
                'trigger_frequency': 15,         # é€‚ä¸­çš„è§¦å‘é¢‘ç‡
                'performance_plateau_threshold': 0.005,
                'min_improvement_epochs': 3
            }
        },
        'framework': {
            'morphogenesis_frequency': 4,       # æ›´é¢‘ç¹çš„å½¢æ€å‘ç”Ÿ
            'performance_tracking_window': 8,
            'early_stopping_patience': 25,
            'target_accuracy_threshold': 95.0, # ç›®æ ‡95%
            'enable_architecture_snapshots': True,
            'adaptive_morphogenesis': True
        }
    }


def run_cifar10_dnm_benchmark():
    """è¿è¡ŒCIFAR-10 DNMåŸºå‡†æµ‹è¯•"""
    
    print("ğŸ§¬ CIFAR-10 DNM Benchmark - Target: 95% Accuracy")
    print("=" * 60)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ Device: {device}")
    
    # å‡†å¤‡æ•°æ®
    train_loader, test_loader = prepare_cifar10_data(batch_size=128)
    
    # åˆ›å»ºå¯æ¼”åŒ–æ¨¡å‹
    model = EvolvableResNet18(num_classes=10, initial_channels=64)
    initial_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ—ï¸ Initial model: {initial_params:,} parameters")
    
    # åˆ›å»ºä¼˜åŒ–çš„DNMé…ç½®
    dnm_config = create_optimized_dnm_config()
    
    # åˆ›å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200, eta_min=1e-6)
    
    # æŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    
    print(f"\nğŸš€ Starting DNM Training (Target: 95% accuracy)")
    print("=" * 60)
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # å›è°ƒå‡½æ•°
    def progress_callback(dnm_framework, model, epoch_record):
        epoch = epoch_record['epoch']
        val_acc = epoch_record['val_acc']
        train_acc = epoch_record['train_acc']
        params = epoch_record['model_params']
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        current_lr = scheduler.get_last_lr()[0]
        
        if epoch % 5 == 0:
            param_growth = (params - initial_params) / initial_params * 100
            print(f"ğŸ“ˆ Epoch {epoch}: Train={train_acc:.2f}%, Val={val_acc:.2f}%, "
                  f"Params={params:,} (+{param_growth:.1f}%), LR={current_lr:.6f}")
    
    # ä½¿ç”¨DNMè®­ç»ƒ
    try:
        result = train_with_dnm(
            model=model,
            train_loader=train_loader,
            val_loader=test_loader,
            epochs=200,
            optimizer=optimizer,
            criterion=criterion,
            config=dnm_config,
            callbacks=[progress_callback]
        )
        
        # è®­ç»ƒå®Œæˆç»Ÿè®¡
        training_time = time.time() - start_time
        final_params = sum(p.numel() for p in result['model'].parameters())
        param_growth = (final_params - initial_params) / initial_params * 100
        
        print("\nğŸ‰ DNM Training Completed!")
        print("=" * 60)
        print(f"ğŸ“Š Results Summary:")
        print(f"   Best Validation Accuracy: {result['best_val_accuracy']:.2f}%")
        print(f"   Final Validation Accuracy: {result['final_val_accuracy']:.2f}%")
        print(f"   Parameter Growth: +{param_growth:.1f}% ({initial_params:,} â†’ {final_params:,})")
        print(f"   Training Time: {training_time/60:.1f} minutes")
        print(f"   Morphogenesis Events: {len(result['morphogenesis_events'])}")
        print(f"   Total Neuron Splits: {result['statistics']['total_neuron_splits']}")
        print(f"   Total Connections Grown: {result['statistics']['total_connections_grown']}")
        
        # è¯¦ç»†åˆ†æå½¢æ€å‘ç”Ÿäº‹ä»¶
        if result['morphogenesis_events']:
            print(f"\nğŸ§¬ Morphogenesis Analysis:")
            for i, event in enumerate(result['morphogenesis_events'][-5:]):  # æ˜¾ç¤ºæœ€å5ä¸ªäº‹ä»¶
                print(f"   Event {i+1} (Epoch {event['epoch']}):")
                print(f"     Neuron splits: {event['neuron_splits']}")
                print(f"     Connections grown: {event['connections_grown']}")
                print(f"     Performance before: {event['performance_before']:.2f}%")
        
        # ä¿å­˜æ¨¡å‹
        model_path = f"cifar10_dnm_evolved_{result['best_val_accuracy']:.1f}percent.pth"
        torch.save({
            'model_state_dict': result['model'].state_dict(),
            'config': dnm_config,
            'results': result,
            'final_accuracy': result['best_val_accuracy']
        }, model_path)
        print(f"ğŸ’¾ Model saved: {model_path}")
        
        # æˆåŠŸæ ‡å¿—
        if result['best_val_accuracy'] >= 95.0:
            print(f"\nğŸ† SUCCESS: Achieved target 95% accuracy! ({result['best_val_accuracy']:.2f}%)")
        elif result['best_val_accuracy'] >= 90.0:
            print(f"\nğŸ¯ GOOD: Close to target! ({result['best_val_accuracy']:.2f}%)")
        else:
            print(f"\nğŸ”„ IMPROVING: Need more optimization ({result['best_val_accuracy']:.2f}%)")
        
        return result
        
    except Exception as e:
        print(f"\nâŒ Training failed: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    torch.cuda.manual_seed(42)
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    result = run_cifar10_dnm_benchmark()
    
    if result:
        print(f"\nâœ… Benchmark completed successfully!")
    else:
        print(f"\nâŒ Benchmark failed!")