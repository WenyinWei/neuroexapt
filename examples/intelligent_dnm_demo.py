#!/usr/bin/env python3
"""
æ™ºèƒ½ç“¶é¢ˆé©±åŠ¨çš„DNMå½¢æ€å‘ç”Ÿæ¼”ç¤º
Intelligent Bottleneck-Driven DNM Morphogenesis Demo

ğŸ§¬ æ¼”ç¤ºå†…å®¹ï¼š
1. æ™ºèƒ½ç“¶é¢ˆæ£€æµ‹ - æ— éœ€å›ºå®šé—´éš”ï¼Œå®æ—¶ç›‘æ§ç½‘ç»œç“¶é¢ˆ
2. Net2Netè¾“å‡ºåå‘æŠ•å½±åˆ†æ - æ£€æµ‹å“ªä¸€å±‚é˜»ç¢äº†å‡†ç¡®ç‡æå‡
3. å¤šä¼˜å…ˆçº§å†³ç­–åˆ¶å®š - åŸºäºç“¶é¢ˆä¸¥é‡ç¨‹åº¦å’Œæ”¹è¿›æ½œåŠ›
4. ç²¾ç¡®çš„å½¢æ€å‘ç”Ÿç­–ç•¥ - é’ˆå¯¹æ€§åœ°è§£å†³ç‰¹å®šç“¶é¢ˆ

ğŸ¯ ç›®æ ‡ï¼šè®©ç¥ç»ç½‘ç»œåƒæ´»è¿‡æ¥ä¸€æ ·è‡ªé€‚åº”ç”Ÿé•¿ï¼Œçªç ´æ€§èƒ½ç“¶é¢ˆ
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import numpy as np
import time
import matplotlib.pyplot as plt
from collections import defaultdict
import logging

# å¯¼å…¥å¢å¼ºçš„DNMç»„ä»¶
from neuroexapt.core import (
    EnhancedDNMFramework,
    AdvancedBottleneckAnalyzer,
    AdvancedMorphogenesisExecutor,
    IntelligentMorphogenesisDecisionMaker,
    MorphogenesisType,
    MorphogenesisDecision
)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)

class IntelligentResNet(nn.Module):
    """æ™ºèƒ½è‡ªé€‚åº”ResNet - ä¸“ä¸ºæ¼”ç¤ºç“¶é¢ˆæ£€æµ‹è®¾è®¡"""
    
    def __init__(self, num_classes=10):
        super(IntelligentResNet, self).__init__()
        
        # æ•…æ„è®¾è®¡ä¸€äº›ç“¶é¢ˆæ¥æ¼”ç¤ºæ™ºèƒ½æ£€æµ‹
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1, bias=False)  # è¾ƒå°çš„åˆå§‹é€šé“æ•°
        self.bn1 = nn.BatchNorm2d(32)
        self.relu = nn.ReLU(inplace=True)
        
        # ç‰¹æ„åˆ›å»ºæ·±åº¦ç“¶é¢ˆ
        self.shallow_block = nn.Sequential(
            nn.Conv2d(32, 64, 3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2)  # 16x16
        )
        
        # ç‰¹æ„åˆ›å»ºå®½åº¦ç“¶é¢ˆ - é€šé“æ•°è¿‡å°‘
        self.narrow_block = nn.Sequential(
            nn.Conv2d(64, 32, 3, padding=1, bias=False),  # å‡å°‘é€šé“æ•°
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2)  # 8x8
        )
        
        # ä¿¡æ¯æµç“¶é¢ˆ - å•ä¸€è·¯å¾„å¤„ç†
        self.bottleneck_conv = nn.Conv2d(32, 64, 3, padding=1, bias=False)
        self.bottleneck_bn = nn.BatchNorm2d(64)
        
        # åˆ†ç±»å™¨ç“¶é¢ˆ - è¾ƒå°çš„éšè—å±‚
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(64, 32),  # å¾ˆå°çš„éšè—å±‚
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(32, num_classes)
        )
        
    def forward(self, x):
        # åˆå§‹ç‰¹å¾æå–
        x = self.relu(self.bn1(self.conv1(x)))
        
        # æµ…å±‚å¤„ç† (æ·±åº¦ç“¶é¢ˆ)
        x = self.shallow_block(x)
        
        # çª„é€šé“å¤„ç† (å®½åº¦ç“¶é¢ˆ) 
        x = self.narrow_block(x)
        
        # å•è·¯å¾„å¤„ç† (ä¿¡æ¯æµç“¶é¢ˆ)
        x = self.relu(self.bottleneck_bn(self.bottleneck_conv(x)))
        
        # åˆ†ç±» (å®¹é‡ç“¶é¢ˆ)
        x = self.classifier(x)
        
        return x

class IntelligentDNMTrainer:
    """æ™ºèƒ½DNMè®­ç»ƒå™¨ - å±•ç¤ºæ™ºèƒ½ç“¶é¢ˆæ£€æµ‹"""
    
    def __init__(self, model, device, train_loader, test_loader):
        self.model = model.to(device)
        self.device = device
        self.train_loader = train_loader
        self.test_loader = test_loader
        
        # ğŸ§  æ™ºèƒ½DNMæ¡†æ¶é…ç½® - æ— å›ºå®šé—´éš”è§¦å‘
        self.dnm_config = {
            # ç§»é™¤å›ºå®šè§¦å‘é—´éš”ï¼Œé‡‡ç”¨æ™ºèƒ½æ£€æµ‹
            'trigger_interval': 1,  # æ¯è½®éƒ½æ£€æŸ¥ï¼Œä½†ç”±æ™ºèƒ½ç®—æ³•å†³å®šæ˜¯å¦è§¦å‘
            'complexity_threshold': 0.3,  # é™ä½é˜ˆå€¼ï¼Œæ›´æ•æ„Ÿ
            'enable_serial_division': True,
            'enable_parallel_division': True,
            'enable_hybrid_division': True,
            'max_parameter_growth_ratio': 5.0,  # å…è®¸æ›´å¤§å¢é•¿
            
            # ğŸ§  æ™ºèƒ½ç“¶é¢ˆæ£€æµ‹é…ç½®
            'enable_intelligent_bottleneck_detection': True,
            'bottleneck_severity_threshold': 0.5,  # ç“¶é¢ˆä¸¥é‡ç¨‹åº¦é˜ˆå€¼
            'stagnation_threshold': 0.005,  # 0.5% åœæ»é˜ˆå€¼
            'net2net_improvement_threshold': 0.3,  # Net2Netæ”¹è¿›æ½œåŠ›é˜ˆå€¼
            
                         # æ¿€è¿›æ¨¡å¼é…ç½® (ä½œä¸ºå¤‡ç”¨) - æš‚æ—¶å…³é—­é¿å…å¯¼å…¥é—®é¢˜
             'enable_aggressive_mode': False,
             'accuracy_plateau_threshold': 0.001,
             'plateau_detection_window': 3,
             'aggressive_trigger_accuracy': 0.88,
             'max_concurrent_mutations': 2,
             'morphogenesis_budget': 15000
        }
        
        self.dnm_framework = EnhancedDNMFramework(self.dnm_config)
        
        # è®­ç»ƒå†å²
        self.train_history = []
        self.test_history = []
        self.morphogenesis_history = []
        self.parameter_history = []
        self.bottleneck_history = []  # è®°å½•ç“¶é¢ˆæ£€æµ‹å†å²
        
    def capture_network_state(self):
        """æ•è·ç½‘ç»œçŠ¶æ€ç”¨äºç“¶é¢ˆåˆ†æ"""
        print("      ğŸ” æ™ºèƒ½ç½‘ç»œçŠ¶æ€æ•è·...")
        activations = {}
        gradients = {}
        
        # æ³¨å†Œé’©å­å‡½æ•°
        def forward_hook(name):
            def hook(module, input, output):
                if isinstance(output, torch.Tensor):
                    activations[name] = output.detach().cpu()
            return hook
        
        def backward_hook(name):
            def hook(module, grad_input, grad_output):
                if grad_output[0] is not None:
                    gradients[name] = grad_output[0].detach().cpu()
            return hook
        
        # æ³¨å†Œé’©å­
        hooks = []
        for name, module in self.model.named_modules():
            if isinstance(module, (nn.Linear, nn.Conv2d)):
                hooks.append(module.register_forward_hook(forward_hook(name)))
                hooks.append(module.register_backward_hook(backward_hook(name)))
        
        # æ‰§è¡Œå‰å‘å’Œåå‘ä¼ æ’­
        try:
            self.model.train()
            data, target = next(iter(self.train_loader))
            data, target = data.to(self.device), target.to(self.device)
            
            output = self.model(data)
            loss = F.cross_entropy(output, target)
            
            self.model.zero_grad()
            loss.backward()
            
        except Exception as e:
            print(f"        âŒ çŠ¶æ€æ•è·å¤±è´¥: {e}")
        
        # æ¸…ç†é’©å­
        for hook in hooks:
            hook.remove()
        
        print(f"        âœ… æ•è·å®Œæˆ: {len(activations)}ä¸ªæ¿€æ´», {len(gradients)}ä¸ªæ¢¯åº¦")
        return activations, gradients, target.detach().cpu()
    
    def train_epoch(self, optimizer, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(self.train_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            optimizer.zero_grad()
            output = self.model(data)
            loss = F.cross_entropy(output, target)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
            
            if batch_idx % 100 == 0:
                print(f'    Batch: {batch_idx:3d}/{len(self.train_loader)}, '
                      f'Loss: {loss.item():.6f}, Acc: {100.*correct/total:.2f}%')
        
        avg_loss = total_loss / len(self.train_loader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def test_epoch(self):
        """æµ‹è¯•ä¸€ä¸ªepoch"""
        self.model.eval()
        test_loss = 0
        correct = 0
        
        with torch.no_grad():
            for data, target in self.test_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                test_loss += F.cross_entropy(output, target, reduction='sum').item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
        
        test_loss /= len(self.test_loader.dataset)
        accuracy = 100. * correct / len(self.test_loader.dataset)
        
        return test_loss, accuracy
    
    def train_with_intelligent_morphogenesis(self, epochs=50):
        """å¸¦æ™ºèƒ½å½¢æ€å‘ç”Ÿçš„è®­ç»ƒ"""
        print("ğŸ§  å¼€å§‹æ™ºèƒ½DNMè®­ç»ƒ...")
        print("=" * 60)
        
        optimizer = optim.SGD(
            self.model.parameters(), 
            lr=0.1,
            momentum=0.9,
            weight_decay=1e-4
        )
        
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)
        
        # è®°å½•åˆå§‹å‚æ•°æ•°é‡
        initial_params = sum(p.numel() for p in self.model.parameters())
        print(f"ğŸ“Š åˆå§‹æ¨¡å‹: {initial_params:,} å‚æ•°")
        self.parameter_history.append(initial_params)
        
        best_test_acc = 0.0
        
        for epoch in range(epochs):
            print(f"\nğŸ§  Epoch {epoch+1}/{epochs}")
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(optimizer, epoch)
            
            # æµ‹è¯•
            test_loss, test_acc = self.test_epoch()
            
            # æ›´æ–°å†å²
            self.train_history.append((train_loss, train_acc))
            self.test_history.append((test_loss, test_acc))
            self.dnm_framework.update_performance_history(test_acc / 100.0)
            
            print(f"  ğŸ“Š Train: {train_acc:.2f}% | Test: {test_acc:.2f}%")
            
            # æ™ºèƒ½å½¢æ€å‘ç”Ÿæ£€æŸ¥ - æ¯è½®éƒ½æ£€æŸ¥ä½†ç”±ç®—æ³•å†³å®š
            print(f"  ğŸ§  æ™ºèƒ½ç“¶é¢ˆæ£€æµ‹åˆ†æ...")
            print(f"    ğŸ“Š å½“å‰æ¨¡å‹: {sum(p.numel() for p in self.model.parameters()):,} å‚æ•°")
            
            try:
                # æ•è·ç½‘ç»œçŠ¶æ€
                activations, gradients, targets = self.capture_network_state()
                
                # æ„å»ºåˆ†æä¸Šä¸‹æ–‡
                context = {
                    'epoch': epoch,
                    'activations': activations,
                    'gradients': gradients,
                    'performance_history': self.dnm_framework.performance_history,
                    'targets': targets
                }
                
                # æ‰§è¡Œæ™ºèƒ½å½¢æ€å‘ç”Ÿ
                print("  ğŸš€ æ‰§è¡Œæ™ºèƒ½å½¢æ€å‘ç”Ÿåˆ†æ...")
                results = self.dnm_framework.execute_morphogenesis(
                    model=self.model,
                    activations_or_context=context,
                    gradients=None,  # contextä¸­å·²åŒ…å«
                    performance_history=None,  # contextä¸­å·²åŒ…å«
                    epoch=None,  # contextä¸­å·²åŒ…å«
                    targets=targets
                )
                
                print(f"    âœ… åˆ†æå®Œæˆ: æ¨¡å‹{'å·²ä¿®æ”¹' if results.get('model_modified', False) else 'æœªä¿®æ”¹'}")
                
                # è®°å½•ç“¶é¢ˆåˆ†æå†å²
                bottleneck_info = {
                    'epoch': epoch,
                    'model_modified': results.get('model_modified', False),
                    'morphogenesis_type': results.get('morphogenesis_type', 'none'),
                    'trigger_reasons': results.get('trigger_reasons', []),
                    'intelligent_decision': results.get('intelligent_decision', False)
                }
                self.bottleneck_history.append(bottleneck_info)
                
                if results['model_modified']:
                    print(f"  ğŸ‰ æ™ºèƒ½å½¢æ€å‘ç”Ÿè§¦å‘!")
                    print(f"    ç±»å‹: {results['morphogenesis_type']}")
                    print(f"    æ–°å¢å‚æ•°: {results['parameters_added']:,}")
                    print(f"    è§¦å‘åŸå› :")
                    for reason in results.get('trigger_reasons', []):
                        print(f"      â€¢ {reason}")
                    
                    # æ›´æ–°æ¨¡å‹
                    old_param_count = sum(p.numel() for p in self.model.parameters())
                    self.model = results['new_model']
                    new_param_count = sum(p.numel() for p in self.model.parameters())
                    
                    print(f"    ğŸ“ˆ å‚æ•°å¢é•¿: {old_param_count:,} â†’ {new_param_count:,}")
                    
                    # é‡å»ºä¼˜åŒ–å™¨
                    current_lr = optimizer.param_groups[0]['lr']
                    optimizer = optim.SGD(
                        self.model.parameters(), 
                        lr=current_lr,
                        momentum=0.9,
                        weight_decay=1e-4
                    )
                    
                    # è®°å½•å½¢æ€å‘ç”Ÿäº‹ä»¶
                    morphogenesis_event = {
                        'epoch': epoch,
                        'type': results['morphogenesis_type'],
                        'parameters_added': results['parameters_added'],
                        'test_acc_before': test_acc,
                        'total_params': new_param_count,
                        'trigger_reasons': results.get('trigger_reasons', []),
                        'intelligent': results.get('intelligent_decision', False)
                    }
                    self.morphogenesis_history.append(morphogenesis_event)
                else:
                    print(f"  âœ… ç“¶é¢ˆæ£€æµ‹: å½“å‰æ— éœ€å½¢æ€å‘ç”Ÿ")
                    if results.get('trigger_reasons'):
                        print(f"    æœªè§¦å‘åŸå› : {results.get('trigger_reasons', [])}")
                
            except Exception as e:
                print(f"    âŒ æ™ºèƒ½åˆ†æå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
            
            # è®°å½•å‚æ•°å†å²
            current_params = sum(p.numel() for p in self.model.parameters())
            self.parameter_history.append(current_params)
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()
            
            # æ€§èƒ½ç›‘æ§
            if test_acc > best_test_acc:
                best_test_acc = test_acc
                print(f"  ğŸ¯ æ–°æœ€ä½³å‡†ç¡®ç‡: {best_test_acc:.2f}%!")
                
                if best_test_acc >= 95.0:
                    print("  ğŸ† è¾¾åˆ°95%+å‡†ç¡®ç‡ç›®æ ‡!")
                elif best_test_acc >= 90.0:
                    print("  ğŸŒŸ è¾¾åˆ°90%+å‡†ç¡®ç‡!")
        
        print(f"\nâœ… æ™ºèƒ½è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“Š æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_test_acc:.2f}%")
        
        return best_test_acc
    
    def analyze_intelligent_morphogenesis(self):
        """åˆ†ææ™ºèƒ½å½¢æ€å‘ç”Ÿæ•ˆæœ"""
        print("\nğŸ§  æ™ºèƒ½å½¢æ€å‘ç”Ÿåˆ†æ")
        print("=" * 50)
        
        # æ€»ä½“ç»Ÿè®¡
        total_events = len(self.morphogenesis_history)
        intelligent_events = len([e for e in self.morphogenesis_history if e.get('intelligent', False)])
        total_params_added = sum(e['parameters_added'] for e in self.morphogenesis_history)
        
        print(f"ğŸ“Š æ™ºèƒ½å½¢æ€å‘ç”Ÿç»Ÿè®¡:")
        print(f"  æ€»äº‹ä»¶æ•°: {total_events}")
        print(f"  æ™ºèƒ½å†³ç­–: {intelligent_events}")
        print(f"  ä¼ ç»Ÿå†³ç­–: {total_events - intelligent_events}")
        print(f"  æ–°å¢å‚æ•°: {total_params_added:,}")
        
        # è§¦å‘åŸå› åˆ†æ
        if self.morphogenesis_history:
            print(f"\nğŸ¯ è§¦å‘åŸå› åˆ†æ:")
            all_reasons = []
            for event in self.morphogenesis_history:
                all_reasons.extend(event.get('trigger_reasons', []))
            
            reason_types = defaultdict(int)
            for reason in all_reasons:
                if 'Net2Net' in reason:
                    reason_types['Net2Netå»ºè®®'] += 1
                elif 'ç“¶é¢ˆæ£€æµ‹' in reason:
                    reason_types['ç“¶é¢ˆæ£€æµ‹'] += 1
                elif 'åœæ»' in reason:
                    reason_types['æ€§èƒ½åœæ»'] += 1
                elif 'æ¿€è¿›' in reason:
                    reason_types['æ¿€è¿›æ¨¡å¼'] += 1
                else:
                    reason_types['å…¶ä»–'] += 1
            
            for reason_type, count in reason_types.items():
                print(f"  {reason_type}: {count}æ¬¡")
        
        # ç“¶é¢ˆæ£€æµ‹æ•ˆæœåˆ†æ
        print(f"\nğŸ”¬ ç“¶é¢ˆæ£€æµ‹æ•ˆæœ:")
        detection_cycles = len(self.bottleneck_history)
        triggered_cycles = len([b for b in self.bottleneck_history if b['model_modified']])
        detection_rate = triggered_cycles / detection_cycles if detection_cycles > 0 else 0
        
        print(f"  æ£€æµ‹å‘¨æœŸ: {detection_cycles}")
        print(f"  è§¦å‘å‘¨æœŸ: {triggered_cycles}")
        print(f"  è§¦å‘ç‡: {detection_rate:.1%}")
        
        # æ€§èƒ½æ”¹è¿›åˆ†æ
        if self.morphogenesis_history:
            print(f"\nğŸ“ˆ æ€§èƒ½æ”¹è¿›åˆ†æ:")
            for i, event in enumerate(self.morphogenesis_history):
                epoch = event['epoch']
                acc_before = event['test_acc_before']
                
                # æŸ¥æ‰¾5è½®åçš„å‡†ç¡®ç‡
                if epoch + 5 < len(self.test_history):
                    acc_after = self.test_history[epoch + 5][1]
                    improvement = acc_after - acc_before
                    
                    print(f"  äº‹ä»¶ {i+1} (Epoch {epoch}):")
                    print(f"    ç±»å‹: {event['type']}")
                    print(f"    æ™ºèƒ½å†³ç­–: {'æ˜¯' if event.get('intelligent', False) else 'å¦'}")
                    print(f"    æ€§èƒ½å˜åŒ–: {acc_before:.2f}% â†’ {acc_after:.2f}% ({improvement:+.2f}%)")
                    print(f"    ä¸»è¦åŸå› : {event.get('trigger_reasons', ['æœªçŸ¥'])[0]}")
        
        return {
            'total_events': total_events,
            'intelligent_events': intelligent_events,
            'total_parameters_added': total_params_added,
            'detection_rate': detection_rate,
            'reason_distribution': dict(reason_types) if 'reason_types' in locals() else {}
        }
    
    def plot_intelligent_training_progress(self):
        """ç»˜åˆ¶æ™ºèƒ½è®­ç»ƒè¿›åº¦å›¾"""
        if len(self.train_history) == 0:
            return
            
        epochs = range(1, len(self.train_history) + 1)
        train_accs = [acc for _, acc in self.train_history]
        test_accs = [acc for _, acc in self.test_history]
        
        plt.figure(figsize=(18, 6))
        
        # å‡†ç¡®ç‡æ›²çº¿
        plt.subplot(1, 3, 1)
        plt.plot(epochs, train_accs, label='Train Accuracy', color='blue', alpha=0.7)
        plt.plot(epochs, test_accs, label='Test Accuracy', color='red', linewidth=2)
        
        # æ ‡è®°æ™ºèƒ½å½¢æ€å‘ç”Ÿäº‹ä»¶
        for event in self.morphogenesis_history:
            if event['epoch'] <= len(self.train_history):
                color = 'green' if event.get('intelligent', False) else 'orange'
                style = '-' if event.get('intelligent', False) else '--'
                plt.axvline(x=event['epoch'], color=color, linestyle=style, alpha=0.8)
                
                # æ·»åŠ äº‹ä»¶æ ‡ç­¾
                plt.text(event['epoch'], max(test_accs) * 0.95, 
                        'ğŸ§ ' if event.get('intelligent', False) else 'ğŸ”„', 
                        rotation=0, fontsize=12, ha='center')
        
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy (%)')
        plt.title('Intelligent DNM Training Progress')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # å‚æ•°å¢é•¿æ›²çº¿
        plt.subplot(1, 3, 2)
        param_growth = [(p - self.parameter_history[0]) / self.parameter_history[0] * 100 
                       for p in self.parameter_history[:len(epochs)]]
        plt.plot(epochs, param_growth, color='purple', linewidth=2)
        
        # æ ‡è®°æ™ºèƒ½vsä¼ ç»Ÿå†³ç­–
        for event in self.morphogenesis_history:
            if event['epoch'] <= len(epochs):
                color = 'green' if event.get('intelligent', False) else 'orange'
                marker = 'o' if event.get('intelligent', False) else 's'
                epoch_idx = event['epoch']
                if epoch_idx < len(param_growth):
                    plt.scatter(epoch_idx, param_growth[epoch_idx], 
                              color=color, marker=marker, s=100, alpha=0.8)
        
        plt.xlabel('Epoch')
        plt.ylabel('Parameter Growth (%)')
        plt.title('Intelligent Parameter Growth')
        plt.grid(True, alpha=0.3)
        
        # è§¦å‘åŸå› åˆ†å¸ƒ
        plt.subplot(1, 3, 3)
        if self.morphogenesis_history:
            all_reasons = []
            for event in self.morphogenesis_history:
                for reason in event.get('trigger_reasons', []):
                    if 'Net2Net' in reason:
                        all_reasons.append('Net2Net')
                    elif 'ç“¶é¢ˆ' in reason:
                        all_reasons.append('ç“¶é¢ˆæ£€æµ‹')
                    elif 'åœæ»' in reason:
                        all_reasons.append('åœæ»æ£€æµ‹')
                    elif 'æ¿€è¿›' in reason:
                        all_reasons.append('æ¿€è¿›æ¨¡å¼')
                    else:
                        all_reasons.append('å…¶ä»–')
            
            if all_reasons:
                reason_counts = defaultdict(int)
                for reason in all_reasons:
                    reason_counts[reason] += 1
                
                colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ff99cc']
                plt.pie(reason_counts.values(), labels=reason_counts.keys(), 
                       colors=colors[:len(reason_counts)], autopct='%1.1f%%')
                plt.title('Trigger Reason Distribution')
            else:
                plt.text(0.5, 0.5, 'No Morphogenesis\nEvents', 
                        ha='center', va='center', transform=plt.gca().transAxes)
                plt.title('Trigger Reason Distribution')
        
        plt.tight_layout()
        plt.savefig('intelligent_dnm_training_progress.png', dpi=150, bbox_inches='tight')
        plt.show()

def prepare_data():
    """å‡†å¤‡CIFAR-10æ•°æ®"""
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])
    
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])
    
    train_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform_train
    )
    
    test_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=False, download=True, transform=transform_test
    )
    
    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)
    test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)
    
    return train_loader, test_loader

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸ§  æ™ºèƒ½ç“¶é¢ˆé©±åŠ¨çš„DNMå½¢æ€å‘ç”Ÿæ¼”ç¤º")
    print("=" * 60)
    
    try:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
        
        # å‡†å¤‡æ•°æ®
        train_loader, test_loader = prepare_data()
        
        # åˆ›å»ºæ•…æ„æœ‰ç“¶é¢ˆçš„æ¨¡å‹
        model = IntelligentResNet()
        trainer = IntelligentDNMTrainer(model, device, train_loader, test_loader)
        
        print(f"\nğŸ“Š åˆå§‹æ¨¡å‹æ¶æ„åˆ†æ:")
        print(f"  æ€»å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
        print(f"  è®¾è®¡ç“¶é¢ˆ:")
        print(f"    - æ·±åº¦ç“¶é¢ˆ: æµ…å±‚ç½‘ç»œè®¾è®¡")
        print(f"    - å®½åº¦ç“¶é¢ˆ: è¿‡çª„çš„é€šé“æ•°")
        print(f"    - ä¿¡æ¯æµç“¶é¢ˆ: å•ä¸€å¤„ç†è·¯å¾„")
        print(f"    - å®¹é‡ç“¶é¢ˆ: å°åˆ†ç±»å™¨éšè—å±‚")
        
        # æ™ºèƒ½è®­ç»ƒ
        print(f"\nğŸ§  å¼€å§‹æ™ºèƒ½ç“¶é¢ˆé©±åŠ¨è®­ç»ƒ...")
        best_acc = trainer.train_with_intelligent_morphogenesis(epochs=30)
        
        # åˆ†æç»“æœ
        summary = trainer.analyze_intelligent_morphogenesis()
        
        # ç»˜åˆ¶å›¾è¡¨
        trainer.plot_intelligent_training_progress()
        
        print(f"\nğŸ‰ æ™ºèƒ½æ¼”ç¤ºå®Œæˆ!")
        print("=" * 60)
        
        print(f"\nğŸ“Š æœ€ç»ˆç»“æœ:")
        print(f"  æœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}%")
        print(f"  å½¢æ€å‘ç”Ÿäº‹ä»¶: {summary['total_events']}")
        print(f"  æ™ºèƒ½å†³ç­–: {summary['intelligent_events']}")
        print(f"  æ–°å¢å‚æ•°: {summary['total_parameters_added']:,}")
        print(f"  æ™ºèƒ½è§¦å‘ç‡: {summary['detection_rate']:.1%}")
        
        print(f"\nğŸ§  æ™ºèƒ½ç‰¹æ€§å±•ç¤º:")
        print(f"  âœ… æ— å›ºå®šé—´éš”é™åˆ¶ - å®æ—¶ç“¶é¢ˆæ£€æµ‹")
        print(f"  âœ… Net2Netè¾“å‡ºåˆ†æ - ç²¾ç¡®å®šä½é—®é¢˜å±‚")
        print(f"  âœ… å¤šä¼˜å…ˆçº§å†³ç­– - æ™ºèƒ½é€‰æ‹©æœ€ä¼˜ç­–ç•¥")
        print(f"  âœ… è‡ªé€‚åº”ç”Ÿé•¿ - ç½‘ç»œåƒæ´»è¿‡æ¥ä¸€æ ·è¿›åŒ–")
        
        if best_acc >= 90.0:
            print(f"  ğŸ† æˆåŠŸçªç ´ç“¶é¢ˆï¼Œè¾¾åˆ°é«˜å‡†ç¡®ç‡!")
        elif summary['total_events'] > 0:
            print(f"  ğŸ”§ æ™ºèƒ½æ£€æµ‹æ­£å¸¸å·¥ä½œï¼Œç½‘ç»œåœ¨æŒç»­è¿›åŒ–")
        else:
            print(f"  âš ï¸ æœªè§¦å‘å½¢æ€å‘ç”Ÿï¼Œå¯èƒ½éœ€è¦è°ƒæ•´æ•æ„Ÿåº¦")
            
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()