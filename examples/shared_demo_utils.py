"""
å…±äº«æ¼”ç¤ºå·¥å…·æ¨¡å—
Shared Demo Utilities Module

æå–å¤šä¸ªæ¼”ç¤ºè„šæœ¬çš„å…±åŒé€»è¾‘ï¼Œå‡å°‘ä»£ç é‡å¤
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import numpy as np
import time
import logging
import os
import sys
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥æ ¸å¿ƒç»„ä»¶
from neuroexapt.core import (
    IntelligentArchitectureEvolutionEngine,
    EvolutionConfig,
    MutualInformationEstimator,
    IntelligentBottleneckDetector,
    IntelligentMutationPlanner,
    AdvancedNet2NetTransfer,
    MonteCarloUncertaintyEstimator,
    BayesianMutationDecision,
    MutationEvidence,
    MutationPrior,
    MutationDecision,
    MCUncertaintyConfig,
    BayesianDecisionConfig
)

# å¯¼å…¥æ¨¡å‹ç»„ä»¶
from neuroexapt.models import (
    create_enhanced_model,
    EnhancedTrainingConfig,
    get_enhanced_transforms,
    LabelSmoothingCrossEntropy,
    mixup_data,
    mixup_criterion
)

logger = logging.getLogger(__name__)


@dataclass
class DemoConfig:
    """æ¼”ç¤ºé…ç½®"""
    # è®¾å¤‡å’ŒåŸºç¡€è®¾ç½®
    device: str = 'auto'  # 'auto', 'cuda', 'cpu'
    seed: int = 42
    
    # æ•°æ®è®¾ç½®
    data_root: str = './data'
    batch_size: int = 128
    num_workers: int = 4
    
    # è®­ç»ƒè®¾ç½®
    initial_epochs: int = 15
    evolution_rounds: int = 3
    additional_epochs_per_round: int = 10
    target_accuracy: float = 95.0
    
    # æ¨¡å‹è®¾ç½®
    model_type: str = 'enhanced_resnet34'
    use_enhanced_features: bool = True
    
    # è¿›åŒ–è®¾ç½®
    use_monte_carlo_uncertainty: bool = True
    use_bayesian_decision: bool = True
    
    # æ—¥å¿—è®¾ç½®
    log_level: str = 'INFO'
    verbose: bool = True


class SharedDataManager:
    """å…±äº«æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self, config: DemoConfig):
        self.config = config
        self.train_loader = None
        self.test_loader = None
        
    def setup_data_loaders(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        if self.config.use_enhanced_features:
            train_transform, test_transform = get_enhanced_transforms()
        else:
            # åŸºç¡€å˜æ¢
            train_transform = transforms.Compose([
                transforms.RandomCrop(32, padding=4),
                transforms.RandomHorizontalFlip(),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], 
                                   std=[0.2023, 0.1994, 0.2010])
            ])
            test_transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], 
                                   std=[0.2023, 0.1994, 0.2010])
            ])
        
        # CIFAR-10æ•°æ®é›†
        train_dataset = torchvision.datasets.CIFAR10(
            root=self.config.data_root, train=True, download=True, 
            transform=train_transform
        )
        test_dataset = torchvision.datasets.CIFAR10(
            root=self.config.data_root, train=False, download=True, 
            transform=test_transform
        )
        
        self.train_loader = DataLoader(
            train_dataset, batch_size=self.config.batch_size,
            shuffle=True, num_workers=self.config.num_workers, pin_memory=True
        )
        self.test_loader = DataLoader(
            test_dataset, batch_size=self.config.batch_size,
            shuffle=False, num_workers=self.config.num_workers, pin_memory=True
        )
        
        logger.info(f"æ•°æ®åŠ è½½å™¨è®¾ç½®å®Œæˆ: Train={len(train_dataset)}, Test={len(test_dataset)}")
        return self.train_loader, self.test_loader


class SharedTrainer:
    """å…±äº«è®­ç»ƒå™¨"""
    
    def __init__(self, model: nn.Module, device: torch.device, 
                 train_loader: DataLoader, test_loader: DataLoader,
                 config: DemoConfig):
        self.model = model.to(device)
        self.device = device
        self.train_loader = train_loader
        self.test_loader = test_loader
        self.config = config
        
        # è®­ç»ƒç»„ä»¶
        self.optimizer = None
        self.scheduler = None
        self.criterion = None
        self.setup_training_components()
        
        # è®­ç»ƒå†å²
        self.train_history = []
        self.test_history = []
        
    def setup_training_components(self):
        """è®¾ç½®è®­ç»ƒç»„ä»¶"""
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.SGD(
            self.model.parameters(),
            lr=0.1,
            momentum=0.9,
            weight_decay=5e-4,
            nesterov=True
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=200, eta_min=1e-6
        )
        
        # æŸå¤±å‡½æ•°
        if self.config.use_enhanced_features:
            self.criterion = LabelSmoothingCrossEntropy(0.1)
        else:
            self.criterion = nn.CrossEntropyLoss()
            
    def train_epoch(self, epoch: int) -> Tuple[float, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (data, targets) in enumerate(self.train_loader):
            data, targets = data.to(self.device), targets.to(self.device)
            
            # æ•°æ®å¢å¼º
            if (self.config.use_enhanced_features and 
                hasattr(self.config, 'use_mixup') and 
                getattr(self.config, 'use_mixup', False) and 
                np.random.rand() < 0.5):
                mixed_data, targets_a, targets_b, lam = mixup_data(data, targets, 0.2)
                self.optimizer.zero_grad()
                outputs = self.model(mixed_data)
                loss = mixup_criterion(self.criterion, outputs, targets_a, targets_b, lam)
            else:
                self.optimizer.zero_grad()
                outputs = self.model(data)
                loss = self.criterion(outputs, targets)
            
            loss.backward()
            self.optimizer.step()
            
            # ç»Ÿè®¡
            running_loss += loss.item()
            if not (self.config.use_enhanced_features and np.random.rand() < 0.5):
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
            
            # æ‰“å°è¿›åº¦
            if self.config.verbose and batch_idx % 100 == 99:
                current_acc = 100. * correct / total if total > 0 else 0
                logger.info(
                    f"Epoch {epoch}, Batch {batch_idx + 1}, "
                    f"Loss: {running_loss/(batch_idx + 1):.6f}, "
                    f"Acc: {current_acc:.2f}%"
                )
        
        train_acc = 100. * correct / total if total > 0 else 0
        avg_loss = running_loss / len(self.train_loader)
        
        return avg_loss, train_acc
        
    def evaluate(self) -> Tuple[float, float]:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        self.model.eval()
        test_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, targets in self.test_loader:
                data, targets = data.to(self.device), targets.to(self.device)
                outputs = self.model(data)
                test_loss += self.criterion(outputs, targets).item()
                
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
        
        test_loss /= len(self.test_loader)
        test_acc = 100. * correct / total
        
        return test_loss, test_acc
        
    def train_epochs(self, num_epochs: int) -> float:
        """è®­ç»ƒå¤šä¸ªepoch"""
        best_acc = 0.0
        
        for epoch in range(num_epochs):
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(epoch + 1)
            
            # è¯„ä¼°
            test_loss, test_acc = self.evaluate()
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()
            
            # è®°å½•å†å²
            self.train_history.append({'loss': train_loss, 'acc': train_acc})
            self.test_history.append({'loss': test_loss, 'acc': test_acc})
            
            # æ›´æ–°æœ€ä½³å‡†ç¡®ç‡
            if test_acc > best_acc:
                best_acc = test_acc
                
            if self.config.verbose:
                logger.info(
                    f"Epoch {epoch + 1}: Train Acc: {train_acc:.2f}%, "
                    f"Test Acc: {test_acc:.2f}%, Test Loss: {test_loss:.4f}"
                )
                
            # æ—©æœŸåœæ­¢æ£€æŸ¥
            if test_acc >= self.config.target_accuracy:
                logger.info(f"ğŸ‰ Target accuracy of {self.config.target_accuracy}% reached! Current: {test_acc:.2f}%")
                break
                
        return best_acc


class SharedEvolutionEngine:
    """å…±äº«è¿›åŒ–å¼•æ“"""
    
    def __init__(self, device: torch.device, config: DemoConfig):
        self.device = device
        self.config = config
        
        # åˆ›å»ºç»„ä»¶
        self.mi_estimator = MutualInformationEstimator()
        
        # ä¸ç¡®å®šæ€§ä¼°è®¡å™¨
        if config.use_monte_carlo_uncertainty:
            mc_config = MCUncertaintyConfig(
                n_samples=30,  # å‡å°‘é‡‡æ ·æ•°é‡ä»¥æé«˜é€Ÿåº¦
                dropout_rate=0.1,
                max_batches=3,
                use_wrapper=True
            )
            self.uncertainty_estimator = MonteCarloUncertaintyEstimator(mc_config)
        else:
            from neuroexapt.core import BayesianUncertaintyEstimator
            self.uncertainty_estimator = BayesianUncertaintyEstimator()
            
        self.bottleneck_detector = IntelligentBottleneckDetector(
            mi_estimator=self.mi_estimator,
            uncertainty_estimator=self.uncertainty_estimator
        )
        self.mutation_planner = IntelligentMutationPlanner()
        self.net2net_transfer = AdvancedNet2NetTransfer()
        
        # è´å¶æ–¯å†³ç­–æ¡†æ¶
        if config.use_bayesian_decision:
            decision_config = BayesianDecisionConfig(
                alpha=1.5,
                beta=1.0, 
                gamma=0.8,
                delta=0.3,
                risk_aversion=1.5,
                confidence_threshold=0.2  # é™ä½é˜ˆå€¼ä»¥å…è®¸æ›´å¤šå˜å¼‚
            )
            self.decision_engine = BayesianMutationDecision(decision_config)
        else:
            self.decision_engine = None
            
        logger.info("ğŸ§¬ Shared evolution engine initialized")
        
    def detect_and_decide_mutations(self, model: nn.Module, data_loader) -> List[Tuple]:
        """æ£€æµ‹ç“¶é¢ˆå¹¶åšå‡ºå˜å¼‚å†³ç­–"""
        logger.info("ğŸ” å¼€å§‹æ£€æµ‹ç“¶é¢ˆ...")
        
        # æ£€æµ‹ç“¶é¢ˆ
        bottlenecks = self.bottleneck_detector.detect_bottlenecks(
            model, data_loader, self.device
        )
        
        logger.info(f"ğŸ¯ æ£€æµ‹åˆ° {len(bottlenecks)} ä¸ªæ½œåœ¨ç“¶é¢ˆ")
        
        if not self.decision_engine:
            # ç®€å•ç­–ç•¥ï¼šé€‰æ‹©å‰å‡ ä¸ªç“¶é¢ˆ
            return [(b, None) for b in bottlenecks[:3]]
            
        # ä½¿ç”¨è´å¶æ–¯å†³ç­–
        bottleneck_decisions = []
        
        for bottleneck in bottlenecks:
            # æ„å»ºå˜å¼‚è¯æ®
            evidence = MutationEvidence(
                mutual_info_gain=bottleneck.mutual_info,
                cond_mutual_info_gain=getattr(bottleneck, 'conditional_mi', 0.0),
                uncertainty_reduction=bottleneck.uncertainty,
                transfer_cost=0.05,
                bottleneck_severity=bottleneck.severity
            )
            
            # åšå‡ºå†³ç­–
            decision = self.decision_engine.make_decision(evidence)
            bottleneck_decisions.append((bottleneck, decision))
            
            if self.config.verbose:
                logger.info(f"ğŸ§  å±‚ {bottleneck.layer_name}: {decision.reasoning}")
                
        return bottleneck_decisions
        
    def execute_mutations(self, model: nn.Module, bottleneck_decisions: List[Tuple]) -> nn.Module:
        """æ‰§è¡Œå˜å¼‚"""
        current_model = model
        mutation_count = 0
        
        for bottleneck, decision in bottleneck_decisions:
            should_mutate = decision is None or (decision.should_mutate and decision.confidence > 0.2)
            
            if should_mutate:
                try:
                    # ç”Ÿæˆå˜å¼‚è®¡åˆ’
                    mutation_plans = self.mutation_planner.plan_mutations(
                        [bottleneck], task_type='vision'
                    )
                    
                    if mutation_plans:
                        mutation_plan = mutation_plans[0]
                        
                        # æ‰§è¡Œå˜å¼‚
                        mutated_model = self.net2net_transfer.apply_mutation(
                            current_model, mutation_plan
                        )
                        
                        if mutated_model is not None:
                            current_model = mutated_model
                            mutation_count += 1
                            
                            if self.config.verbose:
                                logger.info(
                                    f"âœ… æˆåŠŸå˜å¼‚å±‚ {bottleneck.layer_name}: "
                                    f"{mutation_plan.mutation_type}"
                                )
                        else:
                            logger.warning(f"âŒ å˜å¼‚å¤±è´¥: {bottleneck.layer_name}")
                            
                except Exception as e:
                    logger.error(f"å˜å¼‚é”™è¯¯: {bottleneck.layer_name}: {e}")
                    
        logger.info(f"ğŸ”„ æ€»å…±æ‰§è¡Œäº† {mutation_count} ä¸ªå˜å¼‚")
        return current_model
        
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self.uncertainty_estimator, 'cleanup'):
            self.uncertainty_estimator.cleanup()


def setup_demo_environment(config: DemoConfig):
    """è®¾ç½®æ¼”ç¤ºç¯å¢ƒ"""
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(config.seed)
    np.random.seed(config.seed)
    
    # è®¾ç½®è®¾å¤‡
    if config.device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(config.device)
        
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=getattr(logging, config.log_level),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    return device


def create_model_from_config(config: DemoConfig) -> nn.Module:
    """æ ¹æ®é…ç½®åˆ›å»ºæ¨¡å‹"""
    if config.use_enhanced_features:
        # ä½¿ç”¨å¢å¼ºæ¨¡å‹
        model_config = EnhancedTrainingConfig()
        model_config.model_type = config.model_type
        model_config.batch_size = config.batch_size
        return create_enhanced_model(model_config)
    else:
        # ä½¿ç”¨åŸºç¡€ResNet
        from torchvision.models import resnet18
        model = resnet18(num_classes=10)
        # ä¿®æ”¹ç¬¬ä¸€å±‚ä»¥é€‚åº”CIFAR-10
        model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        model.maxpool = nn.Identity()
        return model


def run_complete_demo(config: DemoConfig) -> Dict[str, Any]:
    """è¿è¡Œå®Œæ•´çš„æ¼”ç¤ºæµç¨‹"""
    print("="*60)
    print("ğŸš€ æ™ºèƒ½æ¶æ„è¿›åŒ–æ¼”ç¤º")
    print(f"ğŸ¯ ç›®æ ‡ï¼šCIFAR-10ä¸Š{config.target_accuracy}%å‡†ç¡®ç‡")
    print(f"ğŸ§¬ Monte Carloä¸ç¡®å®šæ€§: {config.use_monte_carlo_uncertainty}")
    print(f"ğŸ² è´å¶æ–¯å†³ç­–: {config.use_bayesian_decision}")
    print("="*60)
    
    # è®¾ç½®ç¯å¢ƒ
    device = setup_demo_environment(config)
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è®¾ç½®æ•°æ®
    data_manager = SharedDataManager(config)
    train_loader, test_loader = data_manager.setup_data_loaders()
    
    # åˆ›å»ºæ¨¡å‹
    model = create_model_from_config(config)
    logger.info(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = SharedTrainer(model, device, train_loader, test_loader, config)
    
    # åŸºç¡€è®­ç»ƒ
    print("\nğŸ“š å¼€å§‹åŸºç¡€è®­ç»ƒ...")
    best_acc = trainer.train_epochs(config.initial_epochs)
    print(f"\nğŸ¯ åŸºç¡€è®­ç»ƒå®Œæˆï¼Œæœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}%")
    
    results = {
        'initial_accuracy': best_acc,
        'evolution_rounds': [],
        'final_accuracy': best_acc,
        'target_reached': best_acc >= config.target_accuracy
    }
    
    # å¦‚æœå·²ç»è¾¾åˆ°ç›®æ ‡ï¼Œç›´æ¥è¿”å›
    if best_acc >= config.target_accuracy:
        print(f"\nğŸ‰ å·²è¾¾åˆ°{config.target_accuracy}%å‡†ç¡®ç‡ç›®æ ‡ï¼")
        results['final_accuracy'] = best_acc
        return results
        
    # æ™ºèƒ½æ¶æ„è¿›åŒ–
    print("\nğŸ§¬ å¼€å§‹æ™ºèƒ½æ¶æ„è¿›åŒ–...")
    evolution_engine = SharedEvolutionEngine(device, config)
    
    current_model = trainer.model
    
    try:
        for round_idx in range(config.evolution_rounds):
            print(f"\nğŸ”„ è¿›åŒ–è½®æ¬¡ {round_idx + 1}/{config.evolution_rounds}")
            
            # æ£€æµ‹å’Œå†³ç­–
            bottleneck_decisions = evolution_engine.detect_and_decide_mutations(
                current_model, test_loader
            )
            
            # è¿‡æ»¤å€¼å¾—å˜å¼‚çš„ç“¶é¢ˆ
            valuable_mutations = [
                (b, d) for b, d in bottleneck_decisions 
                if d is None or (d.should_mutate and d.confidence > 0.2)
            ]
            
            if not valuable_mutations:
                print("\nğŸ›‘ æ²¡æœ‰å‘ç°å€¼å¾—æ‰§è¡Œçš„å˜å¼‚ï¼Œåœæ­¢è¿›åŒ–")
                break
                
            print(f"\nğŸ“‹ å‘ç° {len(valuable_mutations)} ä¸ªå€¼å¾—æ‰§è¡Œçš„å˜å¼‚")
            
            # æ‰§è¡Œå˜å¼‚
            mutated_model = evolution_engine.execute_mutations(
                current_model, valuable_mutations
            )
            
            # æ›´æ–°è®­ç»ƒå™¨
            trainer.model = mutated_model.to(device)
            trainer.setup_training_components()
            
            # ç»§ç»­è®­ç»ƒ
            print("\nğŸ“š è®­ç»ƒå˜å¼‚åçš„æ¨¡å‹...")
            new_best_acc = trainer.train_epochs(config.additional_epochs_per_round)
            
            print(f"\nğŸ“Š å˜å¼‚åæœ€ä½³å‡†ç¡®ç‡: {new_best_acc:.2f}%")
            
            # è®°å½•ç»“æœ
            round_result = {
                'round': round_idx + 1,
                'mutations_applied': len(valuable_mutations),
                'accuracy_before': best_acc,
                'accuracy_after': new_best_acc,
                'improvement': new_best_acc - best_acc
            }
            results['evolution_rounds'].append(round_result)
            
            best_acc = max(best_acc, new_best_acc)
            current_model = trainer.model
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
            if best_acc >= config.target_accuracy:
                print(f"\nğŸ‰ è¾¾åˆ°{config.target_accuracy}%å‡†ç¡®ç‡ç›®æ ‡ï¼æœ€ç»ˆå‡†ç¡®ç‡: {best_acc:.2f}%")
                results['target_reached'] = True
                break
                
    finally:
        # æ¸…ç†èµ„æº
        evolution_engine.cleanup()
    
    results['final_accuracy'] = best_acc
    
    print(f"\nğŸ æ¼”ç¤ºå®Œæˆï¼Œæœ€ç»ˆæœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}%")
    
    if results['target_reached']:
        print(f"\nğŸŠ æˆåŠŸè¾¾æˆ{config.target_accuracy}%å‡†ç¡®ç‡ç›®æ ‡ï¼")
    else:
        print(f"\nğŸ“Š è·ç¦»{config.target_accuracy}%ç›®æ ‡è¿˜å·®: {config.target_accuracy - best_acc:.2f}%")
        
    return results