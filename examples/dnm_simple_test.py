#!/usr/bin/env python3
"""
ç®€åŒ–çš„DNMæµ‹è¯• - ä¿®å¤åŸå§‹é—®é¢˜

è§£å†³çš„é—®é¢˜ï¼š
1. view size is not compatible é”™è¯¯
2. æ¶æ„å®é™…æœªæ¼”åŒ–çš„é—®é¢˜
3. æå‡æ€§èƒ½è¡¨ç°
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import time
import os
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥ä¿®å¤åçš„DNMæ¡†æ¶
from neuroexapt.core.dnm_framework import DNMFramework


class SimpleCNN(nn.Module):
    """ç®€å•ä½†å¯æ¼”åŒ–çš„CNNæ¨¡å‹"""
    
    def __init__(self, num_classes=10):
        super().__init__()
        
        # ç‰¹å¾æå–å±‚
        self.features = nn.Sequential(
            # ç¬¬ä¸€ç»„å·ç§¯
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # ç¬¬äºŒç»„å·ç§¯
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # ç¬¬ä¸‰ç»„å·ç§¯
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
        )
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(128, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(128, num_classes)
        )
        
        # æƒé‡åˆå§‹åŒ–
        self._initialize_weights()
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x


def prepare_small_cifar10(batch_size=64):
    """å‡†å¤‡CIFAR-10æ•°æ®é›†ï¼ˆå°è§„æ¨¡æµ‹è¯•ï¼‰"""
    
    transform_train = transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.RandomCrop(32, padding=4),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))
    ])
    
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))
    ])
    
    # åŠ è½½æ•°æ®é›†
    trainset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform_train
    )
    testset = torchvision.datasets.CIFAR10(
        root='./data', train=False, download=True, transform=transform_test
    )
    
    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)
    
    print(f"ğŸ“Š CIFAR-10 loaded: {len(trainset)} train, {len(testset)} test samples")
    return train_loader, test_loader


def create_aggressive_dnm_config():
    """åˆ›å»ºæ›´æ¿€è¿›çš„DNMé…ç½®æ¥ç¡®ä¿æ¼”åŒ–å‘ç”Ÿ"""
    
    return {
        'neuron_division': {
            'splitter': {
                'entropy_threshold': 0.4,        # é™ä½é˜ˆå€¼ï¼Œæ›´å®¹æ˜“åˆ†è£‚
                'overload_threshold': 0.3,       # é™ä½è¿‡è½½é˜ˆå€¼
                'split_probability': 0.8,        # æé«˜åˆ†è£‚æ¦‚ç‡
                'max_splits_per_layer': 3,       # å…è®¸åˆ†è£‚
                'inheritance_noise': 0.1         # é€‚ä¸­çš„å™ªå£°
            },
            'monitoring': {
                'target_layers': ['conv', 'linear'],
                'analysis_frequency': 3,         # æ›´é¢‘ç¹åˆ†æ
                'min_epoch_before_split': 5      # æ›´æ—©å¼€å§‹
            }
        },
        'connection_growth': {
            'analyzer': {
                'correlation_threshold': 0.1,    # é™ä½ç›¸å…³æ€§é˜ˆå€¼
                'history_length': 6              # è¾ƒçŸ­å†å²
            },
            'growth': {
                'max_new_connections': 2,        # é€‚ä¸­è¿æ¥æ•°
                'min_correlation_threshold': 0.05,
                'growth_frequency': 4,           # æ›´é¢‘ç¹
                'connection_types': ['skip_connection']  # ç®€åŒ–è¿æ¥ç±»å‹
            },
            'filtering': {
                'min_layer_distance': 1,
                'max_layer_distance': 4,
                'avoid_redundant_connections': True
            }
        },
        'multi_objective': {
            'evolution': {
                'population_size': 6,            # è¾ƒå°ç§ç¾¤
                'max_generations': 8,            # è¾ƒå°‘ä»£æ•°
                'mutation_rate': 0.5,
                'crossover_rate': 0.7,
                'elitism_ratio': 0.3
            },
            'optimization': {
                'trigger_frequency': 12,         # é€‚ä¸­é¢‘ç‡
                'performance_plateau_threshold': 0.01,
                'min_improvement_epochs': 2
            }
        },
        'framework': {
            'morphogenesis_frequency': 3,       # æ›´é¢‘ç¹çš„å½¢æ€å‘ç”Ÿ
            'performance_tracking_window': 5,
            'early_stopping_patience': 20,
            'target_accuracy_threshold': 85.0,  # é€‚ä¸­ç›®æ ‡
            'enable_architecture_snapshots': True,
            'adaptive_morphogenesis': True
        }
    }


def run_dnm_simple_test():
    """è¿è¡Œç®€åŒ–çš„DNMæµ‹è¯•"""
    
    print("ğŸ§¬ DNM Simple Test - Fixed Version")
    print("=" * 50)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ Device: {device}")
    
    # å‡†å¤‡æ•°æ®
    train_loader, test_loader = prepare_small_cifar10(batch_size=64)
    
    # åˆ›å»ºæ¨¡å‹
    model = SimpleCNN(num_classes=10)
    initial_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ—ï¸ Initial model: {initial_params:,} parameters")
    
    # åˆ›å»ºDNMæ¡†æ¶
    dnm_config = create_aggressive_dnm_config()
    dnm = DNMFramework(dnm_config)
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)
    criterion = nn.CrossEntropyLoss()
    
    print(f"\nğŸš€ Starting DNM Training")
    print("=" * 50)
    
    # è¿›åº¦å›è°ƒ
    def simple_callback(dnm_framework, model, epoch_record):
        epoch = epoch_record['epoch']
        train_acc = epoch_record['train_acc']
        val_acc = epoch_record['val_acc']
        params = epoch_record['model_params']
        
        param_growth = (params - initial_params) / initial_params * 100
        
        if epoch % 2 == 0:
            print(f"ğŸ“ˆ Epoch {epoch}: Train={train_acc:.2f}%, "
                  f"Val={val_acc:.2f}%, Params={params:,} (+{param_growth:.1f}%)")
    
    # å¼€å§‹è®­ç»ƒ
    start_time = time.time()
    
    try:
        result = dnm.train_with_morphogenesis(
            model=model,
            train_loader=train_loader,
            val_loader=test_loader,
            epochs=30,
            optimizer=optimizer,
            criterion=criterion,
            callbacks=[simple_callback]
        )
        
        # ç»“æœåˆ†æ
        training_time = time.time() - start_time
        final_params = sum(p.numel() for p in result['model'].parameters())
        param_growth = (final_params - initial_params) / initial_params * 100
        
        print(f"\nğŸ‰ DNM Test Completed!")
        print("=" * 50)
        print(f"ğŸ“Š Results:")
        print(f"   Best Accuracy: {result['best_val_accuracy']:.2f}%")
        print(f"   Final Accuracy: {result['final_val_accuracy']:.2f}%")
        print(f"   Parameter Growth: +{param_growth:.1f}%")
        print(f"   Training Time: {training_time:.1f}s")
        print(f"   Morphogenesis Events: {len(result['morphogenesis_events'])}")
        
        # å½¢æ€å‘ç”Ÿåˆ†æ
        if result['morphogenesis_events']:
            print(f"\nğŸ§¬ Morphogenesis Events:")
            for event in result['morphogenesis_events']:
                print(f"   Epoch {event['epoch']}: "
                      f"{event['neuron_splits']} splits, "
                      f"{event['connections_grown']} connections")
        else:
            print(f"\nâš ï¸ No morphogenesis events occurred")
            print(f"   Try lowering thresholds or increasing frequencies")
        
        # æˆåŠŸè¯„ä¼°
        if param_growth > 0:
            print(f"\nâœ… SUCCESS: Model actually evolved! (+{param_growth:.1f}% parameters)")
        else:
            print(f"\nâŒ No evolution detected. Model structure unchanged.")
        
        if result['best_val_accuracy'] >= 60.0:
            print(f"âœ… GOOD: Achieved decent accuracy ({result['best_val_accuracy']:.2f}%)")
        
        return result
        
    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    
    # è¿è¡Œæµ‹è¯•
    result = run_dnm_simple_test()
    
    if result:
        print(f"\nğŸ¯ Test Summary:")
        print(f"   The DNM framework is working!")
        print(f"   Check morphogenesis events for evolution details.")
    else:
        print(f"\nğŸ”§ Need debugging:")
        print(f"   Check error messages above for issues to fix.")