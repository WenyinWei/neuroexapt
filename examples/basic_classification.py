#!/usr/bin/env python3
"""
NeuroExapt Basic Classification Example

This example demonstrates various optimization strategies for neural architecture search:
1. Standard DARTS-style architecture search
2. Fixed architecture training (bypass MixedOp)
3. Optimized architecture search with caching
4. Performance comparison and benchmarking

Use command line arguments to control different optimization strategies.
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import torchvision
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torch.backends.cudnn as cudnn
import numpy as np
import sys
import os
import time
import argparse
from typing import Optional
from tqdm import tqdm
from pathlib import Path

# New high-performance utilities
from neuroexapt.utils.async_dataloader import build_cifar10_pipeline
from neuroexapt.utils.fuse_utils import fuse_model
from neuroexapt.utils.compile_utils import compile_submodules

# Add the parent directory to the path to import neuroexapt
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import NeuroExapt core components
from neuroexapt.core.model import Network
from neuroexapt.core.architect import Architect
from neuroexapt.core.operations import SepConv, DilConv, Identity, Zero, FactorizedReduce, ReLUConvBN
from neuroexapt.utils.minimal_monitor import MinimalMonitor
from neuroexapt.utils.utils import AvgrageMeter, accuracy

class FixedCell(nn.Module):
    """
    Fixed architecture cell that bypasses MixedOp computation
    """
    def __init__(self, steps, block_multiplier, C_prev_prev, C_prev, C, reduction, reduction_prev):
        super(FixedCell, self).__init__()
        self.reduction = reduction
        self.block_multiplier = block_multiplier
        self.steps = steps

        # Preprocessing layers
        if reduction_prev:
            self.preprocess0 = FactorizedReduce(C_prev_prev, C, affine=False)
        else:
            self.preprocess0 = ReLUConvBN(C_prev_prev, C, 1, 1, 0, affine=False)
        self.preprocess1 = ReLUConvBN(C_prev, C, 1, 1, 0, affine=False)

        # Fixed operations (high-performance genotype)
        self.ops = nn.ModuleList()
        
        # Create operations for each connection
        for i in range(self.steps):
            for j in range(2 + i):
                # Choose operation type
                if reduction:
                    if j < 2:
                        op_name = 'max_pool_3x3'
                    else:
                        op_name = 'sep_conv_3x3'
                else:
                    if j == 0:
                        op_name = 'sep_conv_3x3'
                    elif j == 1:
                        op_name = 'sep_conv_5x5'
                    else:
                        op_name = 'sep_conv_3x3'
                
                stride = 2 if reduction and j < 2 else 1
                op = self._create_operation(op_name, C, stride)
                self.ops.append(op)

    def _create_operation(self, op_name, C, stride):
        """Create a single operation"""
        if op_name == 'none':
            return Zero(stride)
        elif op_name == 'avg_pool_3x3':
            return nn.Sequential(
                nn.AvgPool2d(3, stride=stride, padding=1, count_include_pad=False),
                nn.BatchNorm2d(C, affine=False)
            )
        elif op_name == 'max_pool_3x3':
            return nn.Sequential(
                nn.MaxPool2d(3, stride=stride, padding=1),
                nn.BatchNorm2d(C, affine=False)
            )
        elif op_name == 'skip_connect':
            return Identity() if stride == 1 else FactorizedReduce(C, C, affine=False)
        elif op_name == 'sep_conv_3x3':
            return SepConv(C, C, 3, stride, 1, affine=False)
        elif op_name == 'sep_conv_5x5':
            return SepConv(C, C, 5, stride, 2, affine=False)
        else:
            return Identity()

    def forward(self, s0, s1):
        """Forward pass without architecture parameters"""
        s0 = self.preprocess0(s0)
        s1 = self.preprocess1(s1)

        states = [s0, s1]
        offset = 0
        
        for i in range(self.steps):
            s = 0
            for j in range(2 + i):
                op = self.ops[offset + j]
                h = op(states[j])
                s = s + h
            
            offset += len(states)
            states.append(s)

        return torch.cat(states[-self.block_multiplier:], dim=1)

class FixedNetwork(nn.Module):
    """
    Fixed architecture network that bypasses architecture search
    """
    def __init__(self, C, num_classes, layers, steps=4, block_multiplier=4):
        super(FixedNetwork, self).__init__()
        self._C = C
        self._num_classes = num_classes
        self._layers = layers
        self._steps = steps
        self._block_multiplier = block_multiplier

        C_curr = self._block_multiplier * C
        self.stem = nn.Sequential(
            nn.Conv2d(3, C_curr, 3, padding=1, bias=False),
            nn.BatchNorm2d(C_curr)
        )

        C_prev_prev, C_prev, C_curr = C_curr, C_curr, C
        self.cells = nn.ModuleList()
        reduction_prev = False
        
        for i in range(layers):
            if i in [layers // 3, 2 * layers // 3]:
                C_curr *= 2
                reduction = True
            else:
                reduction = False
            
            cell = FixedCell(steps, block_multiplier, C_prev_prev, C_prev, C_curr, reduction, reduction_prev)
            reduction_prev = reduction
            self.cells.append(cell)
            C_prev_prev, C_prev = C_prev, self._block_multiplier * C_curr

        self.global_pooling = nn.AdaptiveAvgPool2d(1)
        self.classifier = nn.Linear(C_prev, num_classes)

    def forward(self, input):
        """Fast forward pass"""
        s0 = s1 = self.stem(input)
        
        for cell in self.cells:
            s0, s1 = s1, cell(s0, s1)
        
        out = self.global_pooling(s1)
        logits = self.classifier(out.view(out.size(0), -1))
        return logits

class CachedNetwork:
    """Wrapper for Network class to provide caching functionality"""
    def __init__(self, model):
        self.model = model
        self.cached_normal_weights = None
        self.cached_reduce_weights = None
        self.cache_valid = False
    
    def update_weight_cache(self):
        """Update weight cache"""
        with torch.no_grad():
            self.cached_normal_weights = torch.softmax(self.model.alphas_normal, dim=-1)
            self.cached_reduce_weights = torch.softmax(self.model.alphas_reduce, dim=-1)
            self.cache_valid = True
    
    def forward(self, input):
        """Forward pass with cached weights"""
        if not self.cache_valid:
            self.update_weight_cache()
        
        # Use cached weights directly in Network's forward logic
        import torch.utils.checkpoint as cp
        from neuroexapt.core.model import GatedCell
        
        s0 = s1 = self.model.stem(input)
        
        for i, cell in enumerate(self.model.cells):
            # Determine which set of precomputed weights to use
            if isinstance(cell, GatedCell):
                if cell.cell.reduction:
                    weights = self.cached_reduce_weights
                else:
                    weights = self.cached_normal_weights
            else:
                weights = self.cached_reduce_weights if cell.reduction else self.cached_normal_weights

            if self.model.use_checkpoint and self.model.training:
                # Wrap cell forward in checkpoint to save memory
                def _cell_forward(a, b):
                    return cell(a, b, weights)

                s1_new = cp.checkpoint(_cell_forward, s0, s1)
            else:
                s1_new = cell(s0, s1, weights)

            s0, s1 = s1, s1_new
        
        out = self.model.global_pooling(s1)
        logits = self.model.classifier(out.view(out.size(0), -1))
        return logits
    
    def __call__(self, input):
        return self.forward(input)
    
    def __getattr__(self, name):
        # ğŸ”§ ä¿®å¤ï¼šé˜²æ­¢__getattr__æ— ç©·é€’å½’
        # æ·»åŠ é»‘åå•å’Œé€’å½’æ·±åº¦æ£€æŸ¥
        if name.startswith('_') or name in {'model', 'cached_normal_weights', 'cached_reduce_weights', 'cache_valid'}:
            raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
        
        # é˜²æ­¢é€’å½’ï¼šæ£€æŸ¥æ˜¯å¦æ­£åœ¨æŸ¥æ‰¾modelå±æ€§
        if not hasattr(self, '_getattr_in_progress'):
            self._getattr_in_progress = set()
        
        if name in self._getattr_in_progress:
            raise AttributeError(f"é€’å½’è°ƒç”¨æ£€æµ‹ï¼š'{name}' å±æ€§æŸ¥æ‰¾å¾ªç¯")
        
        try:
            self._getattr_in_progress.add(name)
            # å®‰å…¨åœ°è·å–modelå±æ€§
            if hasattr(self, 'model') and hasattr(self.model, name):
                return getattr(self.model, name)
            else:
                raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
        finally:
            self._getattr_in_progress.discard(name)

def setup_args():
    """Setup command line arguments"""
    parser = argparse.ArgumentParser(description='NeuroExapt Basic Classification')
    
    # Training parameters
    parser.add_argument('--data', type=str, default='./data', help='dataset location')
    parser.add_argument('--batch_size', type=int, default=64, help='batch size')
    parser.add_argument('--epochs', type=int, default=10, help='number of epochs')
    parser.add_argument('--learning_rate', type=float, default=0.025, help='learning rate')
    parser.add_argument('--momentum', type=float, default=0.9, help='momentum')
    parser.add_argument('--weight_decay', type=float, default=3e-4, help='weight decay')
    parser.add_argument('--grad_clip', type=float, default=5, help='gradient clipping')
    parser.add_argument('--train_portion', type=float, default=0.5, help='training data portion')
    parser.add_argument('--report_freq', type=int, default=20, help='report frequency')
    
    # Architecture parameters
    parser.add_argument('--init_channels', type=int, default=16, help='initial channels')
    parser.add_argument('--layers', type=int, default=8, help='number of layers')
    parser.add_argument('--potential_layers', type=int, default=4, help='potential layers')
    
    # Architecture evolution parameters (renamed from search)
    parser.add_argument('--arch_learning_rate', type=float, default=3e-4, help='arch learning rate')
    parser.add_argument('--arch_weight_decay', type=float, default=1e-3, help='arch weight decay')
    parser.add_argument('--arch_update_freq', type=int, default=50, help='arch update frequency')
    parser.add_argument('--warmup_epochs', type=int, default=5, help='warmup epochs')
    parser.add_argument('--use_first_order', action='store_true', help='use first order approximation')
    
    # Optimization strategy - renamed search to exapt
    parser.add_argument('--mode', type=str, default='exapt', 
                       choices=['exapt', 'fixed', 'cached', 'benchmark'],
                       help='optimization mode: exapt (intelligent evolution), fixed (no evolution), cached (optimized), benchmark (compare all)')
    
    # Performance options
    parser.add_argument('--num_workers', type=int, default=4, help='data loader workers')
    parser.add_argument('--async_loader', action='store_true', help='use async DataPipe loader')
    parser.add_argument('--compile_cell', action='store_true', help='torch.compile Cell submodules')
    parser.add_argument('--pin_memory', action='store_true', default=True, help='pin memory')
    parser.add_argument('--disable_cudnn_benchmark', action='store_true', help='disable cudnn benchmark')
    parser.add_argument('--use_checkpoint', action='store_true', help='use checkpoint for memory optimization')
    parser.add_argument('--use_model_compile', action='store_true', help='use torch.compile for entire model')
    parser.add_argument('--use_optimized_ops', action='store_true', help='use optimized MixedOp operations')
    parser.add_argument('--progress_tracking', action='store_true', default=True, help='enable detailed progress tracking')
    
    # New Exapt-specific optimizations
    parser.add_argument('--lazy_computation', action='store_true', default=True, help='enable lazy computation for operations')
    parser.add_argument('--smart_caching', action='store_true', default=True, help='enable intelligent result caching')
    parser.add_argument('--early_convergence', action='store_true', default=True, help='enable early convergence detection')
    parser.add_argument('--memory_pool', action='store_true', default=True, help='use pre-allocated memory pools')
    parser.add_argument('--operation_pruning', action='store_true', default=True, help='prune low-weight operations during evolution')
    parser.add_argument('--adaptive_update_freq', action='store_true', default=True, help='adaptively adjust architecture update frequency')
    
    # Advanced backward pass optimizations
    parser.add_argument('--gradient_optimized', action='store_true', default=True, help='use gradient-optimized MixedOp for faster backward pass')
    parser.add_argument('--memory_efficient', action='store_true', help='use memory-efficient MixedOp to reduce GPU memory usage')
    parser.add_argument('--gradient_threshold', type=float, default=0.01, help='threshold for selective gradient computation')
    parser.add_argument('--disable_progress_spam', action='store_true', help='reduce progress output to speed up training')
    parser.add_argument('--quiet', action='store_true', help='minimal output for maximum speed')
    
    # Debug options
    parser.add_argument('--seed', type=int, default=42, help='random seed')
    parser.add_argument('--save_dir', type=str, default='./results', help='save directory')
    parser.add_argument('--exp_name', type=str, default='basic_classification', help='experiment name')
    
    return parser.parse_args()

def setup_environment(args):
    """Setup training environment"""
    # Set random seeds
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(args.seed)
    
    # CUDA optimizations
    if torch.cuda.is_available() and not args.disable_cudnn_benchmark:
        cudnn.benchmark = True
        cudnn.enabled = True
        cudnn.deterministic = False
        
        # Enable TensorFloat-32
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
    
    # Create save directory
    os.makedirs(args.save_dir, exist_ok=True)
    
    if not getattr(args, 'quiet', False):
        print(f"âœ… ç¯å¢ƒè®¾ç½®å®Œæˆ")
        print(f"   è®¾å¤‡: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
        print(f"   æ¨¡å¼: {args.mode}")
        print(f"   éšæœºç§å­: {args.seed}")

def check_cifar10_data(data_path: str) -> bool:
    """æ£€æŸ¥CIFAR-10æ•°æ®æ˜¯å¦å·²å­˜åœ¨ä¸”å®Œæ•´"""
    import os
    from pathlib import Path
    
    data_dir = Path(data_path)
    cifar_dir = data_dir / "cifar-10-batches-py"
    
    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not cifar_dir.exists():
        return False
    
    # æ£€æŸ¥å…³é”®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    required_files = [
        "data_batch_1",
        "data_batch_2", 
        "data_batch_3",
        "data_batch_4",
        "data_batch_5",
        "test_batch",
        "batches.meta"
    ]
    
    for file in required_files:
        if not (cifar_dir / file).exists():
            return False
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆç®€å•éªŒè¯ï¼‰
    total_size = sum((cifar_dir / file).stat().st_size for file in required_files)
    if total_size < 100 * 1024 * 1024:  # å°äº100MBè¯´æ˜æ•°æ®ä¸å®Œæ•´
        return False
    
    return True

def download_cifar10_with_xunlei(data_path: str) -> bool:
    """ä½¿ç”¨è¿…é›·ä¸‹è½½CIFAR-10æ•°æ®é›†"""
    try:
        from neuroexapt.utils.xunlei_downloader import XunleiDatasetDownloader
        
        print("ğŸš€ æ£€æµ‹åˆ°è¿…é›·ä¸‹è½½å™¨ï¼Œå°è¯•ä½¿ç”¨è¿…é›·ä¸‹è½½CIFAR-10...")
        downloader = XunleiDatasetDownloader(data_dir=data_path)
        
        if not downloader.xunlei_downloader.is_available:
            print("âš ï¸ è¿…é›·æœªæ£€æµ‹åˆ°ï¼Œå°†ä½¿ç”¨æ ‡å‡†ä¸‹è½½æ–¹å¼")
            print("ğŸ’¡ å¦‚éœ€ä½¿ç”¨è¿…é›·åŠ é€Ÿï¼Œè¯·å…ˆå®‰è£…è¿…é›·: https://www.xunlei.com/")
            return False
        
        print("âœ… è¿…é›·å¯ç”¨ï¼Œå¯åŠ¨ä¸‹è½½...")
        print("=" * 60)
        print("ğŸ“‹ CIFAR-10æ•°æ®é›†ä¿¡æ¯:")
        print("   æ–‡ä»¶: cifar-10-python.tar.gz")
        print("   å¤§å°: ~162MB")
        print("   æ¥æº: https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz")
        print("=" * 60)
        
        success = downloader.download_dataset('cifar10', wait_for_completion=False)
        
        if success:
            print("âœ… è¿…é›·ä¸‹è½½ä»»åŠ¡å·²å¯åŠ¨ï¼")
            print("=" * 60)
            print("ğŸ“‹ è¿…é›·ä¸‹è½½è¯´æ˜:")
            print("1. ğŸ“ ç›®æ ‡ä¿å­˜è·¯å¾„å·²è‡ªåŠ¨å¤åˆ¶åˆ°å‰ªè´´æ¿")
            print("2. ğŸš€ è¯·æ£€æŸ¥è¿…é›·ä¸‹è½½çª—å£")
            print("3. ğŸ“‹ åœ¨ä¸‹è½½çª—å£ä¸­æŒ‰ Ctrl+V ç²˜è´´ä¿å­˜è·¯å¾„")
            print("4. âœ… ç¡®è®¤æ–‡ä»¶åä¸º: cifar-10-python.tar.gz")
            print("5. ğŸ¯ ç‚¹å‡»'ç«‹å³ä¸‹è½½'å¼€å§‹ä¸‹è½½")
            print("=" * 60)
            print("ğŸ’¡ ä¸‹è½½å®Œæˆåï¼Œæ•°æ®ä¼šè‡ªåŠ¨è§£å‹åˆ°æŒ‡å®šç›®å½•")
            print("ğŸ’¡ è¯·ç­‰å¾…ä¸‹è½½å®Œæˆåé‡æ–°è¿è¡Œæ­¤ç¨‹åº")
            return True
        else:
            print("âš ï¸ è¿…é›·ä¸‹è½½å¯åŠ¨å¤±è´¥ï¼Œå°†ä½¿ç”¨æ ‡å‡†ä¸‹è½½æ–¹å¼")
            return False
            
    except ImportError:
        print("âš ï¸ è¿…é›·ä¸‹è½½å™¨æ¨¡å—æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨æ ‡å‡†ä¸‹è½½æ–¹å¼")
        print("ğŸ’¡ è¦ä½¿ç”¨è¿…é›·åŠ é€Ÿï¼Œè¯·ç¡®ä¿ neuroexapt.utils.xunlei_downloader æ¨¡å—å¯ç”¨")
        return False
    except Exception as e:
        print(f"âš ï¸ è¿…é›·ä¸‹è½½å‡ºé”™: {e}")
        print("ğŸ’¡ å°†å›é€€åˆ°æ ‡å‡†ä¸‹è½½æ–¹å¼")
        return False

def create_data_loaders(args):
    """Create data loaders with intelligent data checking and è¿…é›· support"""
    if not getattr(args, 'quiet', False):
        print("ğŸ“Š Creating data loaders...")
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
    data_dir = Path(args.data)
    
    if check_cifar10_data(args.data):
        if not getattr(args, 'quiet', False):
            print("âœ… CIFAR-10æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
        download_flag = False
    else:
        if not getattr(args, 'quiet', False):
            print("âŒ CIFAR-10æ•°æ®ä¸å­˜åœ¨")
        
        # åˆ›å»ºæ•°æ®ç›®å½•
        data_dir.mkdir(parents=True, exist_ok=True)
        
        # å°è¯•ä½¿ç”¨è¿…é›·ä¸‹è½½
        if not getattr(args, 'quiet', False):
            print("ğŸš€ å°è¯•ä½¿ç”¨è¿…é›·åŠ é€Ÿä¸‹è½½...")
        if download_cifar10_with_xunlei(args.data):
            # è¿…é›·ä¸‹è½½å·²å¯åŠ¨ï¼Œè¯¢é—®ç”¨æˆ·é€‰æ‹©
            if not getattr(args, 'quiet', False):
                print("\n" + "="*60)
                print("âš¡ è¿…é›·ä¸‹è½½å·²å¯åŠ¨ï¼æ‚¨æœ‰ä»¥ä¸‹é€‰æ‹©:")
                print("1. [æ¨è] ç­‰å¾…è¿…é›·ä¸‹è½½å®Œæˆåé‡æ–°è¿è¡Œç¨‹åº")
                print("2. ç»§ç»­ä½¿ç”¨PyTorchæ ‡å‡†ä¸‹è½½ï¼ˆè¾ƒæ…¢ä½†è‡ªåŠ¨ï¼‰")
                print("3. é€€å‡ºç¨‹åº")
                print("="*60)
                
                while True:
                    choice = input("è¯·é€‰æ‹© (1/2/3): ").strip()
                    if choice == "1":
                        print("ğŸ‘‹ è¯·ç­‰å¾…è¿…é›·ä¸‹è½½å®Œæˆåé‡æ–°è¿è¡Œç¨‹åº")
                        sys.exit(0)
                    elif choice == "2":
                        print("ğŸ“¥ ç»§ç»­ä½¿ç”¨PyTorchæ ‡å‡†ä¸‹è½½...")
                        download_flag = True
                        break
                    elif choice == "3":
                        print("ğŸ‘‹ ç¨‹åºé€€å‡º")
                        sys.exit(0)
                    else:
                        print("âš ï¸ è¯·è¾“å…¥ 1ã€2 æˆ– 3")
            else:
                # Quiet mode - automatically use standard download
                download_flag = True
        else:
            if not getattr(args, 'quiet', False):
                print("ğŸ“¥ ä½¿ç”¨PyTorchæ ‡å‡†ä¸‹è½½æ–¹å¼è·å–CIFAR-10æ•°æ®...")
            download_flag = True
    
    # Data transforms
    train_transform = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    
    # Load datasets
    if download_flag and not getattr(args, 'quiet', False):
        print("ğŸ“¦ åŠ è½½CIFAR-10æ•°æ®é›†...")
        print("   ğŸ“¥ å¼€å§‹ä¸‹è½½ (é¦–æ¬¡ä¸‹è½½å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
    
    try:
        train_data = dset.CIFAR10(root=args.data, train=True, download=download_flag, transform=train_transform)
        if download_flag and not getattr(args, 'quiet', False):
            print("âœ… CIFAR-10æ•°æ®é›†ä¸‹è½½å®Œæˆï¼")
                
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        if not getattr(args, 'quiet', False):
            print("ğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
            print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print("   2. ä½¿ç”¨è¿…é›·ä¸‹è½½å™¨åŠ é€Ÿä¸‹è½½")
            print("   3. æ‰‹åŠ¨ä¸‹è½½æ•°æ®é›†åˆ°æŒ‡å®šç›®å½•")
        raise e
    
    num_train = len(train_data)
    indices = list(range(num_train))
    split = int(np.floor(args.train_portion * num_train))
    
    if not getattr(args, 'quiet', False):
        print(f"   ğŸ“Š è®­ç»ƒæ ·æœ¬: {num_train}, åˆ†å‰²: {split}/{num_train - split}")
    
    # Create data loaders
    if args.async_loader:
        if not getattr(args, 'quiet', False):
            print("   âš¡ ä½¿ç”¨å¼‚æ­¥æ•°æ®åŠ è½½å™¨...")
        train_queue = build_cifar10_pipeline(batch_size=args.batch_size, num_workers=args.num_workers, prefetch=8)
        valid_queue = build_cifar10_pipeline(batch_size=args.batch_size, num_workers=args.num_workers, prefetch=4)
    else:
        train_queue = data.DataLoader(
            train_data, batch_size=args.batch_size,
            sampler=data.SubsetRandomSampler(indices[:split]),
            num_workers=args.num_workers,
            pin_memory=args.pin_memory,
            drop_last=True
        )
        
        valid_queue = data.DataLoader(
            train_data, batch_size=args.batch_size,
            sampler=data.SubsetRandomSampler(indices[split:num_train]),
            num_workers=args.num_workers,
            pin_memory=args.pin_memory,
            drop_last=True
        )
    
    if not getattr(args, 'quiet', False):
        print(f"   âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ: {len(train_queue)}è®­ç»ƒæ‰¹æ¬¡, {len(valid_queue)}éªŒè¯æ‰¹æ¬¡")
    
    return train_queue, valid_queue

def create_model(args, mode='exapt'):
    """Create model based on mode"""
    if not getattr(args, 'quiet', False):
        print(f"ğŸ§  Creating model (mode: {mode})...")
    
    if mode == 'fixed':
        # Fixed architecture - no evolution
        model = FixedNetwork(
            C=args.init_channels,
            num_classes=10,
            layers=args.layers
        )
    elif mode == 'cached':
        # For now, use fixed architecture to avoid hanging issues
        if not getattr(args, 'quiet', False):
            print("  âš ï¸  Using fixed architecture for cached mode")
        model = FixedNetwork(
            C=args.init_channels,
            num_classes=10,
            layers=args.layers
        )
    elif mode == 'exapt':
        # Exapt mode with intelligent neural architecture evolution
        if not getattr(args, 'quiet', False):
            optimizations = []
            if getattr(args, 'lazy_computation', False):
                optimizations.append("æ‡’è®¡ç®—")
            if getattr(args, 'gradient_optimized', True):
                optimizations.append("æ¢¯åº¦ä¼˜åŒ–")
            if getattr(args, 'memory_efficient', False):
                optimizations.append("å†…å­˜ä¼˜åŒ–")
            if optimizations:
                print(f"  ğŸ§¬ æ™ºèƒ½æ¼”åŒ–æ¨¡å¼: {', '.join(optimizations)}")
        
        model = Network(
            C=args.init_channels,
            num_classes=10,
            layers=args.layers,
            potential_layers=args.potential_layers,
            use_checkpoint=getattr(args, 'use_checkpoint', False),
            use_compile=getattr(args, 'use_model_compile', False),
            use_optimized_ops=getattr(args, 'use_optimized_ops', False),
            use_lazy_ops=getattr(args, 'lazy_computation', False),
            use_gradient_optimized=getattr(args, 'gradient_optimized', True),
            use_memory_efficient=getattr(args, 'memory_efficient', False),
            progress_tracking=not getattr(args, 'disable_progress_spam', False),
            quiet=getattr(args, 'quiet', False)
        )
    else:
        # Legacy search mode (fallback)
        model = Network(
            C=args.init_channels,
            num_classes=10,
            layers=args.layers,
            potential_layers=args.potential_layers,
            use_checkpoint=getattr(args, 'use_checkpoint', False),
            use_compile=getattr(args, 'use_model_compile', False),
            use_optimized_ops=getattr(args, 'use_optimized_ops', False),
            progress_tracking=getattr(args, 'progress_tracking', True),
            quiet=getattr(args, 'quiet', False)
        )
    
    if torch.cuda.is_available():
        if hasattr(model, 'cuda'):
            model = model.cuda()
        elif hasattr(model, 'model'):
            model.model = model.model.cuda()  # type: ignore[attr-defined]

    # Optional compile of Cell submodules for extra speed
    if args.compile_cell and hasattr(torch, 'compile'):
        target = model.model if hasattr(model, 'model') else model
        from neuroexapt.core.model import Cell
        compile_submodules(target, predicate=lambda m: isinstance(m, Cell))  # type: ignore[arg-type]
    
    # Get parameters correctly based on model type
    if hasattr(model, 'model'):  # CachedNetwork
        params = sum(p.numel() for p in model.model.parameters())  # type: ignore[attr-defined]
    else:  # Regular model
        params = sum(p.numel() for p in model.parameters())
    
    if not getattr(args, 'quiet', False):
        print(f"   å‚æ•°é‡: {params:,}")
    
    return model

def create_architect(args, model):
    """Create architect based on configuration"""
    if args.mode == 'fixed' or args.mode == 'cached':
        return None  # No architecture evolution needed
    
    if hasattr(model, 'model'):  # CachedNetwork
        architect = Architect(model.model, args)
    else:
        architect = Architect(model, args)
    
    architect.criterion = nn.CrossEntropyLoss().cuda()
    
    return architect

def train_epoch(train_queue, valid_queue, model, architect, criterion, optimizer, epoch, args, monitor=None):
    """Optimized train epoch with minimal output overhead"""
    model.train()
    
    # é‡ç½®æ¨¡å‹çš„epochè®¡æ•°å™¨
    if hasattr(model, 'reset_epoch_counters'):
        model.reset_epoch_counters()
    
    objs = AvgrageMeter()
    top1 = AvgrageMeter()
    top5 = AvgrageMeter()
    
    # Performance tracking
    batch_times = []
    
    # For architecture search modes
    if architect is not None:
        architect.set_epoch(epoch)
        valid_iter = iter(valid_queue)
        arch_updates = 0
    
    start_time = time.time()
    
    # æç®€è¿›åº¦æ¡
    total_batches = len(train_queue)
    quiet_mode = getattr(args, 'quiet', False)
    
    if quiet_mode:
        # æœ€å°åŒ–çš„è¿›åº¦æ¡
        pbar = tqdm(enumerate(train_queue), total=total_batches, 
                    desc=f"E{epoch}", 
                    bar_format='{desc}: {percentage:3.0f}%|{bar:15}| {n_fmt}/{total_fmt} {rate_fmt}',
                    mininterval=2.0)  # å‡å°‘æ›´æ–°é¢‘ç‡
    else:
        # æ­£å¸¸æ¨¡å¼ä¹Ÿç®€åŒ–
        pbar = tqdm(enumerate(train_queue), total=total_batches, 
                    desc=f"Epoch {epoch}", 
                    bar_format='{desc}: {percentage:3.0f}%|{bar:20}| {n_fmt}/{total_fmt}',
                    mininterval=1.0)
    
    for step, (input, target) in pbar:
        batch_start_time = time.perf_counter()
        
        n = input.size(0)
        input = input.cuda(non_blocking=True)
        target = target.cuda(non_blocking=True)
        
        # Architecture evolution step - ç®€åŒ–é€»è¾‘
        if architect is not None:
            should_update = (step + 1) % getattr(args, 'arch_update_freq', 50) == 0 and epoch >= getattr(args, 'warmup_epochs', 5)
            
            if should_update:
                try:
                    input_search, target_search = next(valid_iter)
                except StopIteration:
                    valid_iter = iter(valid_queue)
                    input_search, target_search = next(valid_iter)
                
                input_search = input_search.cuda(non_blocking=True)
                target_search = target_search.cuda(non_blocking=True)
                
                # Invalidate cache if using cached mode
                if hasattr(model, 'cache_valid'):
                    model.cache_valid = False
                
                architect.step(input, target, input_search, target_search,
                              optimizer.param_groups[0]['lr'], optimizer, args.use_first_order)
                arch_updates += 1
        
        # Weight update step
        optimizer.zero_grad()
        logits = model(input)
        loss = criterion(logits, target)
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)
        optimizer.step()
        
        # Statistics
        prec1, prec5 = accuracy(logits, target, topk=(1, 5))
        objs.update(loss.item(), n)
        top1.update(prec1.item(), n)
        top5.update(prec5.item(), n)
        
        batch_time = time.perf_counter() - batch_start_time
        batch_times.append(batch_time)
        
        # æç®€çš„è¿›åº¦æ¡æ›´æ–°ï¼ˆç§»é™¤æ‰€æœ‰å¤æ‚é€»è¾‘ï¼‰
        if step % 100 == 0:  # å‡å°‘æ›´æ–°é¢‘ç‡
            pbar.set_postfix({
                'Loss': f'{objs.avg:.3f}',
                'Acc': f'{top1.avg:.1f}%'
            })
        
        # GPUç¼“å­˜æ¸…ç†ï¼ˆå‡å°‘é¢‘ç‡ï¼‰
        if step % 500 == 0:
            torch.cuda.empty_cache()
    
    pbar.close()
    epoch_time = time.time() - start_time
    
    # æç®€epochæ€»ç»“
    avg_batch_time = sum(batch_times) / len(batch_times) if batch_times else 0
    
    if quiet_mode:
        print(f"E{epoch}: {objs.avg:.3f} {top1.avg:.1f}% {epoch_time:.0f}s")
    else:
        print(f"âœ… Epoch {epoch}: Loss={objs.avg:.3f} Acc={top1.avg:.1f}% Time={epoch_time:.0f}s")
        if architect and arch_updates > 0:
            print(f"   ğŸ§¬ æ¶æ„æ›´æ–°: {arch_updates}æ¬¡")
    
    return top1.avg, objs.avg, epoch_time

def validate_epoch(valid_queue, model, criterion):
    """Validate one epoch with minimal output"""
    # Clear cache before validation to avoid memory issues
    torch.cuda.empty_cache()
    
    # Switch to evaluation mode
    model.eval()
    
    objs = AvgrageMeter()
    top1 = AvgrageMeter()
    top5 = AvgrageMeter()
    
    with torch.no_grad():
        # ç®€åŒ–çš„è¿›åº¦æ¡ï¼Œä»…æ˜¾ç¤ºç™¾åˆ†æ¯”
        pbar = tqdm(valid_queue, desc="éªŒè¯", 
                   bar_format='{desc}: {percentage:3.0f}%|{bar:20}|{n_fmt}/{total_fmt}',
                   leave=False)  # éªŒè¯å®Œæˆåæ¸…é™¤è¿›åº¦æ¡
        
        for input, target in pbar:
            input = input.cuda(non_blocking=True)
            target = target.cuda(non_blocking=True)
            
            logits = model(input)
            loss = criterion(logits, target)
            
            prec1, prec5 = accuracy(logits, target, topk=(1, 5))
            n = input.size(0)
            objs.update(loss.item(), n)
            top1.update(prec1.item(), n)
            top5.update(prec5.item(), n)
        
        pbar.close()
    
    return top1.avg, objs.avg

def run_single_mode(args, mode):
    """Run training in a single mode with simplified output"""
    print(f"\nğŸš€ Running {mode} mode")
    print("=" * 50)
    
    # Create model and components
    model = create_model(args, mode)
    architect = create_architect(args, model)
    criterion = nn.CrossEntropyLoss().cuda()
    optimizer = torch.optim.SGD(
        model.parameters(),
        lr=args.learning_rate,
        momentum=args.momentum,
        weight_decay=args.weight_decay
    )
    
    # Create data loaders
    train_queue, valid_queue = create_data_loaders(args)
    
    # Training loop
    best_acc = 0.0
    total_time = 0
    
    for epoch in range(args.epochs):
        if not getattr(args, 'quiet', False):
            print(f"\n{'='*50}")
            print(f"ğŸ“Š EPOCH {epoch}/{args.epochs-1} - {mode}")
            print(f"{'='*50}")
        else:
            print(f"\nEpoch {epoch}/{args.epochs-1}")
        
        # Train
        train_acc, train_loss, train_time = train_epoch(
            train_queue, valid_queue, model, architect, criterion, optimizer, epoch, args
        )
        
        # Validate
        val_acc, val_loss = validate_epoch(valid_queue, model, criterion)
        
        if val_acc > best_acc:
            best_acc = val_acc
            if not getattr(args, 'quiet', False):
                print(f"ğŸŒŸ æ–°æœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}%")
        
        total_time += train_time
        
        # ç®€åŒ–epochæ€»ç»“
        if not getattr(args, 'quiet', False):
            print(f"ğŸ“ˆ è®­ç»ƒ={train_acc:.2f}% éªŒè¯={val_acc:.2f}% æœ€ä½³={best_acc:.2f}% æ—¶é—´={train_time:.0f}s")
        else:
            print(f"è®­ç»ƒ{train_acc:.1f}% éªŒè¯{val_acc:.1f}% æœ€ä½³{best_acc:.1f}%")
    
    avg_time = total_time / args.epochs
    
    if not getattr(args, 'quiet', False):
        print(f"\nğŸ“Š {mode}æ¨¡å¼ç»“æœ:")
        print(f"   æœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}%")
        print(f"   å¹³å‡æ¯è½®æ—¶é—´: {avg_time:.1f}s")
        print(f"   æ€»æ—¶é—´: {total_time:.1f}s")
    else:
        print(f"{mode}: {best_acc:.2f}% {avg_time:.1f}s/epoch")
    
    # Get final genotype if available
    genotype = None
    if hasattr(model, 'genotype') and callable(model.genotype):
        genotype = model.genotype()
    elif hasattr(model, 'model') and hasattr(model.model, 'genotype'):
        genotype = model.model.genotype()  # type: ignore[attr-defined]
    
    # Show final architecture for exapt mode (only if not quiet)
    if mode == 'exapt' and genotype is not None and not getattr(args, 'quiet', False):
        print(f"\nğŸ—ï¸  æœ€ç»ˆå‘ç°çš„æ¶æ„:")
        print(f"   Normal Cell: {genotype.normal}")
        print(f"   Reduce Cell: {genotype.reduce}")
        
        # Show the most frequent operations
        normal_ops = [op for op, _ in genotype.normal]
        reduce_ops = [op for op, _ in genotype.reduce]
        
        from collections import Counter
        normal_counts = Counter(normal_ops)
        reduce_counts = Counter(reduce_ops)
        
        print(f"\nğŸ“ˆ æ“ä½œç»Ÿè®¡:")
        print(f"   Normal cell: {normal_counts.most_common(3)}")
        print(f"   Reduce cell: {reduce_counts.most_common(3)}")
    
    # Fuse kernels for inference-ready model
    target_final = model.model if hasattr(model, 'model') else model
    fuse_model(target_final)  # type: ignore[arg-type]

    return {
        'mode': mode,
        'best_acc': best_acc,
        'avg_time': avg_time,
        'total_time': total_time,
        'genotype': genotype
    }

def benchmark_all_modes(args):
    """Benchmark all optimization modes"""
    print("ğŸ Benchmarking all modes")
    print("=" * 70)
    
    modes = ['fixed', 'cached', 'exapt']
    results = []
    
    for mode in modes:
        # Update args for this mode
        args.mode = mode
        result = run_single_mode(args, mode)
        results.append(result)
        
        # Clean up
        torch.cuda.empty_cache()
    
    # Compare results
    print("\nğŸ“ˆ Benchmark Results:")
    print("=" * 70)
    print(f"{'Mode':<10} {'Accuracy':<12} {'Time/Epoch':<12} {'Speedup':<10}")
    print("-" * 50)
    
    baseline_time = results[0]['avg_time']  # Fixed mode as baseline
    
    for result in results:
        speedup = baseline_time / result['avg_time']
        print(f"{result['mode']:<10} {result['best_acc']:<12.2f} {result['avg_time']:<12.1f}s {speedup:<10.1f}x")
    
    # Best mode
    best_mode = max(results, key=lambda x: x['best_acc'])
    fastest_mode = min(results, key=lambda x: x['avg_time'])
    
    print(f"\nğŸ† Best accuracy: {best_mode['mode']} ({best_mode['best_acc']:.2f}%)")
    print(f"âš¡ Fastest: {fastest_mode['mode']} ({fastest_mode['avg_time']:.1f}s/epoch)")
    
    return results

def main():
    """Main function with automated batch size optimization"""
    args = setup_args()
    setup_environment(args)
    
    # ğŸ§  æ™ºèƒ½Batch Sizeè‡ªåŠ¨ä¼˜åŒ– (ä»…åœ¨ç”¨æˆ·æœªæŒ‡å®šbatch_sizeæ—¶)
    import sys  # ç¡®ä¿syså¯ç”¨
    user_specified_batch_size = '--batch_size' in sys.argv
    
    if not getattr(args, 'quiet', False):
        print(f"ğŸš€ NeuroExapt Basic Classification")
        if not user_specified_batch_size:
            print("ğŸ” æ­£åœ¨è‡ªåŠ¨æ£€æµ‹æœ€ä¼˜batch size...")
            print()
    
    if not user_specified_batch_size:
        # å¯¼å…¥å¹¶è¿è¡Œæ™ºèƒ½batch sizeä¼˜åŒ–å™¨
        try:
            # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥intelligent_batch_optimizer
            import sys
            import os
            project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            if project_root not in sys.path:
                sys.path.insert(0, project_root)
            
            from intelligent_batch_optimizer import find_optimal_batch_size
            
            # è‡ªåŠ¨æ‰¾åˆ°æœ€ä¼˜batch size
            optimal_batch_size = find_optimal_batch_size(quiet=getattr(args, 'quiet', False))
            args.batch_size = optimal_batch_size
            
            if not getattr(args, 'quiet', False):
                print(f"\nâœ… å·²è‡ªåŠ¨è®¾ç½®æœ€ä¼˜batch size: {optimal_batch_size}")
                print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
                print()
            
        except Exception as e:
            if not getattr(args, 'quiet', False):
                print(f"âš ï¸ æ™ºèƒ½batch sizeä¼˜åŒ–å¤±è´¥: {e}")
                print(f"ğŸ”„ ä½¿ç”¨é»˜è®¤batch size: {args.batch_size}")
                print()
    else:
        if not getattr(args, 'quiet', False):
            print(f"ğŸ“‹ ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„batch size: {args.batch_size}")
            print()
    
    if not getattr(args, 'quiet', False):
        print(f"ğŸ“Š è®­ç»ƒé…ç½®:")
        print(f"   Mode: {args.mode}")
        print(f"   Epochs: {args.epochs}")
        batch_size_label = "æ™ºèƒ½ä¼˜åŒ–" if not user_specified_batch_size else "ç”¨æˆ·æŒ‡å®š"
        print(f"   Batch size: {args.batch_size} ({batch_size_label})")
        print(f"   Layers: {args.layers}")
        
        # æ£€æŸ¥è¿…é›·ä¸‹è½½å™¨çŠ¶æ€ï¼ˆç®€åŒ–ï¼‰
        try:
            from neuroexapt.utils.xunlei_downloader import XunleiDatasetDownloader
            downloader = XunleiDatasetDownloader()
            if downloader.xunlei_downloader.is_available:
                print(f"âœ… è¿…é›·ä¸‹è½½å™¨: å¯ç”¨")
            else:
                print(f"âš ï¸ è¿…é›·ä¸‹è½½å™¨: ä¸å¯ç”¨")
        except:
            print(f"âš ï¸ è¿…é›·ä¸‹è½½å™¨: æ¨¡å—æœªæ‰¾åˆ°")
        
        print()  # ç©ºè¡Œåˆ†éš”
    
    if args.mode == 'benchmark':
        results = benchmark_all_modes(args)
    else:
        result = run_single_mode(args, args.mode)
        results = [result]
    
    if not getattr(args, 'quiet', False):
        print("\nâœ… è®­ç»ƒå®Œæˆ!")
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡ï¼ˆç®€åŒ–ï¼‰
        optimizations = []
        if hasattr(args, 'gradient_optimized') and args.gradient_optimized:
            optimizations.append("æ¢¯åº¦ä¼˜åŒ–")
        if hasattr(args, 'lazy_computation') and args.lazy_computation:
            optimizations.append("æ‡’è®¡ç®—")
        if hasattr(args, 'memory_efficient') and args.memory_efficient:
            optimizations.append("å†…å­˜ä¼˜åŒ–")
        
        if optimizations:
            print(f"ğŸ”§ å·²å¯ç”¨ä¼˜åŒ–: {', '.join(optimizations)}")
    else:
        print("å®Œæˆ!")

if __name__ == "__main__":
    main() 