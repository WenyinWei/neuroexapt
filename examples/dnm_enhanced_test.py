#!/usr/bin/env python3
"""
DNM Enhanced Test - å¢å¼ºç‰ˆ DNM æ¡†æ¶æµ‹è¯•

ğŸ§¬ ç‰¹æ€§:
1. é›†æˆå¢å¼ºçš„ç“¶é¢ˆæ£€æµ‹å™¨
2. åŸºäºæ€§èƒ½å¯¼å‘çš„ç¥ç»å…ƒåˆ†è£‚
3. å¤šç»´åº¦æ€§èƒ½ç›‘æ§
4. æ™ºèƒ½è§¦å‘æœºåˆ¶
5. è¯¦ç»†çš„åˆ†ææŠ¥å‘Š

ğŸ¯ ç›®æ ‡: éªŒè¯å¢å¼ºçš„ DNM æ¡†æ¶èƒ½å¤Ÿæ˜¾è‘—æå‡åˆ†è£‚æ•ˆæœå’Œå‡†ç¡®ç‡
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import numpy as np
import time
import logging
from tqdm import tqdm
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from neuroexapt.core.dnm_framework import DNMFramework
from neuroexapt.core.enhanced_bottleneck_detector import EnhancedBottleneckDetector
from neuroexapt.core.performance_guided_division import PerformanceGuidedDivision, DivisionStrategy

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(name)s:%(levelname)s:%(message)s')
logger = logging.getLogger('dnm_enhanced_test')

class EnhancedCNNModel(nn.Module):
    """å¢å¼ºçš„CNNæ¨¡å‹ï¼Œä¸“ä¸ºDNMä¼˜åŒ–"""
    
    def __init__(self, num_classes=10):
        super(EnhancedCNNModel, self).__init__()
        
        # ç‰¹å¾æå–éƒ¨åˆ†
        self.features = nn.Sequential(
            # ç¬¬ä¸€ä¸ªå·ç§¯å— - è¾ƒå°çš„åˆå§‹é€šé“æ•°ï¼Œä¾¿äºè§‚å¯Ÿåˆ†è£‚æ•ˆæœ
            nn.Conv2d(3, 16, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(16),
            nn.ReLU(inplace=True),
            nn.Conv2d(16, 32, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout2d(0.25),
            
            # ç¬¬äºŒä¸ªå·ç§¯å—
            nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout2d(0.25),
            
            # ç¬¬ä¸‰ä¸ªå·ç§¯å—
            nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Dropout2d(0.25),
        )
        
        # åˆ†ç±»å™¨éƒ¨åˆ† - è¾ƒå°çš„éšè—å±‚ï¼Œä¾¿äºè§‚å¯Ÿåˆ†è£‚
        self.classifier = nn.Sequential(
            nn.Linear(128 * 4 * 4, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(128, num_classes)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
        
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

class ActivationHook:
    """æ”¹è¿›çš„æ¿€æ´»å€¼æ•è·é’©å­"""
    
    def __init__(self):
        self.activations = {}
        self.hooks = []
    
    def hook_fn(self, name):
        def fn(module, input, output):
            self.activations[name] = output.detach().clone()
        return fn
    
    def register_hooks(self, model):
        self.remove_hooks()  # æ¸…ç†ä¹‹å‰çš„é’©å­
        
        for name, module in model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)):
                # ä¸ºæ‰€æœ‰å·ç§¯å±‚å’Œçº¿æ€§å±‚æ³¨å†Œé’©å­
                hook = module.register_forward_hook(self.hook_fn(name))
                self.hooks.append(hook)
    
    def remove_hooks(self):
        for hook in self.hooks:
            hook.remove()
        self.hooks.clear()
        self.activations.clear()

class GradientHook:
    """æ”¹è¿›çš„æ¢¯åº¦æ•è·é’©å­"""
    
    def __init__(self):
        self.gradients = {}
        self.last_targets = None
    
    def capture_gradients(self, model):
        self.gradients.clear()
        for name, param in model.named_parameters():
            if param.grad is not None:
                self.gradients[name] = param.grad.detach().clone()

def load_cifar10_data(batch_size=64):
    """åŠ è½½CIFAR-10æ•°æ®é›†ï¼Œä½¿ç”¨è¾ƒå°çš„batch sizeä»¥æ›´å¥½åœ°è§‚å¯Ÿåˆ†è£‚æ•ˆæœ"""
    
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)
    
    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)
    
    return trainloader, testloader

def train_epoch(model, train_loader, optimizer, criterion, device, activation_hook, gradient_hook):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    pbar = tqdm(train_loader, desc="Training", leave=False)
    for batch_idx, (data, target) in enumerate(pbar):
        data, target = data.to(device), target.to(device)
        gradient_hook.last_targets = target
        
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        
        # æ•è·æ¢¯åº¦
        gradient_hook.capture_gradients(model)
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        running_loss += loss.item()
        _, predicted = output.max(1)
        total += target.size(0)
        correct += predicted.eq(target).sum().item()
        
        if batch_idx % 100 == 0:
            pbar.set_postfix({
                'Loss': f'{running_loss/(batch_idx+1):.4f}',
                'Acc': f'{100.*correct/total:.2f}%'
            })
        
        # ä¸ºäº†è§‚å¯Ÿåˆ†è£‚æ•ˆæœï¼Œåªè®­ç»ƒä¸€éƒ¨åˆ†æ•°æ®
        if batch_idx >= 200:  # é™åˆ¶æ¯ä¸ªepochçš„æ‰¹æ¬¡æ•°
            break
    
    return running_loss / min(len(train_loader), 201), 100. * correct / total

def validate_epoch(model, val_loader, criterion, device):
    """éªŒè¯ä¸€ä¸ªepoch"""
    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(val_loader):
            data, target = data.to(device), target.to(device)
            output = model(data)
            val_loss += criterion(output, target).item()
            
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()
            
            # é™åˆ¶éªŒè¯æ•°æ®é‡
            if batch_idx >= 50:
                break
    
    return val_loss / min(len(val_loader), 51), 100. * correct / total

def count_parameters(model):
    """è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

def analyze_model_structure(model):
    """åˆ†ææ¨¡å‹ç»“æ„"""
    conv_layers = 0
    linear_layers = 0
    total_params = 0
    
    for name, module in model.named_modules():
        if isinstance(module, nn.Conv2d):
            conv_layers += 1
            total_params += sum(p.numel() for p in module.parameters())
        elif isinstance(module, nn.Linear):
            linear_layers += 1
            total_params += sum(p.numel() for p in module.parameters())
    
    return {
        'conv_layers': conv_layers,
        'linear_layers': linear_layers,
        'total_params': total_params
    }

def main():
    """ä¸»è®­ç»ƒå¾ªç¯"""
    print("ğŸ§¬ DNM Enhanced Framework Test")
    print("=" * 60)
    
    # è®¾å¤‡é…ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ“± è®¾å¤‡: {device}")
    
    # æ•°æ®åŠ è½½
    print("ğŸ“ åŠ è½½CIFAR-10æ•°æ®é›†...")
    train_loader, val_loader = load_cifar10_data(batch_size=64)
    
    # æ¨¡å‹åˆå§‹åŒ–
    print("ğŸ—ï¸ åˆå§‹åŒ–å¢å¼ºæ¨¡å‹...")
    model = EnhancedCNNModel(num_classes=10).to(device)
    initial_params = count_parameters(model)
    initial_structure = analyze_model_structure(model)
    print(f"åˆå§‹å‚æ•°æ•°é‡: {initial_params:,}")
    print(f"åˆå§‹ç»“æ„: Conv={initial_structure['conv_layers']}, Linear={initial_structure['linear_layers']}")
    
    # å¢å¼ºçš„ç“¶é¢ˆæ£€æµ‹å™¨é…ç½®
    bottleneck_detector = EnhancedBottleneckDetector(
        sensitivity_threshold=0.05,   # æ›´æ•æ„Ÿçš„æ£€æµ‹
        diversity_threshold=0.2,      # è¾ƒä½çš„å¤šæ ·æ€§é˜ˆå€¼
        gradient_threshold=1e-7,      # æ›´æ•æ„Ÿçš„æ¢¯åº¦æ£€æµ‹
        info_flow_threshold=0.3       # è¾ƒä½çš„ä¿¡æ¯æµé˜ˆå€¼
    )
    
    # æ€§èƒ½å¯¼å‘åˆ†è£‚å™¨é…ç½®
    guided_division = PerformanceGuidedDivision(
        noise_scale=0.05,            # è¾ƒå°çš„å™ªå£°ï¼Œä¿æŒç¨³å®šæ€§
        progressive_epochs=3,         # è¾ƒå¿«çš„æ¸è¿›æ¿€æ´»
        diversity_threshold=0.7,      # å¤šæ ·æ€§é˜ˆå€¼
        performance_monitoring=True   # å¯ç”¨æ€§èƒ½ç›‘æ§
    )
    
    # DNMæ¡†æ¶é…ç½®
    dnm_config = {
        'morphogenesis_interval': 2,   # æ¯2ä¸ªepochæ£€æŸ¥ä¸€æ¬¡ï¼Œæ›´é¢‘ç¹
        'max_morphogenesis_per_epoch': 2,  # æ¯æ¬¡æœ€å¤š2æ¬¡å½¢æ€å‘ç”Ÿ
        'performance_improvement_threshold': 0.005,  # æ›´æ•æ„Ÿçš„é˜ˆå€¼
        'enhanced_bottleneck_detector': bottleneck_detector,
        'performance_guided_division': guided_division,
        'division_strategy': DivisionStrategy.HYBRID,  # ä½¿ç”¨æ··åˆç­–ç•¥
    }
    
    # åˆå§‹åŒ–DNMæ¡†æ¶
    print("ğŸ§¬ åˆå§‹åŒ–å¢å¼ºDNMæ¡†æ¶...")
    dnm = DNMFramework(model, dnm_config)
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)
    criterion = nn.CrossEntropyLoss()
    
    # é’©å­è®¾ç½®
    activation_hook = ActivationHook()
    activation_hook.register_hooks(model)
    gradient_hook = GradientHook()
    
    # è®­ç»ƒé…ç½®
    epochs = 30  # è¾ƒå°‘çš„epochsä»¥è§‚å¯Ÿå¿«é€Ÿæ•ˆæœ
    best_acc = 0.0
    patience = 15
    patience_counter = 0
    morphogenesis_events = 0
    
    # æ€§èƒ½å†å²è®°å½•
    performance_history = []
    bottleneck_history = []
    division_history = []
    
    print("\nğŸš€ å¼€å§‹å¢å¼ºè®­ç»ƒ...")
    print("=" * 60)
    
    start_time = time.time()
    
    for epoch in range(epochs):
        current_lr = optimizer.param_groups[0]['lr']
        
        print(f"\nğŸ§¬ Epoch {epoch+1}/{epochs} - Enhanced DNM")
        
        # æ›´æ–°ç»„ä»¶çš„epochä¿¡æ¯
        guided_division.update_epoch(epoch)
        
        # è®­ç»ƒ
        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, 
                                          device, activation_hook, gradient_hook)
        
        # éªŒè¯
        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)
        
        # è®°å½•æ€§èƒ½
        performance_history.append(val_acc)
        bottleneck_detector.update_performance_history(val_acc)
        
        # ä½¿ç”¨å¢å¼ºçš„ç“¶é¢ˆæ£€æµ‹
        bottleneck_scores = bottleneck_detector.detect_bottlenecks(
            model, activation_hook.activations, gradient_hook.gradients, gradient_hook.last_targets
        )
        bottleneck_history.append(bottleneck_scores)
        
        # è·å–ç“¶é¢ˆåˆ†ææ‘˜è¦
        bottleneck_summary = bottleneck_detector.get_analysis_summary(bottleneck_scores)
        
        # æ™ºèƒ½è§¦å‘åˆ¤æ–­
        should_trigger, reasons = bottleneck_detector.should_trigger_division(
            bottleneck_scores, performance_history[-5:]  # ä½¿ç”¨æœ€è¿‘5ä¸ªepochçš„æ€§èƒ½
        )
        
        print(f"  ğŸ“Š Train: {train_acc:.2f}% | Val: {val_acc:.2f}% | Loss: {val_loss:.4f}")
        print(f"  ğŸ” ç“¶é¢ˆåˆ†æ: æœ€é«˜={bottleneck_summary.get('max_score', 0):.3f}, å¹³å‡={bottleneck_summary.get('mean_score', 0):.3f}")
        
        if should_trigger:
            print(f"  ğŸ”„ è§¦å‘å¢å¼ºå½¢æ€å‘ç”Ÿ:")
            for reason in reasons:
                print(f"    - {reason}")
            
            # è·å–Topç“¶é¢ˆå±‚
            top_bottlenecks = bottleneck_detector.get_top_bottlenecks(bottleneck_scores, 2)
            
            for layer_name, score in top_bottlenecks:
                print(f"    ğŸ¯ ç›®æ ‡å±‚: {layer_name} (åˆ†æ•°: {score:.3f})")
                
                # æ‰¾åˆ°å¯¹åº”çš„å±‚å’Œç¥ç»å…ƒ
                for name, module in model.named_modules():
                    if name == layer_name and isinstance(module, (nn.Conv2d, nn.Linear)):
                        # é€‰æ‹©åˆ†è£‚çš„ç¥ç»å…ƒï¼ˆè¿™é‡Œç®€å•é€‰æ‹©ä¸­é—´çš„ç¥ç»å…ƒï¼‰
                        if isinstance(module, nn.Conv2d):
                            neuron_idx = module.out_channels // 2
                        else:
                            neuron_idx = module.out_features // 2
                        
                        # æ‰§è¡Œæ€§èƒ½å¯¼å‘åˆ†è£‚
                        success, division_info = guided_division.divide_neuron(
                            module, neuron_idx, DivisionStrategy.HYBRID,
                            activation_hook.activations.get(name),
                            gradient_hook.gradients.get(name + '.weight'),
                            gradient_hook.last_targets
                        )
                        
                        if success:
                            morphogenesis_events += 1
                            division_history.append(division_info)
                            print(f"    âœ… åˆ†è£‚æˆåŠŸ: {division_info.get('division_type', 'unknown')} ç­–ç•¥")
                            
                            # é‡æ–°æ³¨å†Œé’©å­ï¼ˆå› ä¸ºæ¨¡å‹ç»“æ„å¯èƒ½å˜åŒ–ï¼‰
                            activation_hook.register_hooks(model)
                            
                            # æ›´æ–°ä¼˜åŒ–å™¨ï¼ˆå¦‚æœæ¨¡å‹å‚æ•°å˜åŒ–ï¼‰
                            optimizer = optim.Adam(model.parameters(), lr=current_lr, weight_decay=1e-4)
                        else:
                            print(f"    âŒ åˆ†è£‚å¤±è´¥: {division_info.get('error', 'unknown error')}")
                        
                        break
        
        # è®¡ç®—å½“å‰æ¨¡å‹çŠ¶æ€
        current_params = count_parameters(model)
        current_structure = analyze_model_structure(model)
        param_growth = ((current_params - initial_params) / initial_params) * 100
        
        print(f"  ğŸ“ˆ å‚æ•°: {current_params:,} (+{param_growth:.1f}%) | å½¢æ€å‘ç”Ÿ: {morphogenesis_events}")
        
        # æ›´æ–°æœ€ä½³å‡†ç¡®ç‡
        if val_acc > best_acc:
            best_acc = val_acc
            patience_counter = 0
        else:
            patience_counter += 1
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()
        
        # æ—©åœæ£€æŸ¥
        if patience_counter >= patience:
            print(f"  ğŸ›‘ æ—©åœè§¦å‘ (patience: {patience})")
            break
    
    # è®­ç»ƒå®Œæˆåˆ†æ
    training_time = time.time() - start_time
    final_params = count_parameters(model)
    final_structure = analyze_model_structure(model)
    param_growth = ((final_params - initial_params) / initial_params) * 100
    
    print(f"\nğŸ‰ å¢å¼ºDNMè®­ç»ƒå®Œæˆ!")
    print("=" * 60)
    print(f"ğŸ“Š æœ€ç»ˆç»“æœ:")
    print(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.2f}%")
    print(f"   æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
    print(f"   å‚æ•°å¢é•¿: +{param_growth:.1f}% ({initial_params:,} â†’ {final_params:,})")
    print(f"   è®­ç»ƒæ—¶é—´: {training_time/60:.1f}åˆ†é’Ÿ")
    print(f"   å½¢æ€å‘ç”Ÿäº‹ä»¶: {morphogenesis_events}")
    
    # è·å–å¢å¼ºç»„ä»¶çš„æ‘˜è¦
    division_summary = guided_division.get_division_summary()
    
    print(f"\nğŸ§¬ å¢å¼ºç»„ä»¶åˆ†æ:")
    print(f"   ç“¶é¢ˆæ£€æµ‹äº‹ä»¶: {len(bottleneck_history)}")
    print(f"   æ€»åˆ†è£‚æ¬¡æ•°: {division_summary.get('total_divisions', 0)}")
    if division_summary.get('strategies_used'):
        print(f"   åˆ†è£‚ç­–ç•¥åˆ†å¸ƒ:")
        for strategy, count in division_summary['strategies_used'].items():
            print(f"     - {strategy}: {count} æ¬¡")
    
    # æ€§èƒ½è¶‹åŠ¿åˆ†æ
    if len(performance_history) > 5:
        early_avg = np.mean(performance_history[:3])
        late_avg = np.mean(performance_history[-3:])
        improvement = late_avg - early_avg
        print(f"   æ€§èƒ½æ”¹å–„: {improvement:.2f}% (ä» {early_avg:.2f}% åˆ° {late_avg:.2f}%)")
    
    # æ€§èƒ½è¯„ä¼°
    if best_acc >= 75.0:
        print(f"\nğŸ† ä¼˜ç§€è¡¨ç°: æˆåŠŸè¾¾åˆ° {best_acc:.2f}% å‡†ç¡®ç‡!")
    elif best_acc >= 65.0:
        print(f"\nğŸ”„ è‰¯å¥½è¡¨ç°: è¾¾åˆ° {best_acc:.2f}% å‡†ç¡®ç‡ï¼Œæœ‰æ”¹è¿›ç©ºé—´")
    else:
        print(f"\nâš ï¸ éœ€è¦æ”¹è¿›: å½“å‰ {best_acc:.2f}% å‡†ç¡®ç‡")
    
    print(f"\nâœ… å¢å¼ºDNMæµ‹è¯•å®Œæˆ!")
    print(f"   ç“¶é¢ˆæ£€æµ‹å™¨å·¥ä½œæ­£å¸¸: {len(bottleneck_history)} æ¬¡åˆ†æ")
    print(f"   æ€§èƒ½å¯¼å‘åˆ†è£‚å™¨å·¥ä½œæ­£å¸¸: {division_summary.get('total_divisions', 0)} æ¬¡åˆ†è£‚")
    print(f"   æœ€ç»ˆå‡†ç¡®ç‡: {val_acc:.2f}%")
    
    # æ¸…ç†èµ„æº
    activation_hook.remove_hooks()

if __name__ == "__main__":
    main()