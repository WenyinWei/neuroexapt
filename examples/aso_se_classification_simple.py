#!/usr/bin/env python3
"""
ASO-SE ç®€åŒ–ç‰ˆæœ¬ - è§£å†³åŸºç¡€æ€§èƒ½é—®é¢˜

é‡ç‚¹ï¼š
1. Warmupé˜¶æ®µä½¿ç”¨çœŸæ­£çš„å›ºå®šæ¶æ„
2. åˆç†çš„ç½‘ç»œæ¶æ„è®¾è®¡
3. æ­£å¸¸çš„è®­ç»ƒé€Ÿåº¦
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import numpy as np
import time
import os
import sys
import argparse
from datetime import datetime
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# é…ç½®æ—¥å¿—
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(message)s', datefmt='%H:%M:%S')
logger = logging.getLogger()

class SimpleBlock(nn.Module):
    """ç®€å•çš„æ®‹å·®å—"""
    
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
    
    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out

class SimpleNetwork(nn.Module):
    """ç®€å•çš„åŸºå‡†ç½‘ç»œ"""
    
    def __init__(self, num_classes=10):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 64, 3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        
        self.layer1 = self._make_layer(64, 64, 2, stride=1)
        self.layer2 = self._make_layer(64, 128, 2, stride=2)
        self.layer3 = self._make_layer(128, 256, 2, stride=2)
        
        self.avgpool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(256, num_classes)
    
    def _make_layer(self, in_channels, out_channels, num_blocks, stride):
        layers = []
        layers.append(SimpleBlock(in_channels, out_channels, stride))
        for _ in range(1, num_blocks):
            layers.append(SimpleBlock(out_channels, out_channels, 1))
        return nn.Sequential(*layers)
    
    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

class ASOSESimpleTrainer:
    """ç®€åŒ–çš„ASO-SEè®­ç»ƒå™¨"""
    
    def __init__(self, experiment_name="aso_se_simple"):
        self.experiment_name = experiment_name
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # è®­ç»ƒå‚æ•°
        self.batch_size = 128
        self.num_epochs = 50
        self.learning_rate = 0.1
        self.momentum = 0.9
        self.weight_decay = 1e-4
        
        print(f"ğŸš€ ASO-SE ç®€åŒ–è®­ç»ƒå™¨åˆå§‹åŒ–")
        print(f"   å®éªŒåç§°: {experiment_name}")
        print(f"   è®¾å¤‡: {self.device}")
    
    def setup_data(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        transform_train = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])
        
        transform_test = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])
        
        train_dataset = torchvision.datasets.CIFAR10(
            root='./data', train=True, download=True, transform=transform_train)
        test_dataset = torchvision.datasets.CIFAR10(
            root='./data', train=False, download=True, transform=transform_test)
        
        self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, 
                                     shuffle=True, num_workers=4, pin_memory=True)
        self.test_loader = DataLoader(test_dataset, batch_size=self.batch_size, 
                                    shuffle=False, num_workers=4, pin_memory=True)
        
        print(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ: è®­ç»ƒé›† {len(train_dataset)}, æµ‹è¯•é›† {len(test_dataset)}")
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹"""
        self.network = SimpleNetwork(num_classes=10).to(self.device)
        
        total_params = sum(p.numel() for p in self.network.parameters())
        print(f"ğŸ—ï¸ æ¨¡å‹åˆ›å»ºå®Œæˆ")
        print(f"   å‚æ•°é‡: {total_params:,}")
    
    def setup_optimizer(self):
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        self.optimizer = optim.SGD(
            self.network.parameters(),
            lr=self.learning_rate,
            momentum=self.momentum,
            weight_decay=self.weight_decay
        )
        
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=self.num_epochs, eta_min=1e-4)
        
        print(f"âš™ï¸ ä¼˜åŒ–å™¨è®¾ç½®å®Œæˆ")
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.network.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{self.num_epochs}')
        
        for batch_idx, (data, targets) in enumerate(pbar):
            data, targets = data.to(self.device), targets.to(self.device)
            
            self.optimizer.zero_grad()
            outputs = self.network(data)
            loss = F.cross_entropy(outputs, targets)
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
            
            accuracy = 100. * correct / total
            pbar.set_postfix({
                'Loss': f'{total_loss/(batch_idx+1):.3f}',
                'Acc': f'{accuracy:.2f}%'
            })
        
        return total_loss / len(self.train_loader), accuracy
    
    def evaluate(self):
        """è¯„ä¼°æ¨¡å‹"""
        self.network.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, targets in self.test_loader:
                data, targets = data.to(self.device), targets.to(self.device)
                outputs = self.network(data)
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
        
        accuracy = 100. * correct / total
        return accuracy
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"\nğŸ”§ ç®€å•ç½‘ç»œè®­ç»ƒå¼€å§‹ - éªŒè¯åŸºç¡€æ€§èƒ½")
        print(f"{'='*60}")
        
        self.setup_data()
        self.setup_model()
        self.setup_optimizer()
        
        best_accuracy = 0.0
        
        for epoch in range(self.num_epochs):
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(epoch)
            
            # è¯„ä¼°
            test_acc = self.evaluate()
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            
            # æ›´æ–°æœ€ä½³ç²¾åº¦
            if test_acc > best_accuracy:
                best_accuracy = test_acc
            
            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 5 == 0:
                print(f"\nğŸ“Š Epoch {epoch+1}/{self.num_epochs}")
                print(f"   Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")
                print(f"   Test Acc: {test_acc:.2f}% | Best: {best_accuracy:.2f}%")
        
        print(f"\nğŸ‰ ç®€å•ç½‘ç»œè®­ç»ƒå®Œæˆ!")
        print(f"   æœ€ä½³ç²¾åº¦: {best_accuracy:.2f}%")
        print(f"   é¢„æœŸ: åº”è¯¥è¾¾åˆ°85%+çš„å‡†ç¡®ç‡")
        
        return best_accuracy

def main():
    parser = argparse.ArgumentParser(description='ASO-SE ç®€åŒ–åŸºå‡†æµ‹è¯•')
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=128, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=0.1, help='å­¦ä¹ ç‡')
    
    args = parser.parse_args()
    
    print("ğŸ”§ ASO-SE: ç®€åŒ–åŸºå‡†æµ‹è¯•")
    print(f"   ç›®æ ‡: éªŒè¯ç½‘ç»œåŸºç¡€è®­ç»ƒèƒ½åŠ›")
    print(f"   é¢„æœŸ: CIFAR-10 85%+å‡†ç¡®ç‡")
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = ASOSESimpleTrainer("aso_se_simple_baseline")
    trainer.num_epochs = args.epochs
    trainer.batch_size = args.batch_size
    trainer.learning_rate = args.lr
    
    best_acc = trainer.train()
    
    print(f"\nâœ¨ åŸºå‡†æµ‹è¯•å®Œæˆ!")
    print(f"   æœ€ç»ˆç²¾åº¦: {best_acc:.2f}%")
    print(f"   è¯„ä»·: {'âœ… åŸºç¡€è®­ç»ƒæ­£å¸¸' if best_acc > 80 else 'âŒ åŸºç¡€è®­ç»ƒå¼‚å¸¸'}")

if __name__ == '__main__':
    main()