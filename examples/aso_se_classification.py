#!/usr/bin/env python3
"""
ASO-SE ç¥ç»ç½‘ç»œè‡ªç”Ÿé•¿æ¶æ„ç³»ç»Ÿ - å†²å‡»CIFAR-10 95%å‡†ç¡®ç‡

ğŸ§¬ ASO-SEç†è®ºæ¡†æ¶ (Alternating Stable Optimization with Stochastic Exploration):
äº¤æ›¿å¼ç¨³å®šä¼˜åŒ–ä¸éšæœºæ¢ç´¢ï¼Œè§£å†³å¯å¾®æ¶æ„æœç´¢çš„æ ¸å¿ƒçŸ›ç›¾ï¼š
- ç½‘ç»œå‚æ•°å’Œæ¶æ„å‚æ•°è€¦åˆä¼˜åŒ–ä»£ä»·å·¨å¤§
- è§£è€¦ä¼˜åŒ–åˆä¼šå¼•å…¥ç ´åæ€§çš„"æ¶æ„éœ‡è¡"

ğŸŒ± æ ¸å¿ƒæœºåˆ¶ï¼š
1. å‡½æ•°ä¿æŒçªå˜ - å¹³æ»‘æ¶æ„è¿‡æ¸¡ï¼Œé¿å…æ€§èƒ½å‰§é™
2. Gumbel-Softmaxå¼•å¯¼æ¢ç´¢ - çªç ´å±€éƒ¨æœ€ä¼˜ï¼Œæ™ºèƒ½é€‰æ‹©æ¶æ„
3. å››é˜¶æ®µå¾ªç¯è®­ç»ƒ - ç¨³å®šä¼˜åŒ–ä¸æ¢ç´¢çš„å®Œç¾å¹³è¡¡
4. æ¸è¿›å¼ç»“æ„ç”Ÿé•¿ - çœŸæ­£çš„å‚æ•°é‡å’Œæ·±åº¦å¢é•¿

ğŸ¯ ç›®æ ‡ï¼šCIFAR-10æ•°æ®é›†95%+å‡†ç¡®ç‡ï¼Œå±•ç¤ºASO-SEçš„å¼ºå¤§èƒ½åŠ›
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import numpy as np
import time
import os
import sys
import json
import math
from datetime import datetime
from tqdm import tqdm
from typing import Dict, List, Optional, Tuple, Union

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from neuroexapt.core import CheckpointManager, get_checkpoint_manager
from neuroexapt.core.evolution_checkpoint import EvolutionCheckpointManager
from neuroexapt.core.function_preserving_init import FunctionPreservingInitializer

# é…ç½®ç®€æ´æ—¥å¿—æ ¼å¼ï¼Œå»é™¤å¤šä½™å‰ç¼€
import logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s | %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger()

class GumbelSoftmaxSelector:
    """Gumbel-Softmaxæ¶æ„é€‰æ‹©å™¨ - æ ¸å¿ƒæ¢ç´¢æœºåˆ¶"""
    
    def __init__(self, initial_temp=5.0, min_temp=0.1, anneal_rate=0.98):
        self.initial_temp = initial_temp
        self.min_temp = min_temp
        self.anneal_rate = anneal_rate
        self.current_temp = initial_temp
        
    def sample(self, logits: torch.Tensor, hard=True):
        """ä½¿ç”¨Gumbel-Softmaxè¿›è¡Œå¯å¾®é‡‡æ ·"""
        if not self.training:
            # æµ‹è¯•æ—¶ä½¿ç”¨argmax
            return F.one_hot(logits.argmax(dim=-1), logits.size(-1)).float()
        
        # Gumbelå™ªå£°
        gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) + 1e-8) + 1e-8)
        logits_with_noise = (logits + gumbel_noise) / self.current_temp
        
        soft_sample = F.softmax(logits_with_noise, dim=-1)
        
        if hard:
            # ç¡¬é‡‡æ · - å‰å‘æ—¶ç¦»æ•£ï¼Œåå‘æ—¶è¿ç»­
            hard_sample = F.one_hot(soft_sample.argmax(dim=-1), soft_sample.size(-1)).float()
            return hard_sample - soft_sample.detach() + soft_sample
        
        return soft_sample
    
    def anneal_temperature(self):
        """é€€ç«æ¸©åº¦"""
        self.current_temp = max(self.min_temp, self.current_temp * self.anneal_rate)
        return self.current_temp

class AdvancedEvolvableBlock(nn.Module):
    """é«˜çº§å¯æ¼”åŒ–å— - æ•´åˆæ‰€æœ‰å…ˆè¿›ç‰¹æ€§"""
    
    def __init__(self, in_channels, out_channels, block_id, stride=1):
        super().__init__()
        
        self.block_id = block_id
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.stride = stride
        
        # å¤šç§æ“ä½œé€‰æ‹© - æ¶æ„æœç´¢ç©ºé—´
        self.operations = self._build_operation_space()
        
        # æ¶æ„å‚æ•° - ç”¨äºæœç´¢æœ€ä¼˜æ“ä½œç»„åˆ
        self.alpha_ops = nn.Parameter(torch.randn(len(self.operations)))
        
        # è·³è·ƒè¿æ¥é€‰æ‹©
        self.skip_ops = nn.ModuleList([
            nn.Identity(),  # ç›´æ¥è¿æ¥
            nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False) if in_channels != out_channels or stride != 1 else nn.Identity(),  # 1x1æŠ•å½±
        ])
        self.alpha_skip = nn.Parameter(torch.randn(len(self.skip_ops)))
        
        # å¹¶è¡Œåˆ†æ”¯ï¼ˆå¯åŠ¨æ€æ·»åŠ ï¼‰
        self.branches = nn.ModuleList()
        self.alpha_branches = nn.Parameter(torch.zeros(0))  # åŠ¨æ€å¤§å°
        
        # Gumbel-Softmaxé€‰æ‹©å™¨
        self.gumbel_selector = GumbelSoftmaxSelector()
        
        # å‡½æ•°ä¿æŒåˆå§‹åŒ–å™¨
        self.fp_initializer = FunctionPreservingInitializer()
        
        # æ¼”åŒ–å†å²
        self.evolution_history = []
        
        # æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            'forward_count': 0,
            'avg_output_norm': 0.0,
            'gradient_norm': 0.0
        }
        
        print(f"ğŸ§± Block {block_id}: {in_channels}â†’{out_channels}, stride={stride}, {len(self.operations)} ops")
    
    def _build_operation_space(self):
        """æ„å»ºä¸°å¯Œçš„æ“ä½œæœç´¢ç©ºé—´"""
        ops = nn.ModuleList()
        
        # 1. æ ‡å‡†å·ç§¯
        ops.append(nn.Sequential(
            nn.Conv2d(self.in_channels, self.out_channels, 3, 
                     stride=self.stride, padding=1, bias=False),
            nn.BatchNorm2d(self.out_channels),
            nn.ReLU(inplace=False)
        ))
        
        # 2. æ·±åº¦å¯åˆ†ç¦»å·ç§¯
        if self.in_channels == self.out_channels and self.stride == 1:
            ops.append(nn.Sequential(
                nn.Conv2d(self.in_channels, self.in_channels, 3, 
                         stride=self.stride, padding=1, groups=self.in_channels, bias=False),
                nn.Conv2d(self.in_channels, self.out_channels, 1, bias=False),
                nn.BatchNorm2d(self.out_channels),
                nn.ReLU(inplace=False)
            ))
        else:
            # ä¸èƒ½åˆ†ç»„æ—¶ä½¿ç”¨1x1å·ç§¯
            ops.append(nn.Sequential(
                nn.Conv2d(self.in_channels, self.out_channels, 1, 
                         stride=self.stride, bias=False),
                nn.BatchNorm2d(self.out_channels),
                nn.ReLU(inplace=False)
            ))
        
        # 3. æ‰©å¼ å·ç§¯
        ops.append(nn.Sequential(
            nn.Conv2d(self.in_channels, self.out_channels, 3, 
                     stride=self.stride, padding=2, dilation=2, bias=False),
            nn.BatchNorm2d(self.out_channels),
            nn.ReLU(inplace=False)
        ))
        
        # 4. åˆ†ç»„å·ç§¯
        groups = min(self.in_channels, self.out_channels, 8)
        if self.in_channels % groups == 0 and self.out_channels % groups == 0:
            ops.append(nn.Sequential(
                nn.Conv2d(self.in_channels, self.out_channels, 3, 
                         stride=self.stride, padding=1, groups=groups, bias=False),
                nn.BatchNorm2d(self.out_channels),
                nn.ReLU(inplace=False)
            ))
        else:
            # å›é€€åˆ°æ ‡å‡†å·ç§¯
            ops.append(nn.Sequential(
                nn.Conv2d(self.in_channels, self.out_channels, 3, 
                         stride=self.stride, padding=1, bias=False),
                nn.BatchNorm2d(self.out_channels),
                nn.ReLU(inplace=False)
            ))
        
        # 5. 5x5å·ç§¯ï¼ˆç”¨ä¸¤ä¸ª3x3è¿‘ä¼¼ï¼‰
        ops.append(nn.Sequential(
            nn.Conv2d(self.in_channels, self.out_channels, 3, 
                     stride=1, padding=1, bias=False),
            nn.BatchNorm2d(self.out_channels),
            nn.ReLU(inplace=False),
            nn.Conv2d(self.out_channels, self.out_channels, 3, 
                     stride=self.stride, padding=1, bias=False),
            nn.BatchNorm2d(self.out_channels),
            nn.ReLU(inplace=False)
        ))
        
        return ops
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­ - ASO-SEæ¶æ„æœç´¢"""
        # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
        self.performance_stats['forward_count'] += 1
        
        # ä½¿ç”¨Gumbel-Softmaxé€‰æ‹©æ“ä½œ
        op_weights = self.gumbel_selector.sample(self.alpha_ops)
        output = sum(w * op(x) for w, op in zip(op_weights, self.operations))
        
        # è·³è·ƒè¿æ¥é€‰æ‹©
        skip_weights = self.gumbel_selector.sample(self.alpha_skip)
        skip_output = sum(w * op(x) for w, op in zip(skip_weights, self.skip_ops))
        
        # èåˆä¸»è·¯å¾„å’Œè·³è·ƒè¿æ¥
        if skip_output.shape == output.shape:
            output = output + 0.3 * skip_output  # åŠ æƒèåˆ
        
        # å¹¶è¡Œåˆ†æ”¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if len(self.branches) > 0 and len(self.alpha_branches) > 0:
            branch_weights = F.softmax(self.alpha_branches, dim=0)
            branch_outputs = []
            
            for branch in self.branches:
                try:
                    branch_out = branch(x)
                    # å½¢çŠ¶åŒ¹é…
                    if branch_out.shape != output.shape:
                        branch_out = self._match_tensor_shape(branch_out, output)
                    branch_outputs.append(branch_out)
                except Exception as e:
                    print(f"Branch error: {e}")
                    branch_outputs.append(torch.zeros_like(output))
            
            if branch_outputs:
                branch_output = sum(w * out for w, out in zip(branch_weights, branch_outputs))
                output = output + 0.2 * branch_output  # åˆ†æ”¯è´¡çŒ®æƒé‡
        
        # æ›´æ–°è¾“å‡ºç»Ÿè®¡
        with torch.no_grad():
            self.performance_stats['avg_output_norm'] = 0.9 * self.performance_stats['avg_output_norm'] + 0.1 * output.norm().item()
        
        return output
    
    def _match_tensor_shape(self, source, target):
        """æ™ºèƒ½å¼ é‡å½¢çŠ¶åŒ¹é…"""
        if source.shape == target.shape:
            return source
        
        # ç©ºé—´ç»´åº¦åŒ¹é…
        if source.shape[2:] != target.shape[2:]:
            source = F.adaptive_avg_pool2d(source, target.shape[2:])
        
        # é€šé“ç»´åº¦åŒ¹é…
        if source.shape[1] != target.shape[1]:
            if not hasattr(self, '_channel_adapter'):
                self._channel_adapter = nn.Conv2d(
                    source.shape[1], target.shape[1], 1, bias=False
                ).to(source.device)
                
                # å‡½æ•°ä¿æŒåˆå§‹åŒ–
                with torch.no_grad():
                    if source.shape[1] <= target.shape[1]:
                        self._channel_adapter.weight.zero_()
                        min_ch = min(source.shape[1], target.shape[1])
                        for i in range(min_ch):
                            self._channel_adapter.weight[i, i, 0, 0] = 1.0
                    else:
                        # å¹³å‡æ± åŒ–æŠ•å½±
                        self._channel_adapter.weight.fill_(1.0 / source.shape[1])
            
            source = self._channel_adapter(source)
        
        return source
    
    def grow_branches(self, num_branches=1):
        """å¢åŠ å¹¶è¡Œåˆ†æ”¯ - çœŸæ­£çš„ç»“æ„ç”Ÿé•¿"""
        device = next(self.parameters()).device
        
        for _ in range(num_branches):
            # åˆ›å»ºæ–°åˆ†æ”¯
            branch = nn.Sequential(
                nn.Conv2d(self.in_channels, self.out_channels, 5, 
                         stride=self.stride, padding=2, bias=False),
                nn.BatchNorm2d(self.out_channels),
                nn.ReLU(inplace=False)
            ).to(device)
            
            # å‡½æ•°ä¿æŒåˆå§‹åŒ–
            self.fp_initializer.initialize_new_branch(branch)
            
            self.branches.append(branch)
        
        # æ›´æ–°åˆ†æ”¯æƒé‡å‚æ•°
        new_alpha_branches = torch.zeros(len(self.branches), device=device)
        if len(self.alpha_branches) > 0:
            new_alpha_branches[:len(self.alpha_branches)] = self.alpha_branches
        self.alpha_branches = nn.Parameter(new_alpha_branches)
        
        self.evolution_history.append({
            'type': 'branch_growth',
            'num_branches': num_branches,
            'total_branches': len(self.branches),
            'timestamp': time.time()
        })
        
        print(f"ğŸŒ¿ Block {self.block_id}: Added {num_branches} branches (total: {len(self.branches)})")
        return True
    
    def expand_channels(self, expansion_factor=1.5):
        """æ‰©å±•é€šé“æ•° - çœŸæ­£çš„å‚æ•°é‡å¢é•¿"""
        new_out_channels = int(self.out_channels * expansion_factor)
        if new_out_channels <= self.out_channels:
            return False
        
        device = next(self.parameters()).device
        old_channels = self.out_channels
        
        # æ‰©å±•æ‰€æœ‰æ“ä½œçš„è¾“å‡ºé€šé“
        for i, op in enumerate(self.operations):
            new_op = self._expand_operation_channels(op, new_out_channels)
            if new_op is not None:
                self.operations[i] = new_op.to(device)
        
        # æ‰©å±•è·³è·ƒè¿æ¥
        for i, skip_op in enumerate(self.skip_ops):
            if isinstance(skip_op, nn.Conv2d):
                new_skip = self._expand_conv_channels(skip_op, new_out_channels)
                if new_skip is not None:
                    self.skip_ops[i] = new_skip.to(device)
        
        # æ‰©å±•åˆ†æ”¯
        for i, branch in enumerate(self.branches):
            new_branch = self._expand_operation_channels(branch, new_out_channels)
            if new_branch is not None:
                self.branches[i] = new_branch.to(device)
        
        self.out_channels = new_out_channels
        
        self.evolution_history.append({
            'type': 'channel_expansion',
            'old_channels': old_channels,
            'new_channels': new_out_channels,
            'expansion_factor': expansion_factor,
            'timestamp': time.time()
        })
        
        print(f"ğŸŒ± Block {self.block_id}: Channels {old_channels}â†’{new_out_channels}")
        return True
    
    def _expand_operation_channels(self, operation, new_out_channels):
        """æ‰©å±•æ“ä½œçš„è¾“å‡ºé€šé“æ•°"""
        if isinstance(operation, nn.Sequential):
            new_layers = []
            for layer in operation:
                if isinstance(layer, nn.Conv2d):
                    new_conv = self._expand_conv_channels(layer, new_out_channels)
                    new_layers.append(new_conv if new_conv else layer)
                elif isinstance(layer, nn.BatchNorm2d):
                    new_bn = nn.BatchNorm2d(new_out_channels)
                    # å‚æ•°è¿ç§»
                    with torch.no_grad():
                        old_channels = layer.num_features
                        min_channels = min(old_channels, new_out_channels)
                        if hasattr(layer, 'weight') and layer.weight is not None:
                            new_bn.weight[:min_channels] = layer.weight[:min_channels]
                        if hasattr(layer, 'bias') and layer.bias is not None:
                            new_bn.bias[:min_channels] = layer.bias[:min_channels]
                        if hasattr(layer, 'running_mean'):
                            new_bn.running_mean[:min_channels] = layer.running_mean[:min_channels]
                        if hasattr(layer, 'running_var'):
                            new_bn.running_var[:min_channels] = layer.running_var[:min_channels]
                    new_layers.append(new_bn)
                else:
                    new_layers.append(layer)
            return nn.Sequential(*new_layers)
        
        return None
    
    def _expand_conv_channels(self, conv_layer, new_out_channels):
        """æ‰©å±•å·ç§¯å±‚çš„è¾“å‡ºé€šé“æ•°"""
        if not isinstance(conv_layer, nn.Conv2d):
            return None
        
        new_conv = nn.Conv2d(
            conv_layer.in_channels,
            new_out_channels,
            conv_layer.kernel_size,
            stride=conv_layer.stride,
            padding=conv_layer.padding,
            dilation=conv_layer.dilation,
            groups=conv_layer.groups if conv_layer.groups == 1 else min(conv_layer.groups, new_out_channels),
            bias=conv_layer.bias is not None
        )
        
        # å‡½æ•°ä¿æŒå‚æ•°è¿ç§»
        with torch.no_grad():
            old_out_channels = conv_layer.out_channels
            min_out_channels = min(old_out_channels, new_out_channels)
            
            # å¤åˆ¶æƒé‡
            new_conv.weight[:min_out_channels] = conv_layer.weight[:min_out_channels]
            
            # æ–°å¢é€šé“ç”¨å°éšæœºå€¼åˆå§‹åŒ–ï¼Œé¿å…ç ´åå‡½æ•°
            if new_out_channels > old_out_channels:
                nn.init.normal_(new_conv.weight[old_out_channels:], mean=0, std=0.01)
            
            # å¤åˆ¶åç½®
            if conv_layer.bias is not None:
                new_conv.bias[:min_out_channels] = conv_layer.bias[:min_out_channels]
        
        return new_conv
    
    def get_architecture_weights(self):
        """è·å–å½“å‰æ¶æ„æƒé‡ï¼ˆç”¨äºæ¶æ„å‚æ•°è®­ç»ƒï¼‰"""
        return {
            'alpha_ops': self.alpha_ops,
            'alpha_skip': self.alpha_skip,
            'alpha_branches': self.alpha_branches if len(self.alpha_branches) > 0 else None
        }

class ASOSEGrowingNetwork(nn.Module):
    """ASO-SEè‡ªç”Ÿé•¿ç¥ç»ç½‘ç»œ - å®Œæ•´çš„å››é˜¶æ®µè®­ç»ƒæ¡†æ¶"""
    
    def __init__(self, num_classes=10, initial_channels=32, initial_depth=4):
        super().__init__()
        
        self.num_classes = num_classes
        self.initial_channels = initial_channels
        self.current_depth = initial_depth
        
        # è¾“å…¥å¤„ç†
        self.stem = nn.Sequential(
            nn.Conv2d(3, initial_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(initial_channels),
            nn.ReLU(inplace=True)
        )
        
        # å¯æ¼”åŒ–å±‚
        self.layers = nn.ModuleList()
        self._build_initial_architecture()
        
        # å…¨å±€æ± åŒ–å’Œåˆ†ç±»å™¨
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.classifier = nn.Linear(self.layers[-1].out_channels, num_classes)
        
        # ASO-SEç»„ä»¶
        self.gumbel_selector = GumbelSoftmaxSelector()
        self.fp_initializer = FunctionPreservingInitializer()
        
        # è®­ç»ƒé˜¶æ®µçŠ¶æ€
        self.training_phase = "weight_training"  # weight_training, arch_training, mutation, retraining
        self.phase_epoch = 0
        self.cycle_count = 0
        
        # æ¶æ„æœç´¢å†å²
        self.architecture_history = []
        self.performance_history = []
        
        # ç”Ÿé•¿ç»Ÿè®¡
        self.growth_stats = {
            'depth_growths': 0,
            'channel_growths': 0, 
            'branch_growths': 0,
            'total_growths': 0,
            'parameter_evolution': []
        }
        
        # è®°å½•åˆå§‹çŠ¶æ€
        self._record_current_state("initialization")
        
        print(f"ğŸŒ± ASO-SE Network initialized:")
        print(f"   Depth: {self.current_depth}, Channels: {initial_channels}")
        print(f"   Parameters: {sum(p.numel() for p in self.parameters()):,}")
    
    def _build_initial_architecture(self):
        """æ„å»ºåˆå§‹å°ç½‘ç»œ"""
        current_channels = self.initial_channels
        
        for i in range(self.current_depth):
            # ä¸‹é‡‡æ ·ç­–ç•¥ï¼šåœ¨æ·±åº¦çš„1/3å’Œ2/3å¤„ä¸‹é‡‡æ ·
            stride = 2 if i in [self.current_depth//3, 2*self.current_depth//3] else 1
            out_channels = current_channels * (2 if stride == 2 else 1)
            
            block = AdvancedEvolvableBlock(
                current_channels, out_channels, f"layer_{i}", stride
            )
            self.layers.append(block)
            current_channels = out_channels
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        x = self.stem(x)
        
        for layer in self.layers:
            x = layer(x)
        
        x = self.global_pool(x)
        x = x.view(x.size(0), -1)
        return self.classifier(x)
    
    def set_training_phase(self, phase: str):
        """è®¾ç½®è®­ç»ƒé˜¶æ®µ"""
        valid_phases = ["weight_training", "arch_training", "mutation", "retraining"]
        if phase not in valid_phases:
            raise ValueError(f"Invalid phase: {phase}. Must be one of {valid_phases}")
        
        self.training_phase = phase
        self.phase_epoch = 0
        
        # é…ç½®Gumbel-Softmax
        for layer in self.layers:
            layer.gumbel_selector.training = (phase == "arch_training")
        
        print(f"ğŸ”„ Training phase: {phase}")
    
    def get_architecture_parameters(self):
        """è·å–æ‰€æœ‰æ¶æ„å‚æ•°"""
        arch_params = []
        for layer in self.layers:
            weights = layer.get_architecture_weights()
            arch_params.extend([weights['alpha_ops'], weights['alpha_skip']])
            if weights['alpha_branches'] is not None:
                arch_params.append(weights['alpha_branches'])
        return arch_params
    
    def get_weight_parameters(self):
        """è·å–æ‰€æœ‰ç½‘ç»œæƒé‡å‚æ•°ï¼ˆéæ¶æ„å‚æ•°ï¼‰"""
        weight_params = []
        arch_param_ids = {id(p) for p in self.get_architecture_parameters()}
        
        for param in self.parameters():
            if id(param) not in arch_param_ids:
                weight_params.append(param)
        
        return weight_params
    
    def grow_depth(self, position=None):
        """å¢åŠ ç½‘ç»œæ·±åº¦ - ASO-SEæ·±åº¦ç”Ÿé•¿"""
        if position is None:
            position = len(self.layers) - 1
        
        position = max(1, min(position, len(self.layers) - 1))
        
        # ç¡®å®šæ–°å±‚é…ç½®
        if position == 0:
            in_channels = self.initial_channels
            out_channels = self.layers[0].in_channels
        else:
            in_channels = self.layers[position-1].out_channels
            out_channels = self.layers[position].in_channels
        
        # åˆ›å»ºæ–°å±‚
        new_layer = AdvancedEvolvableBlock(
            in_channels, out_channels, f"grown_{len(self.layers)}", stride=1
        )
        
        # å‡½æ•°ä¿æŒåˆå§‹åŒ–
        self.fp_initializer.initialize_new_layer(new_layer)
        
        # è®¾å¤‡è¿ç§»
        device = next(self.parameters()).device
        new_layer = new_layer.to(device)
        
        # æ’å…¥å±‚
        self.layers.insert(position, new_layer)
        self.current_depth += 1
        
        # æ›´æ–°ç»Ÿè®¡
        self.growth_stats['depth_growths'] += 1
        self.growth_stats['total_growths'] += 1
        self._record_current_state("depth_growth")
        
        print(f"ğŸŒ± DEPTH GROWTH: Layer added at position {position}")
        print(f"   New depth: {self.current_depth}")
        print(f"   New parameters: {sum(p.numel() for p in self.parameters()):,}")
        
        return True
    
    def grow_width(self, layer_idx=None, expansion_factor=1.4):
        """å¢åŠ ç½‘ç»œå®½åº¦ - ASO-SEå®½åº¦ç”Ÿé•¿"""
        if layer_idx is None:
            layer_idx = len(self.layers) // 2
        
        if layer_idx >= len(self.layers):
            return False
        
        layer = self.layers[layer_idx]
        success = layer.expand_channels(expansion_factor)
        
        if success:
            # æ›´æ–°åç»­å±‚çš„è¾“å…¥é€šé“
            self._update_subsequent_layers(layer_idx)
            
            # æ›´æ–°ç»Ÿè®¡
            self.growth_stats['channel_growths'] += 1
            self.growth_stats['total_growths'] += 1
            self._record_current_state("width_growth")
            
            print(f"ğŸŒ± WIDTH GROWTH: Layer {layer_idx} expanded by {expansion_factor:.1f}x")
        
        return success
    
    def grow_branches(self, layer_idx=None, num_branches=1):
        """å¢åŠ åˆ†æ”¯ - ASO-SEåˆ†æ”¯ç”Ÿé•¿"""
        if layer_idx is None:
            layer_idx = np.random.randint(0, len(self.layers))
        
        if layer_idx >= len(self.layers):
            return False
        
        layer = self.layers[layer_idx]
        success = layer.grow_branches(num_branches)
        
        if success:
            # æ›´æ–°ç»Ÿè®¡
            self.growth_stats['branch_growths'] += 1
            self.growth_stats['total_growths'] += 1
            self._record_current_state("branch_growth")
            
            print(f"ğŸŒ± BRANCH GROWTH: Layer {layer_idx} added {num_branches} branches")
        
        return success
    
    def _update_subsequent_layers(self, start_idx):
        """æ›´æ–°åç»­å±‚çš„è¾“å…¥é€šé“æ•°"""
        if start_idx >= len(self.layers) - 1:
            return
        
        new_channels = self.layers[start_idx].out_channels
        
        for i in range(start_idx + 1, len(self.layers)):
            layer = self.layers[i]
            device = next(layer.parameters()).device
            
            # æ›´æ–°æ“ä½œçš„è¾“å…¥é€šé“
            for j, op in enumerate(layer.operations):
                new_op = self._update_operation_input_channels(op, new_channels)
                if new_op is not None:
                    layer.operations[j] = new_op.to(device)
            
            # æ›´æ–°è·³è·ƒè¿æ¥
            for j, skip_op in enumerate(layer.skip_ops):
                if isinstance(skip_op, nn.Conv2d):
                    new_skip = self._update_conv_input_channels(skip_op, new_channels)
                    if new_skip is not None:
                        layer.skip_ops[j] = new_skip.to(device)
            
            # æ›´æ–°åˆ†æ”¯
            for j, branch in enumerate(layer.branches):
                new_branch = self._update_operation_input_channels(branch, new_channels)
                if new_branch is not None:
                    layer.branches[j] = new_branch.to(device)
            
            layer.in_channels = new_channels
            new_channels = layer.out_channels
        
        # æ›´æ–°åˆ†ç±»å™¨
        final_channels = self.layers[-1].out_channels
        if self.classifier.in_features != final_channels:
            old_classifier = self.classifier
            self.classifier = nn.Linear(final_channels, self.num_classes)
            
            # å‚æ•°è¿ç§»
            with torch.no_grad():
                min_features = min(old_classifier.in_features, final_channels)
                self.classifier.weight[:, :min_features] = old_classifier.weight[:, :min_features]
                self.classifier.bias.copy_(old_classifier.bias)
            
            device = next(self.parameters()).device
            self.classifier = self.classifier.to(device)
    
    def _update_operation_input_channels(self, operation, new_in_channels):
        """æ›´æ–°æ“ä½œçš„è¾“å…¥é€šé“æ•°"""
        if isinstance(operation, nn.Sequential):
            new_layers = []
            for i, layer in enumerate(operation):
                if isinstance(layer, nn.Conv2d) and i == 0:  # åªæ›´æ–°ç¬¬ä¸€ä¸ªå·ç§¯å±‚çš„è¾“å…¥
                    new_conv = self._update_conv_input_channels(layer, new_in_channels)
                    new_layers.append(new_conv if new_conv else layer)
                else:
                    new_layers.append(layer)
            return nn.Sequential(*new_layers)
        return None
    
    def _update_conv_input_channels(self, conv_layer, new_in_channels):
        """æ›´æ–°å·ç§¯å±‚çš„è¾“å…¥é€šé“æ•°"""
        if not isinstance(conv_layer, nn.Conv2d):
            return None
        
        new_conv = nn.Conv2d(
            new_in_channels,
            conv_layer.out_channels,
            conv_layer.kernel_size,
            stride=conv_layer.stride,
            padding=conv_layer.padding,
            dilation=conv_layer.dilation,
            groups=min(conv_layer.groups, new_in_channels, conv_layer.out_channels),
            bias=conv_layer.bias is not None
        )
        
        # å‡½æ•°ä¿æŒå‚æ•°è¿ç§»
        with torch.no_grad():
            old_in_channels = conv_layer.in_channels
            min_in_channels = min(old_in_channels, new_in_channels)
            
            # å¤åˆ¶æƒé‡
            new_conv.weight[:, :min_in_channels] = conv_layer.weight[:, :min_in_channels]
            
            # æ–°å¢è¾“å…¥é€šé“ç”¨å°å€¼åˆå§‹åŒ–
            if new_in_channels > old_in_channels:
                nn.init.normal_(new_conv.weight[:, old_in_channels:], mean=0, std=0.01)
            
            # å¤åˆ¶åç½®
            if conv_layer.bias is not None:
                new_conv.bias.copy_(conv_layer.bias)
        
        return new_conv
    
    def _record_current_state(self, event_type):
        """è®°å½•å½“å‰ç½‘ç»œçŠ¶æ€"""
        state = {
            'event': event_type,
            'timestamp': time.time(),
            'depth': self.current_depth,
            'parameters': sum(p.numel() for p in self.parameters()),
            'growth_stats': self.growth_stats.copy(),
            'training_phase': self.training_phase
        }
        self.growth_stats['parameter_evolution'].append(state)
        
        # è®°å½•æ¶æ„æƒé‡çŠ¶æ€ï¼ˆç”¨äºåˆ†æï¼‰
        arch_weights = {}
        for i, layer in enumerate(self.layers):
            weights = layer.get_architecture_weights()
            arch_weights[f'layer_{i}'] = {
                'alpha_ops': weights['alpha_ops'].detach().cpu().numpy().tolist(),
                'alpha_skip': weights['alpha_skip'].detach().cpu().numpy().tolist()
            }
        state['architecture_weights'] = arch_weights
        
        self.architecture_history.append(state)
    
    def anneal_gumbel_temperature(self):
        """é€€ç«æ‰€æœ‰å±‚çš„Gumbelæ¸©åº¦"""
        temps = []
        for layer in self.layers:
            temp = layer.gumbel_selector.anneal_temperature()
            temps.append(temp)
        return sum(temps) / len(temps) if temps else 0
    
    def get_dominant_architecture(self):
        """è·å–å½“å‰å ä¸»å¯¼åœ°ä½çš„æ¶æ„"""
        arch_description = []
        
        for i, layer in enumerate(self.layers):
            weights = layer.get_architecture_weights()
            
            # ä¸»æ“ä½œ
            dominant_op = weights['alpha_ops'].argmax().item()
            op_prob = F.softmax(weights['alpha_ops'], dim=0)[dominant_op].item()
            
            # è·³è·ƒè¿æ¥
            dominant_skip = weights['alpha_skip'].argmax().item()
            skip_prob = F.softmax(weights['alpha_skip'], dim=0)[dominant_skip].item()
            
            arch_description.append({
                'layer': i,
                'dominant_op': dominant_op,
                'op_confidence': op_prob,
                'dominant_skip': dominant_skip,
                'skip_confidence': skip_prob,
                'num_branches': len(layer.branches)
            })
        
        return arch_description
    
    def get_architecture_summary(self):
        """è·å–å®Œæ•´æ¶æ„æ‘˜è¦"""
        return {
            'depth': self.current_depth,
            'total_parameters': sum(p.numel() for p in self.parameters()),
            'growth_stats': self.growth_stats,
            'training_phase': self.training_phase,
            'cycle_count': self.cycle_count,
            'dominant_architecture': self.get_dominant_architecture(),
            'layer_details': [
                {
                    'id': layer.block_id,
                    'in_channels': layer.in_channels,
                    'out_channels': layer.out_channels,
                    'num_operations': len(layer.operations),
                    'num_branches': len(layer.branches),
                    'evolution_history': layer.evolution_history
                } for layer in self.layers
            ]
        }

class ASOSETrainingController:
    """ASO-SEå››é˜¶æ®µè®­ç»ƒæ§åˆ¶å™¨"""
    
    def __init__(self):
        # å››é˜¶æ®µé…ç½®
        self.phase_config = {
            'weight_training': {'epochs': 8, 'lr': 0.025},
            'arch_training': {'epochs': 3, 'lr': 3e-4},
            'mutation': {'epochs': 1, 'lr': 0.01},
            'retraining': {'epochs': 6, 'lr': 0.02}
        }
        
        # ç”Ÿé•¿å†³ç­–
        self.growth_decisions = []
        self.performance_trend = []
        self.last_growth_cycle = -1
        
        # ç”Ÿé•¿ç­–ç•¥æƒé‡ï¼ˆåŠ¨æ€è°ƒæ•´ï¼‰
        self.growth_strategy_weights = {
            'grow_depth': 1.0,
            'grow_width': 1.0,
            'grow_branches': 0.8
        }
        
    def should_trigger_growth(self, network, current_cycle, current_accuracy, accuracy_trend):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è§¦å‘ç”Ÿé•¿"""
        # æ¯3-4ä¸ªå‘¨æœŸå¿…é¡»ç”Ÿé•¿ä¸€æ¬¡
        if current_cycle - self.last_growth_cycle >= 4:
            print(f"ğŸŒ± Forced growth trigger (cycle {current_cycle})")
            return True
        
        # æ€§èƒ½åœæ»æ£€æµ‹
        if len(accuracy_trend) >= 3:
            recent_improvement = max(accuracy_trend[-3:]) - min(accuracy_trend[-3:])
            if recent_improvement < 0.5 and current_cycle - self.last_growth_cycle >= 2:
                print(f"ğŸŒ± Stagnation growth trigger (improvement: {recent_improvement:.2f}%)")
                return True
        
        # åŸºäºæ€§èƒ½é˜ˆå€¼çš„è‡ªé€‚åº”ç”Ÿé•¿
        growth_thresholds = {
            30: 2,   # 30%ä»¥ä¸‹ï¼Œæ¯2å‘¨æœŸç”Ÿé•¿
            60: 3,   # 30-60%ï¼Œæ¯3å‘¨æœŸç”Ÿé•¿
            80: 4,   # 60-80%ï¼Œæ¯4å‘¨æœŸç”Ÿé•¿
            95: 5    # 80%+ï¼Œæ¯5å‘¨æœŸç”Ÿé•¿
        }
        
        for threshold, interval in growth_thresholds.items():
            if current_accuracy < threshold:
                if current_cycle - self.last_growth_cycle >= interval:
                    print(f"ğŸŒ± Performance-based growth trigger (acc: {current_accuracy:.1f}%)")
                    return True
                break
        
        return False
    
    def select_growth_strategy(self, network, current_accuracy, cycle_count):
        """é€‰æ‹©ç”Ÿé•¿ç­–ç•¥ - åŸºäºæ€§èƒ½å’Œç½‘ç»œçŠ¶æ€"""
        current_depth = network.current_depth
        total_params = sum(p.numel() for p in network.parameters())
        
        strategies = []
        
        # åŸºäºæ€§èƒ½é˜¶æ®µçš„ç­–ç•¥é€‰æ‹©
        if current_accuracy < 40:
            # ä½æ€§èƒ½ï¼šç§¯æå¢åŠ ç½‘ç»œå®¹é‡
            if current_depth < 10:
                strategies.extend(['grow_depth'] * 3)
            strategies.extend(['grow_width'] * 2)
            strategies.append('grow_branches')
            
        elif current_accuracy < 70:
            # ä¸­ç­‰æ€§èƒ½ï¼šå¹³è¡¡å‘å±•
            if current_depth < 12:
                strategies.extend(['grow_depth'] * 2)
            strategies.extend(['grow_width'] * 2)
            strategies.extend(['grow_branches'] * 2)
            
        elif current_accuracy < 85:
            # è¾ƒé«˜æ€§èƒ½ï¼šç²¾ç»†è°ƒä¼˜
            if current_depth < 15:
                strategies.append('grow_depth')
            strategies.extend(['grow_width', 'grow_branches'] * 2)
            
        else:
            # é«˜æ€§èƒ½ï¼šåˆ†æ”¯æ¢ç´¢ä¸ºä¸»
            strategies.extend(['grow_branches'] * 3)
            if current_depth < 18:
                strategies.append('grow_depth')
            strategies.append('grow_width')
        
        # å‚æ•°é‡é™åˆ¶
        if total_params > 800000:  # 80ä¸‡å‚æ•°é™åˆ¶
            strategies = [s for s in strategies if s != 'grow_depth']
        if total_params > 1200000:  # 120ä¸‡å‚æ•°é™åˆ¶
            strategies = ['grow_branches']
        
        # åº”ç”¨ç­–ç•¥æƒé‡
        weighted_strategies = []
        for strategy in strategies:
            weight = self.growth_strategy_weights.get(strategy, 1.0)
            weighted_strategies.extend([strategy] * max(1, int(weight * 2)))
        
        if not weighted_strategies:
            weighted_strategies = ['grow_branches']  # ä¿åº•ç­–ç•¥
        
        selected = np.random.choice(weighted_strategies)
        
        print(f"ğŸ¯ Growth strategy: {selected}")
        print(f"   Network state: depth={current_depth}, params={total_params:,}")
        print(f"   Strategy weights: {self.growth_strategy_weights}")
        
        return selected
    
    def execute_growth(self, network, strategy, cycle_count):
        """æ‰§è¡Œç”Ÿé•¿ç­–ç•¥"""
        success = False
        growth_details = {}
        
        try:
            pre_growth_params = sum(p.numel() for p in network.parameters())
            pre_growth_depth = network.current_depth
            
            if strategy == 'grow_depth':
                # æ™ºèƒ½é€‰æ‹©æ’å…¥ä½ç½®
                position = self._select_optimal_depth_position(network)
                success = network.grow_depth(position)
                growth_details['position'] = position
                
            elif strategy == 'grow_width':
                # é€‰æ‹©æœ€éœ€è¦æ‰©å±•çš„å±‚
                layer_idx = self._select_optimal_width_layer(network)
                expansion_factor = np.random.uniform(1.3, 1.6)
                success = network.grow_width(layer_idx, expansion_factor)
                growth_details.update({'layer_idx': layer_idx, 'expansion_factor': expansion_factor})
                
            elif strategy == 'grow_branches':
                # é€‰æ‹©åˆé€‚çš„å±‚æ·»åŠ åˆ†æ”¯
                layer_idx = self._select_optimal_branch_layer(network)
                num_branches = np.random.randint(1, 3)
                success = network.grow_branches(layer_idx, num_branches)
                growth_details.update({'layer_idx': layer_idx, 'num_branches': num_branches})
            
            if success:
                self.last_growth_cycle = cycle_count
                
                post_growth_params = sum(p.numel() for p in network.parameters())
                post_growth_depth = network.current_depth
                
                decision = {
                    'strategy': strategy,
                    'cycle': cycle_count,
                    'timestamp': time.time(),
                    'pre_growth': {'depth': pre_growth_depth, 'params': pre_growth_params},
                    'post_growth': {'depth': post_growth_depth, 'params': post_growth_params},
                    'details': growth_details,
                    'param_increase': post_growth_params - pre_growth_params
                }
                self.growth_decisions.append(decision)
                
                # æ›´æ–°ç­–ç•¥æƒé‡ï¼ˆæˆåŠŸçš„ç­–ç•¥æƒé‡å¢åŠ ï¼‰
                self.growth_strategy_weights[strategy] *= 1.1
                
                print(f"âœ… Growth executed successfully!")
                print(f"   Depth: {pre_growth_depth} â†’ {post_growth_depth}")
                print(f"   Parameters: {pre_growth_params:,} â†’ {post_growth_params:,}")
                print(f"   Increase: +{post_growth_params - pre_growth_params:,}")
            else:
                # å¤±è´¥çš„ç­–ç•¥æƒé‡é™ä½
                self.growth_strategy_weights[strategy] *= 0.9
                
        except Exception as e:
            print(f"âŒ Growth failed: {e}")
            self.growth_strategy_weights.get(strategy, 1.0) * 0.8
            success = False
        
        return success
    
    def _select_optimal_depth_position(self, network):
        """é€‰æ‹©æœ€ä¼˜çš„æ·±åº¦æ’å…¥ä½ç½®"""
        # åœ¨ç½‘ç»œååŠéƒ¨åˆ†æ’å…¥ï¼Œé¿å…ç ´åæ—©æœŸç‰¹å¾æå–
        return max(1, len(network.layers) * 2 // 3)
    
    def _select_optimal_width_layer(self, network):
        """é€‰æ‹©æœ€é€‚åˆå®½åº¦æ‰©å±•çš„å±‚"""
        # ä¼˜å…ˆé€‰æ‹©ä¸­é—´å±‚ï¼Œå‚æ•°æ•ˆç‡æ›´é«˜
        return len(network.layers) // 2
    
    def _select_optimal_branch_layer(self, network):
        """é€‰æ‹©æœ€é€‚åˆæ·»åŠ åˆ†æ”¯çš„å±‚"""
        # éšæœºé€‰æ‹©ï¼Œä½†é¿å…æœ€åä¸€å±‚
        return np.random.randint(0, max(1, len(network.layers) - 1))

class AdvancedDataAugmentation:
    """é«˜çº§æ•°æ®å¢å¼ºç­–ç•¥ - å†²å‡»95%å‡†ç¡®ç‡"""
    
    @staticmethod
    def get_train_transforms():
        """è®­ç»ƒæ—¶çš„å¼ºåŒ–æ•°æ®å¢å¼º"""
        return transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
            transforms.RandomRotation(degrees=15),
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
            transforms.RandomErasing(p=0.25, scale=(0.02, 0.33), ratio=(0.3, 3.3))
        ])
    
    @staticmethod
    def get_test_transforms():
        """æµ‹è¯•æ—¶çš„æ ‡å‡†åŒ–"""
        return transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])

class ASOSETrainer:
    """ASO-SEå®Œæ•´è®­ç»ƒå™¨ - å››é˜¶æ®µå¾ªç¯è®­ç»ƒ"""
    
    def __init__(self, experiment_name="aso_se_95"):
        self.experiment_name = experiment_name
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # æ ¸å¿ƒç»„ä»¶
        self.network = None
        self.training_controller = ASOSETrainingController()
        self.evolution_manager = EvolutionCheckpointManager(experiment_name)
        
        # ä¼˜åŒ–å™¨ï¼ˆå°†åŠ¨æ€åˆ›å»ºï¼‰
        self.weight_optimizer = None
        self.arch_optimizer = None
        self.current_optimizer = None
        self.scheduler = None
        
        # è®­ç»ƒçŠ¶æ€
        self.current_cycle = 0
        self.current_phase = "weight_training"
        self.phase_epoch = 0
        self.best_accuracy = 0.0
        
        # å†å²è®°å½•
        self.training_history = []
        self.cycle_results = []
        
        print(f"ğŸŒ± ASO-SE Trainer initialized on {self.device}")
        print(f"ğŸ“Š Target: CIFAR-10 95%+ accuracy")
    
    def setup_data(self, batch_size=128):
        """è®¾ç½®é«˜è´¨é‡æ•°æ®åŠ è½½å™¨"""
        print("ğŸ“Š Setting up enhanced CIFAR-10 data...")
        
        train_transform = AdvancedDataAugmentation.get_train_transforms()
        test_transform = AdvancedDataAugmentation.get_test_transforms()
        
        train_dataset = torchvision.datasets.CIFAR10(
            './data', train=True, download=True, transform=train_transform
        )
        test_dataset = torchvision.datasets.CIFAR10(
            './data', train=False, transform=test_transform
        )
        
        self.train_loader = DataLoader(
            train_dataset, batch_size=batch_size, shuffle=True, 
            num_workers=4, pin_memory=True, persistent_workers=True
        )
        self.test_loader = DataLoader(
            test_dataset, batch_size=batch_size, shuffle=False,
            num_workers=4, pin_memory=True, persistent_workers=True
        )
        
        print(f"âœ… Data ready: {len(train_dataset)} train, {len(test_dataset)} test")
        print(f"   Batch size: {batch_size}, Workers: 4")
    
    def setup_network(self, initial_channels=32, initial_depth=4):
        """è®¾ç½®ASO-SEç½‘ç»œ"""
        self.network = ASOSEGrowingNetwork(
            num_classes=10,
            initial_channels=initial_channels,
            initial_depth=initial_depth
        ).to(self.device)
        
        self._create_optimizers()
        
        total_params = sum(p.numel() for p in self.network.parameters())
        print(f"ğŸ“Š ASO-SE Network ready: {total_params:,} parameters")
    
    def _create_optimizers(self):
        """åˆ›å»ºä¸“ç”¨ä¼˜åŒ–å™¨"""
        # æƒé‡å‚æ•°ä¼˜åŒ–å™¨
        weight_params = self.network.get_weight_parameters()
        self.weight_optimizer = optim.SGD(
            weight_params, lr=0.025, momentum=0.9, weight_decay=1e-4
        )
        
        # æ¶æ„å‚æ•°ä¼˜åŒ–å™¨
        arch_params = self.network.get_architecture_parameters()
        if arch_params:
            self.arch_optimizer = optim.Adam(arch_params, lr=3e-4, weight_decay=1e-3)
        
        # å½“å‰ä½¿ç”¨çš„ä¼˜åŒ–å™¨
        self.current_optimizer = self.weight_optimizer
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.current_optimizer, T_0=50, T_mult=2, eta_min=1e-6
        )
    
    def train_epoch(self, epoch, phase):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.network.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        # é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–å™¨
        if phase == "arch_training":
            optimizer = self.arch_optimizer
            # å†»ç»“æƒé‡å‚æ•°
            for param in self.network.get_weight_parameters():
                param.requires_grad = False
            for param in self.network.get_architecture_parameters():
                param.requires_grad = True
        else:
            optimizer = self.weight_optimizer
            # è®­ç»ƒæƒé‡å‚æ•°
            for param in self.network.get_weight_parameters():
                param.requires_grad = True
            for param in self.network.get_architecture_parameters():
                param.requires_grad = False
        
        criterion = nn.CrossEntropyLoss()
        
        pbar = tqdm(self.train_loader, desc=f"ğŸš€ {phase} Epoch {epoch:02d}")
        
        for batch_idx, (data, target) in enumerate(pbar):
            data, target = data.to(self.device), target.to(self.device)
            
            optimizer.zero_grad()
            output = self.network(data)
            loss = criterion(output, target)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            if phase == "arch_training":
                torch.nn.utils.clip_grad_norm_(self.network.get_architecture_parameters(), 5.0)
            else:
                torch.nn.utils.clip_grad_norm_(self.network.get_weight_parameters(), 5.0)
            
            optimizer.step()
            
            total_loss += loss.item()
            pred = output.argmax(dim=1)
            correct += pred.eq(target).sum().item()
            total += target.size(0)
            
            # å®æ—¶æ˜¾ç¤º
            if batch_idx % 50 == 0:
                arch_summary = self.network.get_architecture_summary()
                pbar.set_postfix({
                    'Loss': f'{total_loss/(batch_idx+1):.4f}',
                    'Acc': f'{100.*correct/total:.2f}%',
                    'Depth': arch_summary['depth'],
                    'Params': f'{arch_summary["total_parameters"]:,}',
                    'Phase': phase.split('_')[0],
                    'Cycle': self.current_cycle
                })
        
        return total_loss/len(self.train_loader), 100.*correct/total
    
    def validate(self):
        """éªŒè¯"""
        self.network.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        criterion = nn.CrossEntropyLoss()
        
        with torch.no_grad():
            for data, target in self.test_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.network(data)
                loss = criterion(output, target)
                
                total_loss += loss.item()
                pred = output.argmax(dim=1)
                correct += pred.eq(target).sum().item()
                total += target.size(0)
        
        return total_loss/len(self.test_loader), 100.*correct/total
    
    def run_training_cycle(self):
        """è¿è¡Œä¸€ä¸ªå®Œæ•´çš„ASO-SEå››é˜¶æ®µè®­ç»ƒå‘¨æœŸ"""
        cycle_start_time = time.time()
        cycle_results = {}
        
        print(f"\n{'='*80}")
        print(f"ğŸ”„ ASO-SE Training Cycle {self.current_cycle + 1}")
        print(f"{'='*80}")
        
        # é˜¶æ®µ1: æƒé‡é¢„çƒ­
        print(f"\nğŸ”¥ Phase 1: Weight Training (Preheating)")
        self.network.set_training_phase("weight_training")
        weight_results = self._run_phase("weight_training", 8)
        cycle_results['weight_training'] = weight_results
        
        # é˜¶æ®µ2: æ¶æ„å‚æ•°å­¦ä¹ 
        print(f"\nğŸ§  Phase 2: Architecture Training (Structure Search)")
        self.network.set_training_phase("arch_training")
        arch_results = self._run_phase("arch_training", 3)
        cycle_results['arch_training'] = arch_results
        
        # é˜¶æ®µ3: æ¶æ„çªå˜ä¸ç¨³å®š
        print(f"\nğŸ§¬ Phase 3: Architecture Mutation (Gumbel-Softmax Exploration)")
        mutation_success = self._architecture_mutation()
        cycle_results['mutation_success'] = mutation_success
        
        # é˜¶æ®µ4: æƒé‡å†é€‚åº”
        print(f"\nğŸ”§ Phase 4: Weight Retraining (Adaptation)")
        self.network.set_training_phase("retraining")
        retrain_results = self._run_phase("retraining", 6)
        cycle_results['retraining'] = retrain_results
        
        cycle_time = time.time() - cycle_start_time
        cycle_results['cycle_time'] = cycle_time
        cycle_results['final_accuracy'] = retrain_results['final_test_acc']
        
        self.cycle_results.append(cycle_results)
        
        print(f"\nâœ… Cycle {self.current_cycle + 1} completed in {cycle_time/60:.1f} minutes")
        print(f"   Final accuracy: {cycle_results['final_accuracy']:.2f}%")
        print(f"   Best so far: {self.best_accuracy:.2f}%")
        
        return cycle_results
    
    def _run_phase(self, phase_name, num_epochs):
        """è¿è¡Œè®­ç»ƒé˜¶æ®µ"""
        phase_results = {'epochs': [], 'final_train_acc': 0, 'final_test_acc': 0}
        
        for epoch in range(num_epochs):
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(epoch, phase_name)
            
            # éªŒè¯
            test_loss, test_acc = self.validate()
            
            # æ›´æ–°å­¦ä¹ ç‡
            if phase_name != "arch_training":
                self.scheduler.step()
            
            # è®°å½•ç»“æœ
            epoch_result = {
                'epoch': epoch,
                'train_loss': train_loss,
                'train_acc': train_acc,
                'test_loss': test_loss,
                'test_acc': test_acc,
                'lr': self.current_optimizer.param_groups[0]['lr']
            }
            phase_results['epochs'].append(epoch_result)
            
            # æ›´æ–°æœ€ä½³æ€§èƒ½
            if test_acc > self.best_accuracy:
                self.best_accuracy = test_acc
            
            # Gumbelæ¸©åº¦é€€ç«
            if phase_name == "arch_training":
                avg_temp = self.network.anneal_gumbel_temperature()
                epoch_result['gumbel_temp'] = avg_temp
            
            print(f"   Epoch {epoch+1}: Train={train_acc:.2f}%, Test={test_acc:.2f}%, Best={self.best_accuracy:.2f}%")
        
        phase_results['final_train_acc'] = phase_results['epochs'][-1]['train_acc']
        phase_results['final_test_acc'] = phase_results['epochs'][-1]['test_acc']
        
        return phase_results
    
    def _architecture_mutation(self):
        """æ¶æ„çªå˜é˜¶æ®µ - Gumbel-Softmaxå¼•å¯¼çš„æ™ºèƒ½ç”Ÿé•¿"""
        # åˆ†æå½“å‰æ€§èƒ½è¶‹åŠ¿
        recent_accuracies = [result['final_accuracy'] for result in self.cycle_results[-3:]]
        if len(recent_accuracies) < 3:
            recent_accuracies = [50.0]  # é»˜è®¤å€¼
        
        current_accuracy = recent_accuracies[-1] if recent_accuracies else 50.0
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦ç”Ÿé•¿
        should_grow = self.training_controller.should_trigger_growth(
            self.network, self.current_cycle, current_accuracy, recent_accuracies
        )
        
        if should_grow:
            print("ğŸŒ± Triggering network growth...")
            
            # é€‰æ‹©ç”Ÿé•¿ç­–ç•¥
            strategy = self.training_controller.select_growth_strategy(
                self.network, current_accuracy, self.current_cycle
            )
            
            # ä¿å­˜ç”Ÿé•¿å‰çŠ¶æ€
            pre_growth_state = self.network.get_architecture_summary()
            
            # æ‰§è¡Œç”Ÿé•¿
            success = self.training_controller.execute_growth(
                self.network, strategy, self.current_cycle
            )
            
            if success:
                # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆå‚æ•°å¯èƒ½å˜åŒ–ï¼‰
                self._create_optimizers()
                
                print("ğŸ‰ Network growth successful!")
                print("   Updated optimizers for new parameters")
                
                return True
            else:
                print("âŒ Network growth failed")
                return False
        else:
            print("ğŸ”„ No growth triggered this cycle")
            
            # å³ä½¿ä¸ç”Ÿé•¿ï¼Œä¹Ÿè¿›è¡ŒGumbel-Softmaxæ¢ç´¢
            print("ğŸ² Performing Gumbel-Softmax architecture exploration...")
            avg_temp = self.network.anneal_gumbel_temperature()
            print(f"   Current Gumbel temperature: {avg_temp:.3f}")
            
            return False
    
    def train(self, max_cycles=20, initial_channels=32, initial_depth=4, batch_size=128, resume_from=None):
        """ä¸»è®­ç»ƒæµç¨‹ - ASO-SEå®Œæ•´å››é˜¶æ®µå¾ªç¯"""
        print(f"\nğŸŒ± ASO-SE Training Started")
        print(f"ğŸ¯ Target: CIFAR-10 95%+ accuracy")
        print(f"âš™ï¸  Config: max_cycles={max_cycles}, channels={initial_channels}, depth={initial_depth}")
        
        start_time = time.time()
        
        # è®¾ç½®æ•°æ®å’Œç½‘ç»œ
        self.setup_data(batch_size)
        self.setup_network(initial_channels, initial_depth)
        
        # æ¢å¤è®­ç»ƒï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if resume_from:
            print(f"ğŸ”„ Resuming from checkpoint: {resume_from}")
            # TODO: å®ç°æ¢å¤é€»è¾‘
        
        try:
            # ä¸»è®­ç»ƒå¾ªç¯
            for cycle in range(max_cycles):
                self.current_cycle = cycle
                
                # è¿è¡Œä¸€ä¸ªå®Œæ•´å‘¨æœŸ
                cycle_result = self.run_training_cycle()
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
                if cycle_result['final_accuracy'] >= 95.0:
                    print(f"\nğŸ‰ TARGET ACHIEVED! Accuracy: {cycle_result['final_accuracy']:.2f}%")
                    break
                
                # æ—©åœæ£€æŸ¥
                if self._should_early_stop():
                    print(f"\nâ¹ï¸  Early stopping triggered")
                    break
                
                # æ˜¾ç¤ºè¿›åº¦æ‘˜è¦
                self._display_progress_summary()
        
        except KeyboardInterrupt:
            print("\nâ¹ï¸  Training interrupted by user")
        
        except Exception as e:
            print(f"\nâŒ Training error: {e}")
            raise
        
        finally:
            # è®­ç»ƒå®Œæˆæ€»ç»“
            total_time = time.time() - start_time
            self._display_final_summary(total_time)
    
    def _should_early_stop(self):
        """æ—©åœæ£€æŸ¥"""
        if len(self.cycle_results) < 5:
            return False
        
        # æ£€æŸ¥æœ€è¿‘5ä¸ªå‘¨æœŸçš„æ”¹è¿›
        recent_accs = [r['final_accuracy'] for r in self.cycle_results[-5:]]
        improvement = max(recent_accs) - min(recent_accs)
        
        return improvement < 0.5  # 5ä¸ªå‘¨æœŸå†…æ”¹è¿›ä¸åˆ°0.5%
    
    def _display_progress_summary(self):
        """æ˜¾ç¤ºè¿›åº¦æ‘˜è¦"""
        print(f"\nğŸ“Š Progress Summary (Cycle {self.current_cycle + 1}):")
        
        if len(self.cycle_results) >= 3:
            recent_results = self.cycle_results[-3:]
            accs = [r['final_accuracy'] for r in recent_results]
            
            print(f"   Recent accuracies: {accs}")
            print(f"   Trend: {accs[-1] - accs[0]:+.2f}% over 3 cycles")
        
        arch_summary = self.network.get_architecture_summary()
        print(f"   Current network: {arch_summary['depth']} layers, {arch_summary['total_parameters']:,} params")
        print(f"   Growth stats: {arch_summary['growth_stats']}")
        
        # æ˜¾ç¤ºå ä¸»å¯¼åœ°ä½çš„æ¶æ„
        dominant_arch = self.network.get_dominant_architecture()
        print(f"   Dominant operations: {[layer['dominant_op'] for layer in dominant_arch[:5]]}")
    
    def _display_final_summary(self, total_time):
        """æ˜¾ç¤ºæœ€ç»ˆæ€»ç»“"""
        print(f"\n{'='*80}")
        print(f"ğŸ‰ ASO-SE Training Completed!")
        print(f"{'='*80}")
        
        print(f"â±ï¸  Total time: {total_time/3600:.1f} hours ({total_time/60:.1f} minutes)")
        print(f"ğŸ”„ Total cycles: {len(self.cycle_results)}")
        print(f"ğŸ† Best accuracy: {self.best_accuracy:.2f}%")
        
        if self.cycle_results:
            final_result = self.cycle_results[-1]
            print(f"ğŸ“Š Final accuracy: {final_result['final_accuracy']:.2f}%")
        
        arch_summary = self.network.get_architecture_summary()
        print(f"ğŸ—ï¸  Final architecture:")
        print(f"   Depth: {arch_summary['depth']} layers")
        print(f"   Parameters: {arch_summary['total_parameters']:,}")
        print(f"   Total growths: {arch_summary['growth_stats']['total_growths']}")
        
        print(f"\nğŸ§¬ Growth breakdown:")
        growth_stats = arch_summary['growth_stats']
        for growth_type in ['depth_growths', 'channel_growths', 'branch_growths']:
            print(f"   {growth_type}: {growth_stats[growth_type]}")
        
        # æ˜¾ç¤ºæœ€ç»ˆå ä¸»å¯¼åœ°ä½çš„æ¶æ„
        print(f"\nğŸ¯ Final dominant architecture:")
        dominant_arch = self.network.get_dominant_architecture()
        for i, layer in enumerate(dominant_arch[:8]):  # æ˜¾ç¤ºå‰8å±‚
            print(f"   Layer {i}: Op{layer['dominant_op']}({layer['op_confidence']:.2f}), Skip{layer['dominant_skip']}({layer['skip_confidence']:.2f}), Branches{layer['num_branches']}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_checkpoint = self.evolution_manager.save_checkpoint(
            network=self.network,
            optimizer=self.weight_optimizer,
            scheduler=self.scheduler,
            epoch=self.current_cycle,
            training_stats={'best_accuracy': self.best_accuracy},
            growth_type="final_model"
        )
        print(f"ğŸ’¾ Final model saved: {final_checkpoint}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ASO-SE Neural Network Training')
    parser.add_argument('--cycles', type=int, default=25, help='Maximum training cycles')
    parser.add_argument('--batch_size', type=int, default=128, help='Batch size')
    parser.add_argument('--initial_channels', type=int, default=32, help='Initial channels')
    parser.add_argument('--initial_depth', type=int, default=4, help='Initial depth')
    parser.add_argument('--experiment', type=str, default='aso_se_95', help='Experiment name')
    parser.add_argument('--resume_from', type=str, default=None, help='Resume from checkpoint')
    
    args = parser.parse_args()
    
    print("ğŸ§¬ ASO-SE: Alternating Stable Optimization with Stochastic Exploration")
    print("ğŸ¯ Target: CIFAR-10 95%+ Accuracy with True Neural Architecture Growth")
    print(f"â° Start: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“‹ Config: {vars(args)}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = ASOSETrainer(args.experiment)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(
        max_cycles=args.cycles,
        initial_channels=args.initial_channels,
        initial_depth=args.initial_depth,
        batch_size=args.batch_size,
        resume_from=args.resume_from
    )

if __name__ == "__main__":
    main() 