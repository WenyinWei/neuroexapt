#!/usr/bin/env python3
"""
è‡ªé€‚åº”ç¥ç»ç½‘ç»œç”Ÿé•¿ç³»ç»Ÿ - çœŸæ­£çš„ç»“æ„å˜åŒ–

ğŸŒ± æ ¸å¿ƒç†å¿µï¼šä»å°ç½‘ç»œå¼€å§‹ï¼ŒåŸºäºæ€§èƒ½éœ€æ±‚çœŸæ­£ç”Ÿé•¿
- ä¸æ˜¯æœç´¢æ“ä½œï¼Œè€Œæ˜¯å¢åŠ ç»“æ„
- å±‚æ•°çœŸæ­£ä»3â†’4â†’5â†’6å±‚å¢é•¿
- å‚æ•°é‡æ˜¾è‘—å¢åŠ ï¼š1ä¸‡â†’3ä¸‡â†’8ä¸‡â†’20ä¸‡
- æ¯æ¬¡ç”Ÿé•¿éƒ½æ˜¯ç»“æ„æ€§çš„æ”¹å˜

ğŸ¯ ç”Ÿé•¿ç­–ç•¥ï¼š
1. æ·±åº¦ç”Ÿé•¿ï¼šåœ¨ç½‘ç»œä¸­æ’å…¥æ–°çš„å·ç§¯å±‚
2. å®½åº¦ç”Ÿé•¿ï¼šæ‰©å±•ç°æœ‰å±‚çš„é€šé“æ•°
3. åˆ†æ”¯ç”Ÿé•¿ï¼šå¢åŠ å¹¶è¡Œå¤„ç†åˆ†æ”¯
4. æ™ºèƒ½å†³ç­–ï¼šåŸºäºæ€§èƒ½ç“¶é¢ˆé€‰æ‹©ç”Ÿé•¿æ–¹å¼
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, random_split
import numpy as np
import time
import logging
from datetime import datetime
import json
import os
import sys
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from neuroexapt.core import CheckpointManager, get_checkpoint_manager
from neuroexapt.core.evolution_checkpoint import EvolutionCheckpointManager

# è®¾ç½®æ—¥å¿— - ç®€æ´æ ¼å¼
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

class GrowableConvBlock(nn.Module):
    """å¯ç”Ÿé•¿çš„å·ç§¯å—"""
    
    def __init__(self, in_channels, out_channels, block_id, stride=1):
        super(GrowableConvBlock, self).__init__()
        
        self.block_id = block_id
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.stride = stride
        
        # åŸºç¡€å·ç§¯å—
        self.conv = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1, bias=False)
        self.bn = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        
        # å¹¶è¡Œåˆ†æ”¯ï¼ˆç”¨äºåˆ†æ”¯ç”Ÿé•¿ï¼‰
        self.branches = nn.ModuleList()
        
        # ç”Ÿé•¿å†å²
        self.growth_history = []
        
        logger.info(f"ğŸ§± Block {block_id} created: {in_channels}â†’{out_channels}, stride={stride}")
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        # ä¸»åˆ†æ”¯
        out = self.conv(x)
        out = self.bn(out)
        out = self.relu(out)
        
        # å¹¶è¡Œåˆ†æ”¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if len(self.branches) > 0:
            branch_outputs = []
            for i, branch in enumerate(self.branches):
                try:
                    branch_out = branch(x)
                    
                    # ğŸ”§ ä¿®å¤ï¼šå®‰å…¨çš„å½¢çŠ¶åŒ¹é…ï¼Œé¿å…gradientç ´å
                    # 1. é¦–å…ˆå¤„ç†ç©ºé—´ç»´åº¦åŒ¹é…
                    if branch_out.shape[2:] != out.shape[2:]:
                        branch_out = F.adaptive_avg_pool2d(branch_out, out.shape[2:])
                    
                    # 2. å¤„ç†é€šé“ç»´åº¦åŒ¹é… - ä½¿ç”¨learnable projectionè€Œéé›¶å¡«å……
                    if branch_out.shape[1] != out.shape[1]:
                        # åˆ›å»ºæˆ–è·å–é€šé“åŒ¹é…å±‚
                        if not hasattr(self, f'_channel_adapter_{i}'):
                            # åŠ¨æ€åˆ›å»ºé€šé“é€‚é…å™¨
                            adapter = nn.Conv2d(
                                branch_out.shape[1], 
                                out.shape[1], 
                                kernel_size=1, 
                                bias=False
                            ).to(branch_out.device)
                            # ä½¿ç”¨identityåˆå§‹åŒ–é¿å…ç ´åå·²å­¦ä¹ ç‰¹å¾
                            with torch.no_grad():
                                if branch_out.shape[1] <= out.shape[1]:
                                    # è¾“å…¥é€šé“å°‘äºè¾“å‡ºé€šé“ï¼šidentity + é›¶åˆå§‹åŒ–
                                    nn.init.zeros_(adapter.weight)
                                    min_channels = min(branch_out.shape[1], out.shape[1])
                                    for c in range(min_channels):
                                        adapter.weight[c, c, 0, 0] = 1.0
                                else:
                                    # è¾“å…¥é€šé“å¤šäºè¾“å‡ºé€šé“ï¼šå–å‰Nä¸ªé€šé“
                                    nn.init.zeros_(adapter.weight)
                                    for c in range(out.shape[1]):
                                        adapter.weight[c, c, 0, 0] = 1.0
                            
                            setattr(self, f'_channel_adapter_{i}', adapter)
                        
                        adapter = getattr(self, f'_channel_adapter_{i}')
                        branch_out = adapter(branch_out)
                    
                    branch_outputs.append(branch_out)
                    
                except Exception as e:
                    logger.warning(f"Branch {i} forward failed: {e}")
                    # ğŸ”§ ä¿®å¤ï¼šå¤±è´¥æ—¶åˆ›å»ºå®‰å…¨çš„é›¶tensorï¼Œé¿å…ç ´ågradient flow
                    safe_output = torch.zeros_like(out)
                    branch_outputs.append(safe_output)
                    continue
            
            # èåˆåˆ†æ”¯è¾“å‡º - ä½¿ç”¨æ›´ç¨³å®šçš„èåˆç­–ç•¥
            if branch_outputs:
                # ä½¿ç”¨å¹³å‡è€Œéæ±‚å’Œï¼Œé¿å…æ¢¯åº¦çˆ†ç‚¸
                branch_avg = torch.stack(branch_outputs).mean(dim=0)
                out = out + 0.2 * branch_avg  # é™ä½åˆ†æ”¯æƒé‡ï¼Œæé«˜ç¨³å®šæ€§
        
        return out
    
    def expand_channels(self, new_out_channels):
        """æ‰©å±•è¾“å‡ºé€šé“æ•°"""
        if new_out_channels <= self.out_channels:
            return False
        
        old_channels = self.out_channels
        
        # è·å–å½“å‰è®¾å¤‡
        device = next(self.conv.parameters()).device
        
        # åˆ›å»ºæ–°çš„å·ç§¯å±‚
        new_conv = nn.Conv2d(self.in_channels, new_out_channels, 3, 
                           stride=self.stride, padding=1, bias=False).to(device)
        new_bn = nn.BatchNorm2d(new_out_channels).to(device)
        
        # å‚æ•°è¿ç§»
        with torch.no_grad():
            # å¤åˆ¶åŸæœ‰å‚æ•°
            new_conv.weight[:old_channels] = self.conv.weight
            new_bn.weight[:old_channels] = self.bn.weight
            new_bn.bias[:old_channels] = self.bn.bias
            if hasattr(self.bn, 'running_mean'):
                new_bn.running_mean[:old_channels] = self.bn.running_mean
                new_bn.running_var[:old_channels] = self.bn.running_var
        
        # æ›¿æ¢å±‚
        self.conv = new_conv
        self.bn = new_bn
        self.out_channels = new_out_channels
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šç²¾ç¡®æ›´æ–°æ‰€æœ‰åˆ†æ”¯çš„è¾“å‡ºé€šé“æ•°
        branches_to_remove = []
        for i, branch in enumerate(self.branches):
            try:
                # æ¸…ç†å¯èƒ½å­˜åœ¨çš„æ—§é€šé“é€‚é…å™¨
                if hasattr(self, f'_channel_adapter_{i}'):
                    delattr(self, f'_channel_adapter_{i}')
                
                # è·å–åˆ†æ”¯çš„ç¬¬ä¸€ä¸ªå·ç§¯å±‚
                if hasattr(branch, '0') and isinstance(branch[0], nn.Conv2d):
                    old_branch_conv = branch[0]
                    old_branch_bn = branch[1] if len(branch) > 1 and isinstance(branch[1], nn.BatchNorm2d) else None
                    
                    # åˆ›å»ºæ–°çš„åˆ†æ”¯å·ç§¯å±‚
                    new_branch_conv = nn.Conv2d(
                        old_branch_conv.in_channels, 
                        new_out_channels,  # ä½¿ç”¨æ–°çš„è¾“å‡ºé€šé“æ•°
                        old_branch_conv.kernel_size,
                        stride=old_branch_conv.stride,
                        padding=old_branch_conv.padding,
                        dilation=old_branch_conv.dilation,
                        groups=old_branch_conv.groups,
                        bias=old_branch_conv.bias is not None
                    ).to(device)
                    
                    # åˆ›å»ºæ–°çš„BNå±‚
                    new_branch_bn = nn.BatchNorm2d(new_out_channels).to(device) if old_branch_bn else None
                    
                    # ğŸ”§ å®‰å…¨çš„å‚æ•°è¿ç§»ï¼Œé¿å…ç»´åº¦ä¸åŒ¹é…
                    with torch.no_grad():
                        # å¤åˆ¶å·ç§¯æƒé‡
                        min_out_channels = min(old_branch_conv.out_channels, new_out_channels)
                        min_in_channels = min(old_branch_conv.in_channels, new_branch_conv.in_channels)
                        
                        # åˆå§‹åŒ–æ–°æƒé‡ä¸ºé›¶
                        nn.init.zeros_(new_branch_conv.weight)
                        
                        # å¤åˆ¶åŸæœ‰æƒé‡åˆ°å¯¹åº”ä½ç½®
                        new_branch_conv.weight[:min_out_channels, :min_in_channels] = \
                            old_branch_conv.weight[:min_out_channels, :min_in_channels]
                        
                        # å¦‚æœæœ‰biasï¼Œä¹Ÿè¦å¤åˆ¶
                        if new_branch_conv.bias is not None and old_branch_conv.bias is not None:
                            new_branch_conv.bias[:min_out_channels] = old_branch_conv.bias[:min_out_channels]
                        
                        # å¤åˆ¶BNå‚æ•°
                        if new_branch_bn and old_branch_bn:
                            new_branch_bn.weight[:min_out_channels] = old_branch_bn.weight[:min_out_channels]
                            new_branch_bn.bias[:min_out_channels] = old_branch_bn.bias[:min_out_channels]
                            if hasattr(old_branch_bn, 'running_mean') and hasattr(new_branch_bn, 'running_mean'):
                                new_branch_bn.running_mean[:min_out_channels] = old_branch_bn.running_mean[:min_out_channels]
                                new_branch_bn.running_var[:min_out_channels] = old_branch_bn.running_var[:min_out_channels]
                                new_branch_bn.num_batches_tracked.copy_(old_branch_bn.num_batches_tracked)
                    
                    # é‡å»ºåˆ†æ”¯ï¼Œä¿æŒåŸæœ‰ç»“æ„
                    branch_layers = []
                    branch_layers.append(new_branch_conv)
                    if new_branch_bn:
                        branch_layers.append(new_branch_bn)
                    
                    # æ·»åŠ æ¿€æ´»å‡½æ•°ï¼ˆå¦‚æœåŸæ¥æœ‰çš„è¯ï¼‰
                    if len(branch) > 2:
                        branch_layers.append(branch[2])
                    elif len(branch) > 1 and not isinstance(branch[1], nn.BatchNorm2d):
                        branch_layers.append(branch[1])
                    else:
                        branch_layers.append(nn.ReLU(inplace=True))
                    
                    self.branches[i] = nn.Sequential(*branch_layers)
                    
                    logger.info(f"ğŸ”§ Updated branch {i} output channels: {old_branch_conv.out_channels}â†’{new_out_channels}")
                    
            except Exception as e:
                logger.warning(f"Failed to update branch {i}: {e}")
                # æ ‡è®°éœ€è¦ç§»é™¤çš„åˆ†æ”¯
                branches_to_remove.append(i)
        
        # å®‰å…¨åœ°ç§»é™¤æœ‰é—®é¢˜çš„åˆ†æ”¯ï¼ˆä»åå¾€å‰ç§»é™¤ï¼Œé¿å…ç´¢å¼•é—®é¢˜ï¼‰
        for i in reversed(branches_to_remove):
            logger.warning(f"Removing problematic branch {i}")
            self.branches.pop(i)
        
        # è®°å½•ç”Ÿé•¿
        self.growth_history.append({
            'type': 'channel_expansion',
            'old_channels': old_channels,
            'new_channels': new_out_channels,
            'timestamp': time.time()
        })
        
        logger.info(f"ğŸŒ± Block {self.block_id} CHANNEL GROWTH: {old_channels}â†’{new_out_channels} on {device}")
        return True
    
    def add_branch(self):
        """å¢åŠ å¹¶è¡Œåˆ†æ”¯"""
        # åˆ›å»ºæ–°åˆ†æ”¯
        branch = nn.Sequential(
            nn.Conv2d(self.in_channels, self.out_channels, 5, 
                     stride=self.stride, padding=2, bias=False),
            nn.BatchNorm2d(self.out_channels),
            nn.ReLU(inplace=True)
        )
        
        # å…³é”®ï¼šå°†æ–°åˆ†æ”¯ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡ä¸Š
        device = next(self.conv.parameters()).device
        branch = branch.to(device)
        
        self.branches.append(branch)
        
        # è®°å½•ç”Ÿé•¿
        self.growth_history.append({
            'type': 'branch_addition',
            'branch_count': len(self.branches),
            'timestamp': time.time()
        })
        
        logger.info(f"ğŸŒ¿ Block {self.block_id} BRANCH GROWTH: Added branch #{len(self.branches)} on {device}")
        return True

class GrowingNetwork(nn.Module):
    """ä¼šçœŸæ­£ç”Ÿé•¿çš„ç¥ç»ç½‘ç»œ"""
    
    def __init__(self, num_classes=10, initial_channels=16, initial_depth=3):
        super(GrowingNetwork, self).__init__()
        
        self.num_classes = num_classes
        self.initial_channels = initial_channels
        self.current_depth = initial_depth
        
        # Stem
        self.stem = nn.Sequential(
            nn.Conv2d(3, initial_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(initial_channels),
            nn.ReLU(inplace=True)
        )
        
        # åŠ¨æ€å±‚åˆ—è¡¨
        self.layers = nn.ModuleList()
        
        # æ„å»ºåˆå§‹ç½‘ç»œï¼ˆå¾ˆå°ï¼ï¼‰
        current_channels = initial_channels
        for i in range(initial_depth):
            stride = 2 if i == 1 else 1  # åªåœ¨ç¬¬äºŒå±‚é™é‡‡æ ·
            out_channels = current_channels * (2 if i == 1 else 1)
            
            block = GrowableConvBlock(current_channels, out_channels, i, stride)
            self.layers.append(block)
            current_channels = out_channels
        
        # åˆ†ç±»å¤´
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.classifier = nn.Linear(current_channels, num_classes)
        
        # ç”Ÿé•¿ç»Ÿè®¡
        self.growth_stats = {
            'depth_growths': 0,
            'channel_growths': 0,
            'branch_growths': 0,
            'total_growths': 0,
            'parameter_history': []
        }
        
        # è®°å½•åˆå§‹å‚æ•°é‡
        initial_params = sum(p.numel() for p in self.parameters())
        self.growth_stats['parameter_history'].append({
            'depth': initial_depth,
            'params': initial_params,
            'timestamp': time.time()
        })
        
        logger.info(f"ğŸŒ± Growing Network initialized:")
        logger.info(f"   Initial depth: {initial_depth} layers")
        logger.info(f"   Initial channels: {initial_channels}")
        logger.info(f"   Initial parameters: {initial_params:,}")
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        x = self.stem(x)
        
        for layer in self.layers:
            x = layer(x)
        
        x = self.global_pool(x)
        x = x.view(x.size(0), -1)
        return self.classifier(x)
    
    def grow_depth(self, position=None):
        """å¢åŠ ç½‘ç»œæ·±åº¦ - çœŸæ­£çš„å±‚æ•°å¢é•¿ï¼"""
        if position is None:
            position = len(self.layers) - 1  # åœ¨å€’æ•°ç¬¬äºŒä¸ªä½ç½®æ’å…¥
        
        position = max(1, min(position, len(self.layers) - 1))
        
        # ç¡®å®šæ–°å±‚çš„é€šé“é…ç½®
        if position == 0:
            in_channels = self.initial_channels
            out_channels = self.layers[0].in_channels
        else:
            in_channels = self.layers[position - 1].out_channels
            out_channels = self.layers[position].in_channels
        
        # åˆ›å»ºæ–°å±‚
        new_layer = GrowableConvBlock(in_channels, out_channels, f"grown_{len(self.layers)}", stride=1)
        
        # ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
        if len(self.layers) > 0:
            device = next(self.layers[0].conv.parameters()).device
            new_layer = new_layer.to(device)
        
        # æ’å…¥æ–°å±‚
        self.layers.insert(position, new_layer)
        self.current_depth += 1
        
        # æ›´æ–°ç»Ÿè®¡
        self.growth_stats['depth_growths'] += 1
        self.growth_stats['total_growths'] += 1
        
        # è®°å½•å‚æ•°å˜åŒ–
        new_params = sum(p.numel() for p in self.parameters())
        self.growth_stats['parameter_history'].append({
            'depth': self.current_depth,
            'params': new_params,
            'timestamp': time.time()
        })
        
        logger.info(f"ğŸŒ± DEPTH GROWTH: Added layer at position {position}")
        logger.info(f"   New depth: {self.current_depth} layers")
        logger.info(f"   New parameters: {new_params:,}")
        
        return True
    
    def grow_width(self, layer_idx=None, expansion_factor=1.5):
        """å¢åŠ ç½‘ç»œå®½åº¦ - çœŸæ­£çš„é€šé“æ•°å¢é•¿ï¼"""
        if layer_idx is None:
            layer_idx = len(self.layers) // 2  # é€‰æ‹©ä¸­é—´å±‚
        
        if layer_idx >= len(self.layers):
            return False
        
        layer = self.layers[layer_idx]
        new_channels = int(layer.out_channels * expansion_factor)
        
        success = layer.expand_channels(new_channels)
        
        if success:
            # æ›´æ–°åç»­å±‚çš„è¾“å…¥é€šé“æ•°
            self._update_subsequent_layers(layer_idx, new_channels)
            
            # æ›´æ–°ç»Ÿè®¡
            self.growth_stats['channel_growths'] += 1
            self.growth_stats['total_growths'] += 1
            
            # è®°å½•å‚æ•°å˜åŒ–
            new_params = sum(p.numel() for p in self.parameters())
            self.growth_stats['parameter_history'].append({
                'depth': self.current_depth,
                'params': new_params,
                'timestamp': time.time()
            })
            
            logger.info(f"ğŸŒ± WIDTH GROWTH: Layer {layer_idx} channels expanded")
            logger.info(f"   New parameters: {new_params:,}")
        
        return success
    
    def grow_branches(self, layer_idx=None):
        """å¢åŠ åˆ†æ”¯ - çœŸæ­£çš„å¹¶è¡Œå¤„ç†å¢é•¿ï¼"""
        if layer_idx is None:
            layer_idx = np.random.randint(0, len(self.layers))
        
        if layer_idx >= len(self.layers):
            return False
        
        layer = self.layers[layer_idx]
        success = layer.add_branch()
        
        if success:
            # æ›´æ–°ç»Ÿè®¡
            self.growth_stats['branch_growths'] += 1
            self.growth_stats['total_growths'] += 1
            
            # è®°å½•å‚æ•°å˜åŒ–
            new_params = sum(p.numel() for p in self.parameters())
            self.growth_stats['parameter_history'].append({
                'depth': self.current_depth,
                'params': new_params,
                'timestamp': time.time()
            })
            
            logger.info(f"ğŸŒ± BRANCH GROWTH: Layer {layer_idx} added branch")
            logger.info(f"   New parameters: {new_params:,}")
        
        return success
    
    def _update_subsequent_layers(self, start_idx, new_channels):
        """æ›´æ–°åç»­å±‚çš„è¾“å…¥é€šé“æ•°"""
        for i in range(start_idx + 1, len(self.layers)):
            layer = self.layers[i]
            
            # è·å–è®¾å¤‡
            device = next(layer.conv.parameters()).device
            
            # åˆ›å»ºæ–°çš„å·ç§¯å±‚
            new_conv = nn.Conv2d(new_channels, layer.out_channels, 3,
                               stride=layer.stride, padding=1, bias=False).to(device)
            
            # å‚æ•°è¿ç§»ï¼ˆéƒ¨åˆ†ï¼‰
            with torch.no_grad():
                min_channels = min(new_channels, layer.in_channels)
                new_conv.weight[:, :min_channels] = layer.conv.weight[:, :min_channels]
            
            layer.conv = new_conv
            layer.in_channels = new_channels
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šå®‰å…¨æ›´æ–°è¯¥å±‚æ‰€æœ‰åˆ†æ”¯çš„è¾“å…¥é€šé“æ•°
            branches_to_remove = []
            for j, branch in enumerate(layer.branches):
                try:
                    # æ¸…ç†å¯èƒ½å­˜åœ¨çš„æ—§é€šé“é€‚é…å™¨
                    if hasattr(layer, f'_channel_adapter_{j}'):
                        delattr(layer, f'_channel_adapter_{j}')
                    
                    # è·å–åˆ†æ”¯çš„ç¬¬ä¸€ä¸ªå·ç§¯å±‚
                    if hasattr(branch, '0') and isinstance(branch[0], nn.Conv2d):
                        old_branch_conv = branch[0]
                        old_branch_bn = branch[1] if len(branch) > 1 and isinstance(branch[1], nn.BatchNorm2d) else None
                        
                        # åˆ›å»ºæ–°çš„åˆ†æ”¯å·ç§¯å±‚ï¼ˆæ›´æ–°è¾“å…¥é€šé“æ•°ï¼‰
                        new_branch_conv = nn.Conv2d(
                            new_channels,  # ä½¿ç”¨æ–°çš„è¾“å…¥é€šé“æ•°
                            old_branch_conv.out_channels,
                            old_branch_conv.kernel_size,
                            stride=old_branch_conv.stride,
                            padding=old_branch_conv.padding,
                            dilation=old_branch_conv.dilation,
                            groups=old_branch_conv.groups,
                            bias=old_branch_conv.bias is not None
                        ).to(device)
                        
                        # ğŸ”§ å®‰å…¨çš„å‚æ•°è¿ç§»
                        with torch.no_grad():
                            # è®¡ç®—å¯ä»¥å¤åˆ¶çš„æœ€å°é€šé“æ•°
                            min_in_channels = min(new_channels, old_branch_conv.in_channels)
                            min_out_channels = min(old_branch_conv.out_channels, new_branch_conv.out_channels)
                            
                            # åˆå§‹åŒ–ä¸ºé›¶
                            nn.init.zeros_(new_branch_conv.weight)
                            
                            # å¤åˆ¶åŸæœ‰æƒé‡åˆ°å¯¹åº”ä½ç½®
                            new_branch_conv.weight[:min_out_channels, :min_in_channels] = \
                                old_branch_conv.weight[:min_out_channels, :min_in_channels]
                            
                            # å¦‚æœæœ‰biasï¼Œä¹Ÿè¦å¤åˆ¶
                            if new_branch_conv.bias is not None and old_branch_conv.bias is not None:
                                new_branch_conv.bias[:min_out_channels] = old_branch_conv.bias[:min_out_channels]
                        
                        # é‡å»ºåˆ†æ”¯ï¼Œä¿æŒåŸæœ‰ç»“æ„
                        branch_layers = []
                        branch_layers.append(new_branch_conv)
                        if old_branch_bn:
                            branch_layers.append(old_branch_bn)  # BNå±‚ä¸éœ€è¦æ”¹å˜
                        
                        # æ·»åŠ æ¿€æ´»å‡½æ•°
                        if len(branch) > 2:
                            branch_layers.append(branch[2])
                        elif len(branch) > 1 and not isinstance(branch[1], nn.BatchNorm2d):
                            branch_layers.append(branch[1])
                        else:
                            branch_layers.append(nn.ReLU(inplace=True))
                        
                        layer.branches[j] = nn.Sequential(*branch_layers)
                        
                        logger.info(f"ğŸ”§ Updated layer {i} branch {j} input channels: {old_branch_conv.in_channels}â†’{new_channels}")
                        
                except Exception as e:
                    logger.warning(f"Failed to update layer {i} branch {j}: {e}")
                    # æ ‡è®°éœ€è¦ç§»é™¤çš„åˆ†æ”¯
                    branches_to_remove.append(j)
            
            # å®‰å…¨åœ°ç§»é™¤æœ‰é—®é¢˜çš„åˆ†æ”¯ï¼ˆä»åå¾€å‰ç§»é™¤ï¼Œé¿å…ç´¢å¼•é—®é¢˜ï¼‰
            for j in reversed(branches_to_remove):
                logger.warning(f"Removing problematic branch {j} from layer {i}")
                layer.branches.pop(j)
            
            new_channels = layer.out_channels  # ä¸ºä¸‹ä¸€å±‚å‡†å¤‡
    
    def get_architecture_summary(self):
        """è·å–æ¶æ„æ‘˜è¦"""
        layer_info = []
        for i, layer in enumerate(self.layers):
            layer_info.append({
                'id': layer.block_id,
                'in_channels': layer.in_channels,
                'out_channels': layer.out_channels,
                'branches': len(layer.branches),
                'growth_history': layer.growth_history
            })
        
        return {
            'depth': self.current_depth,
            'total_parameters': sum(p.numel() for p in self.parameters()),
            'growth_stats': self.growth_stats,
            'layers': layer_info
        }

class GrowthController:
    """ç”Ÿé•¿æ§åˆ¶å™¨ - å†³å®šä½•æ—¶ä½•åœ°ç”Ÿé•¿"""
    
    def __init__(self):
        self.performance_history = []
        self.growth_decisions = []
        self.last_growth_epoch = -1
        
        # æ›´æ¿€è¿›çš„ç”Ÿé•¿è§¦å‘å‚æ•° - å†²å‡»95%å‡†ç¡®ç‡ï¼
        self.min_epochs_between_growth = 6  # æ¯6ä¸ªepochæœ€å°‘ç”Ÿé•¿1æ¬¡
        self.improvement_threshold = 0.2  # æ›´ä½çš„æ”¹è¿›é˜ˆå€¼
        self.forced_growth_interval = 12  # æ¯12ä¸ªepochå¼ºåˆ¶ç”Ÿé•¿ä¸€æ¬¡
        
    def should_grow(self, current_accuracy, epoch):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç”Ÿé•¿ - æ›´æ¿€è¿›çš„ç­–ç•¥"""
        self.performance_history.append(current_accuracy)
        
        # å¼ºåˆ¶ç”Ÿé•¿ï¼šå®šæœŸå¿…é¡»ç”Ÿé•¿
        if epoch % self.forced_growth_interval == 10:
            logger.info(f"ğŸŒ± FORCED GROWTH at epoch {epoch} (scheduled growth)")
            return True
        
        # å¦‚æœè·ç¦»ä¸Šæ¬¡ç”Ÿé•¿å¤ªä¹…ï¼Œå¼ºåˆ¶ç”Ÿé•¿
        if epoch - self.last_growth_epoch >= self.min_epochs_between_growth and epoch > 5:
            logger.info(f"ğŸŒ± GROWTH TRIGGER at epoch {epoch} (interval-based)")
            return True
        
        # å¦‚æœè®­ç»ƒæ—©æœŸï¼Œç§¯æç”Ÿé•¿
        if epoch < 20 and epoch % 6 == 5:
            logger.info(f"ğŸŒ± EARLY GROWTH at epoch {epoch} (early phase)")
            return True
        
        # æ€§èƒ½åœæ»æ£€æµ‹ï¼ˆæ›´å®½æ¾ï¼‰
        if len(self.performance_history) >= 4:
            recent_performance = self.performance_history[-3:]
            improvement = max(recent_performance) - min(recent_performance)
            
            if improvement < self.improvement_threshold and epoch - self.last_growth_epoch >= 6:
                logger.info(f"ğŸŒ± STAGNATION GROWTH at epoch {epoch}")
                logger.info(f"   Recent improvement: {improvement:.2f}%")
                return True
        
        return False
    
    def select_growth_strategy(self, network, current_accuracy):
        """é€‰æ‹©ç”Ÿé•¿ç­–ç•¥"""
        current_depth = network.current_depth
        total_params = sum(p.numel() for p in network.parameters())
        
        strategies = []
        
        # åŸºäºç½‘ç»œçŠ¶æ€å’Œæ€§èƒ½é€‰æ‹©ç­–ç•¥
        if current_accuracy < 30:
            # ä½æ€§èƒ½ï¼šä¼˜å…ˆå¢åŠ æ·±åº¦å’Œå®½åº¦
            if current_depth < 8:
                strategies.extend(['grow_depth'] * 3)
            strategies.extend(['grow_width'] * 2)
            strategies.append('grow_branches')
            
        elif current_accuracy < 60:
            # ä¸­ç­‰æ€§èƒ½ï¼šå¹³è¡¡å‘å±•
            if current_depth < 10:
                strategies.extend(['grow_depth'] * 2)
            strategies.extend(['grow_width'] * 2)
            strategies.extend(['grow_branches'] * 2)
            
        else:
            # é«˜æ€§èƒ½ï¼šç²¾ç»†è°ƒä¼˜
            if current_depth < 12:
                strategies.append('grow_depth')
            strategies.extend(['grow_width', 'grow_branches'] * 2)
        
        # å‚æ•°é‡é™åˆ¶
        if total_params > 500000:  # 50ä¸‡å‚æ•°é™åˆ¶
            strategies = [s for s in strategies if s != 'grow_depth']
        
        if not strategies:
            strategies = ['grow_branches']  # ä¿åº•ç­–ç•¥
        
        selected = np.random.choice(strategies)
        
        logger.info(f"ğŸ¯ Selected growth strategy: {selected}")
        logger.info(f"   Current depth: {current_depth}, Parameters: {total_params:,}")
        
        return selected
    
    def execute_growth(self, network, strategy, current_epoch):
        """æ‰§è¡Œç”Ÿé•¿ç­–ç•¥"""
        success = False
        
        try:
            if strategy == 'grow_depth':
                success = network.grow_depth()
                
            elif strategy == 'grow_width':
                success = network.grow_width(expansion_factor=np.random.uniform(1.3, 1.8))
                
            elif strategy == 'grow_branches':
                success = network.grow_branches()
            
            if success:
                # æ›´æ–°ä¸Šæ¬¡ç”Ÿé•¿æ—¶é—´
                self.last_growth_epoch = current_epoch
                
                decision = {
                    'strategy': strategy,
                    'timestamp': time.time(),
                    'epoch': current_epoch,
                    'depth': network.current_depth,
                    'parameters': sum(p.numel() for p in network.parameters())
                }
                self.growth_decisions.append(decision)
                
                logger.info(f"âœ… Growth executed successfully!")
                logger.info(f"   Strategy: {strategy}")
                logger.info(f"   New depth: {network.current_depth}")
                logger.info(f"   New parameters: {decision['parameters']:,}")
            
        except Exception as e:
            logger.error(f"âŒ Growth failed: {e}")
            success = False
        
        return success

class GrowingNetworkTrainer:
    """ç”Ÿé•¿ç½‘ç»œè®­ç»ƒå™¨"""
    
    def __init__(self, experiment_name="growing_network"):
        self.experiment_name = experiment_name
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ç»„ä»¶
        self.network = None
        self.growth_controller = GrowthController()
        self.optimizer = None
        self.scheduler = None
        self.criterion = nn.CrossEntropyLoss()
        
        # è®­ç»ƒçŠ¶æ€
        self.best_accuracy = 0.0
        self.training_history = []
        
        # è¿›åŒ–checkpointç®¡ç†å™¨
        self.evolution_manager = EvolutionCheckpointManager(experiment_name)
        
        logger.info(f"ğŸŒ± Growing Network Trainer initialized")
        logger.info(f"ğŸ”§ Device: {self.device}")
        logger.info(f"ğŸ“š Evolution checkpoint manager ready")
    
    def setup_data(self, batch_size=128):
        """è®¾ç½®æ•°æ®"""
        logger.info("ğŸ“Š Setting up CIFAR-10...")
        
        transform_train = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])
        
        transform_test = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])
        
        train_dataset = torchvision.datasets.CIFAR10('./data', train=True, download=True, transform=transform_train)
        test_dataset = torchvision.datasets.CIFAR10('./data', train=False, transform=transform_test)
        
        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)
        self.test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)
        
        logger.info(f"âœ… Data ready: {len(train_dataset)} train, {len(test_dataset)} test")
    
    def setup_network(self, initial_channels=16, initial_depth=3):
        """è®¾ç½®ç½‘ç»œ"""
        self.network = GrowingNetwork(
            num_classes=10,
            initial_channels=initial_channels,
            initial_depth=initial_depth
        ).to(self.device)
        
        self.optimizer = optim.SGD(self.network.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=200)
        
        total_params = sum(p.numel() for p in self.network.parameters())
        logger.info(f"ğŸ“Š Network setup complete: {total_params:,} parameters")
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.network.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        pbar = tqdm(self.train_loader, desc=f"ğŸš€ Epoch {epoch:02d}")
        
        for batch_idx, (data, target) in enumerate(pbar):
            data, target = data.to(self.device), target.to(self.device)
            
            self.optimizer.zero_grad()
            output = self.network(data)
            loss = self.criterion(output, target)
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            pred = output.argmax(dim=1)
            correct += pred.eq(target).sum().item()
            total += target.size(0)
            
            pbar.set_postfix({
                'Loss': f'{total_loss/(batch_idx+1):.4f}',
                'Acc': f'{100.*correct/total:.2f}%',
                'Depth': self.network.current_depth,
                'Params': f'{sum(p.numel() for p in self.network.parameters()):,}'
            })
        
        return total_loss/len(self.train_loader), 100.*correct/total
    
    def validate(self):
        """éªŒè¯"""
        self.network.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in self.test_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.network(data)
                loss = self.criterion(output, target)
                
                total_loss += loss.item()
                pred = output.argmax(dim=1)
                correct += pred.eq(target).sum().item()
                total += target.size(0)
        
        return total_loss/len(self.test_loader), 100.*correct/total
    
    def train(self, epochs=100, initial_channels=16, initial_depth=3, batch_size=128, resume_from=None):
        """ä¸»è®­ç»ƒæµç¨‹"""
        logger.info(f"ğŸŒ± GROWING NETWORK TRAINING START")
        logger.info(f"ğŸ“Š Config: epochs={epochs}, initial_channels={initial_channels}, initial_depth={initial_depth}")
        
        start_time = time.time()
        start_epoch = 0
        
        # è®¾ç½®
        self.setup_data(batch_size)
        self.setup_network(initial_channels, initial_depth)
        
        # ğŸ”„ æ¢å¤è®­ç»ƒ
        if resume_from:
            logger.info(f"ğŸ”„ Resuming from checkpoint: {resume_from}")
            try:
                network_state, optimizer_state, scheduler_state, metadata = self.evolution_manager.load_checkpoint(resume_from)
                
                # æ¢å¤ç½‘ç»œçŠ¶æ€
                self.network.load_state_dict(network_state)
                self.optimizer.load_state_dict(optimizer_state)
                if scheduler_state and self.scheduler:
                    self.scheduler.load_state_dict(scheduler_state)
                
                # æ¢å¤è®­ç»ƒç»Ÿè®¡
                start_epoch = metadata['epoch'] + 1
                self.best_accuracy = metadata['training_stats'].get('best_accuracy', 0.0)
                
                logger.info(f"âœ… Successfully resumed from epoch {start_epoch}")
                logger.info(f"   Best accuracy so far: {self.best_accuracy:.2f}%")
                
                # æ˜¾ç¤ºå½“å‰æ¶æ„
                self.display_detailed_architecture()
                
            except Exception as e:
                logger.error(f"âŒ Failed to resume from checkpoint: {e}")
                logger.info("ğŸ”„ Starting fresh training instead...")
                start_epoch = 0
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(start_epoch, epochs):
            logger.info(f"\n{'='*80}")
            logger.info(f"Epoch {epoch+1}/{epochs}")
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(epoch)
            
            # éªŒè¯
            test_loss, test_acc = self.validate()
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            
            # è®°å½•ç»Ÿè®¡
            stats = {
                'epoch': epoch,
                'train_loss': train_loss,
                'train_acc': train_acc,
                'test_loss': test_loss,
                'test_accuracy': test_acc,
                'network_summary': self.network.get_architecture_summary()
            }
            self.training_history.append(stats)
            
            # æ›´æ–°æœ€ä½³æ€§èƒ½ï¼ˆåœ¨æ˜¾ç¤ºä¹‹å‰ï¼‰
            if test_acc > self.best_accuracy:
                self.best_accuracy = test_acc
            
            # è¾“å‡ºç»“æœ
            arch_summary = self.network.get_architecture_summary()
            logger.info(f"ğŸ“Š Results:")
            logger.info(f"   Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%")
            logger.info(f"   Test:  Loss={test_loss:.4f}, Acc={test_acc:.2f}%")
            logger.info(f"   Best:  {self.best_accuracy:.2f}%")
            logger.info(f"   ğŸ—ï¸ Architecture: {arch_summary['depth']} layers, {arch_summary['total_parameters']:,} params")
            
            # æ˜¾ç¤ºç”Ÿé•¿çŠ¶æ€
            epochs_since_growth = epoch - self.growth_controller.last_growth_epoch
            logger.info(f"   ğŸŒ± Growth: {arch_summary['growth_stats']['total_growths']} total, {epochs_since_growth} epochs since last")
            
            # ç”Ÿé•¿å†³ç­–
            if self.growth_controller.should_grow(test_acc, epoch):
                strategy = self.growth_controller.select_growth_strategy(self.network, test_acc)
                
                # ğŸ”¥ å…³é”®ï¼šç”Ÿé•¿å‰ä¿å­˜checkpoint
                logger.info(f"ğŸ’¾ Saving checkpoint before {strategy}...")
                current_stats = {
                    'epoch': epoch,
                    'train_accuracy': train_acc,
                    'test_accuracy': test_acc,
                    'best_accuracy': self.best_accuracy,
                    'train_loss': train_loss,
                    'test_loss': test_loss
                }
                
                pre_growth_checkpoint_id = self.evolution_manager.save_checkpoint(
                    network=self.network,
                    optimizer=self.optimizer, 
                    scheduler=self.scheduler,
                    epoch=epoch,
                    training_stats=current_stats,
                    growth_type=f"pre_{strategy}",
                    parent_id=None  # å½“å‰çº¿æ€§è¿›åŒ–
                )
                
                # ä¿å­˜ç”Ÿé•¿å‰çš„çŠ¶æ€ï¼ˆç”¨äºæ˜¾ç¤ºå¯¹æ¯”ï¼‰
                pre_growth_params = sum(p.numel() for p in self.network.parameters())
                pre_growth_depth = self.network.current_depth
                
                # æ‰§è¡Œç”Ÿé•¿
                success = self.growth_controller.execute_growth(self.network, strategy, epoch)
                
                if success:
                    # ç¡®ä¿æ•´ä¸ªç½‘ç»œåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                    self.network = self.network.to(self.device)
                    
                    # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆå‚æ•°å˜äº†ï¼‰
                    self.optimizer = optim.SGD(self.network.parameters(), lr=0.05, momentum=0.9, weight_decay=1e-4)
                    self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=epochs-epoch)
                    
                    # æ˜¾ç¤ºç”Ÿé•¿æ•ˆæœ
                    post_growth_params = sum(p.numel() for p in self.network.parameters())
                    post_growth_depth = self.network.current_depth
                    
                    logger.info(f"ğŸ‰ NETWORK GROWN SUCCESSFULLY!")
                    logger.info(f"   Depth: {pre_growth_depth} â†’ {post_growth_depth}")
                    logger.info(f"   Parameters: {pre_growth_params:,} â†’ {post_growth_params:,}")
                    logger.info(f"   Parameter increase: +{post_growth_params-pre_growth_params:,}")
                    logger.info(f"   Device check: Network on {next(self.network.parameters()).device}")
            
                    # ğŸ’¾ ç”Ÿé•¿åä¹Ÿä¿å­˜checkpoint
                    post_growth_stats = current_stats.copy()
                    post_growth_checkpoint_id = self.evolution_manager.save_checkpoint(
                        network=self.network,
                        optimizer=self.optimizer,
                        scheduler=self.scheduler, 
                        epoch=epoch,
                        training_stats=post_growth_stats,
                        growth_type=strategy,
                        parent_id=pre_growth_checkpoint_id  # è®¾ç½®çˆ¶èŠ‚ç‚¹å…³ç³»
                    )
            
            # å®šæœŸæ˜¾ç¤ºè¯¦ç»†æ¶æ„
            if epoch % 20 == 19:
                self.display_detailed_architecture()
        
        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        logger.info(f"\nğŸ‰ GROWING NETWORK TRAINING COMPLETED!")
        logger.info(f"â±ï¸  Total time: {total_time/60:.1f} minutes")
        logger.info(f"ğŸ† Best accuracy: {self.best_accuracy:.2f}%")
        
        final_summary = self.network.get_architecture_summary()
        logger.info(f"ğŸŒ± Final network:")
        logger.info(f"   Depth: {final_summary['depth']} layers")
        logger.info(f"   Parameters: {final_summary['total_parameters']:,}")
        logger.info(f"   Total growths: {final_summary['growth_stats']['total_growths']}")
        
        # ğŸ“Š æ˜¾ç¤ºè¿›åŒ–æ ‘
        self.evolution_manager.display_evolution_tree()
        
        self.display_detailed_architecture()
    
    def display_detailed_architecture(self):
        """æ˜¾ç¤ºè¯¦ç»†æ¶æ„ä¿¡æ¯"""
        summary = self.network.get_architecture_summary()
        
        logger.info(f"\nğŸ—ï¸ DETAILED ARCHITECTURE:")
        logger.info(f"   Total depth: {summary['depth']} layers")
        logger.info(f"   Total parameters: {summary['total_parameters']:,}")
        logger.info(f"   Growth statistics:")
        for key, value in summary['growth_stats'].items():
            if key != 'parameter_history':
                logger.info(f"     {key}: {value}")
        
        logger.info(f"   Layer details:")
        for i, layer_info in enumerate(summary['layers']):
            branches_info = f", {layer_info['branches']} branches" if layer_info['branches'] > 0 else ""
            logger.info(f"     Layer {i}: {layer_info['in_channels']}â†’{layer_info['out_channels']}{branches_info}")

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='Growing Network Training')
    parser.add_argument('--epochs', type=int, default=150, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=128, help='æ‰¹å¤§å°')
    parser.add_argument('--initial_channels', type=int, default=32, help='åˆå§‹é€šé“æ•°')
    parser.add_argument('--initial_depth', type=int, default=4, help='åˆå§‹æ·±åº¦')
    parser.add_argument('--experiment', type=str, default='growing_network_95', help='å®éªŒåç§°')
    parser.add_argument('--resume_from', type=str, default=None, help='ä»æŒ‡å®šcheckpointæ¢å¤è®­ç»ƒ')
    parser.add_argument('--target_accuracy', type=float, default=95.0, help='ç›®æ ‡å‡†ç¡®ç‡')
    
    args = parser.parse_args()
    
    logger.info("ğŸŒ± GROWING NEURAL NETWORK - REAL STRUCTURAL GROWTH!")
    logger.info(f"â° Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"ğŸ“Š Configuration: {vars(args)}")
    
    trainer = GrowingNetworkTrainer(args.experiment)
    
    logger.info(f"ğŸ¯ Target accuracy: {args.target_accuracy}%")
    if args.resume_from:
        logger.info(f"ğŸ”„ Will resume from: {args.resume_from}")
    
    trainer.train(
        epochs=args.epochs,
        initial_channels=args.initial_channels,
        initial_depth=args.initial_depth,
        batch_size=args.batch_size,
        resume_from=args.resume_from
    )

if __name__ == "__main__":
    main() 