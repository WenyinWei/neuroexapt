#!/usr/bin/env python3
"""
ASO-SE (Alternating Stable Optimization with Stochastic Exploration) ç¥ç»ç½‘ç»œè®­ç»ƒ

æ ¸å¿ƒç‰¹æ€§ï¼š
ğŸš€ çœŸæ­£çš„æ¶æ„æœç´¢å’Œç½‘ç»œç»“æ„åŠ¨æ€ç”Ÿé•¿
ğŸ”§ åŸºäºNet2Netçš„å¹³æ»‘å‚æ•°è¿ç§»
âš¡ Gumbel-Softmaxå¼•å¯¼çš„å¯å¾®åˆ†æ¶æ„é‡‡æ ·
ğŸ¯ å››é˜¶æ®µè®­ç»ƒå¾ªç¯ï¼šé¢„çƒ­â†’æœç´¢â†’ç”Ÿé•¿â†’ä¼˜åŒ–

æ¶æ„ç”Ÿé•¿ç­–ç•¥ï¼š
- æ·±åº¦ç”Ÿé•¿ï¼šæ·»åŠ æ–°çš„å¯è¿›åŒ–å±‚
- å®½åº¦ç”Ÿé•¿ï¼šæ‰©å±•ç°æœ‰å±‚çš„é€šé“æ•°
- åˆ†æ”¯ç”Ÿé•¿ï¼šå¢åŠ æ“ä½œåˆ†æ”¯çš„å¤æ‚åº¦
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import numpy as np
import time
import os
import sys
import argparse
from datetime import datetime
from tqdm import tqdm
from typing import Dict, List, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥æ¨¡å—
from neuroexapt.core.genotypes import PRIMITIVES
from neuroexapt.core.operations import OPS
from neuroexapt.core.net2net_transfer import Net2NetTransfer

# é…ç½®æ—¥å¿—
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(message)s', datefmt='%H:%M:%S')
logger = logging.getLogger()

class GumbelSoftmax(nn.Module):
    """Gumbel-Softmaxé‡‡æ ·å™¨"""
    
    def __init__(self, hard=True, temperature=1.0, min_temperature=0.1):  # é™ä½åˆå§‹æ¸©åº¦
        super().__init__()
        self.hard = hard
        self.temperature = temperature
        self.min_temperature = min_temperature
        self.anneal_rate = 0.98  # æ›´æ…¢çš„é€€ç«é€Ÿåº¦
    
    def forward(self, logits):
        """å‰å‘ä¼ æ’­"""
        hard = self.hard and self.training
        
        if not self.training:
            # æ¨ç†æ—¶ç›´æ¥è¿”å›one-hot
            y_hard = torch.zeros_like(logits)
            y_hard.scatter_(-1, torch.argmax(logits, dim=-1, keepdim=True), 1.0)
            return y_hard
        
        # è®­ç»ƒæ—¶ä½¿ç”¨Gumbel-Softmax
        gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) + 1e-8) + 1e-8)
        y_soft = F.softmax((logits + gumbel_noise) / self.temperature, dim=-1)
        
        if hard:
            # Straight-through estimator
            max_indices = torch.argmax(y_soft, dim=-1, keepdim=True)
            y_hard = torch.zeros_like(y_soft).scatter_(-1, max_indices, 1.0)
            return y_hard - y_soft.detach() + y_soft
        else:
            return y_soft
    
    def anneal_temperature(self):
        """æ¸©åº¦é€€ç«"""
        self.temperature = max(self.min_temperature, self.temperature * self.anneal_rate)

class MixedOperation(nn.Module):
    """æ··åˆæ“ä½œå±‚ - æ”¯æŒå¤šç§åŸå§‹æ“ä½œ"""
    
    def __init__(self, C, stride):
        super().__init__()
        self.operations = nn.ModuleList()
        self.C = C
        self.stride = stride
        
        # åˆ›å»ºæ‰€æœ‰å€™é€‰æ“ä½œ
        for primitive in PRIMITIVES:
            op = self._create_operation(primitive, C, stride)
            self.operations.append(op)
        
        self.num_ops = len(PRIMITIVES)
        print(f"ğŸ”§ MixedOperation åˆ›å»º: {self.num_ops} ä¸ªæ“ä½œ, C={C}, stride={stride}")
    
    def _create_operation(self, primitive, C, stride):
        """åˆ›å»ºå•ä¸ªæ“ä½œ"""
        if primitive in OPS:
            return OPS[primitive](C, stride, False)
        elif primitive == 'none':
            return Zero(stride)
        elif primitive == 'skip_connect':
            if stride == 1:
                return Identity()
            else:
                return FactorizedReduce(C, C)
        elif primitive == 'sep_conv_3x3':
            return SepConv(C, C, 3, stride, 1)
        elif primitive == 'sep_conv_5x5':
            return SepConv(C, C, 5, stride, 2)
        elif primitive == 'sep_conv_7x7':
            return SepConv(C, C, 7, stride, 3)
        elif primitive == 'dil_conv_3x3':
            return DilConv(C, C, 3, stride, 2, 2)
        elif primitive == 'dil_conv_5x5':
            return DilConv(C, C, 5, stride, 4, 2)
        elif primitive == 'conv_7x1_1x7':
            return Conv7x1_1x7(C, C, stride)
        elif primitive == 'avg_pool_3x3':
            return nn.AvgPool2d(3, stride=stride, padding=1, count_include_pad=False)
        elif primitive == 'max_pool_3x3':
            return nn.MaxPool2d(3, stride=stride, padding=1)
        else:
            raise ValueError(f"Unknown primitive: {primitive}")
    
    def forward(self, x, arch_weights):
        """å‰å‘ä¼ æ’­"""
        # æ£€æŸ¥æƒé‡æœ‰æ•ˆæ€§
        if torch.isnan(arch_weights).any() or torch.isinf(arch_weights).any():
            # å¦‚æœæƒé‡æ— æ•ˆï¼Œä½¿ç”¨skipè¿æ¥ä½œä¸ºå®‰å…¨å›é€€
            return self.operations[3](x)  # skip_connect
        
        # æ™ºèƒ½æ“ä½œé€‰æ‹©ï¼šå¦‚æœæŸä¸ªæ“ä½œæƒé‡å ä¸»å¯¼ï¼Œä¼˜å…ˆè®¡ç®—è¯¥æ“ä½œ
        max_weight_idx = torch.argmax(arch_weights).item()
        max_weight = arch_weights[max_weight_idx].item()
        
        # å¦‚æœæœ‰æ“ä½œæƒé‡è¶…è¿‡0.8ï¼Œä¸»è¦è®¡ç®—è¯¥æ“ä½œï¼ˆé«˜æ•ˆæ¨¡å¼ï¼‰
        if max_weight > 0.8:
            dominant_result = self.operations[max_weight_idx](x)
            
            # ä»ç„¶è®¡ç®—å…¶ä»–æœ‰æ„ä¹‰çš„æ“ä½œï¼Œä½†æƒé‡è¾ƒä½
            if max_weight < 0.95:  # ä¸æ˜¯å®Œå…¨ç¡®å®šçš„æƒ…å†µä¸‹
                other_results = []
                for i, op in enumerate(self.operations):
                    if i != max_weight_idx and arch_weights[i] > 0.05:
                        other_results.append(arch_weights[i] * op(x))
                
                if other_results:
                    other_contribution = sum(other_results)
                    return max_weight * dominant_result + other_contribution
            
            return dominant_result
        
        # å¦åˆ™è®¡ç®—æ‰€æœ‰æœ‰æ„ä¹‰æƒé‡çš„æ“ä½œï¼ˆæœç´¢æ¨¡å¼ï¼‰
        results = []
        total_computed_weight = 0.0
        
        for i, op in enumerate(self.operations):
            weight = arch_weights[i]
            if weight > 0.02:  # åªè®¡ç®—æƒé‡è¶…è¿‡2%çš„æ“ä½œ
                try:
                    op_result = op(x)
                    results.append(weight * op_result)
                    total_computed_weight += weight
                except Exception as e:
                    # å¦‚æœæŸä¸ªæ“ä½œå¤±è´¥ï¼Œè·³è¿‡å®ƒ
                    print(f"âš ï¸ æ“ä½œ {i} è®¡ç®—å¤±è´¥: {e}")
                    continue
        
        if not results or total_computed_weight < 0.1:
            # å›é€€ï¼šå¦‚æœæ²¡æœ‰è¶³å¤Ÿæƒé‡çš„æ“ä½œï¼Œä½¿ç”¨skipè¿æ¥
            return self.operations[3](x)  # skip_connect
        
        return sum(results)

class ArchitectureManager(nn.Module):
    """æ¶æ„å‚æ•°ç®¡ç†å™¨"""
    
    def __init__(self, num_layers, num_ops):
        super().__init__()
        self.num_layers = num_layers
        self.num_ops = num_ops
        
        # ä¸ºæ¯å±‚åˆ›å»ºæ¶æ„å‚æ•°
        self.arch_params = nn.ParameterList()
        for i in range(num_layers):
            # æ¯å±‚çš„æ¶æ„å‚æ•° - é¿å…noneæ“ä½œè¢«é€‰ä¸­ï¼Œç»™skip_connectæ›´é«˜çš„åˆå§‹æƒé‡
            layer_params = nn.Parameter(torch.randn(num_ops) * 0.5)
            # ç»™skip_connect(ç´¢å¼•3)æ›´é«˜çš„åˆå§‹å€¼ï¼Œé¿å…none(ç´¢å¼•0)
            with torch.no_grad():
                layer_params[0] = -2.0  # noneæ“ä½œæƒé‡é™ä½
                if num_ops > 3:
                    layer_params[3] = 1.0   # skip_connectæƒé‡æé«˜
            self.arch_params.append(layer_params)
        
        print(f"ğŸ”§ ArchitectureManager: {num_layers} å±‚, æ¯å±‚ {num_ops} ä¸ªæ“ä½œ")
    
    def get_arch_weights(self, layer_idx, selector, training_phase='warmup'):
        """è·å–æŒ‡å®šå±‚çš„æ¶æ„æƒé‡"""
        if layer_idx >= len(self.arch_params):
            # å¦‚æœå±‚æ•°å¢åŠ äº†ï¼Œæ·»åŠ æ–°çš„æ¶æ„å‚æ•°
            while len(self.arch_params) <= layer_idx:
                new_params = nn.Parameter(torch.randn(self.num_ops) * 0.5)
                with torch.no_grad():
                    new_params[0] = -2.0  # noneæ“ä½œæƒé‡é™ä½
                    if self.num_ops > 3:
                        new_params[3] = 1.0   # skip_connectæƒé‡æé«˜
                if len(self.arch_params) > 0 and self.arch_params[0].device != torch.device('cpu'):
                    new_params = new_params.to(self.arch_params[0].device)
                self.arch_params.append(new_params)
        
        logits = self.arch_params[layer_idx]
        
        # åœ¨warmupé˜¶æ®µä½¿ç”¨å›ºå®šæ¶æ„ï¼ˆsoftmax without gumbel noiseï¼‰
        if training_phase == 'warmup':
            # åœ¨warmupé˜¶æ®µå¼ºåˆ¶ä½¿ç”¨skipè¿æ¥ï¼Œé¿å…noneæ“ä½œ
            weights = torch.zeros_like(logits)
            weights[3] = 1.0  # å¼ºåˆ¶ä½¿ç”¨skip_connect (index 3)
            return weights.detach()  # ä¸éœ€è¦æ¢¯åº¦
        else:
            # åœ¨æœç´¢é˜¶æ®µä½¿ç”¨Gumbel-Softmaxï¼Œä½†ä¿æŒæ¸©å’Œè¿‡æ¸¡
            if training_phase == 'search':
                # å¹³æ»‘è¿‡æ¸¡ï¼šæ··åˆlearned logitså’ŒGumbelé‡‡æ ·
                with torch.no_grad():
                    # ä»å½“å‰å­¦åˆ°çš„å‚æ•°å¼€å§‹ï¼Œé¿å…çªç„¶è·³è·ƒ
                    current_best = torch.argmax(logits).item()
                    if current_best == 3:  # å¦‚æœå½“å‰æœ€ä¼˜æ˜¯skip_connect
                        # ç»™å…¶ä»–æ“ä½œä¸€äº›æœºä¼šï¼Œä½†ä¸è¦å®Œå…¨éšæœº
                        logits = logits + torch.randn_like(logits) * 0.1
                
            return selector(logits.unsqueeze(0)).squeeze(0)
    
    def preserve_architecture_knowledge(self):
        """ä¿å­˜å½“å‰æ¶æ„çŸ¥è¯†ï¼Œç”¨äºå¹³æ»‘è¿‡æ¸¡"""
        preserved_logits = []
        for params in self.arch_params:
            preserved_logits.append(params.data.clone())
        return preserved_logits
    
    def smooth_transition_to_search(self, preserved_logits=None):
        """å¹³æ»‘è¿‡æ¸¡åˆ°æœç´¢é˜¶æ®µ"""
        if preserved_logits is not None:
            for i, preserved in enumerate(preserved_logits):
                if i < len(self.arch_params):
                    # ä¿æŒå­¦åˆ°çš„çŸ¥è¯†ï¼Œä½†å¢åŠ å°‘é‡æ¢ç´¢å™ªå£°
                    with torch.no_grad():
                        self.arch_params[i].data = preserved + torch.randn_like(preserved) * 0.05
    
    def get_current_genotype(self):
        """è·å–å½“å‰åŸºå› å‹"""
        genotype = []
        arch_weights_info = []  # æ·»åŠ æ¶æ„æƒé‡ä¿¡æ¯
        for i, layer_params in enumerate(self.arch_params):
            best_op_idx = torch.argmax(layer_params).item()
            best_op_name = PRIMITIVES[best_op_idx]
            genotype.append(best_op_name)
            
            # æ”¶é›†æƒé‡ä¿¡æ¯ç”¨äºè°ƒè¯•
            weights = F.softmax(layer_params, dim=0)
            max_weight = weights[best_op_idx].item()
            arch_weights_info.append({
                'layer': i,
                'best_op': best_op_name,
                'weight': max_weight,
                'entropy': -torch.sum(weights * torch.log(weights + 1e-8)).item()
            })
        
        return genotype, arch_weights_info
    
    def print_architecture_analysis(self):
        """æ‰“å°æ¶æ„åˆ†æä¿¡æ¯"""
        genotype, weights_info = self.get_current_genotype()
        
        print(f"\nğŸ” æ¶æ„åˆ†æ:")
        op_counts = {}
        avg_entropy = 0.0
        
        for info in weights_info:
            op = info['best_op']
            op_counts[op] = op_counts.get(op, 0) + 1
            avg_entropy += info['entropy']
            
            if info['weight'] < 0.5:  # æƒé‡ä¸ç¡®å®šçš„å±‚
                print(f"  âš ï¸ å±‚ {info['layer']}: {op} (æƒé‡: {info['weight']:.3f}, ç†µ: {info['entropy']:.3f})")
        
        avg_entropy /= len(weights_info)
        print(f"  ğŸ“Š æ“ä½œåˆ†å¸ƒ: {op_counts}")
        print(f"  ğŸ² å¹³å‡æ¶æ„ç†µ: {avg_entropy:.3f}")
        
        return genotype

class EvolvableBlock(nn.Module):
    """å¯è¿›åŒ–çš„ç½‘ç»œå—"""
    
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.stride = stride
        
        # é¢„å¤„ç†å±‚ï¼ˆå¦‚æœé€šé“æ•°ä¸åŒ¹é…æˆ–éœ€è¦ä¸‹é‡‡æ ·ï¼‰
        self.preprocess = None
        if in_channels != out_channels or stride != 1:
            self.preprocess = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
        
        # ä¸»è¦çš„æ··åˆæ“ä½œ - ä¼ é€’æ­£ç¡®çš„stride
        self.mixed_op = MixedOperation(out_channels, stride=1)  # æ··åˆæ“ä½œå†…éƒ¨ä¸ä¸‹é‡‡æ ·
        
        # æ®‹å·®è¿æ¥
        self.use_residual = (in_channels == out_channels and stride == 1)
    
    def forward(self, x, arch_weights):
        """å‰å‘ä¼ æ’­"""
        # ä¿å­˜è¾“å…¥ç”¨äºæ®‹å·®è¿æ¥
        identity = x
        
        # é¢„å¤„ç†ï¼ˆä¸‹é‡‡æ ·å’Œé€šé“è°ƒæ•´ï¼‰
        if self.preprocess is not None:
            x = self.preprocess(x)
            identity = x  # æ›´æ–°æ®‹å·®è¿æ¥çš„åŸºå‡†
        
        # æ··åˆæ“ä½œ
        out = self.mixed_op(x, arch_weights)
        
        # æ®‹å·®è¿æ¥ï¼ˆä»…åœ¨ç»´åº¦åŒ¹é…æ—¶ï¼‰
        if self.use_residual:
            out = out + identity
        
        return out

class ASOSENetwork(nn.Module):
    """ASO-SEå¯ç”Ÿé•¿ç¥ç»ç½‘ç»œ"""
    
    def __init__(self, input_channels=3, initial_channels=64, num_classes=10, initial_depth=6):
        super().__init__()
        self.input_channels = input_channels
        self.initial_channels = initial_channels
        self.num_classes = num_classes
        self.current_depth = initial_depth
        self.current_channels = initial_channels
        
        # æ”¹è¿›çš„åˆå§‹ç‰¹å¾æå–
        self.stem = nn.Sequential(
            nn.Conv2d(input_channels, initial_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(initial_channels),
            nn.ReLU(inplace=True)
        )
        
        # å¯è¿›åŒ–å±‚
        self.layers = nn.ModuleList()
        current_channels = initial_channels
        
        for i in range(initial_depth):
            # æ”¹è¿›çš„ä¸‹é‡‡æ ·ç­–ç•¥ï¼šåªåœ¨ç¬¬2å±‚å’Œç¬¬4å±‚ä¸‹é‡‡æ ·
            stride = 2 if i in [1, 3] else 1
            if stride == 2:
                next_channels = min(current_channels * 2, 256)
            else:
                next_channels = current_channels
            
            layer = EvolvableBlock(current_channels, next_channels, stride)
            self.layers.append(layer)
            current_channels = next_channels
        
        # å…¨å±€å¹³å‡æ± åŒ–å’Œåˆ†ç±»å™¨
        self.global_pooling = nn.AdaptiveAvgPool2d(1)
        self.classifier = nn.Linear(current_channels, num_classes)
        
        # æ¶æ„ç®¡ç†å™¨
        self.arch_manager = ArchitectureManager(self.current_depth, len(PRIMITIVES))
        
        # Gumbel-Softmaxé€‰æ‹©å™¨
        self.gumbel_selector = GumbelSoftmax(hard=True, temperature=1.0, min_temperature=0.1)
        
        # Net2Netè¿ç§»å·¥å…·
        self.net2net_transfer = Net2NetTransfer()
        
        # è®­ç»ƒé˜¶æ®µçŠ¶æ€
        self.training_phase = 'warmup'
        
        print(f"ğŸš€ ASO-SE ç½‘ç»œåˆå§‹åŒ–:")
        print(f"   æ·±åº¦: {self.current_depth} å±‚")
        print(f"   åˆå§‹é€šé“: {initial_channels}")
        print(f"   å½“å‰é€šé“: {current_channels}")
        print(f"   å‚æ•°é‡: {sum(p.numel() for p in self.parameters() if p.requires_grad):,}")
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        x = self.stem(x)
        
        for i, layer in enumerate(self.layers):
            arch_weights = self.arch_manager.get_arch_weights(i, self.gumbel_selector, self.training_phase)
            x = layer(x, arch_weights)
        
        x = self.global_pooling(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        
        return x
    
    def set_training_phase(self, phase):
        """è®¾ç½®è®­ç»ƒé˜¶æ®µ"""
        self.training_phase = phase
        print(f"ğŸ”„ è®¾ç½®è®­ç»ƒé˜¶æ®µ: {phase}")
    
    def grow_depth(self, num_new_layers=1):
        """æ·±åº¦ç”Ÿé•¿ - æ·»åŠ æ–°å±‚"""
        print(f"ğŸŒ± ç½‘ç»œæ·±åº¦ç”Ÿé•¿: æ·»åŠ  {num_new_layers} å±‚")
        
        for _ in range(num_new_layers):
            # åœ¨å€’æ•°ç¬¬äºŒå±‚åæ’å…¥æ–°å±‚
            insert_pos = len(self.layers) - 1
            if insert_pos <= 0:
                insert_pos = len(self.layers) // 2  # åœ¨ä¸­é—´æ’å…¥
            
            # è·å–å½“å‰å±‚çš„é€šé“æ•°
            reference_layer = self.layers[insert_pos]
            current_channels = reference_layer.out_channels
            
            # åˆ›å»ºæ–°å±‚å¹¶ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
            new_layer = EvolvableBlock(current_channels, current_channels, stride=1)
            new_layer = new_layer.to(next(self.parameters()).device)
            
            self.layers.insert(insert_pos, new_layer)
            self.current_depth += 1
        
        # æ›´æ–°æ¶æ„ç®¡ç†å™¨ï¼ˆä¿æŒç°æœ‰å‚æ•°ï¼‰
        # ArchitectureManagerå·²ç»èƒ½å¤ŸåŠ¨æ€æ‰©å±•å‚æ•°ï¼Œæ— éœ€é‡æ–°åˆ›å»º
        
        print(f"   æ–°æ·±åº¦: {self.current_depth}")
    
    def grow_width(self, growth_factor=1.5):
        """å®½åº¦ç”Ÿé•¿ - æ‰©å±•é€šé“æ•°ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        print(f"ğŸŒ± ç½‘ç»œå®½åº¦ç”Ÿé•¿: å¢é•¿å› å­ {growth_factor}")
        
        # ç®€åŒ–å®ç°ï¼šåªæ‰©å±•åˆ†ç±»å™¨çš„è¾“å…¥ç‰¹å¾æ•°
        # çœŸæ­£çš„å®½åº¦æ‰©å±•éœ€è¦æ›´å¤æ‚çš„Net2Netæ“ä½œï¼Œè¿™é‡Œå…ˆè®°å½•æ„å›¾
        old_classifier = self.classifier
        current_features = old_classifier.in_features
        new_features = int(current_features * growth_factor)
        
        if new_features > current_features:
            print(f"   åˆ†ç±»å™¨æ‰©å±•: {current_features} -> {new_features} ç‰¹å¾")
            # è¿™é‡Œå¯ä»¥åœ¨æœªæ¥é›†æˆçœŸæ­£çš„Net2Netå®½åº¦æ‰©å±•
        else:
            print(f"   å®½åº¦ç”Ÿé•¿è·³è¿‡ï¼ˆå¢é•¿å› å­å¤ªå°ï¼‰")
    
    def get_architecture_info(self):
        """è·å–æ¶æ„ä¿¡æ¯"""
        genotype, _ = self.arch_manager.get_current_genotype()  # è§£åŒ…å…ƒç»„
        params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'depth': self.current_depth,
            'genotype': genotype,
            'parameters': params,
            'temperature': self.gumbel_selector.temperature
        }

class ASOSETrainingController:
    """ASO-SEè®­ç»ƒæ§åˆ¶å™¨"""
    
    def __init__(self, network, growth_patience=5, performance_threshold=0.02):
        self.network = network
        self.growth_patience = growth_patience
        self.performance_threshold = performance_threshold
        
        self.best_accuracy = 0.0
        self.patience_counter = 0
        self.growth_history = []
    
    def should_grow(self, current_accuracy):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç”Ÿé•¿"""
        improvement = current_accuracy - self.best_accuracy
        
        if improvement > self.performance_threshold:
            self.best_accuracy = current_accuracy
            self.patience_counter = 0
            return False
        else:
            self.patience_counter += 1
            return self.patience_counter >= self.growth_patience
    
    def trigger_growth(self, growth_type='depth'):
        """è§¦å‘ç½‘ç»œç”Ÿé•¿"""
        print(f"ğŸŒ± è§¦å‘ {growth_type} ç”Ÿé•¿")
        
        if growth_type == 'depth':
            self.network.grow_depth(1)
        elif growth_type == 'width':
            self.network.grow_width(1.2)
        
        self.growth_history.append({
            'type': growth_type,
            'step': len(self.growth_history),
            'architecture': self.network.get_architecture_info()
        })
        
        # é‡ç½®æ§åˆ¶å™¨
        self.patience_counter = 0

class ASOSETrainer:
    """ASO-SEè®­ç»ƒå™¨"""
    
    def __init__(self, experiment_name="aso_se_neural_growth"):
        self.experiment_name = experiment_name
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # è®­ç»ƒå‚æ•°
        self.batch_size = 128
        self.num_epochs = 100
        self.weight_lr = 0.025
        self.arch_lr = 3e-4
        self.momentum = 0.9
        self.weight_decay = 3e-4
        
        # é˜¶æ®µæ§åˆ¶
        self.phase_durations = {
            'warmup': 10,      # é¢„çƒ­é˜¶æ®µ
            'search': 30,      # æ¶æ„æœç´¢é˜¶æ®µ
            'growth': 40,      # ç”Ÿé•¿é˜¶æ®µ
            'optimize': 20     # ä¼˜åŒ–é˜¶æ®µ
        }
        
        self.current_phase = 'warmup'
        self.phase_epochs = 0
        
        print(f"ğŸš€ ASO-SE è®­ç»ƒå™¨åˆå§‹åŒ–")
        print(f"   å®éªŒåç§°: {experiment_name}")
        print(f"   è®¾å¤‡: {self.device}")
    
    def setup_data(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        # CIFAR-10æ•°æ®å¢å¼º
        train_transform = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])
        
        test_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])
        
        # æ•°æ®é›†
        train_dataset = torchvision.datasets.CIFAR10(
            root='./data', train=True, download=True, transform=train_transform)
        test_dataset = torchvision.datasets.CIFAR10(
            root='./data', train=False, download=True, transform=test_transform)
        
        # æ•°æ®åŠ è½½å™¨
        self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, 
                                     shuffle=True, num_workers=4, pin_memory=True)
        self.test_loader = DataLoader(test_dataset, batch_size=self.batch_size, 
                                    shuffle=False, num_workers=4, pin_memory=True)
        
        print(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ: è®­ç»ƒé›† {len(train_dataset)}, æµ‹è¯•é›† {len(test_dataset)}")
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹"""
        self.network = ASOSENetwork(
            input_channels=3,
            initial_channels=64,
            num_classes=10,
            initial_depth=6
        ).to(self.device)
        
        # è®­ç»ƒæ§åˆ¶å™¨
        self.training_controller = ASOSETrainingController(self.network)
        
        print(f"ğŸ—ï¸ æ¨¡å‹è®¾ç½®å®Œæˆ")
    
    def setup_optimizers(self):
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        # è·å–æ¶æ„å‚æ•°çš„IDé›†åˆï¼Œé¿å…å¼ é‡æ¯”è¾ƒ
        arch_param_ids = {id(p) for p in self.network.arch_manager.parameters()}
        
        # æƒé‡ä¼˜åŒ–å™¨ - æ’é™¤æ¶æ„å‚æ•°
        weight_params = [p for p in self.network.parameters() if id(p) not in arch_param_ids]
        self.weight_optimizer = optim.SGD(
            weight_params,
            lr=self.weight_lr,
            momentum=self.momentum,
            weight_decay=self.weight_decay
        )
        
        # æ¶æ„ä¼˜åŒ–å™¨
        self.arch_optimizer = optim.Adam(
            self.network.arch_manager.parameters(),
            lr=self.arch_lr,
            betas=(0.5, 0.999),
            weight_decay=1e-3
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.weight_scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.weight_optimizer, T_max=self.num_epochs, eta_min=1e-4)
        self.arch_scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.arch_optimizer, T_max=self.num_epochs, eta_min=1e-5)
        
        print(f"âš™ï¸ ä¼˜åŒ–å™¨è®¾ç½®å®Œæˆ")
    
    def _update_optimizers_after_growth(self):
        """ç”Ÿé•¿åå®‰å…¨åœ°æ›´æ–°ä¼˜åŒ–å™¨"""
        try:
            # ä¿å­˜å½“å‰å­¦ä¹ ç‡
            current_weight_lr = self.weight_optimizer.param_groups[0]['lr']
            current_arch_lr = self.arch_optimizer.param_groups[0]['lr']
            
            # é‡æ–°è®¾ç½®ä¼˜åŒ–å™¨
            self.setup_optimizers()
            
            # æ¢å¤å­¦ä¹ ç‡
            for param_group in self.weight_optimizer.param_groups:
                param_group['lr'] = current_weight_lr
            for param_group in self.arch_optimizer.param_groups:
                param_group['lr'] = current_arch_lr
                
            print(f"âœ… ä¼˜åŒ–å™¨å·²æ›´æ–°ä»¥åŒ…å«æ–°å‚æ•°")
            
        except Exception as e:
            print(f"âš ï¸ ä¼˜åŒ–å™¨æ›´æ–°è­¦å‘Š: {e}")
            # å¦‚æœæ›´æ–°å¤±è´¥ï¼Œè‡³å°‘å°è¯•åŸºæœ¬è®¾ç½®
            self.setup_optimizers()
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.network.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{self.num_epochs}')
        
        for batch_idx, (data, targets) in enumerate(pbar):
            data, targets = data.to(self.device), targets.to(self.device)
            
            # åœ¨warmupå’Œoptimizeé˜¶æ®µï¼Œåªä¼˜åŒ–æƒé‡å‚æ•°
            if self.current_phase in ['warmup', 'optimize']:
                self.weight_optimizer.zero_grad()
                outputs = self.network(data)
                loss = F.cross_entropy(outputs, targets)
                loss.backward()
                self.weight_optimizer.step()
                
            # åœ¨searchå’Œgrowthé˜¶æ®µï¼Œäº¤æ›¿ä¼˜åŒ–æƒé‡å’Œæ¶æ„å‚æ•°ï¼ˆé¿å…å¹²æ‰°ï¼‰
            elif self.current_phase in ['search', 'growth']:
                if batch_idx % 3 == 0:  # æ¶æ„ä¼˜åŒ–é¢‘ç‡é™ä½ï¼Œé¿å…è¿‡åº¦å¹²æ‰°
                    # æ¶æ„å‚æ•°ä¼˜åŒ–
                    self.arch_optimizer.zero_grad()
                    arch_outputs = self.network(data)
                    arch_loss = F.cross_entropy(arch_outputs, targets)
                    arch_loss.backward()
                    self.arch_optimizer.step()
                    
                    # æ¯æ¬¡æ¶æ„æ›´æ–°åè¿›è¡Œæ¸©åº¦é€€ç«
                    self.network.gumbel_selector.anneal_temperature()
                    
                else:
                    # æƒé‡å‚æ•°ä¼˜åŒ–
                    self.weight_optimizer.zero_grad()
                    outputs = self.network(data)
                    loss = F.cross_entropy(outputs, targets)
                    loss.backward()
                    self.weight_optimizer.step()
            
            # ç»Ÿè®¡ï¼ˆä½¿ç”¨æœ€åçš„å‰å‘ä¼ æ’­ç»“æœï¼‰
            with torch.no_grad():
                if 'outputs' not in locals():
                    outputs = self.network(data)
                total_loss += F.cross_entropy(outputs, targets).item()
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
            
            # æ›´æ–°è¿›åº¦æ¡
            accuracy = 100. * correct / total
            current_temp = self.network.gumbel_selector.temperature
            pbar.set_postfix({
                'Loss': f'{total_loss/(batch_idx+1):.3f}',
                'Acc': f'{accuracy:.2f}%',
                'Phase': self.current_phase,
                'Temp': f'{current_temp:.3f}'
            })
        
        return total_loss / len(self.train_loader), accuracy
    
    def evaluate(self):
        """è¯„ä¼°æ¨¡å‹"""
        self.network.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, targets in self.test_loader:
                data, targets = data.to(self.device), targets.to(self.device)
                outputs = self.network(data)
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
        
        accuracy = 100. * correct / total
        return accuracy
    
    def update_phase(self, epoch):
        """æ›´æ–°è®­ç»ƒé˜¶æ®µ"""
        self.phase_epochs += 1
        
        # é˜¶æ®µè½¬æ¢é€»è¾‘
        old_phase = self.current_phase
        
        if self.current_phase == 'warmup' and self.phase_epochs >= self.phase_durations['warmup']:
            # ä¿å­˜warmupé˜¶æ®µå­¦åˆ°çš„æ¶æ„çŸ¥è¯†
            preserved_knowledge = self.network.arch_manager.preserve_architecture_knowledge()
            
            self.current_phase = 'search'
            self.phase_epochs = 0
            print(f"ğŸ”„ è¿›å…¥æ¶æ„æœç´¢é˜¶æ®µ")
            
            # å®ç°å¹³æ»‘è¿‡æ¸¡åˆ°æœç´¢é˜¶æ®µ
            self.network.arch_manager.smooth_transition_to_search(preserved_knowledge)
            
            # é‡ç½®Gumbelæ¸©åº¦ä¸ºæœç´¢é˜¶æ®µé€‚åˆçš„å€¼
            self.network.gumbel_selector.temperature = 0.8  # é€‚ä¸­çš„æ¸©åº¦å¼€å§‹æœç´¢
            print(f"ğŸŒ¡ï¸ é‡ç½®Gumbelæ¸©åº¦ä¸º {self.network.gumbel_selector.temperature}")
        
        elif self.current_phase == 'search' and self.phase_epochs >= self.phase_durations['search']:
            self.current_phase = 'growth'
            self.phase_epochs = 0
            print(f"ğŸ”„ è¿›å…¥ç½‘ç»œç”Ÿé•¿é˜¶æ®µ")
        
        elif self.current_phase == 'growth' and self.phase_epochs >= self.phase_durations['growth']:
            self.current_phase = 'optimize'
            self.phase_epochs = 0
            print(f"ğŸ”„ è¿›å…¥æœ€ç»ˆä¼˜åŒ–é˜¶æ®µ")
            
            # åœ¨ä¼˜åŒ–é˜¶æ®µå›ºå®šæ¶æ„ï¼Œä¸“æ³¨äºæƒé‡ä¼˜åŒ–
            self.network.gumbel_selector.temperature = 0.01  # æä½æ¸©åº¦ï¼Œå‡ ä¹ç¡®å®šæ€§
        
        # åŒæ­¥ç½‘ç»œçš„è®­ç»ƒé˜¶æ®µ
        if old_phase != self.current_phase:
            self.network.set_training_phase(self.current_phase)
            print(f"âœ… é˜¶æ®µè½¬æ¢: {old_phase} â†’ {self.current_phase}")
            
            # æ‰“å°å½“å‰æ¶æ„çŠ¶æ€
            genotype = self.network.arch_manager.print_architecture_analysis()
            print(f"ğŸ“‹ å½“å‰åŸºå› å‹: {genotype[:5]}...")  # æ˜¾ç¤ºå‰5ä¸ªæ“ä½œ
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"\nğŸ”§ ASO-SE è®­ç»ƒå¼€å§‹")
        print(f"{'='*60}")
        
        self.setup_data()
        self.setup_model()
        self.setup_optimizers()
        
        best_accuracy = 0.0
        training_history = []
        
        for epoch in range(self.num_epochs):
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(epoch)
            
            # è¯„ä¼°
            test_acc = self.evaluate()
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.weight_scheduler.step()
            if self.current_phase in ['search', 'growth']:
                self.arch_scheduler.step()
            
            # è®°å½•å†å²
            training_history.append({
                'epoch': epoch,
                'phase': self.current_phase,
                'train_loss': train_loss,
                'train_acc': train_acc,
                'test_acc': test_acc,
                'architecture': self.network.get_architecture_info()
            })
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿé•¿
            if self.current_phase == 'growth':
                if self.training_controller.should_grow(test_acc):
                    growth_type = 'depth' if epoch % 2 == 0 else 'width'
                    self.training_controller.trigger_growth(growth_type)
                    # å®‰å…¨åœ°æ›´æ–°ä¼˜åŒ–å™¨ä»¥åŒ…å«æ–°å‚æ•°
                    self._update_optimizers_after_growth()
            
            # æ›´æ–°æœ€ä½³ç²¾åº¦
            if test_acc > best_accuracy:
                best_accuracy = test_acc
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.network.state_dict(),
                    'optimizer_state_dict': self.weight_optimizer.state_dict(),
                    'architecture': self.network.get_architecture_info(),
                    'accuracy': best_accuracy
                }, f'{self.experiment_name}_best.pth')
            
            # æ›´æ–°é˜¶æ®µ
            self.update_phase(epoch)
            
            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 5 == 0:
                arch_info = self.network.get_architecture_info()
                print(f"\nğŸ“Š Epoch {epoch+1}/{self.num_epochs} | Phase: {self.current_phase}")
                print(f"   Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")
                print(f"   Test Acc: {test_acc:.2f}% | Best: {best_accuracy:.2f}%")
                print(f"   ç½‘ç»œæ·±åº¦: {arch_info['depth']} | å‚æ•°é‡: {arch_info['parameters']:,}")
                print(f"   å½“å‰åŸºå› å‹: {arch_info['genotype'][:3]}...")
                
                # åœ¨æœç´¢é˜¶æ®µæ‰“å°è¯¦ç»†æ¶æ„åˆ†æ
                if self.current_phase == 'search':
                    self.network.arch_manager.print_architecture_analysis()
        
        print(f"\nğŸ‰ ASO-SE è®­ç»ƒå®Œæˆ!")
        print(f"   æœ€ä½³ç²¾åº¦: {best_accuracy:.2f}%")
        print(f"   æœ€ç»ˆæ¶æ„: {self.network.get_architecture_info()}")
        
        return training_history, best_accuracy

# åŸºç¡€æ“ä½œå®ç°
class Identity(nn.Module):
    def forward(self, x):
        return x

class FactorizedReduce(nn.Module):
    def __init__(self, C_in, C_out):
        super().__init__()
        assert C_out % 2 == 0
        self.relu = nn.ReLU(inplace=False)
        self.conv_1 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)
        self.conv_2 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)
        self.bn = nn.BatchNorm2d(C_out, affine=True)

    def forward(self, x):
        x = self.relu(x)
        out = torch.cat([self.conv_1(x), self.conv_2(x[:, :, 1:, 1:])], dim=1)
        out = self.bn(out)
        return out

class SepConv(nn.Module):
    def __init__(self, C_in, C_out, kernel_size, stride, padding):
        super().__init__()
        self.op = nn.Sequential(
            nn.ReLU(inplace=False),
            nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, 
                     padding=padding, groups=C_in, bias=False),
            nn.Conv2d(C_in, C_in, kernel_size=1, padding=0, bias=False),
            nn.BatchNorm2d(C_in, affine=True),
            nn.ReLU(inplace=False),
            nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=1, 
                     padding=padding, groups=C_in, bias=False),
            nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False),
            nn.BatchNorm2d(C_out, affine=True),
        )

    def forward(self, x):
        return self.op(x)

class DilConv(nn.Module):
    def __init__(self, C_in, C_out, kernel_size, stride, padding, dilation):
        super().__init__()
        self.op = nn.Sequential(
            nn.ReLU(inplace=False),
            nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, 
                     padding=padding, dilation=dilation, groups=C_in, bias=False),
            nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False),
            nn.BatchNorm2d(C_out, affine=True),
        )

    def forward(self, x):
        return self.op(x)

class Zero(nn.Module):
    def __init__(self, stride):
        super().__init__()
        self.stride = stride

    def forward(self, x):
        if self.stride == 1:
            return x.mul(0.)
        return x[:, :, ::self.stride, ::self.stride].mul(0.)

class Conv7x1_1x7(nn.Module):
    def __init__(self, C_in, C_out, stride):
        super().__init__()
        self.op = nn.Sequential(
            nn.ReLU(inplace=False),
            nn.Conv2d(C_in, C_out, (1, 7), stride=(1, stride), padding=(0, 3), bias=False),
            nn.Conv2d(C_out, C_out, (7, 1), stride=(stride, 1), padding=(3, 0), bias=False),
            nn.BatchNorm2d(C_out, affine=True)
        )

    def forward(self, x):
        return self.op(x)

def main():
    parser = argparse.ArgumentParser(description='ASO-SE ç¥ç»ç½‘ç»œè‡ªé€‚åº”æ¶æ„æœç´¢')
    parser.add_argument('--epochs', type=int, default=100, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=128, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=0.025, help='å­¦ä¹ ç‡')
    parser.add_argument('--experiment', type=str, default='aso_se_neural_growth', help='å®éªŒåç§°')
    
    args = parser.parse_args()
    
    print("ğŸ”§ ASO-SE: çœŸæ­£çš„ç¥ç»æ¶æ„æœç´¢ä¸ç½‘ç»œç”Ÿé•¿")
    print(f"   ç›®æ ‡: CIFAR-10 95%å‡†ç¡®ç‡")
    print(f"   ç­–ç•¥: å››é˜¶æ®µè®­ç»ƒ + Net2Netå¹³æ»‘è¿ç§»")
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = ASOSETrainer(args.experiment)
    trainer.batch_size = args.batch_size
    trainer.num_epochs = args.epochs
    trainer.weight_lr = args.lr
    
    history, best_acc = trainer.train()
    
    print(f"\nâœ¨ å®éªŒå®Œæˆ!")
    print(f"   æœ€ç»ˆç²¾åº¦: {best_acc:.2f}%")

if __name__ == '__main__':
    main() 