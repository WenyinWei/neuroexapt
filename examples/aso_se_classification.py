#!/usr/bin/env python3
"""
ASO-SE (Alternating Stable Optimization with Stochastic Exploration) ç¥ç»ç½‘ç»œè®­ç»ƒ

æ ¸å¿ƒç‰¹æ€§ï¼š
ğŸš€ çœŸæ­£çš„æ¶æ„æœç´¢å’Œç½‘ç»œç»“æ„åŠ¨æ€ç”Ÿé•¿
ğŸ”§ åŸºäºNet2Netçš„å¹³æ»‘å‚æ•°è¿ç§»
âš¡ Gumbel-Softmaxå¼•å¯¼çš„å¯å¾®åˆ†æ¶æ„é‡‡æ ·
ğŸ¯ å››é˜¶æ®µè®­ç»ƒå¾ªç¯ï¼šé¢„çƒ­â†’æœç´¢â†’ç”Ÿé•¿â†’ä¼˜åŒ–

æ¶æ„ç”Ÿé•¿ç­–ç•¥ï¼š
- æ·±åº¦ç”Ÿé•¿ï¼šæ·»åŠ æ–°çš„å¯è¿›åŒ–å±‚
- å®½åº¦ç”Ÿé•¿ï¼šæ‰©å±•ç°æœ‰å±‚çš„é€šé“æ•°
- åˆ†æ”¯ç”Ÿé•¿ï¼šå¢åŠ æ“ä½œåˆ†æ”¯çš„å¤æ‚åº¦
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import numpy as np
import time
import os
import sys
import argparse
from datetime import datetime
from tqdm import tqdm
from typing import Dict, List, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥æ¨¡å—
from neuroexapt.core.genotypes import PRIMITIVES
from neuroexapt.core.operations import OPS
from neuroexapt.core.net2net_transfer import Net2NetTransfer

# é…ç½®æ—¥å¿—
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(message)s', datefmt='%H:%M:%S')
logger = logging.getLogger()

class GumbelSoftmaxSelector(nn.Module):
    """Gumbel-Softmaxæ¶æ„é‡‡æ ·å™¨"""
    
    def __init__(self, initial_temp=5.0, min_temp=0.1, anneal_rate=0.98):
        super().__init__()
        self.temperature = initial_temp
        self.min_temperature = min_temp
        self.anneal_rate = anneal_rate
        
    def forward(self, logits, hard=True):
        """Gumbel-Softmaxé‡‡æ ·"""
        if not self.training:
            # æ¨ç†æ—¶ä½¿ç”¨argmax
            y_hard = torch.zeros_like(logits)
            y_hard.scatter_(-1, torch.argmax(logits, dim=-1, keepdim=True), 1.0)
            return y_hard
        
        # è®­ç»ƒæ—¶ä½¿ç”¨Gumbel-Softmax
        gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) + 1e-8) + 1e-8)
        y_soft = F.softmax((logits + gumbel_noise) / self.temperature, dim=-1)
        
        if hard:
            # Straight-through estimator
            max_indices = torch.argmax(y_soft, dim=-1, keepdim=True)
            y_hard = torch.zeros_like(y_soft).scatter_(-1, max_indices, 1.0)
            return y_hard - y_soft.detach() + y_soft
        else:
            return y_soft
    
    def anneal_temperature(self):
        """æ¸©åº¦é€€ç«"""
        self.temperature = max(self.min_temperature, self.temperature * self.anneal_rate)

class MixedOperation(nn.Module):
    """æ··åˆæ“ä½œå±‚ - æ”¯æŒå¤šç§åŸå§‹æ“ä½œ"""
    
    def __init__(self, C, stride):
        super().__init__()
        self.operations = nn.ModuleList()
        self.C = C
        self.stride = stride
        
        # åˆ›å»ºæ‰€æœ‰å€™é€‰æ“ä½œ
        for primitive in PRIMITIVES:
            op = self._create_operation(primitive, C, stride)
            self.operations.append(op)
        
        self.num_ops = len(PRIMITIVES)
        print(f"ğŸ”§ MixedOperation åˆ›å»º: {self.num_ops} ä¸ªæ“ä½œ, C={C}, stride={stride}")
    
    def _create_operation(self, primitive, C, stride):
        """åˆ›å»ºå•ä¸ªæ“ä½œ"""
        if primitive in OPS:
            return OPS[primitive](C, stride, False)
        elif primitive == 'none':
            return Zero(stride)
        elif primitive == 'skip_connect':
            if stride == 1:
                return Identity()
            else:
                return FactorizedReduce(C, C)
        elif primitive == 'sep_conv_3x3':
            return SepConv(C, C, 3, stride, 1)
        elif primitive == 'sep_conv_5x5':
            return SepConv(C, C, 5, stride, 2)
        elif primitive == 'sep_conv_7x7':
            return SepConv(C, C, 7, stride, 3)
        elif primitive == 'dil_conv_3x3':
            return DilConv(C, C, 3, stride, 2, 2)
        elif primitive == 'dil_conv_5x5':
            return DilConv(C, C, 5, stride, 4, 2)
        elif primitive == 'conv_7x1_1x7':
            return Conv7x1_1x7(C, C, stride)
        elif primitive == 'avg_pool_3x3':
            return nn.AvgPool2d(3, stride=stride, padding=1, count_include_pad=False)
        elif primitive == 'max_pool_3x3':
            return nn.MaxPool2d(3, stride=stride, padding=1)
        else:
            raise ValueError(f"Unknown primitive: {primitive}")
    
    def forward(self, x, arch_weights):
        """å‰å‘ä¼ æ’­"""
        # å¯¹æ¯ä¸ªæ“ä½œè®¡ç®—ç»“æœå¹¶åŠ æƒæ±‚å’Œ
        results = []
        for i, op in enumerate(self.operations):
            results.append(arch_weights[i] * op(x))
        
        return sum(results)

class ArchitectureManager(nn.Module):
    """æ¶æ„å‚æ•°ç®¡ç†å™¨"""
    
    def __init__(self, num_layers, num_ops):
        super().__init__()
        self.num_layers = num_layers
        self.num_ops = num_ops
        
        # ä¸ºæ¯å±‚åˆ›å»ºæ¶æ„å‚æ•°
        self.arch_params = nn.ParameterList()
        for i in range(num_layers):
            # æ¯å±‚çš„æ¶æ„å‚æ•° - é¿å…noneæ“ä½œè¢«é€‰ä¸­ï¼Œç»™skip_connectæ›´é«˜çš„åˆå§‹æƒé‡
            layer_params = nn.Parameter(torch.randn(num_ops) * 0.5)
            # ç»™skip_connect(ç´¢å¼•3)æ›´é«˜çš„åˆå§‹å€¼ï¼Œé¿å…none(ç´¢å¼•0)
            with torch.no_grad():
                layer_params[0] = -2.0  # noneæ“ä½œæƒé‡é™ä½
                if num_ops > 3:
                    layer_params[3] = 1.0   # skip_connectæƒé‡æé«˜
            self.arch_params.append(layer_params)
        
        print(f"ğŸ”§ ArchitectureManager: {num_layers} å±‚, æ¯å±‚ {num_ops} ä¸ªæ“ä½œ")
    
    def get_arch_weights(self, layer_idx, selector, training_phase='warmup'):
        """è·å–æŒ‡å®šå±‚çš„æ¶æ„æƒé‡"""
        if layer_idx >= len(self.arch_params):
            # å¦‚æœå±‚æ•°å¢åŠ äº†ï¼Œæ·»åŠ æ–°çš„æ¶æ„å‚æ•°
            while len(self.arch_params) <= layer_idx:
                new_params = nn.Parameter(torch.randn(self.num_ops) * 0.5)
                with torch.no_grad():
                    new_params[0] = -2.0  # noneæ“ä½œæƒé‡é™ä½
                    if self.num_ops > 3:
                        new_params[3] = 1.0   # skip_connectæƒé‡æé«˜
                if len(self.arch_params) > 0 and self.arch_params[0].device != torch.device('cpu'):
                    new_params = new_params.to(self.arch_params[0].device)
                self.arch_params.append(new_params)
        
        logits = self.arch_params[layer_idx]
        
        # åœ¨warmupé˜¶æ®µä½¿ç”¨å›ºå®šæ¶æ„ï¼ˆsoftmax without gumbel noiseï¼‰
        if training_phase == 'warmup':
            # åœ¨warmupé˜¶æ®µå¼ºåˆ¶ä½¿ç”¨skipè¿æ¥ï¼Œé¿å…noneæ“ä½œ
            weights = torch.zeros_like(logits)
            weights[3] = 1.0  # å¼ºåˆ¶ä½¿ç”¨skip_connect (index 3)
            return weights.detach()  # ä¸éœ€è¦æ¢¯åº¦
        else:
            # åœ¨æœç´¢å’Œç”Ÿé•¿é˜¶æ®µä½¿ç”¨Gumbel-Softmax
            return selector(logits.unsqueeze(0)).squeeze(0)
    
    def get_current_genotype(self):
        """è·å–å½“å‰åŸºå› å‹"""
        genotype = []
        for layer_params in self.arch_params:
            best_op_idx = torch.argmax(layer_params).item()
            best_op = PRIMITIVES[best_op_idx]
            genotype.append(best_op)
        return genotype

class EvolvableBlock(nn.Module):
    """å¯è¿›åŒ–çš„ç½‘ç»œå—"""
    
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.stride = stride
        
        # é¢„å¤„ç†å±‚ï¼ˆå¦‚æœé€šé“æ•°ä¸åŒ¹é…ï¼‰
        self.preprocess = None
        if in_channels != out_channels:
            self.preprocess = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
        
        # ä¸»è¦çš„æ··åˆæ“ä½œ
        self.mixed_op = MixedOperation(out_channels, stride=1)
    
    def forward(self, x, arch_weights):
        """å‰å‘ä¼ æ’­"""
        if self.preprocess is not None:
            x = self.preprocess(x)
        
        out = self.mixed_op(x, arch_weights)
        return out

class ASOSENetwork(nn.Module):
    """ASO-SEå¯ç”Ÿé•¿ç¥ç»ç½‘ç»œ"""
    
    def __init__(self, input_channels=3, initial_channels=16, num_classes=10, initial_depth=8):
        super().__init__()
        self.input_channels = input_channels
        self.initial_channels = initial_channels
        self.num_classes = num_classes
        self.current_depth = initial_depth
        self.current_channels = initial_channels
        
        # åˆå§‹ç‰¹å¾æå–
        self.stem = nn.Sequential(
            nn.Conv2d(input_channels, initial_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(initial_channels),
            nn.ReLU(inplace=True)
        )
        
        # å¯è¿›åŒ–å±‚
        self.layers = nn.ModuleList()
        current_channels = initial_channels
        
        for i in range(initial_depth):
            # æ¯éš”å‡ å±‚è¿›è¡Œä¸‹é‡‡æ ·
            stride = 2 if i in [initial_depth//3, 2*initial_depth//3] else 1
            if stride == 2:
                next_channels = min(current_channels * 2, 512)
            else:
                next_channels = current_channels
            
            layer = EvolvableBlock(current_channels, next_channels, stride)
            self.layers.append(layer)
            current_channels = next_channels
        
        # å…¨å±€å¹³å‡æ± åŒ–å’Œåˆ†ç±»å™¨
        self.global_pooling = nn.AdaptiveAvgPool2d(1)
        self.classifier = nn.Linear(current_channels, num_classes)
        
        # æ¶æ„ç®¡ç†å™¨
        self.arch_manager = ArchitectureManager(self.current_depth, len(PRIMITIVES))
        
        # Gumbel-Softmaxé€‰æ‹©å™¨
        self.gumbel_selector = GumbelSoftmaxSelector()
        
        # Net2Netè¿ç§»å·¥å…·
        self.net2net_transfer = Net2NetTransfer()
        
        # è®­ç»ƒé˜¶æ®µçŠ¶æ€
        self.training_phase = 'warmup'
        
        print(f"ğŸš€ ASO-SE ç½‘ç»œåˆå§‹åŒ–:")
        print(f"   æ·±åº¦: {self.current_depth} å±‚")
        print(f"   åˆå§‹é€šé“: {initial_channels}")
        print(f"   å½“å‰é€šé“: {current_channels}")
        print(f"   å‚æ•°é‡: {sum(p.numel() for p in self.parameters() if p.requires_grad):,}")
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        x = self.stem(x)
        
        for i, layer in enumerate(self.layers):
            arch_weights = self.arch_manager.get_arch_weights(i, self.gumbel_selector, self.training_phase)
            x = layer(x, arch_weights)
        
        x = self.global_pooling(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        
        return x
    
    def set_training_phase(self, phase):
        """è®¾ç½®è®­ç»ƒé˜¶æ®µ"""
        self.training_phase = phase
        print(f"ğŸ”„ è®¾ç½®è®­ç»ƒé˜¶æ®µ: {phase}")
    
    def grow_depth(self, num_new_layers=1):
        """æ·±åº¦ç”Ÿé•¿ - æ·»åŠ æ–°å±‚"""
        print(f"ğŸŒ± ç½‘ç»œæ·±åº¦ç”Ÿé•¿: æ·»åŠ  {num_new_layers} å±‚")
        
        for _ in range(num_new_layers):
            # åœ¨å€’æ•°ç¬¬äºŒå±‚åæ’å…¥æ–°å±‚
            insert_pos = len(self.layers) - 1
            if insert_pos <= 0:
                insert_pos = len(self.layers) // 2  # åœ¨ä¸­é—´æ’å…¥
            
            # è·å–å½“å‰å±‚çš„é€šé“æ•°
            reference_layer = self.layers[insert_pos]
            current_channels = reference_layer.out_channels
            
            # åˆ›å»ºæ–°å±‚å¹¶ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
            new_layer = EvolvableBlock(current_channels, current_channels, stride=1)
            new_layer = new_layer.to(next(self.parameters()).device)
            
            self.layers.insert(insert_pos, new_layer)
            self.current_depth += 1
        
        # æ›´æ–°æ¶æ„ç®¡ç†å™¨ï¼ˆä¿æŒç°æœ‰å‚æ•°ï¼‰
        # ArchitectureManagerå·²ç»èƒ½å¤ŸåŠ¨æ€æ‰©å±•å‚æ•°ï¼Œæ— éœ€é‡æ–°åˆ›å»º
        
        print(f"   æ–°æ·±åº¦: {self.current_depth}")
    
    def grow_width(self, growth_factor=1.5):
        """å®½åº¦ç”Ÿé•¿ - æ‰©å±•é€šé“æ•°ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        print(f"ğŸŒ± ç½‘ç»œå®½åº¦ç”Ÿé•¿: å¢é•¿å› å­ {growth_factor}")
        
        # ç®€åŒ–å®ç°ï¼šåªæ‰©å±•åˆ†ç±»å™¨çš„è¾“å…¥ç‰¹å¾æ•°
        # çœŸæ­£çš„å®½åº¦æ‰©å±•éœ€è¦æ›´å¤æ‚çš„Net2Netæ“ä½œï¼Œè¿™é‡Œå…ˆè®°å½•æ„å›¾
        old_classifier = self.classifier
        current_features = old_classifier.in_features
        new_features = int(current_features * growth_factor)
        
        if new_features > current_features:
            print(f"   åˆ†ç±»å™¨æ‰©å±•: {current_features} -> {new_features} ç‰¹å¾")
            # è¿™é‡Œå¯ä»¥åœ¨æœªæ¥é›†æˆçœŸæ­£çš„Net2Netå®½åº¦æ‰©å±•
        else:
            print(f"   å®½åº¦ç”Ÿé•¿è·³è¿‡ï¼ˆå¢é•¿å› å­å¤ªå°ï¼‰")
    
    def get_architecture_info(self):
        """è·å–æ¶æ„ä¿¡æ¯"""
        genotype = self.arch_manager.get_current_genotype()
        params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'depth': self.current_depth,
            'genotype': genotype,
            'parameters': params,
            'temperature': self.gumbel_selector.temperature
        }

class ASOSETrainingController:
    """ASO-SEè®­ç»ƒæ§åˆ¶å™¨"""
    
    def __init__(self, network, growth_patience=5, performance_threshold=0.02):
        self.network = network
        self.growth_patience = growth_patience
        self.performance_threshold = performance_threshold
        
        self.best_accuracy = 0.0
        self.patience_counter = 0
        self.growth_history = []
    
    def should_grow(self, current_accuracy):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç”Ÿé•¿"""
        improvement = current_accuracy - self.best_accuracy
        
        if improvement > self.performance_threshold:
            self.best_accuracy = current_accuracy
            self.patience_counter = 0
            return False
        else:
            self.patience_counter += 1
            return self.patience_counter >= self.growth_patience
    
    def trigger_growth(self, growth_type='depth'):
        """è§¦å‘ç½‘ç»œç”Ÿé•¿"""
        print(f"ğŸŒ± è§¦å‘ {growth_type} ç”Ÿé•¿")
        
        if growth_type == 'depth':
            self.network.grow_depth(1)
        elif growth_type == 'width':
            self.network.grow_width(1.2)
        
        self.growth_history.append({
            'type': growth_type,
            'step': len(self.growth_history),
            'architecture': self.network.get_architecture_info()
        })
        
        # é‡ç½®æ§åˆ¶å™¨
        self.patience_counter = 0

class ASOSETrainer:
    """ASO-SEè®­ç»ƒå™¨"""
    
    def __init__(self, experiment_name="aso_se_neural_growth"):
        self.experiment_name = experiment_name
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # è®­ç»ƒå‚æ•°
        self.batch_size = 128
        self.num_epochs = 100
        self.weight_lr = 0.025
        self.arch_lr = 3e-4
        self.momentum = 0.9
        self.weight_decay = 3e-4
        
        # é˜¶æ®µæ§åˆ¶
        self.phase_durations = {
            'warmup': 10,      # é¢„çƒ­é˜¶æ®µ
            'search': 30,      # æ¶æ„æœç´¢é˜¶æ®µ
            'growth': 40,      # ç”Ÿé•¿é˜¶æ®µ
            'optimize': 20     # ä¼˜åŒ–é˜¶æ®µ
        }
        
        self.current_phase = 'warmup'
        self.phase_epochs = 0
        
        print(f"ğŸš€ ASO-SE è®­ç»ƒå™¨åˆå§‹åŒ–")
        print(f"   å®éªŒåç§°: {experiment_name}")
        print(f"   è®¾å¤‡: {self.device}")
    
    def setup_data(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        # CIFAR-10æ•°æ®å¢å¼º
        train_transform = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])
        
        test_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])
        
        # æ•°æ®é›†
        train_dataset = torchvision.datasets.CIFAR10(
            root='./data', train=True, download=True, transform=train_transform)
        test_dataset = torchvision.datasets.CIFAR10(
            root='./data', train=False, download=True, transform=test_transform)
        
        # æ•°æ®åŠ è½½å™¨
        self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, 
                                     shuffle=True, num_workers=4, pin_memory=True)
        self.test_loader = DataLoader(test_dataset, batch_size=self.batch_size, 
                                    shuffle=False, num_workers=4, pin_memory=True)
        
        print(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ: è®­ç»ƒé›† {len(train_dataset)}, æµ‹è¯•é›† {len(test_dataset)}")
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹"""
        self.network = ASOSENetwork(
            input_channels=3,
            initial_channels=16,
            num_classes=10,
            initial_depth=8
        ).to(self.device)
        
        # è®­ç»ƒæ§åˆ¶å™¨
        self.training_controller = ASOSETrainingController(self.network)
        
        print(f"ğŸ—ï¸ æ¨¡å‹è®¾ç½®å®Œæˆ")
    
    def setup_optimizers(self):
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        # è·å–æ¶æ„å‚æ•°çš„IDé›†åˆï¼Œé¿å…å¼ é‡æ¯”è¾ƒ
        arch_param_ids = {id(p) for p in self.network.arch_manager.parameters()}
        
        # æƒé‡ä¼˜åŒ–å™¨ - æ’é™¤æ¶æ„å‚æ•°
        weight_params = [p for p in self.network.parameters() if id(p) not in arch_param_ids]
        self.weight_optimizer = optim.SGD(
            weight_params,
            lr=self.weight_lr,
            momentum=self.momentum,
            weight_decay=self.weight_decay
        )
        
        # æ¶æ„ä¼˜åŒ–å™¨
        self.arch_optimizer = optim.Adam(
            self.network.arch_manager.parameters(),
            lr=self.arch_lr,
            betas=(0.5, 0.999),
            weight_decay=1e-3
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.weight_scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.weight_optimizer, T_max=self.num_epochs, eta_min=1e-4)
        self.arch_scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.arch_optimizer, T_max=self.num_epochs, eta_min=1e-5)
        
        print(f"âš™ï¸ ä¼˜åŒ–å™¨è®¾ç½®å®Œæˆ")
    
    def _update_optimizers_after_growth(self):
        """ç”Ÿé•¿åå®‰å…¨åœ°æ›´æ–°ä¼˜åŒ–å™¨"""
        try:
            # ä¿å­˜å½“å‰å­¦ä¹ ç‡
            current_weight_lr = self.weight_optimizer.param_groups[0]['lr']
            current_arch_lr = self.arch_optimizer.param_groups[0]['lr']
            
            # é‡æ–°è®¾ç½®ä¼˜åŒ–å™¨
            self.setup_optimizers()
            
            # æ¢å¤å­¦ä¹ ç‡
            for param_group in self.weight_optimizer.param_groups:
                param_group['lr'] = current_weight_lr
            for param_group in self.arch_optimizer.param_groups:
                param_group['lr'] = current_arch_lr
                
            print(f"âœ… ä¼˜åŒ–å™¨å·²æ›´æ–°ä»¥åŒ…å«æ–°å‚æ•°")
            
        except Exception as e:
            print(f"âš ï¸ ä¼˜åŒ–å™¨æ›´æ–°è­¦å‘Š: {e}")
            # å¦‚æœæ›´æ–°å¤±è´¥ï¼Œè‡³å°‘å°è¯•åŸºæœ¬è®¾ç½®
            self.setup_optimizers()
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.network.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{self.num_epochs}')
        
        for batch_idx, (data, targets) in enumerate(pbar):
            data, targets = data.to(self.device), targets.to(self.device)
            
            # æƒé‡ä¼˜åŒ–æ­¥éª¤
            self.weight_optimizer.zero_grad()
            outputs = self.network(data)
            loss = F.cross_entropy(outputs, targets)
            loss.backward()
            self.weight_optimizer.step()
            
            # æ¶æ„ä¼˜åŒ–æ­¥éª¤(åœ¨æœç´¢å’Œç”Ÿé•¿é˜¶æ®µ)
            if self.current_phase in ['search', 'growth'] and batch_idx % 2 == 0:
                self.arch_optimizer.zero_grad()
                arch_outputs = self.network(data)
                arch_loss = F.cross_entropy(arch_outputs, targets)
                arch_loss.backward()
                self.arch_optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
            
            # æ›´æ–°è¿›åº¦æ¡
            accuracy = 100. * correct / total
            pbar.set_postfix({
                'Loss': f'{total_loss/(batch_idx+1):.3f}',
                'Acc': f'{accuracy:.2f}%',
                'Phase': self.current_phase,
                'Temp': f'{self.network.gumbel_selector.temperature:.3f}'
            })
        
        # æ¸©åº¦é€€ç«
        if self.current_phase in ['search', 'growth']:
            self.network.gumbel_selector.anneal_temperature()
        
        return total_loss / len(self.train_loader), accuracy
    
    def evaluate(self):
        """è¯„ä¼°æ¨¡å‹"""
        self.network.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, targets in self.test_loader:
                data, targets = data.to(self.device), targets.to(self.device)
                outputs = self.network(data)
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
        
        accuracy = 100. * correct / total
        return accuracy
    
    def update_phase(self, epoch):
        """æ›´æ–°è®­ç»ƒé˜¶æ®µ"""
        self.phase_epochs += 1
        
        # é˜¶æ®µè½¬æ¢é€»è¾‘
        old_phase = self.current_phase
        
        if self.current_phase == 'warmup' and self.phase_epochs >= self.phase_durations['warmup']:
            self.current_phase = 'search'
            self.phase_epochs = 0
            print(f"ğŸ”„ è¿›å…¥æ¶æ„æœç´¢é˜¶æ®µ")
        
        elif self.current_phase == 'search' and self.phase_epochs >= self.phase_durations['search']:
            self.current_phase = 'growth'
            self.phase_epochs = 0
            print(f"ğŸ”„ è¿›å…¥ç½‘ç»œç”Ÿé•¿é˜¶æ®µ")
        
        elif self.current_phase == 'growth' and self.phase_epochs >= self.phase_durations['growth']:
            self.current_phase = 'optimize'
            self.phase_epochs = 0
            print(f"ğŸ”„ è¿›å…¥æœ€ç»ˆä¼˜åŒ–é˜¶æ®µ")
        
        # åŒæ­¥ç½‘ç»œçš„è®­ç»ƒé˜¶æ®µ
        if old_phase != self.current_phase:
            self.network.set_training_phase(self.current_phase)
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"\nğŸ”§ ASO-SE è®­ç»ƒå¼€å§‹")
        print(f"{'='*60}")
        
        self.setup_data()
        self.setup_model()
        self.setup_optimizers()
        
        best_accuracy = 0.0
        training_history = []
        
        for epoch in range(self.num_epochs):
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(epoch)
            
            # è¯„ä¼°
            test_acc = self.evaluate()
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.weight_scheduler.step()
            if self.current_phase in ['search', 'growth']:
                self.arch_scheduler.step()
            
            # è®°å½•å†å²
            training_history.append({
                'epoch': epoch,
                'phase': self.current_phase,
                'train_loss': train_loss,
                'train_acc': train_acc,
                'test_acc': test_acc,
                'architecture': self.network.get_architecture_info()
            })
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿé•¿
            if self.current_phase == 'growth':
                if self.training_controller.should_grow(test_acc):
                    growth_type = 'depth' if epoch % 2 == 0 else 'width'
                    self.training_controller.trigger_growth(growth_type)
                    # å®‰å…¨åœ°æ›´æ–°ä¼˜åŒ–å™¨ä»¥åŒ…å«æ–°å‚æ•°
                    self._update_optimizers_after_growth()
            
            # æ›´æ–°æœ€ä½³ç²¾åº¦
            if test_acc > best_accuracy:
                best_accuracy = test_acc
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.network.state_dict(),
                    'optimizer_state_dict': self.weight_optimizer.state_dict(),
                    'architecture': self.network.get_architecture_info(),
                    'accuracy': best_accuracy
                }, f'{self.experiment_name}_best.pth')
            
            # æ›´æ–°é˜¶æ®µ
            self.update_phase(epoch)
            
            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 5 == 0:
                arch_info = self.network.get_architecture_info()
                print(f"\nğŸ“Š Epoch {epoch+1}/{self.num_epochs} | Phase: {self.current_phase}")
                print(f"   Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")
                print(f"   Test Acc: {test_acc:.2f}% | Best: {best_accuracy:.2f}%")
                print(f"   ç½‘ç»œæ·±åº¦: {arch_info['depth']} | å‚æ•°é‡: {arch_info['parameters']:,}")
                print(f"   å½“å‰åŸºå› å‹: {arch_info['genotype'][:3]}...")
        
        print(f"\nğŸ‰ ASO-SE è®­ç»ƒå®Œæˆ!")
        print(f"   æœ€ä½³ç²¾åº¦: {best_accuracy:.2f}%")
        print(f"   æœ€ç»ˆæ¶æ„: {self.network.get_architecture_info()}")
        
        return training_history, best_accuracy

# åŸºç¡€æ“ä½œå®ç°
class Identity(nn.Module):
    def forward(self, x):
        return x

class FactorizedReduce(nn.Module):
    def __init__(self, C_in, C_out):
        super().__init__()
        assert C_out % 2 == 0
        self.relu = nn.ReLU(inplace=False)
        self.conv_1 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)
        self.conv_2 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)
        self.bn = nn.BatchNorm2d(C_out, affine=True)

    def forward(self, x):
        x = self.relu(x)
        out = torch.cat([self.conv_1(x), self.conv_2(x[:, :, 1:, 1:])], dim=1)
        out = self.bn(out)
        return out

class SepConv(nn.Module):
    def __init__(self, C_in, C_out, kernel_size, stride, padding):
        super().__init__()
        self.op = nn.Sequential(
            nn.ReLU(inplace=False),
            nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, 
                     padding=padding, groups=C_in, bias=False),
            nn.Conv2d(C_in, C_in, kernel_size=1, padding=0, bias=False),
            nn.BatchNorm2d(C_in, affine=True),
            nn.ReLU(inplace=False),
            nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=1, 
                     padding=padding, groups=C_in, bias=False),
            nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False),
            nn.BatchNorm2d(C_out, affine=True),
        )

    def forward(self, x):
        return self.op(x)

class DilConv(nn.Module):
    def __init__(self, C_in, C_out, kernel_size, stride, padding, dilation):
        super().__init__()
        self.op = nn.Sequential(
            nn.ReLU(inplace=False),
            nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, 
                     padding=padding, dilation=dilation, groups=C_in, bias=False),
            nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False),
            nn.BatchNorm2d(C_out, affine=True),
        )

    def forward(self, x):
        return self.op(x)

class Zero(nn.Module):
    def __init__(self, stride):
        super().__init__()
        self.stride = stride

    def forward(self, x):
        if self.stride == 1:
            return x.mul(0.)
        return x[:, :, ::self.stride, ::self.stride].mul(0.)

class Conv7x1_1x7(nn.Module):
    def __init__(self, C_in, C_out, stride):
        super().__init__()
        self.op = nn.Sequential(
            nn.ReLU(inplace=False),
            nn.Conv2d(C_in, C_out, (1, 7), stride=(1, stride), padding=(0, 3), bias=False),
            nn.Conv2d(C_out, C_out, (7, 1), stride=(stride, 1), padding=(3, 0), bias=False),
            nn.BatchNorm2d(C_out, affine=True)
        )

    def forward(self, x):
        return self.op(x)

def main():
    parser = argparse.ArgumentParser(description='ASO-SE ç¥ç»ç½‘ç»œè‡ªé€‚åº”æ¶æ„æœç´¢')
    parser.add_argument('--epochs', type=int, default=100, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=128, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=0.025, help='å­¦ä¹ ç‡')
    parser.add_argument('--experiment', type=str, default='aso_se_neural_growth', help='å®éªŒåç§°')
    
    args = parser.parse_args()
    
    print("ğŸ”§ ASO-SE: çœŸæ­£çš„ç¥ç»æ¶æ„æœç´¢ä¸ç½‘ç»œç”Ÿé•¿")
    print(f"   ç›®æ ‡: CIFAR-10 95%å‡†ç¡®ç‡")
    print(f"   ç­–ç•¥: å››é˜¶æ®µè®­ç»ƒ + Net2Netå¹³æ»‘è¿ç§»")
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = ASOSETrainer(args.experiment)
    trainer.batch_size = args.batch_size
    trainer.num_epochs = args.epochs
    trainer.weight_lr = args.lr
    
    history, best_acc = trainer.train()
    
    print(f"\nâœ¨ å®éªŒå®Œæˆ!")
    print(f"   æœ€ç»ˆç²¾åº¦: {best_acc:.2f}%")

if __name__ == '__main__':
    main() 