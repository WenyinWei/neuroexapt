#!/usr/bin/env python3
"""
é‡æ„ç‰ˆASO-SEç¥ç»æ¶æ„æœç´¢è®­ç»ƒè„šæœ¬
ä½¿ç”¨å…¨æ–°è®¾è®¡çš„ç¨³å®šæ¶æ„æœç´¢æ¡†æ¶
è§£å†³æ¶æ„æœç´¢é˜¶æ®µæ€§èƒ½å´©æºƒé—®é¢˜
"""

import os
import sys
import time
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
from tqdm import tqdm

# æ·»åŠ neuroexaptåˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥é‡æ„åçš„ç»„ä»¶
try:
    from neuroexapt.core.aso_se_operators import StableMixedOp, PRIMITIVES, create_operation
    from neuroexapt.core.aso_se_architecture import (
        StableGumbelSampler, 
        ArchitectureParameterManager, 
        ProgressiveArchitectureNetwork
    )
    from neuroexapt.core.aso_se_trainer import StableASO_SETrainer
    print("âœ… ä½¿ç”¨é‡æ„åçš„ASO-SEæ¡†æ¶")
except ImportError as e:
    print(f"âš ï¸ é‡æ„æ¡†æ¶å¯¼å…¥å¤±è´¥: {e}")
    print("ğŸ”„ ä½¿ç”¨å†…è”é‡æ„å®ç°")
    
    # å†…è”é‡æ„å®ç°
    class StableOp(nn.Module):
        """ç¨³å®šçš„åŸºç¡€æ“ä½œç±»"""
        
        def __init__(self, C, stride, affine=True):
            super().__init__()
            self.C = C
            self.stride = stride
            self.affine = affine
        
        def forward(self, x):
            raise NotImplementedError
    
    class Identity(StableOp):
        """æ’ç­‰æ˜ å°„"""
        
        def forward(self, x):
            if self.stride == 1:
                return x
            else:
                return x[:, :, ::self.stride, ::self.stride]
    
    class Zero(StableOp):
        """é›¶æ“ä½œ"""
        
        def forward(self, x):
            if self.stride == 1:
                return torch.zeros_like(x)
            else:
                shape = list(x.shape)
                shape[2] = (shape[2] + self.stride - 1) // self.stride
                shape[3] = (shape[3] + self.stride - 1) // self.stride
                return torch.zeros(shape, dtype=x.dtype, device=x.device)
    
    class ReLUConvBN(StableOp):
        """ReLU + Conv + BatchNorm"""
        
        def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):
            super().__init__(C_out, stride, affine)
            self.op = nn.Sequential(
                nn.ReLU(inplace=False),
                nn.Conv2d(C_in, C_out, kernel_size, stride=stride, padding=padding, bias=False),
                nn.BatchNorm2d(C_out, affine=affine)
            )
        
        def forward(self, x):
            return self.op(x)
    
    class SepConv(StableOp):
        """æ·±åº¦å¯åˆ†ç¦»å·ç§¯"""
        
        def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):
            super().__init__(C_out, stride, affine)
            self.op = nn.Sequential(
                nn.ReLU(inplace=False),
                nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, 
                         padding=padding, groups=C_in, bias=False),
                nn.Conv2d(C_in, C_in, kernel_size=1, padding=0, bias=False),
                nn.BatchNorm2d(C_in, affine=affine),
                nn.ReLU(inplace=False),
                nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=1, 
                         padding=padding, groups=C_in, bias=False),
                nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False),
                nn.BatchNorm2d(C_out, affine=affine),
            )
        
        def forward(self, x):
            return self.op(x)
    
    class FactorizedReduce(StableOp):
        """å› å¼åŒ–é™ç»´"""
        
        def __init__(self, C_in, C_out, affine=True):
            super().__init__(C_out, 2, affine)
            assert C_out % 2 == 0
            self.relu = nn.ReLU(inplace=False)
            self.conv_1 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)
            self.conv_2 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False) 
            self.bn = nn.BatchNorm2d(C_out, affine=affine)
        
        def forward(self, x):
            x = self.relu(x)
            out = torch.cat([self.conv_1(x), self.conv_2(x[:,:,1:,1:])], dim=1)
            out = self.bn(out)
            return out
    
    # å®šä¹‰æ“ä½œç¬¦æ˜ å°„
    PRIMITIVES = [
        'none',
        'max_pool_3x3',
        'avg_pool_3x3', 
        'skip_connect',
        'sep_conv_3x3',
        'sep_conv_5x5',
        'conv_1x1',
        'conv_3x3',
    ]
    
    def create_operation(primitive, C_in, C_out, stride, affine=True):
        """åˆ›å»ºæ“ä½œå®ä¾‹"""
        if primitive == 'none':
            return Zero(C_out, stride, affine)
        elif primitive == 'max_pool_3x3':
            if C_in == C_out:
                return nn.Sequential(
                    nn.MaxPool2d(3, stride=stride, padding=1),
                    nn.BatchNorm2d(C_in, affine=affine)
                )
            else:
                return nn.Sequential(
                    nn.MaxPool2d(3, stride=stride, padding=1),
                    nn.Conv2d(C_in, C_out, 1, bias=False),
                    nn.BatchNorm2d(C_out, affine=affine)
                )
        elif primitive == 'avg_pool_3x3':
            if C_in == C_out:
                return nn.Sequential(
                    nn.AvgPool2d(3, stride=stride, padding=1, count_include_pad=False),
                    nn.BatchNorm2d(C_in, affine=affine)
                )
            else:
                return nn.Sequential(
                    nn.AvgPool2d(3, stride=stride, padding=1, count_include_pad=False),
                    nn.Conv2d(C_in, C_out, 1, bias=False),
                    nn.BatchNorm2d(C_out, affine=affine)
                )
        elif primitive == 'skip_connect':
            if stride == 1 and C_in == C_out:
                return Identity(C_out, stride, affine)
            else:
                return FactorizedReduce(C_in, C_out, affine)
        elif primitive == 'sep_conv_3x3':
            return SepConv(C_in, C_out, 3, stride, 1, affine)
        elif primitive == 'sep_conv_5x5':
            return SepConv(C_in, C_out, 5, stride, 2, affine)
        elif primitive == 'conv_1x1':
            return ReLUConvBN(C_in, C_out, 1, stride, 0, affine)
        elif primitive == 'conv_3x3':
            return ReLUConvBN(C_in, C_out, 3, stride, 1, affine)
        else:
            raise ValueError(f"Unknown primitive: {primitive}")
    
    class StableMixedOp(nn.Module):
        """ç¨³å®šçš„æ··åˆæ“ä½œ"""
        
        def __init__(self, C_in, C_out, stride, primitives=None):
            super().__init__()
            self.C_in = C_in
            self.C_out = C_out
            self.stride = stride
            
            if primitives is None:
                primitives = PRIMITIVES
            
            self.primitives = primitives
            self.operations = nn.ModuleList()
            
            # åˆ›å»ºæ‰€æœ‰å€™é€‰æ“ä½œ
            for primitive in primitives:
                op = create_operation(primitive, C_in, C_out, stride)
                self.operations.append(op)
        
        def forward(self, x, weights):
            """å‰å‘ä¼ æ’­ï¼Œä½¿ç”¨ç¨³å®šçš„åŠ æƒæ±‚å’Œ"""
            # è¾“å…¥éªŒè¯
            if torch.isnan(weights).any() or torch.isinf(weights).any():
                # å®‰å…¨å›é€€ï¼šä½¿ç”¨skip_connect
                skip_idx = 3 if len(self.operations) > 3 else 0
                return self.operations[skip_idx](x)
            
            # ç¡®ä¿æƒé‡å’Œä¸º1
            weights = F.softmax(weights, dim=0)
            
            # æ™ºèƒ½æ“ä½œé€‰æ‹©ç­–ç•¥
            max_weight_idx = torch.argmax(weights).item()
            max_weight = weights[max_weight_idx].item()
            
            # å¦‚æœæœ‰æ˜æ˜¾çš„ä¸»å¯¼æ“ä½œ (>0.8)ï¼Œä¸»è¦ä½¿ç”¨è¯¥æ“ä½œ
            if max_weight > 0.8:
                try:
                    return self.operations[max_weight_idx](x)
                except Exception:
                    # å›é€€åˆ°skipè¿æ¥
                    skip_idx = 3 if len(self.operations) > 3 else 0
                    return self.operations[skip_idx](x)
            
            # è®¡ç®—åŠ æƒå’Œï¼Œä½†åªä½¿ç”¨æƒé‡>0.02çš„æ“ä½œ
            result = 0.0
            total_weight = 0.0
            
            for i, op in enumerate(self.operations):
                weight = weights[i]
                if weight > 0.02:  # å¿½ç•¥æƒé‡å¤ªå°çš„æ“ä½œ
                    try:
                        op_result = op(x)
                        result += weight * op_result
                        total_weight += weight
                    except Exception:
                        continue
            
            if total_weight < 0.1:
                # å¦‚æœæ‰€æœ‰æ“ä½œéƒ½å¤±è´¥ï¼Œä½¿ç”¨skipè¿æ¥
                skip_idx = 3 if len(self.operations) > 3 else 0
                return self.operations[skip_idx](x)
            
            return result / total_weight if total_weight > 0 else result
    
    class StableGumbelSampler(nn.Module):
        """ç¨³å®šçš„Gumbelé‡‡æ ·å™¨"""
        
        def __init__(self, tau_max=1.2, tau_min=0.2, anneal_rate=0.999):
            super().__init__()
            self.tau_max = tau_max
            self.tau_min = tau_min
            self.tau = tau_max
            self.anneal_rate = anneal_rate
            
        def forward(self, logits, hard=True):
            """Gumbel Softmaxé‡‡æ ·"""
            if not self.training:
                # æ¨ç†æ—¶ä½¿ç”¨argmax
                y_hard = torch.zeros_like(logits)
                y_hard.scatter_(-1, torch.argmax(logits, dim=-1, keepdim=True), 1.0)
                return y_hard
            
            # è®­ç»ƒæ—¶ä½¿ç”¨æ›´ç¨³å®šçš„Gumbel-Softmax
            gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) + 1e-10) + 1e-10)
            y_soft = F.softmax((logits + gumbel_noise) / self.tau, dim=-1)
            
            if hard:
                # æ›´ç¨³å®šçš„straight-through estimator
                y_hard = torch.zeros_like(y_soft)
                y_hard.scatter_(-1, torch.argmax(y_soft, dim=-1, keepdim=True), 1.0)
                return y_hard - y_soft.detach() + y_soft
            else:
                return y_soft
        
        def anneal_temperature(self):
            """æ¸©åº¦é€€ç«"""
            self.tau = max(self.tau_min, self.tau * self.anneal_rate)
    
    class ArchitectureParameterManager(nn.Module):
        """æ¶æ„å‚æ•°ç®¡ç†å™¨"""
        
        def __init__(self, num_nodes, primitives=None):
            super().__init__()
            if primitives is None:
                primitives = PRIMITIVES
            
            self.num_nodes = num_nodes
            self.primitives = primitives
            self.num_ops = len(primitives)
            
            # æ¶æ„å‚æ•° - ä½¿ç”¨æ›´ä¿å®ˆçš„åˆå§‹åŒ–
            self.alpha = nn.ParameterList()
            for i in range(num_nodes):
                alpha = nn.Parameter(torch.randn(self.num_ops) * 0.05)  # æ›´å°çš„åˆå§‹åŒ–
                with torch.no_grad():
                    # ç»™skip_connectæ›´é«˜çš„åˆå§‹æƒé‡
                    skip_idx = 3 if 'skip_connect' in primitives else 0
                    alpha[skip_idx] += 0.5
                    # é™ä½noneæ“ä½œçš„æƒé‡
                    if 'none' in primitives:
                        none_idx = 0
                        alpha[none_idx] -= 0.5
                self.alpha.append(alpha)
            
            # Gumbelé‡‡æ ·å™¨
            self.sampler = StableGumbelSampler()
            
            # è®­ç»ƒé˜¶æ®µæ§åˆ¶
            self.training_phase = 'warmup'
        
        def get_architecture_weights(self, node_idx, mode='gumbel'):
            """è·å–æŒ‡å®šèŠ‚ç‚¹çš„æ¶æ„æƒé‡"""
            if node_idx >= len(self.alpha):
                raise IndexError(f"Node index {node_idx} out of range")
            
            logits = self.alpha[node_idx]
            
            if self.training_phase == 'warmup':
                # warmupé˜¶æ®µä½¿ç”¨å›ºå®šçš„skip_connect
                weights = torch.zeros_like(logits)
                skip_idx = 3 if 'skip_connect' in self.primitives else 0
                weights[skip_idx] = 1.0
                return weights.detach()
            
            elif self.training_phase in ['search', 'growth']:
                # æœç´¢é˜¶æ®µä½¿ç”¨æ›´ä¿å®ˆçš„Gumbelé‡‡æ ·
                return self.sampler(logits.unsqueeze(0), hard=True).squeeze(0)
            
            elif self.training_phase == 'optimize':
                # ä¼˜åŒ–é˜¶æ®µä½¿ç”¨ç¡®å®šæ€§é€‰æ‹©
                weights = torch.zeros_like(logits)
                best_idx = torch.argmax(logits).item()
                weights[best_idx] = 1.0
                return weights.detach()
            
            else:
                # é»˜è®¤å›é€€
                weights = torch.zeros_like(logits)
                skip_idx = 3 if 'skip_connect' in self.primitives else 0
                weights[skip_idx] = 1.0
                return weights.detach()
        
        def set_training_phase(self, phase):
            """è®¾ç½®è®­ç»ƒé˜¶æ®µ"""
            print(f"ğŸ”„ è®¾ç½®è®­ç»ƒé˜¶æ®µ: {phase}")
            self.training_phase = phase
            if phase == 'search':
                self.sampler.tau = 1.0  # é‡ç½®æ¸©åº¦
        
        def get_current_architecture(self):
            """è·å–å½“å‰æ¶æ„"""
            architecture = []
            for alpha in self.alpha:
                best_op_idx = torch.argmax(alpha).item()
                best_op_name = self.primitives[best_op_idx]
                architecture.append(best_op_name)
            return architecture
        
        def get_architecture_entropy(self):
            """è®¡ç®—æ¶æ„ç†µ"""
            total_entropy = 0.0
            for alpha in self.alpha:
                probs = F.softmax(alpha, dim=0)
                entropy = -torch.sum(probs * torch.log(probs + 1e-8))
                total_entropy += entropy.item()
            return total_entropy / len(self.alpha)
        
        def anneal_temperature(self):
            """æ¸©åº¦é€€ç«"""
            self.sampler.anneal_temperature()


class StableASO_SECell(nn.Module):
    """ç¨³å®šçš„ASO-SEå•å…ƒ"""
    
    def __init__(self, C_in, C_out, stride, node_id, arch_manager):
        super().__init__()
        self.C_in = C_in
        self.C_out = C_out
        self.stride = stride
        self.node_id = node_id
        self.arch_manager = arch_manager
        
        # é¢„å¤„ç†å±‚ï¼ˆç¡®ä¿è¾“å…¥è¾“å‡ºé€šé“åŒ¹é…ï¼‰
        if C_in != C_out or stride != 1:
            self.preprocess = nn.Sequential(
                nn.ReLU(inplace=False),
                nn.Conv2d(C_in, C_out, 1, stride=stride, bias=False),
                nn.BatchNorm2d(C_out)
            )
        else:
            self.preprocess = None
        
        # æ··åˆæ“ä½œ
        self.mixed_op = StableMixedOp(C_out, C_out, 1, PRIMITIVES)
        
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        # é¢„å¤„ç†
        if self.preprocess is not None:
            x = self.preprocess(x)
        
        # è·å–æ¶æ„æƒé‡
        try:
            weights = self.arch_manager.get_architecture_weights(self.node_id)
        except Exception as e:
            print(f"âš ï¸ èŠ‚ç‚¹ {self.node_id} æƒé‡è·å–å¤±è´¥: {e}")
            # å›é€€åˆ°skipè¿æ¥
            return x
        
        # æ··åˆæ“ä½œ
        return self.mixed_op(x, weights)


class StableProgressiveNetwork(nn.Module):
    """ç¨³å®šçš„æ¸è¿›å¼æ¶æ„ç½‘ç»œ"""
    
    def __init__(self, input_channels=3, init_channels=32, num_classes=10, init_depth=4):
        super().__init__()
        self.input_channels = input_channels
        self.init_channels = init_channels
        self.num_classes = num_classes
        self.current_depth = init_depth
        
        # ä¸»å¹²ç½‘ç»œ
        self.stem = nn.Sequential(
            nn.Conv2d(input_channels, init_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(init_channels),
            nn.ReLU(inplace=False)
        )
        
        # æ¶æ„ç®¡ç†å™¨
        self.arch_manager = ArchitectureParameterManager(init_depth)
        
        # æœç´¢å•å…ƒ
        self.cells = nn.ModuleList()
        current_channels = init_channels
        
        for i in range(init_depth):
            # æ¯éš”å‡ å±‚å¢åŠ é€šé“æ•°å¹¶é™é‡‡æ ·
            if i > 0 and i % 2 == 0:
                stride = 2
                out_channels = min(current_channels * 2, 256)  # é™åˆ¶æœ€å¤§é€šé“æ•°
            else:
                stride = 1
                out_channels = current_channels
            
            cell = StableASO_SECell(current_channels, out_channels, stride, i, self.arch_manager)
            self.cells.append(cell)
            current_channels = out_channels
        
        # åˆ†ç±»å¤´
        self.global_pooling = nn.AdaptiveAvgPool2d(1)
        self.classifier = nn.Linear(current_channels, num_classes)
        
        print(f"ğŸš€ ç¨³å®šç½‘ç»œåˆå§‹åŒ–:")
        print(f"   æ·±åº¦: {self.current_depth} å±‚")
        print(f"   åˆå§‹é€šé“: {init_channels}")
        print(f"   å½“å‰é€šé“: {current_channels}")
        print(f"   å‚æ•°é‡: {sum(p.numel() for p in self.parameters() if p.requires_grad):,}")
        
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        x = self.stem(x)
        
        for cell in self.cells:
            x = cell(x)
        
        x = self.global_pooling(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        
        return x
    
    def set_training_phase(self, phase):
        """è®¾ç½®è®­ç»ƒé˜¶æ®µ"""
        self.arch_manager.set_training_phase(phase)
    
    def get_architecture_info(self):
        """è·å–æ¶æ„ä¿¡æ¯"""
        architecture = self.arch_manager.get_current_architecture()
        entropy = self.arch_manager.get_architecture_entropy()
        
        total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'depth': self.current_depth,
            'architecture': architecture,
            'entropy': entropy,
            'parameters': total_params,
            'temperature': self.arch_manager.sampler.tau
        }


def set_seed(seed=42):
    """è®¾ç½®éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def setup_data(batch_size=128):
    """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
    # æ•°æ®å¢å¼º
    train_transform = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])
    
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])
    
    train_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=train_transform
    )
    test_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=False, download=True, transform=test_transform
    )
    
    train_loader = DataLoader(
        train_dataset, batch_size=batch_size, 
        shuffle=True, num_workers=2, pin_memory=True
    )
    test_loader = DataLoader(
        test_dataset, batch_size=batch_size, 
        shuffle=False, num_workers=2, pin_memory=True
    )
    
    print(f"ğŸ“Š CIFAR-10æ•°æ®åŠ è½½å®Œæˆ: è®­ç»ƒé›† {len(train_dataset)}, æµ‹è¯•é›† {len(test_dataset)}")
    return train_loader, test_loader


def main():
    print("ğŸ”§ é‡æ„ç‰ˆASO-SE: ç¨³å®šçš„ç¥ç»æ¶æ„æœç´¢")
    print("   ç›®æ ‡: CIFAR-10 é«˜å‡†ç¡®ç‡")
    print("   ç­–ç•¥: å››é˜¶æ®µç¨³å®šè®­ç»ƒ")
    print("   æ¡†æ¶: å…¨æ–°é‡æ„æ¶æ„")
    
    # è®¾ç½®ç¯å¢ƒ
    set_seed(42)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"   è®¾å¤‡: {device}")
    
    # é…ç½®
    config = {
        'batch_size': 128,
        'num_epochs': 60,  # å‡å°‘epochæ•°ä»¥ä¾¿å¿«é€Ÿæµ‹è¯•
        'warmup_epochs': 10,
        'search_epochs': 20,
        'growth_epochs': 20,
        'optimize_epochs': 10,
        'weight_lr': 0.025,
        'arch_lr': 3e-4,
        'arch_update_freq': 8,  # æ›´ä½çš„æ¶æ„æ›´æ–°é¢‘ç‡
    }
    
    # æ•°æ®å’Œæ¨¡å‹
    train_loader, test_loader = setup_data(config['batch_size'])
    network = StableProgressiveNetwork(
        input_channels=3,
        init_channels=32,
        num_classes=10,
        init_depth=4
    ).to(device)
    
    # ä¼˜åŒ–å™¨
    weight_params = []
    arch_params = []
    
    for name, param in network.named_parameters():
        if 'arch_manager.alpha' in name:
            arch_params.append(param)
        else:
            weight_params.append(param)
    
    weight_optimizer = optim.SGD(
        weight_params, lr=config['weight_lr'], 
        momentum=0.9, weight_decay=3e-4
    )
    arch_optimizer = optim.Adam(
        arch_params, lr=config['arch_lr'], 
        betas=(0.5, 0.999), weight_decay=1e-3
    )
    
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        weight_optimizer, T_max=config['num_epochs']
    )
    
    print(f"âš™ï¸ ä¼˜åŒ–å™¨è®¾ç½®:")
    print(f"   æƒé‡å‚æ•°: {len(weight_params)}")
    print(f"   æ¶æ„å‚æ•°: {len(arch_params)}")
    
    # è®­ç»ƒå¾ªç¯
    best_accuracy = 0.0
    current_phase = 'warmup'
    phase_epochs = 0
    
    print(f"\nğŸ”§ å¼€å§‹ç¨³å®šASO-SEè®­ç»ƒ")
    print(f"{'='*60}")
    
    for epoch in range(config['num_epochs']):
        # æ›´æ–°é˜¶æ®µ
        phase_epochs += 1
        old_phase = current_phase
        
        if current_phase == 'warmup' and phase_epochs >= config['warmup_epochs']:
            current_phase = 'search'
            phase_epochs = 0
            network.set_training_phase('search')
            print(f"ğŸ”„ è¿›å…¥æ¶æ„æœç´¢é˜¶æ®µ")
        
        elif current_phase == 'search' and phase_epochs >= config['search_epochs']:
            current_phase = 'optimize'  # è·³è¿‡growthé˜¶æ®µè¿›è¡Œæµ‹è¯•
            phase_epochs = 0
            network.set_training_phase('optimize')
            print(f"ğŸ”„ è¿›å…¥ä¼˜åŒ–é˜¶æ®µ")
        
        # è®­ç»ƒ
        network.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}')
        
        for batch_idx, (data, targets) in enumerate(pbar):
            data, targets = data.to(device), targets.to(device)
            
            # æ ¹æ®é˜¶æ®µé€‰æ‹©ä¼˜åŒ–ç­–ç•¥
            if current_phase == 'warmup' or current_phase == 'optimize':
                # åªä¼˜åŒ–æƒé‡
                weight_optimizer.zero_grad()
                outputs = network(data)
                loss = F.cross_entropy(outputs, targets)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(network.parameters(), 5.0)
                weight_optimizer.step()
                
            elif current_phase == 'search':
                # äº¤æ›¿ä¼˜åŒ–ï¼Œæ›´ä½é¢‘ç‡çš„æ¶æ„æ›´æ–°
                if batch_idx % config['arch_update_freq'] == 0:
                    # æ¶æ„ä¼˜åŒ–
                    arch_optimizer.zero_grad()
                    outputs = network(data)
                    loss = F.cross_entropy(outputs, targets)
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(arch_params, 5.0)
                    arch_optimizer.step()
                    network.arch_manager.anneal_temperature()
                else:
                    # æƒé‡ä¼˜åŒ–
                    weight_optimizer.zero_grad()
                    outputs = network(data)
                    loss = F.cross_entropy(outputs, targets)
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(weight_params, 5.0)
                    weight_optimizer.step()
            
            # ç»Ÿè®¡
            with torch.no_grad():
                if 'outputs' not in locals() or outputs is None:
                    outputs = network(data)
                total_loss += F.cross_entropy(outputs, targets).item()
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
            
            # æ›´æ–°è¿›åº¦æ¡
            accuracy = 100. * correct / total
            arch_info = network.get_architecture_info()
            
            pbar.set_postfix({
                'Loss': f'{total_loss/(batch_idx+1):.3f}',
                'Acc': f'{accuracy:.2f}%',
                'Phase': current_phase,
                'Temp': f'{arch_info["temperature"]:.3f}',
            })
        
        # è¯„ä¼°
        network.eval()
        test_correct = 0
        test_total = 0
        
        with torch.no_grad():
            for data, targets in test_loader:
                data, targets = data.to(device), targets.to(device)
                outputs = network(data)
                _, predicted = outputs.max(1)
                test_total += targets.size(0)
                test_correct += predicted.eq(targets).sum().item()
        
        test_acc = 100. * test_correct / test_total
        train_acc = 100. * correct / total
        train_loss = total_loss / len(train_loader)
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # æ›´æ–°æœ€ä½³ç²¾åº¦
        if test_acc > best_accuracy:
            best_accuracy = test_acc
        
        # å®šæœŸæ±‡æŠ¥
        if (epoch + 1) % 3 == 0:
            arch_info = network.get_architecture_info()
            print(f"\nğŸ“Š Epoch {epoch+1}/{config['num_epochs']} | é˜¶æ®µ: {current_phase}")
            print(f"   è®­ç»ƒæŸå¤±: {train_loss:.4f} | è®­ç»ƒç²¾åº¦: {train_acc:.2f}%")
            print(f"   æµ‹è¯•ç²¾åº¦: {test_acc:.2f}% | æœ€ä½³: {best_accuracy:.2f}%")
            print(f"   æ¶æ„ç†µ: {arch_info['entropy']:.3f} | æ¸©åº¦: {arch_info['temperature']:.3f}")
            
            if current_phase == 'search':
                print(f"   å½“å‰æ¶æ„: {arch_info['architecture']}")
    
    # è®­ç»ƒå®Œæˆ
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"   æœ€ä½³ç²¾åº¦: {best_accuracy:.2f}%")
    print(f"   æœ€ç»ˆæ¶æ„: {network.get_architecture_info()['architecture']}")


if __name__ == '__main__':
    main() 