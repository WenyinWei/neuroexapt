#!/usr/bin/env python3
"""
åŠ¨æ€æ¶æ„æ¼”è¿›ç³»ç»Ÿ - çœŸæ­£çš„æ¶æ„è‡ªå‘å¢é•¿

ğŸ§¬ æ ¸å¿ƒèƒ½åŠ›ï¼š
1. åŠ¨æ€å¢åŠ /ç§»é™¤å±‚ï¼ˆæ·±åº¦æ¼”è¿›ï¼‰
2. åŠ¨æ€è°ƒæ•´é€šé“æ•°ï¼ˆå®½åº¦æ¼”è¿›ï¼‰ 
3. åŠ¨æ€å¢å‡åˆ†æ”¯ï¼ˆæ‹“æ‰‘æ¼”è¿›ï¼‰
4. åŸºäºæ€§èƒ½çš„æ™ºèƒ½æ¶æ„å†³ç­–
5. è‡ªåŠ¨å½¢çŠ¶åŒ¹é…å’Œå‚æ•°è¿ç§»

ğŸ¯ ç›®æ ‡ï¼šè®©ç¥ç»ç½‘ç»œè‡ªå‘å¯»æ‰¾æœ€é€‚åˆçš„æ¶æ„ï¼
"""

import argparse
import os
import sys
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, random_split
from tqdm import tqdm
import json
import copy
from datetime import datetime
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from neuroexapt.core import CheckpointManager, get_checkpoint_manager

class EvolvableBlock(nn.Module):
    """å¯æ¼”è¿›çš„åŸºç¡€å—"""
    
    def __init__(self, in_channels, out_channels, block_id, stride=1):
        super(EvolvableBlock, self).__init__()
        
        self.block_id = block_id
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.stride = stride
        
        # å¯æ‰©å±•çš„æ“ä½œåˆ—è¡¨
        self.operations = nn.ModuleList()
        self.skip_connections = nn.ModuleList()
        
        # åˆå§‹åŸºç¡€æ“ä½œ
        self._add_basic_operations()
        
        # æ¶æ„å‚æ•°ï¼šæ§åˆ¶æ“ä½œé€‰æ‹©å’Œè·³è·ƒè¿æ¥
        self.op_weights = nn.Parameter(torch.randn(len(self.operations)))
        self.skip_weights = nn.Parameter(torch.randn(3))  # [no_skip, add, concat]
        
        # æ¼”è¿›å†å²
        self.evolution_history = []
        
    def _add_basic_operations(self):
        """æ·»åŠ åŸºç¡€æ“ä½œ"""
        # 3x3 Conv
        self.operations.append(nn.Sequential(
            nn.Conv2d(self.in_channels, self.out_channels, 3, 
                     stride=self.stride, padding=1, bias=False),
            nn.BatchNorm2d(self.out_channels),
            nn.ReLU(inplace=False)
        ))
        
        # 1x1 Convï¼ˆé€šé“è°ƒæ•´ï¼‰
        self.operations.append(nn.Sequential(
            nn.Conv2d(self.in_channels, self.out_channels, 1, 
                     stride=self.stride, bias=False),
            nn.BatchNorm2d(self.out_channels),
            nn.ReLU(inplace=False)
        ))
        
        # Depthwise Separable
        self.operations.append(nn.Sequential(
            nn.Conv2d(self.in_channels, self.in_channels, 3, 
                     stride=self.stride, padding=1, groups=self.in_channels, bias=False),
            nn.Conv2d(self.in_channels, self.out_channels, 1, bias=False),
            nn.BatchNorm2d(self.out_channels),
            nn.ReLU(inplace=False)
        ))
    
    def forward(self, x, skip_input=None):
        """å‰å‘ä¼ æ’­"""
        # è®¡ç®—æ‰€æœ‰æ“ä½œçš„åŠ æƒè¾“å‡º
        op_weights = F.softmax(self.op_weights, dim=0)
        output = sum(w * op(x) for w, op in zip(op_weights, self.operations))
        
        # å¤„ç†è·³è·ƒè¿æ¥
        if skip_input is not None and skip_input.shape[1] == output.shape[1]:
            skip_weights = F.softmax(self.skip_weights, dim=0)
            
            # å½¢çŠ¶åŒ¹é…
            if skip_input.shape[2:] != output.shape[2:]:
                skip_input = F.adaptive_avg_pool2d(skip_input, output.shape[2:])
            
            # è·³è·ƒè¿æ¥æ¨¡å¼
            if skip_weights[1] > 0.5:  # Add
                output = output + skip_weights[1] * skip_input
            elif skip_weights[2] > 0.3:  # Concatï¼ˆéœ€è¦é€šé“è°ƒæ•´ï¼‰
                if skip_input.shape[1] <= output.shape[1]:
                    padding = output.shape[1] - skip_input.shape[1]
                    skip_input = F.pad(skip_input, (0, 0, 0, 0, 0, padding))
                    output = output + skip_weights[2] * skip_input
        
        return output
    
    def add_operation(self, operation):
        """åŠ¨æ€æ·»åŠ æ–°æ“ä½œ"""
        self.operations.append(operation)
        
        # æ‰©å±•æƒé‡å‚æ•°
        old_weights = self.op_weights.data
        new_weights = torch.randn(len(self.operations))
        new_weights[:-1] = old_weights
        new_weights[-1] = old_weights.mean()  # åˆå§‹åŒ–ä¸ºå¹³å‡å€¼
        
        self.op_weights = nn.Parameter(new_weights)
        
        self.evolution_history.append({
            'action': 'add_operation',
            'operation_type': str(type(operation)),
            'timestamp': time.time()
        })
        
        print(f"ğŸ§¬ Block {self.block_id}: Added new operation, total={len(self.operations)}")
    
    def get_dominant_operation(self):
        """è·å–ä¸»å¯¼æ“ä½œ"""
        with torch.no_grad():
            weights = F.softmax(self.op_weights, dim=0)
            dominant_idx = torch.argmax(weights).item()
            return {
                'index': dominant_idx,
                'weight': weights[dominant_idx].item(),
                'entropy': (-weights * torch.log(weights + 1e-8)).sum().item()
            }

class DynamicArchitecture(nn.Module):
    """åŠ¨æ€æ¼”è¿›æ¶æ„"""
    
    def __init__(self, initial_channels=16, num_classes=10):
        super(DynamicArchitecture, self).__init__()
        
        self.initial_channels = initial_channels
        self.num_classes = num_classes
        
        # Stem
        self.stem = nn.Sequential(
            nn.Conv2d(3, initial_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(initial_channels),
            nn.ReLU(inplace=False)
        )
        
        # åŠ¨æ€å—åˆ—è¡¨
        self.blocks = nn.ModuleList()
        
        # åˆå§‹æ¶æ„ï¼š3ä¸ªåŸºç¡€å—
        current_channels = initial_channels
        for i in range(3):
            stride = 2 if i > 0 else 1
            out_channels = current_channels * (2 if i > 0 else 1)
            
            block = EvolvableBlock(current_channels, out_channels, i, stride)
            self.blocks.append(block)
            current_channels = out_channels
        
        # åˆ†ç±»å¤´
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.classifier = nn.Linear(current_channels, num_classes)
        
        # æ¼”è¿›çŠ¶æ€
        self.evolution_stats = {
            'total_evolutions': 0,
            'depth_changes': 0,
            'channel_changes': 0,
            'operation_additions': 0
        }
        
        print(f"ğŸ—ï¸ Dynamic Architecture initialized:")
        print(f"   Initial blocks: {len(self.blocks)}")
        print(f"   Initial channels: {initial_channels}")
        print(f"   Current channels: {current_channels}")
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        x = self.stem(x)
        
        skip_inputs = [None]  # ç”¨äºè·³è·ƒè¿æ¥
        
        for i, block in enumerate(self.blocks):
            skip_input = skip_inputs[-2] if len(skip_inputs) >= 2 else None
            x = block(x, skip_input)
            skip_inputs.append(x)
        
        x = self.global_pool(x)
        x = x.view(x.size(0), -1)
        return self.classifier(x)
    
    def add_depth(self, position=None):
        """å¢åŠ ç½‘ç»œæ·±åº¦"""
        if position is None:
            position = len(self.blocks)  # åœ¨æœ«å°¾æ·»åŠ 
        
        # ç¡®å®šæ–°å—çš„é€šé“æ•°
        if position == 0:
            in_channels = self.initial_channels
            out_channels = self.initial_channels
        else:
            prev_block = self.blocks[position - 1]
            in_channels = prev_block.out_channels
            out_channels = in_channels
        
        # åˆ›å»ºæ–°å—
        new_block = EvolvableBlock(in_channels, out_channels, 
                                 f"evolved_{len(self.blocks)}", stride=1)
        
        # æ’å…¥æ–°å—
        self.blocks.insert(position, new_block)
        
        # æ›´æ–°åˆ†ç±»å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if position == len(self.blocks) - 1:
            old_classifier = self.classifier
            self.classifier = nn.Linear(out_channels, self.num_classes)
            
            # å‚æ•°è¿ç§»
            with torch.no_grad():
                if old_classifier.weight.shape == self.classifier.weight.shape:
                    self.classifier.weight.copy_(old_classifier.weight)
                    self.classifier.bias.copy_(old_classifier.bias)
        
        self.evolution_stats['depth_changes'] += 1
        self.evolution_stats['total_evolutions'] += 1
        
        print(f"ğŸ§¬ DEPTH EVOLUTION: Added block at position {position}")
        print(f"   New depth: {len(self.blocks)} blocks")
        return True
    
    def expand_channels(self, block_idx, factor=1.5):
        """æ‰©å±•æŒ‡å®šå—çš„é€šé“æ•°"""
        if block_idx >= len(self.blocks):
            return False
        
        block = self.blocks[block_idx]
        old_out_channels = block.out_channels
        new_out_channels = int(old_out_channels * factor)
        
        # åˆ›å»ºæ–°çš„å—
        new_block = EvolvableBlock(
            block.in_channels, new_out_channels, 
            block.block_id, block.stride
        )
        
        # å‚æ•°è¿ç§»ï¼šå¤åˆ¶ç°æœ‰æ“ä½œçš„æƒé‡
        with torch.no_grad():
            for i, (old_op, new_op) in enumerate(zip(block.operations, new_block.operations)):
                for old_param, new_param in zip(old_op.parameters(), new_op.parameters()):
                    if old_param.shape == new_param.shape:
                        new_param.copy_(old_param)
                    elif len(old_param.shape) == 4:  # Convæƒé‡
                        min_out = min(old_param.shape[0], new_param.shape[0])
                        min_in = min(old_param.shape[1], new_param.shape[1])
                        new_param[:min_out, :min_in] = old_param[:min_out, :min_in]
                    elif len(old_param.shape) == 1:  # BNæƒé‡/åç½®
                        min_dim = min(old_param.shape[0], new_param.shape[0])
                        new_param[:min_dim] = old_param[:min_dim]
            
            # å¤åˆ¶æ¶æ„å‚æ•°
            new_block.op_weights.copy_(block.op_weights)
            new_block.skip_weights.copy_(block.skip_weights)
        
        # æ›¿æ¢å—
        self.blocks[block_idx] = new_block
        
        # æ›´æ–°åç»­å—çš„è¾“å…¥é€šé“æ•°
        self._update_subsequent_blocks(block_idx, new_out_channels)
        
        self.evolution_stats['channel_changes'] += 1
        self.evolution_stats['total_evolutions'] += 1
        
        print(f"ğŸ§¬ CHANNEL EXPANSION: Block {block_idx}")
        print(f"   Channels: {old_out_channels} â†’ {new_out_channels}")
        return True
    
    def _update_subsequent_blocks(self, start_idx, new_channels):
        """æ›´æ–°åç»­å—çš„è¾“å…¥é€šé“æ•°"""
        for i in range(start_idx + 1, len(self.blocks)):
            old_block = self.blocks[i]
            
            # åˆ›å»ºæ–°å—ï¼Œæ›´æ–°è¾“å…¥é€šé“æ•°
            new_block = EvolvableBlock(
                new_channels, old_block.out_channels,
                old_block.block_id, old_block.stride
            )
            
            # å‚æ•°è¿ç§»ï¼ˆå°½åŠ›è€Œä¸ºï¼‰
            with torch.no_grad():
                try:
                    new_block.op_weights.copy_(old_block.op_weights)
                    new_block.skip_weights.copy_(old_block.skip_weights)
                except:
                    pass  # å½¢çŠ¶ä¸åŒ¹é…æ—¶ä½¿ç”¨é»˜è®¤åˆå§‹åŒ–
            
            self.blocks[i] = new_block
            new_channels = old_block.out_channels
    
    def add_advanced_operation(self, block_idx):
        """ä¸ºæŒ‡å®šå—æ·»åŠ é«˜çº§æ“ä½œ"""
        if block_idx >= len(self.blocks):
            return False
        
        block = self.blocks[block_idx]
        
        # éšæœºé€‰æ‹©ä¸€ç§é«˜çº§æ“ä½œ
        advanced_ops = [
            # 5x5 Conv
            nn.Sequential(
                nn.Conv2d(block.in_channels, block.out_channels, 5, 
                         stride=block.stride, padding=2, bias=False),
                nn.BatchNorm2d(block.out_channels),
                nn.ReLU(inplace=False)
            ),
            # 7x7 Conv
            nn.Sequential(
                nn.Conv2d(block.in_channels, block.out_channels, 7, 
                         stride=block.stride, padding=3, bias=False),
                nn.BatchNorm2d(block.out_channels),
                nn.ReLU(inplace=False)
            ),
            # Dilated Conv
            nn.Sequential(
                nn.Conv2d(block.in_channels, block.out_channels, 3, 
                         stride=block.stride, padding=2, dilation=2, bias=False),
                nn.BatchNorm2d(block.out_channels),
                nn.ReLU(inplace=False)
            ),
            # Grouped Conv
            nn.Sequential(
                nn.Conv2d(block.in_channels, block.out_channels, 3, 
                         stride=block.stride, padding=1, 
                         groups=min(block.in_channels, 4), bias=False),
                nn.BatchNorm2d(block.out_channels),
                nn.ReLU(inplace=False)
            )
        ]
        
        # éšæœºé€‰æ‹©å¹¶æ·»åŠ 
        selected_op = np.random.choice(advanced_ops)
        block.add_operation(selected_op)
        
        self.evolution_stats['operation_additions'] += 1
        self.evolution_stats['total_evolutions'] += 1
        
        return True
    
    def get_architecture_summary(self):
        """è·å–æ¶æ„æ‘˜è¦"""
        summary = {
            'depth': len(self.blocks),
            'evolution_stats': self.evolution_stats,
            'blocks': []
        }
        
        for i, block in enumerate(self.blocks):
            block_info = {
                'id': block.block_id,
                'in_channels': block.in_channels,
                'out_channels': block.out_channels,
                'num_operations': len(block.operations),
                'dominant_op': block.get_dominant_operation()
            }
            summary['blocks'].append(block_info)
        
        return summary

class EvolutionController:
    """æ¼”è¿›æ§åˆ¶å™¨ - åŸºäºæ€§èƒ½æŒ‡å¯¼æ¶æ„æ¼”è¿›"""
    
    def __init__(self):
        self.performance_history = []
        self.evolution_decisions = []
        
        # æ¼”è¿›ç­–ç•¥å‚æ•°
        self.patience = 3  # æ€§èƒ½åœæ»è½®æ•°
        self.improvement_threshold = 0.5  # æ”¹è¿›é˜ˆå€¼(%)
        self.evolution_probability = 0.3  # æ¼”è¿›æ¦‚ç‡
        
    def should_evolve(self, current_accuracy, epoch):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿›è¡Œæ¶æ„æ¼”è¿›"""
        self.performance_history.append(current_accuracy)
        
        # è‡³å°‘è®­ç»ƒ5ä¸ªepochåå†è€ƒè™‘æ¼”è¿›
        if len(self.performance_history) < 5:
            return False
        
        # æ£€æŸ¥æ€§èƒ½åœæ»
        recent_performance = self.performance_history[-self.patience:]
        max_recent = max(recent_performance)
        min_recent = min(recent_performance)
        
        improvement = max_recent - min_recent
        
        # æ€§èƒ½åœæ»ä¸”éšæœºè§¦å‘
        should_evolve = (improvement < self.improvement_threshold and 
                        np.random.random() < self.evolution_probability)
        
        if should_evolve:
            print(f"ğŸ§¬ EVOLUTION TRIGGER at epoch {epoch}")
            print(f"   Recent improvement: {improvement:.2f}%")
            print(f"   Performance plateau detected")
        
        return should_evolve
    
    def select_evolution_strategy(self, model, current_accuracy):
        """é€‰æ‹©æ¼”è¿›ç­–ç•¥"""
        strategies = []
        
        # åŸºäºå½“å‰æ€§èƒ½é€‰æ‹©ç­–ç•¥
        if current_accuracy < 40:
            # ä½æ€§èƒ½ï¼šå¢åŠ æ·±åº¦å’Œå®½åº¦
            strategies.extend(['add_depth', 'expand_channels'] * 2)
            strategies.append('add_operation')
        elif current_accuracy < 70:
            # ä¸­ç­‰æ€§èƒ½ï¼šå¹³è¡¡å¢é•¿
            strategies.extend(['add_depth', 'expand_channels', 'add_operation'])
        else:
            # é«˜æ€§èƒ½ï¼šç²¾ç»†è°ƒä¼˜
            strategies.extend(['add_operation'] * 2)
            strategies.append('expand_channels')
        
        return np.random.choice(strategies)
    
    def execute_evolution(self, model, strategy):
        """æ‰§è¡Œæ¼”è¿›ç­–ç•¥"""
        success = False
        
        try:
            if strategy == 'add_depth':
                # åœ¨ç½‘ç»œä¸­é—´æ·»åŠ æ–°å±‚
                position = np.random.randint(1, len(model.blocks))
                success = model.add_depth(position)
                
            elif strategy == 'expand_channels':
                # æ‰©å±•éšæœºå—çš„é€šé“æ•°
                block_idx = np.random.randint(0, len(model.blocks))
                factor = np.random.uniform(1.2, 1.8)
                success = model.expand_channels(block_idx, factor)
                
            elif strategy == 'add_operation':
                # ä¸ºéšæœºå—æ·»åŠ é«˜çº§æ“ä½œ
                block_idx = np.random.randint(0, len(model.blocks))
                success = model.add_advanced_operation(block_idx)
            
            if success:
                decision = {
                    'strategy': strategy,
                    'timestamp': time.time(),
                    'model_depth': len(model.blocks),
                    'total_params': sum(p.numel() for p in model.parameters())
                }
                self.evolution_decisions.append(decision)
                
                print(f"âœ… Evolution executed: {strategy}")
                print(f"   Current depth: {len(model.blocks)}")
                print(f"   Total parameters: {decision['total_params']:,}")
                
        except Exception as e:
            print(f"âŒ Evolution failed: {e}")
            success = False
        
        return success

class DynamicArchTrainer:
    """åŠ¨æ€æ¶æ„è®­ç»ƒå™¨"""
    
    def __init__(self, experiment_name="dynamic_arch"):
        self.experiment_name = experiment_name
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # æ£€æŸ¥ç‚¹ç®¡ç†
        self.checkpoint_manager = get_checkpoint_manager(
            "./dynamic_arch_checkpoints", experiment_name
        )
        
        # ç»„ä»¶
        self.model = None
        self.evolution_controller = EvolutionController()
        self.weight_optimizer = None
        self.arch_optimizer = None
        self.criterion = nn.CrossEntropyLoss()
        
        # è®­ç»ƒçŠ¶æ€
        self.best_accuracy = 0.0
        self.training_stats = []
        
        print(f"ğŸš€ Dynamic Architecture Trainer initialized")
        print(f"ğŸ”§ Device: {self.device}")
    
    def setup_data(self, batch_size=96):
        """è®¾ç½®æ•°æ®"""
        print("ğŸ“Š Setting up CIFAR-10...")
        
        transform_train = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])
        
        transform_test = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])
        
        train_dataset = torchvision.datasets.CIFAR10(
            './data', train=True, download=True, transform=transform_train
        )
        test_dataset = torchvision.datasets.CIFAR10(
            './data', train=False, transform=transform_test
        )
        
        # åˆ†å‰²è®­ç»ƒæ•°æ®
        train_size = int(0.8 * len(train_dataset))
        valid_size = len(train_dataset) - train_size
        train_subset, valid_subset = random_split(train_dataset, [train_size, valid_size])
        
        self.train_loader = DataLoader(train_subset, batch_size=batch_size, 
                                     shuffle=True, num_workers=2, pin_memory=True)
        self.valid_loader = DataLoader(valid_subset, batch_size=batch_size, 
                                     shuffle=False, num_workers=2, pin_memory=True)
        self.test_loader = DataLoader(test_dataset, batch_size=batch_size, 
                                    shuffle=False, num_workers=2, pin_memory=True)
        
        print(f"âœ… Data ready: {len(train_subset)} train, {len(valid_subset)} valid, {len(test_dataset)} test")
    
    def setup_model(self, initial_channels=16):
        """è®¾ç½®æ¨¡å‹"""
        print(f"ğŸ—ï¸ Creating Dynamic Architecture: C={initial_channels}")
        
        self.model = DynamicArchitecture(initial_channels=initial_channels).to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.weight_optimizer = optim.SGD(
            [p for p in self.model.parameters() if not any(
                'op_weights' in n or 'skip_weights' in n 
                for n, _ in self.model.named_parameters() if p is _[1]
            )],
            lr=0.025, momentum=0.9, weight_decay=3e-4
        )
        
        # æ¶æ„å‚æ•°ä¼˜åŒ–å™¨
        arch_params = []
        for block in self.model.blocks:
            arch_params.extend([block.op_weights, block.skip_weights])
        
        self.arch_optimizer = optim.Adam(arch_params, lr=3e-4, weight_decay=1e-3)
        
        total_params = sum(p.numel() for p in self.model.parameters())
        print(f"ğŸ“Š Total parameters: {total_params:,}")
    
    def train_epoch(self, epoch, mode='weight'):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        if mode == 'weight':
            # æƒé‡è®­ç»ƒï¼šå†»ç»“æ¶æ„å‚æ•°
            for block in self.model.blocks:
                block.op_weights.requires_grad = False
                block.skip_weights.requires_grad = False
            optimizer = self.weight_optimizer
            data_loader = self.train_loader
            desc = f"ğŸ”§ E{epoch:02d} Weight Training"
        else:
            # æ¶æ„è®­ç»ƒï¼šå†»ç»“æƒé‡å‚æ•°
            for p in self.model.parameters():
                p.requires_grad = False
            for block in self.model.blocks:
                block.op_weights.requires_grad = True
                block.skip_weights.requires_grad = True
            optimizer = self.arch_optimizer
            data_loader = self.valid_loader
            desc = f"ğŸ§¬ E{epoch:02d} Architecture Search"
        
        total_loss = 0.0
        correct = 0
        total = 0
        
        pbar = tqdm(data_loader, desc=desc)
        
        for batch_idx, (data, target) in enumerate(pbar):
            data, target = data.to(self.device), target.to(self.device)
            
            optimizer.zero_grad()
            output = self.model(data)
            loss = self.criterion(output, target)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            pred = output.argmax(dim=1)
            correct += pred.eq(target).sum().item()
            total += target.size(0)
            
            pbar.set_postfix({
                'Loss': f'{total_loss/(batch_idx+1):.4f}',
                'Acc': f'{100.*correct/total:.2f}%'
            })
        
        # æ¢å¤æ¢¯åº¦çŠ¶æ€
        for p in self.model.parameters():
            p.requires_grad = True
        
        return total_loss/len(data_loader), 100.*correct/total
    
    def validate(self):
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in self.test_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                loss = self.criterion(output, target)
                
                total_loss += loss.item()
                pred = output.argmax(dim=1)
                correct += pred.eq(target).sum().item()
                total += target.size(0)
        
        return total_loss/len(self.test_loader), 100.*correct/total
    
    def train(self, epochs=80, initial_channels=16, batch_size=96):
        """ä¸»è®­ç»ƒæµç¨‹"""
        print(f"ğŸ¯ DYNAMIC ARCHITECTURE EVOLUTION TRAINING")
        print(f"ğŸ“Š Config: epochs={epochs}, channels={initial_channels}, batch_size={batch_size}")
        
        start_time = time.time()
        
        # è®¾ç½®
        self.setup_data(batch_size)
        self.setup_model(initial_channels)
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(epochs):
            print(f"\n{'='*80}")
            print(f"Epoch {epoch+1}/{epochs}")
            
            # äº¤æ›¿è®­ç»ƒæ¨¡å¼
            if epoch % 4 == 3:  # æ¯4ä¸ªepochè¿›è¡Œ1æ¬¡æ¶æ„æœç´¢
                train_loss, train_acc = self.train_epoch(epoch, 'arch')
                mode = "Architecture Search"
            else:
                train_loss, train_acc = self.train_epoch(epoch, 'weight')
                mode = "Weight Training"
            
            # éªŒè¯
            valid_loss, valid_acc = self.validate()
            
            # è®°å½•ç»Ÿè®¡
            stats = {
                'epoch': epoch,
                'mode': mode,
                'train_loss': train_loss,
                'train_acc': train_acc,
                'valid_loss': valid_loss,
                'valid_acc': valid_acc,
                'architecture': self.model.get_architecture_summary()
            }
            self.training_stats.append(stats)
            
            # è¾“å‡ºç»“æœ
            print(f"ğŸ“Š {mode}")
            print(f"   Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%")
            print(f"   Valid: Loss={valid_loss:.4f}, Acc={valid_acc:.2f}%")
            print(f"   Best:  {self.best_accuracy:.2f}%")
            
            # æ¶æ„æ¼”è¿›å†³ç­–
            if self.evolution_controller.should_evolve(valid_acc, epoch):
                strategy = self.evolution_controller.select_evolution_strategy(
                    self.model, valid_acc
                )
                success = self.evolution_controller.execute_evolution(
                    self.model, strategy
                )
                
                if success:
                    # é‡æ–°è®¾ç½®ä¼˜åŒ–å™¨ï¼ˆå‚æ•°å¯èƒ½æ”¹å˜ï¼‰
                    self.setup_model(initial_channels)
            
            # æ›´æ–°æœ€ä½³æ€§èƒ½
            if valid_acc > self.best_accuracy:
                self.best_accuracy = valid_acc
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                try:
                    checkpoint_path = self.checkpoint_manager.save_checkpoint(
                        epoch=epoch,
                        model_state=self.model.state_dict(),
                        optimizer_states={
                            'weight': self.weight_optimizer.state_dict(),
                            'arch': self.arch_optimizer.state_dict()
                        },
                        scheduler_states={},  # æ·»åŠ ç¼ºå¤±çš„å‚æ•°
                        training_stats=stats,
                        framework_state={     # æ·»åŠ ç¼ºå¤±çš„å‚æ•°
                            'evolution_stats': self.model.evolution_stats,
                            'evolution_controller': {
                                'performance_history': self.evolution_controller.performance_history,
                                'evolution_decisions': self.evolution_controller.evolution_decisions
                            }
                        },
                        performance_metric=valid_acc,
                        architecture_info=self.model.get_architecture_summary()
                    )
                    print(f"ğŸ’¾ New best model saved: {valid_acc:.2f}%")
                except Exception as e:
                    print(f"âš ï¸ Save failed: {e}")
            
            # æ˜¾ç¤ºå½“å‰æ¶æ„
            if epoch % 10 == 9:
                self._display_architecture()
            
            # å†…å­˜æ¸…ç†
            if epoch % 5 == 0:
                torch.cuda.empty_cache()
        
        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        self._display_final_results(total_time)
    
    def _display_architecture(self):
        """æ˜¾ç¤ºå½“å‰æ¶æ„"""
        summary = self.model.get_architecture_summary()
        print(f"\nğŸ—ï¸ CURRENT ARCHITECTURE:")
        print(f"   Depth: {summary['depth']} blocks")
        print(f"   Evolution stats: {summary['evolution_stats']}")
        
        for i, block_info in enumerate(summary['blocks']):
            dom_op = block_info['dominant_op']
            print(f"   Block {i}: {block_info['in_channels']}â†’{block_info['out_channels']} "
                  f"({block_info['num_operations']} ops, dominant: {dom_op['weight']:.3f})")
    
    def _display_final_results(self, total_time):
        """æ˜¾ç¤ºæœ€ç»ˆç»“æœ"""
        print(f"\nğŸ‰ DYNAMIC EVOLUTION COMPLETED!")
        print(f"â±ï¸  Total time: {total_time/60:.1f} minutes")
        print(f"ğŸ† Best accuracy: {self.best_accuracy:.2f}%")
        
        final_summary = self.model.get_architecture_summary()
        print(f"\nğŸ§¬ FINAL EVOLVED ARCHITECTURE:")
        print(f"   Final depth: {final_summary['depth']} blocks")
        print(f"   Total evolutions: {final_summary['evolution_stats']['total_evolutions']}")
        print(f"   Depth changes: {final_summary['evolution_stats']['depth_changes']}")
        print(f"   Channel changes: {final_summary['evolution_stats']['channel_changes']}")
        print(f"   Operation additions: {final_summary['evolution_stats']['operation_additions']}")
        
        total_params = sum(p.numel() for p in self.model.parameters())
        print(f"   Final parameters: {total_params:,}")

def main():
    parser = argparse.ArgumentParser(description='Dynamic Architecture Evolution Training')
    
    parser.add_argument('--epochs', type=int, default=60, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=96, help='æ‰¹å¤§å°')
    parser.add_argument('--channels', type=int, default=16, help='åˆå§‹é€šé“æ•°')
    parser.add_argument('--experiment', type=str, default='dynamic_evolution', help='å®éªŒåç§°')
    
    args = parser.parse_args()
    
    print("ğŸ§¬ DYNAMIC ARCHITECTURE EVOLUTION - REAL ARCHITECTURE CHANGES!")
    print(f"â° Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“Š Configuration: {vars(args)}")
    
    trainer = DynamicArchTrainer(args.experiment)
    trainer.train(
        epochs=args.epochs,
        initial_channels=args.channels,
        batch_size=args.batch_size
    )

if __name__ == "__main__":
    main() 