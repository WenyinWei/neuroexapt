#!/usr/bin/env python3
"""
DNM Enhanced Components Basic Test

åŸºç¡€åŠŸèƒ½æµ‹è¯•ï¼ŒéªŒè¯å¢å¼ºç»„ä»¶æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import torch
import torch.nn as nn
import numpy as np
from neuroexapt.core import (
    EnhancedBottleneckDetector, 
    PerformanceGuidedDivision, 
    DivisionStrategy
)

def test_enhanced_bottleneck_detector():
    """æµ‹è¯•å¢å¼ºç“¶é¢ˆæ£€æµ‹å™¨"""
    print("ğŸ” æµ‹è¯•å¢å¼ºç“¶é¢ˆæ£€æµ‹å™¨...")
    
    # åˆ›å»ºç®€å•æ¨¡å‹
    model = nn.Sequential(
        nn.Conv2d(3, 16, 3, padding=1),
        nn.ReLU(),
        nn.Conv2d(16, 32, 3, padding=1),
        nn.ReLU(),
        nn.AdaptiveAvgPool2d((1, 1)),
        nn.Flatten(),
        nn.Linear(32, 10)
    )
    
    # åˆ›å»ºæ£€æµ‹å™¨
    detector = EnhancedBottleneckDetector(
        sensitivity_threshold=0.1,
        diversity_threshold=0.3,
        gradient_threshold=1e-6,
        info_flow_threshold=0.5
    )
    
    # æ¨¡æ‹Ÿæ¿€æ´»å€¼å’Œæ¢¯åº¦
    activations = {}
    gradients = {}
    
    # æ¨¡æ‹Ÿä¸€äº›æ¿€æ´»å€¼
    batch_size = 8
    activations['0'] = torch.randn(batch_size, 16, 32, 32)  # Conv2d
    activations['2'] = torch.randn(batch_size, 32, 32, 32)  # Conv2d
    activations['6'] = torch.randn(batch_size, 10)          # Linear
    
    # æ¨¡æ‹Ÿä¸€äº›æ¢¯åº¦
    gradients['0.weight'] = torch.randn(16, 3, 3, 3) * 0.01
    gradients['2.weight'] = torch.randn(32, 16, 3, 3) * 0.01
    gradients['6.weight'] = torch.randn(10, 32) * 0.01
    
    # æ¨¡æ‹Ÿç›®æ ‡
    targets = torch.randint(0, 10, (batch_size,))
    
    # æ£€æµ‹ç“¶é¢ˆ
    bottleneck_scores = detector.detect_bottlenecks(model, activations, gradients, targets)
    
    print(f"   æ£€æµ‹åˆ° {len(bottleneck_scores)} ä¸ªå±‚")
    for layer_name, score in bottleneck_scores.items():
        print(f"   å±‚ {layer_name}: ç“¶é¢ˆåˆ†æ•° = {score:.3f}")
    
    # è·å–åˆ†ææ‘˜è¦
    summary = detector.get_analysis_summary(bottleneck_scores)
    print(f"   åˆ†ææ‘˜è¦: {summary}")
    
    # æµ‹è¯•è§¦å‘åˆ¤æ–­
    performance_trend = [70.0, 72.0, 73.0, 73.1, 73.2]  # æ€§èƒ½åœæ»
    should_trigger, reasons = detector.should_trigger_division(bottleneck_scores, performance_trend)
    print(f"   æ˜¯å¦è§¦å‘åˆ†è£‚: {should_trigger}")
    if reasons:
        for reason in reasons:
            print(f"     - {reason}")
    
    print("   âœ… ç“¶é¢ˆæ£€æµ‹å™¨æµ‹è¯•å®Œæˆ")
    return True

def test_performance_guided_division():
    """æµ‹è¯•æ€§èƒ½å¯¼å‘åˆ†è£‚å™¨"""
    print("âš¡ æµ‹è¯•æ€§èƒ½å¯¼å‘åˆ†è£‚å™¨...")
    
    # åˆ›å»ºåˆ†è£‚å™¨
    divider = PerformanceGuidedDivision(
        noise_scale=0.1,
        progressive_epochs=3,
        diversity_threshold=0.7,
        performance_monitoring=True
    )
    
    # åˆ›å»ºç®€å•å±‚è¿›è¡Œæµ‹è¯•
    conv_layer = nn.Conv2d(16, 32, 3, padding=1)
    linear_layer = nn.Linear(64, 10)
    
    # æ¨¡æ‹Ÿæ¿€æ´»å€¼å’Œæ¢¯åº¦
    activations = torch.randn(8, 16, 16, 16)  # Conv layer activations
    gradients = torch.randn(32, 16, 3, 3) * 0.01  # Conv layer gradients
    targets = torch.randint(0, 10, (8,))
    
    # æµ‹è¯•ä¸åŒçš„åˆ†è£‚ç­–ç•¥
    strategies = [
        DivisionStrategy.GRADIENT_BASED,
        DivisionStrategy.ACTIVATION_BASED,
        DivisionStrategy.HYBRID,
        DivisionStrategy.INFORMATION_GUIDED
    ]
    
    for strategy in strategies:
        print(f"   æµ‹è¯•ç­–ç•¥: {strategy.value}")
        
        # é€‰æ‹©ä¸­é—´ç¥ç»å…ƒè¿›è¡Œåˆ†è£‚
        neuron_idx = conv_layer.out_channels // 2
        
        try:
            success, division_info = divider.divide_neuron(
                conv_layer, neuron_idx, strategy,
                activations, gradients, targets
            )
            
            if success:
                print(f"     âœ… åˆ†è£‚æˆåŠŸ: {division_info.get('strategy', 'unknown')}")
            else:
                print(f"     âŒ åˆ†è£‚å¤±è´¥: {division_info.get('error', 'unknown')}")
                
        except Exception as e:
            print(f"     âŒ å¼‚å¸¸: {e}")
    
    # æµ‹è¯•çº¿æ€§å±‚åˆ†è£‚
    print("   æµ‹è¯•çº¿æ€§å±‚åˆ†è£‚...")
    linear_activations = torch.randn(8, 64)
    linear_gradients = torch.randn(10, 64) * 0.01
    
    success, division_info = divider.divide_neuron(
        linear_layer, 5, DivisionStrategy.HYBRID,
        linear_activations, linear_gradients, targets
    )
    
    if success:
        print(f"     âœ… çº¿æ€§å±‚åˆ†è£‚æˆåŠŸ")
    else:
        print(f"     âŒ çº¿æ€§å±‚åˆ†è£‚å¤±è´¥: {division_info.get('error', 'unknown')}")
    
    # è·å–åˆ†è£‚æ‘˜è¦
    summary = divider.get_division_summary()
    print(f"   åˆ†è£‚æ‘˜è¦: {summary}")
    
    print("   âœ… æ€§èƒ½å¯¼å‘åˆ†è£‚å™¨æµ‹è¯•å®Œæˆ")
    return True

def test_integration():
    """æµ‹è¯•ç»„ä»¶é›†æˆ"""
    print("ğŸ§¬ æµ‹è¯•ç»„ä»¶é›†æˆ...")
    
    # åˆ›å»ºæ£€æµ‹å™¨å’Œåˆ†è£‚å™¨
    detector = EnhancedBottleneckDetector()
    divider = PerformanceGuidedDivision()
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    model = nn.Sequential(
        nn.Conv2d(3, 8, 3, padding=1),   # è¾ƒå°çš„æ¨¡å‹ä¾¿äºæµ‹è¯•
        nn.ReLU(),
        nn.AdaptiveAvgPool2d((4, 4)),
        nn.Flatten(),
        nn.Linear(8 * 4 * 4, 5),
        nn.ReLU(),
        nn.Linear(5, 2)
    )
    
    # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
    batch_size = 4
    x = torch.randn(batch_size, 3, 8, 8)
    y = torch.randint(0, 2, (batch_size,))
    
    # å‰å‘ä¼ æ’­
    output = model(x)
    loss = nn.CrossEntropyLoss()(output, y)
    
    # åå‘ä¼ æ’­
    loss.backward()
    
    # æ”¶é›†æ¿€æ´»å€¼å’Œæ¢¯åº¦
    activations = {}
    gradients = {}
    
    # ç®€å•çš„æ¿€æ´»å€¼æ”¶é›†
    with torch.no_grad():
        x_temp = x
        for i, layer in enumerate(model):
            x_temp = layer(x_temp)
            activations[str(i)] = x_temp.clone()
    
    # æ”¶é›†æ¢¯åº¦
    for name, param in model.named_parameters():
        if param.grad is not None:
            gradients[name] = param.grad.clone()
    
    # ç“¶é¢ˆæ£€æµ‹
    bottleneck_scores = detector.detect_bottlenecks(model, activations, gradients, y)
    print(f"   æ£€æµ‹åˆ°ç“¶é¢ˆå±‚: {len(bottleneck_scores)}")
    
    # è·å–æœ€é«˜åˆ†æ•°çš„å±‚
    if bottleneck_scores:
        top_layer = max(bottleneck_scores.items(), key=lambda x: x[1])
        print(f"   æœ€é«˜ç“¶é¢ˆåˆ†æ•°: {top_layer[0]} = {top_layer[1]:.3f}")
        
        # æ¨¡æ‹Ÿåˆ†è£‚ï¼ˆè¿™é‡Œåªæ˜¯æµ‹è¯•æ¥å£ï¼Œä¸åšå®é™…åˆ†è£‚ï¼‰
        for name, module in model.named_modules():
            if name == top_layer[0] and isinstance(module, (nn.Conv2d, nn.Linear)):
                print(f"   ç›®æ ‡å±‚ç±»å‹: {type(module).__name__}")
                break
    
    print("   âœ… ç»„ä»¶é›†æˆæµ‹è¯•å®Œæˆ")
    return True

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§¬ DNM å¢å¼ºç»„ä»¶åŸºç¡€æµ‹è¯•")
    print("=" * 50)
    
    try:
        # è¿è¡Œæµ‹è¯•
        test1 = test_enhanced_bottleneck_detector()
        print()
        
        test2 = test_performance_guided_division()
        print()
        
        test3 = test_integration()
        print()
        
        # æ€»ç»“
        all_passed = test1 and test2 and test3
        
        print("=" * 50)
        if all_passed:
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼DNM å¢å¼ºç»„ä»¶å·¥ä½œæ­£å¸¸")
        else:
            print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥")
        
        print("ğŸ“Š æµ‹è¯•æ‘˜è¦:")
        print(f"   ç“¶é¢ˆæ£€æµ‹å™¨: {'âœ…' if test1 else 'âŒ'}")
        print(f"   æ€§èƒ½å¯¼å‘åˆ†è£‚å™¨: {'âœ…' if test2 else 'âŒ'}")
        print(f"   ç»„ä»¶é›†æˆ: {'âœ…' if test3 else 'âŒ'}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å‡ºç°å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()