#!/usr/bin/env python3
"""
æ™ºèƒ½Batch Sizeä¼˜åŒ–å™¨

åŸºäºç”¨æˆ·å»ºè®®çš„æ”¹è¿›ç‰ˆæœ¬ï¼š
1. å¿«é€Ÿæ£€æµ‹GPUå†…å­˜è…¾æŒªçŠ¶æ€
2. ç†è®ºè®¡ç®—æœ€å¤§batch sizeä¸Šé™  
3. æ™ºèƒ½é€€å‡ºæœºåˆ¶é¿å…é•¿æ—¶é—´ç­‰å¾…

ç‰¹ç‚¹ï¼š
- ç›‘æ§GPUè®¡ç®—å ç”¨ç‡æ¨¡å¼è¯†åˆ«å†…å­˜ç“¶é¢ˆ
- åŸºäºGPUå†…å­˜ç†è®ºè®¡ç®—batch sizeä¸Šé™
- å¿«é€Ÿè¯†åˆ«å¹¶è·³è¿‡å†…å­˜è…¾æŒªçŠ¶æ€
"""

import torch
import torch.nn as nn
import torch.optim as optim
import time
import gc
import sys
import os
import threading
import subprocess
from typing import Optional, Dict, List, Tuple
import psutil
from concurrent.futures import ThreadPoolExecutor, TimeoutError, as_completed
import signal

# Add the project directory to the path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from neuroexapt.core.model import Network

class GPUMonitor:
    """GPUç›‘æ§å™¨ï¼Œæ£€æµ‹å†…å­˜è…¾æŒªçŠ¶æ€"""
    
    def __init__(self):
        self.monitoring = False
        self.gpu_utilizations = []
        self.memory_utilizations = []
        self.monitor_thread = None
        
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§GPUçŠ¶æ€"""
        self.monitoring = True
        self.gpu_utilizations = []
        self.memory_utilizations = []
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
        
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1.0)
    
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            try:
                # ä½¿ç”¨nvidia-smiè·å–GPUåˆ©ç”¨ç‡
                result = subprocess.run(
                    ['nvidia-smi', '--query-gpu=utilization.gpu,utilization.memory', 
                     '--format=csv,noheader,nounits'],
                    capture_output=True, text=True, timeout=1.0
                )
                
                if result.returncode == 0:
                    gpu_util, mem_util = map(int, result.stdout.strip().split(', '))
                    self.gpu_utilizations.append(gpu_util)
                    self.memory_utilizations.append(mem_util)
                    
                    # åªä¿ç•™æœ€è¿‘20ä¸ªæ ·æœ¬
                    if len(self.gpu_utilizations) > 20:
                        self.gpu_utilizations = self.gpu_utilizations[-20:]
                        self.memory_utilizations = self.memory_utilizations[-20:]
                        
            except (subprocess.TimeoutExpired, FileNotFoundError, ValueError):
                pass  # nvidia-smiä¸å¯ç”¨æˆ–è¶…æ—¶ï¼Œç»§ç»­ç›‘æ§
                
            time.sleep(0.1)  # 100msé‡‡æ ·é—´éš”
    
    def detect_memory_thrashing(self, min_samples=12) -> Tuple[bool, str]:
        """
        æ£€æµ‹æ˜¯å¦å¤„äºå†…å­˜è…¾æŒªçŠ¶æ€ (æ”¹è¿›ç‰ˆ - é¿å…çŸ­æœŸå³°å€¼è¯¯åˆ¤)
        
        ç‰¹å¾è¯†åˆ«ï¼š
        1. GPUåˆ©ç”¨ç‡æŒç»­ç¨³å®šé«˜ä½ (é¿å…çŸ­æœŸå³°å€¼)
        2. å†…å­˜åˆ©ç”¨ç‡å¤§å¹…ä¸”æŒç»­æ³¢åŠ¨
        3. GPUé«˜ä½¿ç”¨ç‡ä½†è®­ç»ƒæ•ˆç‡æ˜æ˜¾ä½ä¸‹
        
        Returns:
            (æ˜¯å¦æ£€æµ‹åˆ°è…¾æŒª, æ£€æµ‹åŸå› )
        """
        if len(self.gpu_utilizations) < min_samples:
            return False, "æ ·æœ¬ä¸è¶³"
            
        recent_gpu = self.gpu_utilizations[-min_samples:]
        recent_mem = self.memory_utilizations[-min_samples:]
        
        # ç‰¹å¾1: GPUæŒç»­ç¨³å®šé«˜åˆ©ç”¨ç‡ï¼ˆ>80%ï¼‰ï¼Œé¿å…çŸ­æœŸå³°å€¼
        very_high_gpu_count = sum(1 for x in recent_gpu if x > 80)
        sustained_very_high_gpu = very_high_gpu_count >= min_samples * 0.8  # 80%çš„æ—¶é—´éƒ½>80%
        
        # ç‰¹å¾2: å†…å­˜åˆ©ç”¨ç‡æŒç»­æ³¢åŠ¨ä¸”åœ¨é«˜ä½
        if len(recent_mem) > 1:
            mem_variance = max(recent_mem) - min(recent_mem)
            high_mem_variance = mem_variance > 25  # æé«˜é˜ˆå€¼ï¼Œé¿å…è¯¯åˆ¤
            
            # å†…å­˜åˆ©ç”¨ç‡æŒç»­åœ¨å±é™©é«˜ä½ï¼ˆ>90%ï¼‰
            very_high_mem_count = sum(1 for x in recent_mem if x > 90)
            sustained_very_high_mem = very_high_mem_count >= min_samples * 0.6
            
            # å†…å­˜åˆ©ç”¨ç‡å¹³å‡å€¼ä¹Ÿè¦å¾ˆé«˜
            avg_mem = sum(recent_mem) / len(recent_mem)
        else:
            high_mem_variance = False
            sustained_very_high_mem = False
            avg_mem = 0
        
        # ç‰¹å¾3: GPUå¹³å‡åˆ©ç”¨ç‡
        avg_gpu = sum(recent_gpu) / len(recent_gpu)
        
        # æ›´ä¸¥æ ¼çš„è…¾æŒªåˆ¤æ–­æ¡ä»¶
        reasons = []
        is_thrashing = False
        
        # æ¡ä»¶1: æé«˜GPUä½¿ç”¨ç‡ + å†…å­˜åœ¨å±é™©åŒºåŸŸä¸”æ³¢åŠ¨å¤§
        if sustained_very_high_gpu and sustained_very_high_mem and high_mem_variance:
            reasons.append(f"GPUæŒç»­>{very_high_gpu_count}/{min_samples}æ¬¡>80% + å†…å­˜å±é™©åŒºåŸŸæ³¢åŠ¨{mem_variance:.0f}%")
            is_thrashing = True
        
        # æ¡ä»¶2: å¹³å‡GPUå¾ˆé«˜ä½†å†…å­˜åˆ©ç”¨ç‡å¼‚å¸¸
        elif avg_gpu > 85 and avg_mem > 85 and high_mem_variance:
            reasons.append(f"åŒé«˜ä½è¿è¡Œ GPU({avg_gpu:.0f}%) + å†…å­˜({avg_mem:.0f}%) + æ³¢åŠ¨({mem_variance:.0f}%)")
            is_thrashing = True
        
        # æ¡ä»¶3: æç«¯æƒ…å†µ - GPUå’Œå†…å­˜éƒ½æ¥è¿‘æ»¡è½½
        elif avg_gpu > 90 and avg_mem > 95:
            reasons.append(f"ç³»ç»Ÿæ¥è¿‘æ»¡è½½ GPU({avg_gpu:.0f}%) + å†…å­˜({avg_mem:.0f}%)")
            is_thrashing = True
        
        reason = "; ".join(reasons) if reasons else "æ­£å¸¸"
        return is_thrashing, reason
    
    def get_stats(self) -> Dict:
        """è·å–ç›‘æ§ç»Ÿè®¡ä¿¡æ¯"""
        if not self.gpu_utilizations:
            return {'gpu_avg': 0, 'mem_avg': 0, 'samples': 0}
            
        return {
            'gpu_avg': sum(self.gpu_utilizations) / len(self.gpu_utilizations),
            'mem_avg': sum(self.memory_utilizations) / len(self.memory_utilizations),
            'gpu_max': max(self.gpu_utilizations),
            'mem_max': max(self.memory_utilizations),
            'samples': len(self.gpu_utilizations)
        }

class BatchSizeCalculator:
    """Batch Sizeç†è®ºè®¡ç®—å™¨"""
    
    @staticmethod
    def get_gpu_memory_info() -> Tuple[int, int, int]:
        """
        è·å–GPUå†…å­˜ä¿¡æ¯
        
        Returns:
            (æ€»å†…å­˜MB, å¯ç”¨å†…å­˜MB, ç³»ç»Ÿå ç”¨MB)
        """
        if not torch.cuda.is_available():
            return 0, 0, 0
            
        # è·å–GPUæ€»å†…å­˜
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024 / 1024  # MB
        
        # æ¸…ç†ç¼“å­˜åè·å–å½“å‰å†…å­˜ä½¿ç”¨
        torch.cuda.empty_cache()
        current_allocated = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        
        # ä¼°ç®—ç³»ç»Ÿå ç”¨ï¼ˆæ˜¾å­˜çš„åŸºç¡€å ç”¨ï¼‰
        system_overhead = max(500, total_memory * 0.1)  # è‡³å°‘500MBæˆ–10%
        
        available_memory = total_memory - system_overhead - current_allocated
        
        return int(total_memory), int(available_memory), int(system_overhead + current_allocated)
    
    @staticmethod
    def estimate_model_memory_per_sample(model: nn.Module) -> Tuple[float, float]:
        """
        ç²¾ç¡®ä¼°ç®—æ¨¡å‹æ¯ä¸ªæ ·æœ¬çš„å†…å­˜å ç”¨ï¼ˆMBï¼‰
        
        åŸºäºå®é™…æµ‹è¯•çš„æ”¹è¿›ä¼°ç®—ï¼š
        - å‰å‘æ¿€æ´»å†…å­˜ï¼šåŸºäºç½‘ç»œå±‚æ•°å’Œé€šé“æ•°
        - åå‘æ¢¯åº¦å†…å­˜ï¼šä¸æ¿€æ´»å†…å­˜ç›¸å½“
        - å‚æ•°å’Œä¼˜åŒ–å™¨çŠ¶æ€ï¼šå›ºå®šå¼€é”€
        
        Returns:
            (æ¯æ ·æœ¬å†…å­˜å ç”¨MB, å›ºå®šå†…å­˜å ç”¨MB)
        """
        # æ¨¡å‹å‚æ•°å†…å­˜ (float32 = 4 bytes)
        param_memory = sum(p.numel() * 4 for p in model.parameters()) / 1024 / 1024
        
        # æ¢¯åº¦å†…å­˜ï¼ˆä¸å‚æ•°åŒæ ·å¤§å°ï¼‰
        gradient_memory = param_memory
        
        # ä¼˜åŒ–å™¨çŠ¶æ€å†…å­˜ï¼ˆSGD + momentumï¼‰
        optimizer_memory = param_memory  # åŠ¨é‡ç¼“å­˜
        
        # æ¿€æ´»å†…å­˜ç²¾ç¡®ä¼°ç®—
        # åŸºäºNeuroExaptæ¨¡å‹ç‰¹æ€§ï¼šNetwork with layers=6, C=16
        # ä»å®é™…æµ‹è¯•ä¸­è§‚å¯Ÿåˆ°çš„å†…å­˜ä½¿ç”¨æ¨¡å¼ï¼š
        
        # CIFAR-10 (3, 32, 32) è¾“å…¥
        input_size = 3 * 32 * 32 * 4 / 1024 / 1024  # MB
        
        # æ¯å±‚æ¿€æ´»å†…å­˜ï¼ˆç»éªŒä¼°ç®—ï¼‰
        # stem: 3->64 channels, 32x32
        # cell: progressive channel increase + spatial reduction
        
        # åŸºäºè§‚å¯Ÿåˆ°çš„å†…å­˜å¢é•¿æ¨¡å¼ï¼š
        # batch_size=16: 672MB, batch_size=32: 1272MB
        # å·®å€¼: 600MB for 16 samples = 37.5MB per sample
        
        # ä½†è¿™åŒ…å«äº†æ‰€æœ‰å†…å­˜ï¼Œéœ€è¦åˆ†ç¦»å‡ºæ¿€æ´»éƒ¨åˆ†
        total_per_sample_observed = 37.5  # MB from actual testing
        
        # ä»ä¸­å‡å»å…¶ä»–å›ºå®šå†…å­˜çš„åˆ†æ‘Š
        fixed_memory = param_memory + gradient_memory + optimizer_memory
        activation_per_sample = total_per_sample_observed - (fixed_memory / 64)  # å‡è®¾åŸºå‡†batch=64
        
        # ç¡®ä¿åˆç†èŒƒå›´
        activation_per_sample = max(20.0, min(activation_per_sample, 100.0))  # 20-100MB per sample
        
        return activation_per_sample, fixed_memory
    
    @classmethod
    def calculate_max_batch_size(cls, model: nn.Module, safety_margin: float = 0.8) -> int:
        """
        ç†è®ºè®¡ç®—æœ€å¤§batch size
        
        Args:
            model: ç¥ç»ç½‘ç»œæ¨¡å‹
            safety_margin: å®‰å…¨è¾¹ç•Œï¼ˆ0.8è¡¨ç¤ºåªä½¿ç”¨80%çš„å¯ç”¨å†…å­˜ï¼‰
        """
        total_mem, available_mem, used_mem = cls.get_gpu_memory_info()
        
        if available_mem <= 0:
            return 32  # ä¿å®ˆä¼°è®¡
        
        # ä¼°ç®—å†…å­˜éœ€æ±‚
        per_sample_mem, fixed_mem = cls.estimate_model_memory_per_sample(model)
        
        # è®¡ç®—å¯ç”¨äºbatchçš„å†…å­˜
        usable_memory = available_mem * safety_margin - fixed_mem
        
        if usable_memory <= 0 or per_sample_mem <= 0:
            return 32  # ä¿å®ˆä¼°è®¡
        
        # è®¡ç®—ç†è®ºæœ€å¤§batch size
        max_batch_size = int(usable_memory / per_sample_mem)
        
        # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
        max_batch_size = max(16, min(max_batch_size, 512))
        
        # è°ƒæ•´ä¸º8çš„å€æ•°ï¼ˆGPUè®¡ç®—æ•ˆç‡æ›´å¥½ï¼‰
        max_batch_size = (max_batch_size // 8) * 8
        
        return max_batch_size

def create_test_model() -> nn.Module:
    """åˆ›å»ºæµ‹è¯•ç”¨çš„æ¨¡å‹"""
    return Network(
        C=16,
        num_classes=10,
        layers=6,  # å‡å°‘å±‚æ•°ä»¥åŠ å¿«æµ‹è¯•
        potential_layers=2,  # å‡å°‘æ½œåœ¨å±‚æ•°
        use_gradient_optimized=True,
        quiet=True
    ).cuda()

def _run_batch_test_core(batch_size: int, model: nn.Module) -> Optional[Dict]:
    """
    æ ¸å¿ƒbatch sizeæµ‹è¯•å‡½æ•°ï¼ˆåœ¨çº¿ç¨‹ä¸­è¿è¡Œï¼‰
    """
    try:
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
        
        criterion = nn.CrossEntropyLoss().cuda()
        optimizer = optim.SGD(model.parameters(), lr=0.025, momentum=0.9, weight_decay=3e-4)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_input = torch.randn(batch_size, 3, 32, 32, device='cuda')
        test_target = torch.randint(0, 10, (batch_size,), device='cuda')
        
        # é¢„çƒ­ä¸€æ¬¡
        model.train()
        output = model(test_input)
        loss = criterion(output, target=test_target)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        
        torch.cuda.synchronize()  # ç¡®ä¿é¢„çƒ­å®Œæˆ
        
        # å¤šæ¬¡æµ‹è¯•å–å¹³å‡
        num_runs = 3
        times = []
        
        for i in range(num_runs):
            torch.cuda.synchronize()
            start_time = time.perf_counter()
            
            model.train()
            output = model(test_input)
            loss = criterion(output, target=test_target)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            
            torch.cuda.synchronize()
            end_time = time.perf_counter()
            
            batch_time = end_time - start_time
            times.append(batch_time)
        
        # è·å–å†…å­˜ä¿¡æ¯
        peak_memory_mb = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        avg_time = sum(times) / len(times)
        time_std = (sum((t - avg_time) ** 2 for t in times) / len(times)) ** 0.5
        time_variance = time_std / avg_time if avg_time > 0 else 0
        samples_per_sec = batch_size / avg_time
        
        return {
            'batch_size': batch_size,
            'avg_time': avg_time,
            'time_variance': time_variance,
            'peak_memory_mb': peak_memory_mb,
            'samples_per_sec': samples_per_sec,
            'times': times
        }
        
    except Exception as e:
        return None

def test_batch_with_monitoring(batch_size: int, model: nn.Module, monitor: GPUMonitor, timeout_seconds: float = 15.0) -> Optional[Dict]:
    """
    å¸¦ç›‘æ§å’Œè¶…æ—¶çš„batch sizeæµ‹è¯•
    
    ä½¿ç”¨çº¿ç¨‹æ± å’Œè¶…æ—¶æœºåˆ¶é¿å…å†…å­˜è…¾æŒªæ—¶é•¿æ—¶é—´ç­‰å¾…
    """
    print(f"æµ‹è¯• batch_size={batch_size:3d}... ", end="", flush=True)
    
    # å¼€å§‹ç›‘æ§
    monitor.start_monitoring()
    
    try:
        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œæµ‹è¯•ï¼Œè®¾ç½®è¶…æ—¶
        with ThreadPoolExecutor(max_workers=1) as executor:
            # æäº¤æµ‹è¯•ä»»åŠ¡
            future = executor.submit(_run_batch_test_core, batch_size, model)
            
            try:
                # ç­‰å¾…ç»“æœï¼Œè®¾ç½®è¶…æ—¶
                result = future.result(timeout=timeout_seconds)
                
                if result is not None:
                    # æ£€æŸ¥æ˜¯å¦æœ‰å†…å­˜è…¾æŒª
                    time.sleep(0.3)  # ç»™ç›‘æ§å™¨æ”¶é›†æ•°æ®çš„æ—¶é—´
                    is_thrashing, reason = monitor.detect_memory_thrashing()
                    
                    if is_thrashing:
                        monitor.stop_monitoring()
                        print(f"âŒ å†…å­˜è…¾æŒª ({reason})")
                        return None
                    else:
                        monitor.stop_monitoring()
                        print(f"âœ… {result['avg_time']:.2f}s/batch, {result['samples_per_sec']:4.0f} samples/s, {result['peak_memory_mb']:4.0f}MB")
                        return result
                else:
                    monitor.stop_monitoring()
                    print(f"âŒ æµ‹è¯•å¤±è´¥")
                    return None
                    
            except TimeoutError:
                # è¶…æ—¶äº†ï¼Œå¼ºåˆ¶å–æ¶ˆä»»åŠ¡
                future.cancel()
                monitor.stop_monitoring()
                print(f"ğŸ•’ è¶…æ—¶ (>{timeout_seconds:.0f}s, å¯èƒ½å†…å­˜è…¾æŒª)")
                
                # æ¸…ç†CUDAç¼“å­˜
                torch.cuda.empty_cache()
                return None
                
    except Exception as e:
        monitor.stop_monitoring()
        print(f"âŒ é”™è¯¯: {str(e)}")
        return None
                    monitor.stop_monitoring()
                    print(f"âŒ å†…å­˜è…¾æŒª (ç¬¬{i+1}æ¬¡å: {reason})")
                    return None
            
            # è¶…æ—¶æ£€æŸ¥
            if batch_time > 15.0:  # 15ç§’è¶…æ—¶
                monitor.stop_monitoring()
                print(f"âŒ è¶…æ—¶ ({batch_time:.1f}s)")
                return None
        
        monitor.stop_monitoring()
        
        # è®¡ç®—ç»“æœ
        avg_time = sum(times) / len(times)
        samples_per_sec = batch_size / avg_time
        peak_memory = torch.cuda.max_memory_allocated() / 1024 / 1024
        time_variance = max(times) - min(times) if len(times) > 1 else 0
        
        # è·å–GPUç›‘æ§ç»Ÿè®¡
        monitor_stats = monitor.get_stats()
        
        print(f"âœ… {avg_time:.2f}s/batch, {samples_per_sec:5.0f} samples/s, {peak_memory:4.0f}MB")
        if monitor_stats['samples'] > 0:
            print(f"       GPUå¹³å‡:{monitor_stats['gpu_avg']:.0f}% å†…å­˜å¹³å‡:{monitor_stats['mem_avg']:.0f}%")
        
        # æ¸…ç†
        del test_input, test_target
        torch.cuda.empty_cache()
        gc.collect()
        
        return {
            'batch_size': batch_size,
            'avg_time': avg_time,
            'samples_per_sec': samples_per_sec,
            'peak_memory_mb': peak_memory,
            'warmup_time': warmup_time,
            'time_variance': time_variance,
            'gpu_stats': monitor_stats
        }
        
    except RuntimeError as e:
        monitor.stop_monitoring()
        if "out of memory" in str(e).lower():
            print("âŒ OOM")
        else:
            print(f"âŒ Error: {str(e)[:30]}...")
        return None
    except Exception as e:
        monitor.stop_monitoring()
        print(f"âŒ Exception: {str(e)[:30]}...")
        return None

def find_optimal_batch_size(quiet: bool = False) -> int:
    """
    é™é»˜æ‰¾åˆ°æœ€ä¼˜batch size
    
    Returns:
        æœ€ä¼˜çš„batch sizeæ•°å€¼
    """
    if not quiet:
        print("ğŸ§  æ™ºèƒ½Batch Sizeä¼˜åŒ–å™¨")
        print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    if not quiet:
        print("ğŸ“Š åˆ›å»ºæµ‹è¯•æ¨¡å‹...")
    model = create_test_model()
    
    # è·å–GPUå†…å­˜ä¿¡æ¯
    total_mem, available_mem, used_mem = BatchSizeCalculator.get_gpu_memory_info()
    if not quiet:
        print(f"ğŸ’¾ GPUå†…å­˜: æ€»è®¡{total_mem}MB, å¯ç”¨{available_mem}MB, å·²ç”¨{used_mem}MB")
    
    # ç†è®ºè®¡ç®—æœ€å¤§batch size
    theoretical_max = BatchSizeCalculator.calculate_max_batch_size(model)
    if not quiet:
        print(f"ğŸ§® ç†è®ºæœ€å¤§batch size: {theoretical_max}")
    
    # åˆ›å»ºGPUç›‘æ§å™¨
    monitor = GPUMonitor()
    
    # æ™ºèƒ½æµ‹è¯•ç­–ç•¥ - æ‰©å±•å€™é€‰èŒƒå›´æ”¯æŒå„ç§GPUé…ç½®
    candidates = [16, 32, 48, 64, 80, 96, 128, 160, 192, 224, 256, 320, 384, 512]
    
    # åªæµ‹è¯•ä¸è¶…è¿‡ç†è®ºæœ€å¤§å€¼çš„batch size
    valid_candidates = [bs for bs in candidates if bs <= theoretical_max * 1.2]  # å…è®¸20%è¶…å‡º
    
    if not valid_candidates:
        valid_candidates = [16, 32]  # ä¿å®ˆæµ‹è¯•
    
    if not quiet:
        print(f"ğŸ¯ æµ‹è¯•å€™é€‰: {valid_candidates}")
        print("=" * 60)
    
    results = []
    peak_samples_per_sec = 0  # è®°å½•å³°å€¼ååé‡
    declining_count = 0  # è¿ç»­ä¸‹é™è®¡æ•°
    last_two_results = []  # è®°å½•æœ€è¿‘ä¸¤æ¬¡ç»“æœï¼Œç”¨äºè¶‹åŠ¿åˆ¤æ–­
    
    for batch_size in valid_candidates:
        result = test_batch_with_monitoring(batch_size, model, monitor)
        
        if result is not None:
            results.append(result)
            current_samples_per_sec = result['samples_per_sec']
            current_time_per_batch = result['avg_time']
            
            # æ£€æµ‹æ€§èƒ½ä¸‹é™çš„å¤šç§æŒ‡æ ‡
            should_stop = False
            stop_reason = ""
            
            # 1. ååé‡ä¸‹é™æ£€æµ‹
            if current_samples_per_sec > peak_samples_per_sec:
                peak_samples_per_sec = current_samples_per_sec
                declining_count = 0  # é‡ç½®ä¸‹é™è®¡æ•°
            else:
                declining_count += 1
                decline_ratio = (peak_samples_per_sec - current_samples_per_sec) / peak_samples_per_sec
                
                # æ›´æ•æ„Ÿçš„æ—©åœæ¡ä»¶
                if decline_ratio > 0.12:  # ä¸‹é™è¶…è¿‡12%å°±åœæ­¢
                    should_stop = True
                    stop_reason = f"ååé‡ä¸‹é™{decline_ratio*100:.1f}%"
                elif declining_count >= 2:  # è¿ç»­2æ¬¡ä¸‹é™
                    should_stop = True
                    stop_reason = f"è¿ç»­{declining_count}æ¬¡ä¸‹é™"
            
            # 2. æ—¶é—´å‰§å¢æ£€æµ‹ï¼ˆå†…å­˜è…¾æŒªçš„å…¸å‹è¡¨ç°ï¼‰
            if len(last_two_results) >= 2:
                recent_avg_time = sum(r['avg_time'] for r in last_two_results) / len(last_two_results)
                time_increase_ratio = (current_time_per_batch - recent_avg_time) / recent_avg_time
                
                if time_increase_ratio > 0.5:  # æ—¶é—´å¢é•¿è¶…è¿‡50%
                    should_stop = True
                    stop_reason = f"è¿è¡Œæ—¶é—´å‰§å¢{time_increase_ratio*100:.1f}%"
            
            # 3. æ—¶é—´æ–¹å·®æ£€æµ‹ï¼ˆä¸ç¨³å®šæ€§ï¼‰
            if result['time_variance'] > 0.8:  # æ—¶é—´æ–¹å·®è¿‡å¤§ï¼Œè¯´æ˜å†…å­˜è…¾æŒªä¸¥é‡
                should_stop = True
                stop_reason = f"æ—¶é—´æ–¹å·®è¿‡å¤§({result['time_variance']:.2f})"
            
            # 4. å†…å­˜å‹åŠ›æ£€æµ‹
            memory_usage_ratio = result['peak_memory_mb'] / total_mem
            if memory_usage_ratio > 0.85:  # å†…å­˜ä½¿ç”¨è¶…è¿‡85%
                should_stop = True
                stop_reason = f"å†…å­˜å‹åŠ›è¿‡å¤§({memory_usage_ratio*100:.1f}%)"
            
            # æ‰§è¡Œæ—©åœ
            if should_stop:
                if not quiet:
                    print(f"ğŸ›‘ æ™ºèƒ½æ—©åœ: {stop_reason}")
                    print(f"   å½“å‰: {current_samples_per_sec:.0f} samples/s, {current_time_per_batch:.2f}s/batch")
                    print(f"   å³°å€¼: {peak_samples_per_sec:.0f} samples/s")
                    print(f"   è·³è¿‡å‰©ä½™æ›´å¤§çš„batch sizeæµ‹è¯•")
                break
            
            # æ›´æ–°æœ€è¿‘ç»“æœè®°å½•
            last_two_results.append(result)
            if len(last_two_results) > 2:
                last_two_results.pop(0)
                
        else:
            # å¦‚æœå¤±è´¥äº†ï¼Œè·³è¿‡æ›´å¤§çš„batch size
            if not quiet:
                print(f"âš ï¸  è·³è¿‡æ›´å¤§çš„batch size (å½“å‰{batch_size}å¤±è´¥)")
            break
        
        # çŸ­æš‚ä¼‘æ¯è®©GPUå†·å´
        time.sleep(0.3)  # è¿›ä¸€æ­¥å‡å°‘ç­‰å¾…æ—¶é—´
    
    # æ¸…ç†æµ‹è¯•æ¨¡å‹
    del model
    torch.cuda.empty_cache()
    
    # åˆ†æç»“æœå¹¶è¿”å›æœ€ä½³batch size
    if not results:
        if not quiet:
            print("âŒ æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•ç»“æœï¼Œä½¿ç”¨é»˜è®¤å€¼32")
        return 32
    
    # è®¡ç®—ç»¼åˆæœ€ä½³batch size
    best_throughput = max(results, key=lambda x: x['samples_per_sec'])
    
    efficiency_scores = []
    for result in results:
        # ç»¼åˆè¯„åˆ†ï¼šååé‡ + å†…å­˜æ•ˆç‡ + ç¨³å®šæ€§
        throughput_score = result['samples_per_sec'] / best_throughput['samples_per_sec']
        memory_score = 1.0 - (result['peak_memory_mb'] / total_mem)
        stability_score = 1.0 - min(result['time_variance'], 1.0)
        
        overall_score = throughput_score * 0.5 + memory_score * 0.3 + stability_score * 0.2
        efficiency_scores.append((result, overall_score))
    
    best_overall = max(efficiency_scores, key=lambda x: x[1])
    optimal_batch_size = best_overall[0]['batch_size']
    
    if not quiet:
        print(f"\nğŸ“ˆ æµ‹è¯•ç»“æœåˆ†æ:")
        print("=" * 60)
        print(f"{'Batch':<6} {'æ—¶é—´':<8} {'é€Ÿåº¦':<10} {'å†…å­˜':<8} {'æ•ˆç‡':<6} {'ç¨³å®šæ€§':<8}")
        print("-" * 60)
        
        for result in results:
            efficiency = result['samples_per_sec'] / result['peak_memory_mb'] * 1000
            stability = "å¥½" if result['time_variance'] < 0.2 else "ä¸€èˆ¬" if result['time_variance'] < 0.5 else "å·®"
            
            marker = ""
            if result == best_throughput:
                marker += "ğŸš€"
            if result == best_overall[0]:
                marker += "âš¡"
            
            print(f"{result['batch_size']:<6} {result['avg_time']:<8.2f} "
                  f"{result['samples_per_sec']:<10.0f} {result['peak_memory_mb']:<8.0f} "
                  f"{efficiency:<6.1f} {stability:<8} {marker}")
        
        print(f"\nğŸ† æœ€ä¼˜é€‰æ‹©: batch_size={optimal_batch_size}")
        print(f"   æœ€é«˜ååé‡: batch_size={best_throughput['batch_size']} "
              f"({best_throughput['samples_per_sec']:.0f} samples/s)")
        print(f"   ç»¼åˆæœ€ä½³: batch_size={optimal_batch_size} "
              f"(è¯„åˆ†: {best_overall[1]:.3f})")
    
    return optimal_batch_size

def intelligent_batch_size_optimization():
    """æ™ºèƒ½batch sizeä¼˜åŒ–ï¼ˆè¯¦ç»†ç‰ˆæœ¬ï¼‰"""
    optimal_batch_size = find_optimal_batch_size(quiet=False)
    return optimal_batch_size

if __name__ == "__main__":
    intelligent_batch_size_optimization() 