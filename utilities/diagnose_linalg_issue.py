"""
è¯Šæ–­ torch.linalg.matrix_norm è°ƒç”¨é”™è¯¯çš„é—®é¢˜

è¿™ä¸ªè„šæœ¬å°†æµ‹è¯•æ‰€æœ‰å¯èƒ½å¯¼è‡´ linalg.matrix_norm é”™è¯¯çš„åŸå› ï¼š
1. PyTorch ç‰ˆæœ¬å…¼å®¹æ€§
2. è¾“å…¥å¼ é‡ç±»å‹å’Œå½¢çŠ¶
3. è®¾å¤‡å…¼å®¹æ€§
4. æ¢¯åº¦è®¡ç®—é—®é¢˜
5. å†…å­˜é—®é¢˜
"""

import torch
import torch.nn as nn
import numpy as np
import traceback
import sys

def check_pytorch_version():
    """æ£€æŸ¥PyTorchç‰ˆæœ¬å…¼å®¹æ€§"""
    print("=" * 60)
    print("1. PyTorchç‰ˆæœ¬æ£€æŸ¥")
    print("=" * 60)
    
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"Current device: {torch.cuda.current_device()}")
        print(f"Device name: {torch.cuda.get_device_name()}")
        # Skip CUDA version check due to API differences
    
    # æ£€æŸ¥ torch.linalg.matrix_norm æ˜¯å¦å­˜åœ¨
    try:
        hasattr(torch.linalg, 'matrix_norm')
        print("âœ… torch.linalg.matrix_norm å­˜åœ¨")
        
        # æ£€æŸ¥ç‰ˆæœ¬è¦æ±‚
        version_parts = torch.__version__.split('.')
        major, minor = int(version_parts[0]), int(version_parts[1])
        
        if major < 2:
            print("âŒ PyTorchç‰ˆæœ¬è¿‡ä½ï¼Œéœ€è¦ >= 2.0.0")
            print("   matrix_norm åœ¨ PyTorch 1.x ä¸­ä¸å¯ç”¨")
            return False
        else:
            print("âœ… PyTorchç‰ˆæœ¬ç¬¦åˆè¦æ±‚")
            
    except Exception as e:
        print(f"âŒ torch.linalg.matrix_norm æ£€æŸ¥å¤±è´¥: {e}")
        return False
    
    return True

def test_basic_matrix_norm():
    """æµ‹è¯•åŸºæœ¬çš„ matrix_norm åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("2. åŸºæœ¬ matrix_norm åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    try:
        # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
        test_matrix = torch.randn(3, 3)
        print(f"æµ‹è¯•çŸ©é˜µ: {test_matrix.shape}")
        
        # æµ‹è¯•ä¸åŒçš„ norm ç±»å‹
        norm_types = ['fro', 'nuc', 2, -2, 1, -1, float('inf'), float('-inf')]
        
        for norm_type in norm_types:
            try:
                result = torch.linalg.matrix_norm(test_matrix, ord=norm_type)
                print(f"âœ… norm={norm_type}: {result:.4f}")
            except Exception as e:
                print(f"âŒ norm={norm_type} å¤±è´¥: {e}")
                
    except Exception as e:
        print(f"âŒ åŸºæœ¬æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False
    
    return True

def test_tensor_types():
    """æµ‹è¯•ä¸åŒå¼ é‡ç±»å‹"""
    print("\n" + "=" * 60)
    print("3. å¼ é‡ç±»å‹æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•ä¸åŒæ•°æ®ç±»å‹
    dtypes = [torch.float32, torch.float64, torch.int32, torch.int64]
    
    for dtype in dtypes:
        try:
            test_tensor = torch.randn(3, 3).to(dtype)
            print(f"æµ‹è¯•æ•°æ®ç±»å‹: {dtype}")
            
            if dtype in [torch.int32, torch.int64]:
                # æ•´æ•°ç±»å‹å¯èƒ½ä¸æ”¯æŒ
                try:
                    result = torch.linalg.matrix_norm(test_tensor)
                    print(f"âœ… {dtype}: {result:.4f}")
                except Exception as e:
                    print(f"âŒ {dtype} ä¸æ”¯æŒ: {e}")
            else:
                result = torch.linalg.matrix_norm(test_tensor)
                print(f"âœ… {dtype}: {result:.4f}")
                
        except Exception as e:
            print(f"âŒ {dtype} æµ‹è¯•å¤±è´¥: {e}")

def test_tensor_shapes():
    """æµ‹è¯•ä¸åŒå¼ é‡å½¢çŠ¶"""
    print("\n" + "=" * 60)
    print("4. å¼ é‡å½¢çŠ¶æµ‹è¯•")
    print("=" * 60)
    
    test_shapes = [
        (2, 2),      # 2x2 çŸ©é˜µ
        (3, 3),      # 3x3 çŸ©é˜µ
        (4, 4),      # 4x4 çŸ©é˜µ
        (2, 3),      # éæ–¹é˜µ
        (3, 2),      # éæ–¹é˜µ
        (1, 5),      # 1ç»´ç±»ä¼¼
        (5, 1),      # 1ç»´ç±»ä¼¼
        (2, 3, 4),   # 3ç»´å¼ é‡
        (2, 3, 3),   # æ‰¹é‡çŸ©é˜µ
        (1,),        # 1ç»´å¼ é‡
        (5,),        # 1ç»´å¼ é‡
        ()           # æ ‡é‡
    ]
    
    for shape in test_shapes:
        try:
            test_tensor = torch.randn(*shape) if shape else torch.tensor(5.0)
            print(f"æµ‹è¯•å½¢çŠ¶: {shape}")
            
            result = torch.linalg.matrix_norm(test_tensor)
            print(f"âœ… {shape}: {result}")
            
        except Exception as e:
            print(f"âŒ {shape} å¤±è´¥: {e}")

def test_device_compatibility():
    """æµ‹è¯•è®¾å¤‡å…¼å®¹æ€§"""
    print("\n" + "=" * 60)
    print("5. è®¾å¤‡å…¼å®¹æ€§æµ‹è¯•")
    print("=" * 60)
    
    # CPU æµ‹è¯•
    try:
        cpu_tensor = torch.randn(3, 3)
        result = torch.linalg.matrix_norm(cpu_tensor)
        print(f"âœ… CPU: {result:.4f}")
    except Exception as e:
        print(f"âŒ CPU å¤±è´¥: {e}")
    
    # GPU æµ‹è¯•
    if torch.cuda.is_available():
        try:
            gpu_tensor = torch.randn(3, 3).cuda()
            result = torch.linalg.matrix_norm(gpu_tensor)
            print(f"âœ… GPU: {result:.4f}")
        except Exception as e:
            print(f"âŒ GPU å¤±è´¥: {e}")
    else:
        print("âŒ GPU ä¸å¯ç”¨")

def test_gradient_computation():
    """æµ‹è¯•æ¢¯åº¦è®¡ç®—"""
    print("\n" + "=" * 60)
    print("6. æ¢¯åº¦è®¡ç®—æµ‹è¯•")
    print("=" * 60)
    
    try:
        # æµ‹è¯•éœ€è¦æ¢¯åº¦çš„å¼ é‡
        test_tensor = torch.randn(3, 3, requires_grad=True)
        print(f"æµ‹è¯•å¼ é‡: {test_tensor.shape}, requires_grad={test_tensor.requires_grad}")
        
        norm_result = torch.linalg.matrix_norm(test_tensor)
        print(f"âœ… è®¡ç®—norm: {norm_result:.4f}")
        
        # æµ‹è¯•åå‘ä¼ æ’­
        loss = norm_result.sum()
        loss.backward()
        print(f"âœ… åå‘ä¼ æ’­æˆåŠŸ")
        if test_tensor.grad is not None:
            print(f"æ¢¯åº¦å½¢çŠ¶: {test_tensor.grad.shape}")
        else:
            print("âŒ æ¢¯åº¦ä¸ºNone")
        
    except Exception as e:
        print(f"âŒ æ¢¯åº¦è®¡ç®—å¤±è´¥: {e}")
        traceback.print_exc()

def test_memory_issues():
    """æµ‹è¯•å†…å­˜ç›¸å…³é—®é¢˜"""
    print("\n" + "=" * 60)
    print("7. å†…å­˜é—®é¢˜æµ‹è¯•")
    print("=" * 60)
    
    try:
        # æµ‹è¯•å¤§çŸ©é˜µ
        large_sizes = [100, 500, 1000]
        
        for size in large_sizes:
            try:
                large_tensor = torch.randn(size, size)
                result = torch.linalg.matrix_norm(large_tensor)
                print(f"âœ… å¤§å° {size}x{size}: {result:.4f}")
                
                # æ¸…ç†å†…å­˜
                del large_tensor
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    
            except Exception as e:
                print(f"âŒ å¤§å° {size}x{size} å¤±è´¥: {e}")
                
    except Exception as e:
        print(f"âŒ å†…å­˜æµ‹è¯•å¤±è´¥: {e}")

def test_special_cases():
    """æµ‹è¯•ç‰¹æ®Šæƒ…å†µ"""
    print("\n" + "=" * 60)
    print("8. ç‰¹æ®Šæƒ…å†µæµ‹è¯•")
    print("=" * 60)
    
    special_cases = [
        ("é›¶çŸ©é˜µ", torch.zeros(3, 3)),
        ("å•ä½çŸ©é˜µ", torch.eye(3)),
        ("åŒ…å«NaN", torch.tensor([[1., float('nan')], [3., 4.]])),
        ("åŒ…å«Inf", torch.tensor([[1., float('inf')], [3., 4.]])),
        ("æå°å€¼", torch.full((3, 3), 1e-10)),
        ("æå¤§å€¼", torch.full((3, 3), 1e10))
    ]
    
    for name, tensor in special_cases:
        try:
            result = torch.linalg.matrix_norm(tensor)
            print(f"âœ… {name}: {result}")
        except Exception as e:
            print(f"âŒ {name} å¤±è´¥: {e}")

def test_existing_code_patterns():
    """æµ‹è¯•ç°æœ‰ä»£ç ä¸­çš„ä½¿ç”¨æ¨¡å¼"""
    print("\n" + "=" * 60)
    print("9. ç°æœ‰ä»£ç æ¨¡å¼æµ‹è¯•")
    print("=" * 60)
    
    try:
        # æ¨¡æ‹Ÿ radical_architecture_evolution.py ä¸­çš„ä½¿ç”¨
        print("æµ‹è¯• NTK çŸ©é˜µè®¡ç®—æ¨¡å¼...")
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„ NTK çŸ©é˜µ
        n = 10
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        ntk_matrix = torch.randn(n, n, device=device)
        
        # æµ‹è¯• matrix_rank
        try:
            rank = torch.linalg.matrix_rank(ntk_matrix)
            print(f"âœ… matrix_rank: {rank.item()}")
        except Exception as e:
            print(f"âŒ matrix_rank å¤±è´¥: {e}")
            
        # æµ‹è¯• eigvals
        try:
            eigenvalues = torch.linalg.eigvals(ntk_matrix)
            print(f"âœ… eigvals: shape={eigenvalues.shape}")
        except Exception as e:
            print(f"âŒ eigvals å¤±è´¥: {e}")
            
        # æµ‹è¯• pinv
        try:
            ntk_inv = torch.linalg.pinv(ntk_matrix)
            print(f"âœ… pinv: shape={ntk_inv.shape}")
        except Exception as e:
            print(f"âŒ pinv å¤±è´¥: {e}")
            
        # æµ‹è¯• norm
        try:
            norm_result = torch.norm(ntk_inv)
            print(f"âœ… norm: {norm_result.item():.4f}")
        except Exception as e:
            print(f"âŒ norm å¤±è´¥: {e}")
            
    except Exception as e:
        print(f"âŒ ç°æœ‰ä»£ç æ¨¡å¼æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” torch.linalg.matrix_norm é”™è¯¯è¯Šæ–­")
    print("è¿™ä¸ªè„šæœ¬å°†æ£€æŸ¥æ‰€æœ‰å¯èƒ½å¯¼è‡´é”™è¯¯çš„åŸå› ")
    print()
    
    try:
        # 1. ç‰ˆæœ¬æ£€æŸ¥
        if not check_pytorch_version():
            print("\nâŒ ç‰ˆæœ¬æ£€æŸ¥å¤±è´¥ï¼Œè¯·å‡çº§PyTorchåˆ°2.0+")
            return
            
        # 2. åŸºæœ¬åŠŸèƒ½æµ‹è¯•
        if not test_basic_matrix_norm():
            print("\nâŒ åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥")
            return
            
        # 3. å…¶ä»–æµ‹è¯•
        test_tensor_types()
        test_tensor_shapes()
        test_device_compatibility()
        test_gradient_computation()
        test_memory_issues()
        test_special_cases()
        test_existing_code_patterns()
        
        print("\n" + "=" * 60)
        print("âœ… è¯Šæ–­å®Œæˆ")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nâŒ è¯Šæ–­è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main() 