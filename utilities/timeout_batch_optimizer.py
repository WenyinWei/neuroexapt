#!/usr/bin/env python3
"""
è¶…æ—¶æœºåˆ¶çš„Batch Sizeä¼˜åŒ–å™¨

ç‰¹ç‚¹ï¼š
1. çº¿ç¨‹æ±  + è¶…æ—¶æœºåˆ¶é¿å…å†…å­˜è…¾æŒªæ—¶é•¿æ—¶é—´ç­‰å¾…
2. æ•æ„Ÿçš„æ—©åœæ£€æµ‹
3. å¿«é€Ÿè¯†åˆ«æœ€ä¼˜batch size
"""

import torch
import torch.nn as nn
import torch.optim as optim
import time
import gc
import sys
import os
from concurrent.futures import ThreadPoolExecutor, TimeoutError
from typing import Optional, Dict, List

# Add the project directory to the path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from neuroexapt.core.model import Network

class TimeoutBatchOptimizer:
    """è¶…æ—¶æœºåˆ¶çš„batch sizeä¼˜åŒ–å™¨"""
    
    def __init__(self, timeout_seconds: float = 12.0):
        self.timeout_seconds = timeout_seconds
        
    def get_gpu_memory_info(self):
        """è·å–GPUå†…å­˜ä¿¡æ¯"""
        if not torch.cuda.is_available():
            return 0, 0, 0
        
        total_mem = torch.cuda.get_device_properties(0).total_memory / 1024 / 1024
        allocated_mem = torch.cuda.memory_allocated() / 1024 / 1024
        available_mem = total_mem - allocated_mem
        
        return total_mem, available_mem, allocated_mem
    
    def _test_batch_core(self, batch_size: int, model: nn.Module) -> Optional[Dict]:
        """æ ¸å¿ƒæµ‹è¯•å‡½æ•°ï¼ˆåœ¨çº¿ç¨‹ä¸­è¿è¡Œï¼‰"""
        try:
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
            
            criterion = nn.CrossEntropyLoss().cuda()
            optimizer = optim.SGD(model.parameters(), lr=0.025, momentum=0.9, weight_decay=3e-4)
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            test_input = torch.randn(batch_size, 3, 32, 32, device='cuda')
            test_target = torch.randint(0, 10, (batch_size,), device='cuda')
            
            # é¢„çƒ­
            model.train()
            output = model(test_input)
            loss = criterion(output, test_target)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            torch.cuda.synchronize()
            
            # å¤šæ¬¡æµ‹è¯•
            times = []
            for _ in range(3):
                torch.cuda.synchronize()
                start_time = time.perf_counter()
                
                model.train()
                output = model(test_input)
                loss = criterion(output, test_target)
                loss.backward()
                optimizer.step()
                optimizer.zero_grad()
                
                torch.cuda.synchronize()
                end_time = time.perf_counter()
                times.append(end_time - start_time)
            
            # ç»Ÿè®¡
            peak_memory_mb = torch.cuda.max_memory_allocated() / 1024 / 1024
            avg_time = sum(times) / len(times)
            time_std = (sum((t - avg_time) ** 2 for t in times) / len(times)) ** 0.5
            time_variance = time_std / avg_time if avg_time > 0 else 0
            samples_per_sec = batch_size / avg_time
            
            return {
                'batch_size': batch_size,
                'avg_time': avg_time,
                'time_variance': time_variance,
                'peak_memory_mb': peak_memory_mb,
                'samples_per_sec': samples_per_sec
            }
            
        except Exception:
            return None
    
    def test_batch_with_timeout(self, batch_size: int, model: nn.Module) -> Optional[Dict]:
        """å¸¦è¶…æ—¶çš„batchæµ‹è¯•"""
        print(f"æµ‹è¯• batch_size={batch_size:3d}... ", end="", flush=True)
        
        try:
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(self._test_batch_core, batch_size, model)
                
                try:
                    result = future.result(timeout=self.timeout_seconds)
                    
                    if result is not None:
                        print(f"âœ… {result['avg_time']:.2f}s/batch, {result['samples_per_sec']:4.0f} samples/s, {result['peak_memory_mb']:4.0f}MB")
                        return result
                    else:
                        print(f"âŒ æµ‹è¯•å¤±è´¥")
                        return None
                        
                                 except TimeoutError:
                     future.cancel()
                     print(f"ğŸ•’ è¶…æ—¶ (>{self.timeout_seconds:.0f}s, å†…å­˜è…¾æŒª)")
                     torch.cuda.empty_cache()
                     return None
                    
        except Exception as e:
            print(f"âŒ é”™è¯¯: {str(e)}")
            return None
    
    def find_optimal_batch_size(self, quiet: bool = False) -> int:
        """å¯»æ‰¾æœ€ä¼˜batch size"""
        if not quiet:
            print("ğŸ§  è¶…æ—¶æœºåˆ¶Batch Sizeä¼˜åŒ–å™¨")
            print("=" * 60)
        
        # åˆ›å»ºæµ‹è¯•æ¨¡å‹
        model = Network(
            C=16,
            num_classes=10,
            layers=4,  # å‡å°‘åˆ°4å±‚åŠ å¿«æµ‹è¯•
            potential_layers=2,
            use_gradient_optimized=True,
            quiet=True
        ).cuda()
        
        # è·å–å†…å­˜ä¿¡æ¯
        total_mem, available_mem, used_mem = self.get_gpu_memory_info()
        if not quiet:
            print(f"ğŸ’¾ GPUå†…å­˜: æ€»è®¡{total_mem:.0f}MB, å¯ç”¨{available_mem:.0f}MB")
        
        # æµ‹è¯•å€™é€‰
        candidates = [16, 32, 48, 64, 80, 96, 128, 160, 192, 256]
        
        # åŸºäºå¯ç”¨å†…å­˜è¿‡æ»¤å€™é€‰
        max_candidate = min(256, int(available_mem / 20))  # ç²—ç•¥ä¼°ç®—
        valid_candidates = [bs for bs in candidates if bs <= max_candidate]
        
        if not quiet:
            print(f"ğŸ¯ æµ‹è¯•å€™é€‰: {valid_candidates}")
            print(f"â° è¶…æ—¶è®¾ç½®: {self.timeout_seconds}s")
            print("=" * 60)
        
        results = []
        peak_samples_per_sec = 0
        declining_count = 0
        
        for batch_size in valid_candidates:
            result = self.test_batch_with_timeout(batch_size, model)
            
            if result is not None:
                results.append(result)
                current_samples_per_sec = result['samples_per_sec']
                
                # æ—©åœæ£€æµ‹
                if current_samples_per_sec > peak_samples_per_sec:
                    peak_samples_per_sec = current_samples_per_sec
                    declining_count = 0
                else:
                    declining_count += 1
                    decline_ratio = (peak_samples_per_sec - current_samples_per_sec) / peak_samples_per_sec
                    
                    # æ›´æ•æ„Ÿçš„æ—©åœ
                    if decline_ratio > 0.10 or declining_count >= 2:
                        if not quiet:
                            print(f"ğŸ›‘ æ—©åœ: æ€§èƒ½ä¸‹é™{decline_ratio*100:.1f}% æˆ–è¿ç»­{declining_count}æ¬¡ä¸‹é™")
                        break
                
                # æ—¶é—´å¼‚å¸¸æ£€æµ‹
                if result['time_variance'] > 0.6:
                    if not quiet:
                        print(f"ğŸ›‘ æ—©åœ: æ—¶é—´æ–¹å·®è¿‡å¤§({result['time_variance']:.2f})")
                    break
                    
                # å†…å­˜å‹åŠ›æ£€æµ‹
                if result['peak_memory_mb'] / total_mem > 0.85:
                    if not quiet:
                        print(f"ğŸ›‘ æ—©åœ: å†…å­˜ä½¿ç”¨è¿‡é«˜({result['peak_memory_mb']/total_mem*100:.1f}%)")
                    break
                         else:
                 # æµ‹è¯•å¤±è´¥æˆ–è¶…æ—¶ï¼Œç«‹å³åœæ­¢
                 if not quiet:
                     print(f"ğŸ›‘ åœæ­¢æµ‹è¯• (batch_size={batch_size}å¤±è´¥æˆ–è¶…æ—¶)")
                 break
            
            time.sleep(0.2)  # çŸ­æš‚ä¼‘æ¯
        
        # æ¸…ç†
        del model
        torch.cuda.empty_cache()
        
        # é€‰æ‹©æœ€ä¼˜batch size
        if not results:
            if not quiet:
                print("âŒ æ²¡æœ‰æˆåŠŸç»“æœï¼Œä½¿ç”¨é»˜è®¤å€¼32")
            return 32
        
        # ç»¼åˆè¯„åˆ†
        best_throughput = max(results, key=lambda x: x['samples_per_sec'])
        
        efficiency_scores = []
        for result in results:
            throughput_score = result['samples_per_sec'] / best_throughput['samples_per_sec']
            memory_score = 1.0 - (result['peak_memory_mb'] / total_mem)
            stability_score = 1.0 - min(result['time_variance'], 1.0)
            
            overall_score = throughput_score * 0.6 + memory_score * 0.25 + stability_score * 0.15
            efficiency_scores.append((result, overall_score))
        
        best_overall = max(efficiency_scores, key=lambda x: x[1])
        optimal_batch_size = best_overall[0]['batch_size']
        
        if not quiet:
            print(f"\nğŸ“ˆ æµ‹è¯•ç»“æœ:")
            print("=" * 60)
            print(f"{'Batch':<6} {'æ—¶é—´':<8} {'é€Ÿåº¦':<10} {'å†…å­˜':<8} {'ç¨³å®šæ€§':<8}")
            print("-" * 50)
            
            for result in results:
                stability = "å¥½" if result['time_variance'] < 0.2 else "ä¸€èˆ¬" if result['time_variance'] < 0.5 else "å·®"
                marker = ""
                if result == best_throughput:
                    marker += "ğŸš€"
                if result == best_overall[0]:
                    marker += "âš¡"
                
                print(f"{result['batch_size']:<6} {result['avg_time']:<8.2f} "
                      f"{result['samples_per_sec']:<10.0f} {result['peak_memory_mb']:<8.0f} "
                      f"{stability:<8} {marker}")
            
            print(f"\nğŸ† æœ€ä¼˜é€‰æ‹©: batch_size={optimal_batch_size}")
            print(f"   (è¯„åˆ†: {best_overall[1]:.3f})")
        
        return optimal_batch_size

def find_optimal_batch_size(quiet: bool = False) -> int:
    """å¿«é€Ÿæ‰¾åˆ°æœ€ä¼˜batch size"""
    optimizer = TimeoutBatchOptimizer(timeout_seconds=12.0)
    return optimizer.find_optimal_batch_size(quiet=quiet)

if __name__ == "__main__":
    find_optimal_batch_size(quiet=False) 