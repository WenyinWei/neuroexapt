#!/usr/bin/env python3
"""
å¿«é€Ÿè¶…æ—¶Batch Sizeä¼˜åŒ–å™¨

ç‰¹ç‚¹ï¼š
1. çº¿ç¨‹æ±  + è¶…æ—¶æœºåˆ¶ï¼Œè¶…æ—¶ç«‹å³åœæ­¢
2. æ•æ„Ÿçš„æ—©åœæ£€æµ‹
3. å¿«é€Ÿè¯†åˆ«æœ€ä¼˜batch size
"""

import torch
import torch.nn as nn
import torch.optim as optim
import time
import sys
import os
from concurrent.futures import ThreadPoolExecutor, TimeoutError
from typing import Optional, Dict

# Add the project directory to the path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from neuroexapt.core.model import Network

def test_single_batch(batch_size: int, timeout: float = 10.0) -> Optional[Dict]:
    """æµ‹è¯•å•ä¸ªbatch sizeï¼Œå¸¦è¶…æ—¶"""
    
    def _core_test():
        model = Network(C=16, num_classes=10, layers=4, potential_layers=2, 
                       use_gradient_optimized=True, use_optimized_ops=True, 
                       use_lazy_ops=True, use_memory_efficient=True,
                       use_compile=True, quiet=True).cuda()
        
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
        
        criterion = nn.CrossEntropyLoss().cuda()
        optimizer = optim.SGD(model.parameters(), lr=0.025, momentum=0.9, weight_decay=3e-4)
        
        test_input = torch.randn(batch_size, 3, 32, 32, device='cuda')
        test_target = torch.randint(0, 10, (batch_size,), device='cuda')
        
        # é¢„çƒ­
        model.train()
        output = model(test_input)
        loss = criterion(output, test_target)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        torch.cuda.synchronize()
        
        # æµ‹è¯•3æ¬¡
        times = []
        for _ in range(3):
            torch.cuda.synchronize()
            start = time.perf_counter()
            
            model.train()
            output = model(test_input)
            loss = criterion(output, test_target)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            
            torch.cuda.synchronize()
            times.append(time.perf_counter() - start)
        
        peak_memory = torch.cuda.max_memory_allocated() / 1024 / 1024
        avg_time = sum(times) / len(times)
        samples_per_sec = batch_size / avg_time
        
        del model
        torch.cuda.empty_cache()
        
        return {
            'batch_size': batch_size,
            'avg_time': avg_time,
            'peak_memory_mb': peak_memory,
            'samples_per_sec': samples_per_sec
        }
    
    print(f"æµ‹è¯• batch_size={batch_size:3d}... ", end="", flush=True)
    
    try:
        with ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(_core_test)
            try:
                result = future.result(timeout=timeout)
                print(f"âœ… {result['avg_time']:.2f}s/batch, {result['samples_per_sec']:4.0f} samples/s, {result['peak_memory_mb']:4.0f}MB")
                return result
            except TimeoutError:
                future.cancel()
                print(f"ğŸ•’ è¶…æ—¶ (>{timeout:.0f}s, å†…å­˜è…¾æŒª)")
                torch.cuda.empty_cache()
                return None
    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")
        return None

def find_optimal_batch_size(quiet: bool = False) -> int:
    """å¿«é€Ÿæ‰¾åˆ°æœ€ä¼˜batch size"""
    if not quiet:
        print("ğŸš€ å¿«é€ŸBatch Sizeä¼˜åŒ–å™¨")
        print("=" * 50)
    
    # å€™é€‰batch sizes
    candidates = [16, 32, 48, 64, 80, 96, 128, 160, 192, 256]
    
    results = []
    peak_throughput = 0
    declining_count = 0
    
    for batch_size in candidates:
        result = test_single_batch(batch_size, timeout=10.0)
        
        if result is not None:
            results.append(result)
            
            # æ—©åœæ£€æµ‹
            current_throughput = result['samples_per_sec']
            if current_throughput > peak_throughput:
                peak_throughput = current_throughput
                declining_count = 0
            else:
                declining_count += 1
                decline_ratio = (peak_throughput - current_throughput) / peak_throughput
                
                # æ€§èƒ½ä¸‹é™è¶…è¿‡10%æˆ–è¿ç»­2æ¬¡ä¸‹é™å°±åœæ­¢
                if decline_ratio > 0.10 or declining_count >= 2:
                    if not quiet:
                        print(f"ğŸ›‘ æ—©åœ: æ€§èƒ½ä¸‹é™{decline_ratio*100:.1f}%")
                    break
        else:
            # è¶…æ—¶æˆ–å¤±è´¥ï¼Œç«‹å³åœæ­¢
            if not quiet:
                print(f"ğŸ›‘ åœæ­¢: batch_size={batch_size}è¶…æ—¶æˆ–å¤±è´¥")
            break
        
        time.sleep(0.1)  # çŸ­æš‚ä¼‘æ¯
    
    # é€‰æ‹©æœ€ä¼˜
    if not results:
        if not quiet:
            print("âŒ æ²¡æœ‰æˆåŠŸç»“æœï¼Œä½¿ç”¨é»˜è®¤å€¼32")
        return 32
    
    # é€‰æ‹©ååé‡æœ€é«˜çš„
    best = max(results, key=lambda x: x['samples_per_sec'])
    optimal_batch_size = best['batch_size']
    
    if not quiet:
        print(f"\nğŸ“ˆ æµ‹è¯•ç»“æœ:")
        print(f"{'Batch':<6} {'æ—¶é—´':<8} {'é€Ÿåº¦':<10} {'å†…å­˜':<8}")
        print("-" * 40)
        
        for result in results:
            marker = "ğŸ†" if result == best else ""
            print(f"{result['batch_size']:<6} {result['avg_time']:<8.2f} "
                  f"{result['samples_per_sec']:<10.0f} {result['peak_memory_mb']:<8.0f} {marker}")
        
        print(f"\nğŸ† æœ€ä¼˜é€‰æ‹©: batch_size={optimal_batch_size}")
    
    return optimal_batch_size

if __name__ == "__main__":
    find_optimal_batch_size(quiet=False) 