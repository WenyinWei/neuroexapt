#!/usr/bin/env python3
"""
å…¨é¢çš„CUDAç¯å¢ƒè¯Šæ–­å·¥å…·
ç”¨äºæ’æŸ¥PyTorchåŸºç¡€ç®—å­CUDAé”™è¯¯çš„æ ¹æœ¬åŸå› 
"""

import os
import sys
import torch
import subprocess
import gc
from pathlib import Path

def print_section(title):
    print(f"\n{'='*60}")
    print(f"ğŸ” {title}")
    print(f"{'='*60}")

def print_subsection(title):
    print(f"\n{'â”€'*40}")
    print(f"ğŸ“‹ {title}")
    print(f"{'â”€'*40}")

def run_command(cmd, capture_output=True):
    """è¿è¡Œç³»ç»Ÿå‘½ä»¤å¹¶è¿”å›ç»“æœ"""
    try:
        result = subprocess.run(cmd, shell=True, capture_output=capture_output, text=True, timeout=30)
        return result.stdout.strip() if result.returncode == 0 else f"Error: {result.stderr.strip()}"
    except subprocess.TimeoutExpired:
        return "Error: Command timeout"
    except Exception as e:
        return f"Error: {str(e)}"

def check_system_info():
    """æ£€æŸ¥ç³»ç»ŸåŸºç¡€ä¿¡æ¯"""
    print_section("ç³»ç»Ÿç¯å¢ƒä¿¡æ¯")
    
    print(f"æ“ä½œç³»ç»Ÿ: {os.name}")
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print(f"Pythonè·¯å¾„: {sys.executable}")
    
    # æ£€æŸ¥WSL
    wsl_info = run_command("uname -r")
    if "microsoft" in wsl_info.lower():
        print(f"WSLç¯å¢ƒ: {wsl_info}")
        print("âš ï¸ æ£€æµ‹åˆ°WSLç¯å¢ƒï¼Œå¯èƒ½å­˜åœ¨GPUè®¿é—®é™åˆ¶")
    else:
        print(f"å†…æ ¸ç‰ˆæœ¬: {wsl_info}")

def check_cuda_installation():
    """æ£€æŸ¥CUDAå®‰è£…æƒ…å†µ"""
    print_section("CUDAå®‰è£…æ£€æŸ¥")
    
    # CUDAç‰ˆæœ¬
    cuda_version = run_command("nvcc --version")
    print(f"NVCCç‰ˆæœ¬:\n{cuda_version}")
    
    # NVIDIAé©±åŠ¨
    nvidia_smi = run_command("nvidia-smi")
    print(f"\nNVIDIA-SMIè¾“å‡º:\n{nvidia_smi}")
    
    # CUDAè¿è¡Œæ—¶åº“
    print(f"\nğŸ“¦ CUDAåº“æ£€æŸ¥:")
    for lib in ["/usr/local/cuda/lib64/libcudart.so", "/usr/lib/x86_64-linux-gnu/libcuda.so"]:
        if Path(lib).exists():
            print(f"âœ… {lib} - å­˜åœ¨")
        else:
            print(f"âŒ {lib} - ä¸å­˜åœ¨")

def check_pytorch_installation():
    """æ£€æŸ¥PyTorchå®‰è£…æƒ…å†µ"""
    print_section("PyTorchç¯å¢ƒæ£€æŸ¥")
    
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"PyTorchè·¯å¾„: {torch.__file__}")
    
    # CUDAæ”¯æŒ
    print(f"\nğŸ”§ PyTorch CUDAæ”¯æŒ:")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDAè®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
        print(f"å½“å‰CUDAè®¾å¤‡: {torch.cuda.current_device()}")
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
        
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            print(f"\nğŸ“± GPU {i}: {props.name}")
            print(f"   è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
            print(f"   æ€»å†…å­˜: {props.total_memory / 1024**3:.1f} GB")
            print(f"   å¤šå¤„ç†å™¨æ•°é‡: {props.multi_processor_count}")
    else:
        print("âŒ CUDAä¸å¯ç”¨")

def check_gpu_memory():
    """æ£€æŸ¥GPUå†…å­˜çŠ¶æ€"""
    if not torch.cuda.is_available():
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUå†…å­˜æ£€æŸ¥")
        return
        
    print_section("GPUå†…å­˜çŠ¶æ€")
    
    # æ¸…ç†ç¼“å­˜
    torch.cuda.empty_cache()
    gc.collect()
    
    for i in range(torch.cuda.device_count()):
        torch.cuda.set_device(i)
        total_memory = torch.cuda.get_device_properties(i).total_memory
        allocated = torch.cuda.memory_allocated(i)
        cached = torch.cuda.memory_reserved(i)
        
        print(f"\nğŸ“± GPU {i}:")
        print(f"   æ€»å†…å­˜: {total_memory / 1024**3:.2f} GB")
        print(f"   å·²åˆ†é…: {allocated / 1024**3:.2f} GB ({allocated/total_memory*100:.1f}%)")
        print(f"   å·²ç¼“å­˜: {cached / 1024**3:.2f} GB ({cached/total_memory*100:.1f}%)")
        print(f"   å¯ç”¨å†…å­˜: {(total_memory - allocated) / 1024**3:.2f} GB")

def test_basic_cuda_operations():
    """æµ‹è¯•åŸºç¡€CUDAæ“ä½œ"""
    print_section("åŸºç¡€CUDAæ“ä½œæµ‹è¯•")
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒCUDAæ“ä½œæµ‹è¯•")
        return False
    
    try:
        print("ğŸ”¸ æµ‹è¯•1: åˆ›å»ºCUDAå¼ é‡...")
        x = torch.randn(10, device='cuda')
        print(f"âœ… æˆåŠŸåˆ›å»ºCUDAå¼ é‡: {x.shape}")
        
        print("ğŸ”¸ æµ‹è¯•2: åŸºç¡€æ•°å­¦è¿ç®—...")
        y = x + 1
        print(f"âœ… åŠ æ³•è¿ç®—æˆåŠŸ: {y.mean().item():.4f}")
        
        print("ğŸ”¸ æµ‹è¯•3: çŸ©é˜µä¹˜æ³•...")
        A = torch.randn(100, 100, device='cuda')
        B = torch.randn(100, 100, device='cuda')
        C = torch.mm(A, B)
        print(f"âœ… çŸ©é˜µä¹˜æ³•æˆåŠŸ: {C.shape}")
        
        print("ğŸ”¸ æµ‹è¯•4: GPU-CPUæ•°æ®ä¼ è¾“...")
        cpu_data = C.cpu()
        gpu_data = cpu_data.cuda()
        print(f"âœ… æ•°æ®ä¼ è¾“æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ åŸºç¡€CUDAæ“ä½œå¤±è´¥: {e}")
        return False

def test_pytorch_basic_ops():
    """æµ‹è¯•PyTorchåŸºç¡€ç®—å­"""
    print_section("PyTorchåŸºç¡€ç®—å­æµ‹è¯•")
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPUæµ‹è¯•")
        device = 'cpu'
    else:
        device = 'cuda'
    
    tests = [
        ("ReLUæ¿€æ´»", lambda: torch.relu(torch.randn(10, 10, device=device))),
        ("Conv2Då·ç§¯", lambda: torch.nn.functional.conv2d(
            torch.randn(1, 3, 32, 32, device=device),
            torch.randn(16, 3, 3, 3, device=device)
        )),
        ("BatchNorm2D", lambda: torch.nn.functional.batch_norm(
            torch.randn(1, 16, 32, 32, device=device),
            torch.zeros(16, device=device),
            torch.ones(16, device=device),
            torch.zeros(16, device=device),
            torch.ones(16, device=device)
        )),
        ("MaxPool2D", lambda: torch.nn.functional.max_pool2d(
            torch.randn(1, 16, 32, 32, device=device), 3, stride=1, padding=1
        )),
        ("AdaptiveAvgPool2D", lambda: torch.nn.functional.adaptive_avg_pool2d(
            torch.randn(1, 16, 32, 32, device=device), (1, 1)
        ))
    ]
    
    success_count = 0
    for test_name, test_func in tests:
        try:
            print(f"ğŸ”¸ æµ‹è¯•: {test_name}...")
            result = test_func()
            print(f"âœ… {test_name} æˆåŠŸ: {result.shape}")
            success_count += 1
        except Exception as e:
            print(f"âŒ {test_name} å¤±è´¥: {e}")
    
    print(f"\nğŸ“Š åŸºç¡€ç®—å­æµ‹è¯•ç»“æœ: {success_count}/{len(tests)} é€šè¿‡")
    return success_count == len(tests)

def test_minimal_network():
    """æµ‹è¯•æœ€å°ç¥ç»ç½‘ç»œ"""
    print_section("æœ€å°ç¥ç»ç½‘ç»œæµ‹è¯•")
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPUæµ‹è¯•")
        device = 'cpu'
    else:
        device = 'cuda'
    
    try:
        print("ğŸ”¸ åˆ›å»ºæœ€å°ç½‘ç»œ...")
        
        class MinimalNet(torch.nn.Module):
            def __init__(self):
                super().__init__()
                self.conv = torch.nn.Conv2d(3, 16, 3, padding=1, bias=False)
                self.bn = torch.nn.BatchNorm2d(16)
                self.relu = torch.nn.ReLU()
                self.pool = torch.nn.AdaptiveAvgPool2d(1)
                self.fc = torch.nn.Linear(16, 10)
            
            def forward(self, x):
                x = self.conv(x)
                x = self.bn(x)
                x = self.relu(x)
                x = self.pool(x)
                x = x.view(x.size(0), -1)
                x = self.fc(x)
                return x
        
        model = MinimalNet().to(device)
        print(f"âœ… ç½‘ç»œåˆ›å»ºæˆåŠŸ")
        
        print("ğŸ”¸ å‰å‘ä¼ æ’­æµ‹è¯•...")
        input_data = torch.randn(2, 3, 32, 32, device=device)
        output = model(input_data)
        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ: {output.shape}")
        
        print("ğŸ”¸ åå‘ä¼ æ’­æµ‹è¯•...")
        loss = output.sum()
        loss.backward()
        print(f"âœ… åå‘ä¼ æ’­æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ æœ€å°ç½‘ç»œæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_with_cuda_dsa():
    """ä½¿ç”¨CUDA DSAé‡æ–°æµ‹è¯•å¤±è´¥çš„æ“ä½œ"""
    print_section("CUDA DSAè¯Šæ–­æµ‹è¯•")
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒDSAæµ‹è¯•")
        return
    
    # å¯ç”¨CUDA DSA
    os.environ['TORCH_USE_CUDA_DSA'] = '1'
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
    
    print("ğŸ”§ å·²å¯ç”¨ TORCH_USE_CUDA_DSA å’Œ CUDA_LAUNCH_BLOCKING")
    print("ğŸ”¸ é‡æ–°æµ‹è¯•ReLUæ“ä½œ...")
    
    try:
        # æµ‹è¯•æœ€ç®€å•çš„ReLUæ“ä½œ
        x = torch.randn(10, 10, device='cuda')
        y = torch.relu(x)
        print("âœ… ReLUæ“ä½œæˆåŠŸ")
        
        # æµ‹è¯•Conv2Dæ“ä½œ
        print("ğŸ”¸ é‡æ–°æµ‹è¯•Conv2Dæ“ä½œ...")
        input_tensor = torch.randn(1, 3, 32, 32, device='cuda')
        weight = torch.randn(16, 3, 3, 3, device='cuda')
        output = torch.nn.functional.conv2d(input_tensor, weight)
        print("âœ… Conv2Dæ“ä½œæˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ DSAè¯Šæ–­ä¸­å‘ç°é”™è¯¯: {e}")
        print("è¿™å¯èƒ½æä¾›äº†æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯")

def check_environment_variables():
    """æ£€æŸ¥ç›¸å…³ç¯å¢ƒå˜é‡"""
    print_section("ç¯å¢ƒå˜é‡æ£€æŸ¥")
    
    important_vars = [
        'CUDA_VISIBLE_DEVICES',
        'CUDA_DEVICE_ORDER', 
        'TORCH_USE_CUDA_DSA',
        'CUDA_LAUNCH_BLOCKING',
        'PYTHONPATH',
        'LD_LIBRARY_PATH',
        'CUDA_HOME',
        'CUDA_PATH'
    ]
    
    for var in important_vars:
        value = os.environ.get(var)
        if value:
            print(f"âœ… {var} = {value}")
        else:
            print(f"âšª {var} = æœªè®¾ç½®")

def generate_recommendations():
    """ç”Ÿæˆä¿®å¤å»ºè®®"""
    print_section("ä¿®å¤å»ºè®®")
    
    print("åŸºäºè¯Šæ–­ç»“æœï¼Œå»ºè®®æŒ‰ä»¥ä¸‹é¡ºåºå°è¯•ä¿®å¤:")
    print()
    print("ğŸ”§ ç«‹å³å°è¯•çš„ä¿®å¤æ–¹æ¡ˆ:")
    print("1. é‡å¯Pythonè¿›ç¨‹å’Œæ¸…ç†GPUå†…å­˜")
    print("2. è®¾ç½®ç¯å¢ƒå˜é‡åé‡è¯•:")
    print("   export TORCH_USE_CUDA_DSA=1")
    print("   export CUDA_LAUNCH_BLOCKING=1")
    print("3. é™ä½batch sizeåˆ°æœ€å°(å¦‚16æˆ–32)")
    print("4. ä½¿ç”¨CPUæ¨¡å¼éªŒè¯ä»£ç é€»è¾‘")
    print()
    print("ğŸ”§ ä¸­çº§ä¿®å¤æ–¹æ¡ˆ:")
    print("1. é‡æ–°å®‰è£…PyTorch:")
    print("   pip uninstall torch torchvision")
    print("   pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118")
    print("2. æ›´æ–°NVIDIAé©±åŠ¨")
    print("3. æ£€æŸ¥WSL2çš„GPUæ”¯æŒé…ç½®")
    print()
    print("ğŸ”§ é«˜çº§ä¿®å¤æ–¹æ¡ˆ:")
    print("1. é‡è£…CUDA Toolkit")
    print("2. æ£€æŸ¥GPUç¡¬ä»¶çŠ¶æ€")
    print("3. å°è¯•ä½¿ç”¨Dockerå®¹å™¨éš”ç¦»ç¯å¢ƒ")

def main():
    """ä¸»è¯Šæ–­æµç¨‹"""
    print("ğŸ¥ CUDAç¯å¢ƒå…¨é¢è¯Šæ–­å·¥å…·")
    print("=" * 60)
    
    # åŸºç¡€ä¿¡æ¯æ£€æŸ¥
    check_system_info()
    check_environment_variables()
    check_cuda_installation()
    check_pytorch_installation()
    
    # GPUå’Œå†…å­˜æ£€æŸ¥
    check_gpu_memory()
    
    # åŠŸèƒ½æµ‹è¯•
    cuda_basic_ok = test_basic_cuda_operations()
    pytorch_ops_ok = test_pytorch_basic_ops()
    network_ok = test_minimal_network()
    
    # å¦‚æœåŸºç¡€æ“ä½œå¤±è´¥ï¼Œå°è¯•DSAè¯Šæ–­
    if not (cuda_basic_ok and pytorch_ops_ok):
        test_with_cuda_dsa()
    
    # ç”Ÿæˆå»ºè®®
    generate_recommendations()
    
    # æ€»ç»“
    print_section("è¯Šæ–­æ€»ç»“")
    print(f"CUDAåŸºç¡€æ“ä½œ: {'âœ… æ­£å¸¸' if cuda_basic_ok else 'âŒ å¼‚å¸¸'}")
    print(f"PyTorchç®—å­: {'âœ… æ­£å¸¸' if pytorch_ops_ok else 'âŒ å¼‚å¸¸'}")
    print(f"ç¥ç»ç½‘ç»œ: {'âœ… æ­£å¸¸' if network_ok else 'âŒ å¼‚å¸¸'}")
    
    if cuda_basic_ok and pytorch_ops_ok and network_ok:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼é—®é¢˜å¯èƒ½åœ¨äºç‰¹å®šçš„æ¨¡å‹é…ç½®æˆ–æ•°æ®")
        print("å»ºè®®æ£€æŸ¥:")
        print("- æ¨¡å‹å¤æ‚åº¦å’Œå†…å­˜ä½¿ç”¨")
        print("- ç‰¹å®šçš„ç½‘ç»œæ¶æ„ç»„ä»¶")
        print("- æ•°æ®åŠ è½½å’Œé¢„å¤„ç†æ­¥éª¤")
    else:
        print("\nâš ï¸ å‘ç°ç¯å¢ƒé—®é¢˜ï¼Œéœ€è¦ä¿®å¤CUDAç¯å¢ƒåå†è¿è¡ŒNeuroExapt")

if __name__ == "__main__":
    main() 