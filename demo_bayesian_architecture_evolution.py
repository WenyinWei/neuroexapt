"""
è´å¶æ–¯æ¶æ„æ¼”åŒ–æ¼”ç¤º

è¿™ä¸ªæ¼”ç¤ºå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨è´å¶æ–¯æ¨æ–­å’Œä¿¡æ¯è®ºåŸç†æ¥æŒ‡å¯¼ç¥ç»ç½‘ç»œæ¶æ„çš„æ¼”åŒ–ã€‚
æ¯æ¬¡è®­ç»ƒ/æ¶æ„è°ƒæ•´éƒ½ä¼šæ›´æ–°æœ€ä¼˜æ¶æ„ä¼°è®¡çš„åéªŒåˆ†å¸ƒã€‚

ä¸»è¦ç‰¹ç‚¹ï¼š
1. åŸºäºè´å¶æ–¯æ¨æ–­çš„æ¶æ„å†³ç­–
2. ä¿¡æ¯è®ºæŒ‡æ ‡ï¼ˆäº’ä¿¡æ¯ã€ç†µï¼‰æŒ‡å¯¼æ¼”åŒ–
3. åéªŒåˆ†å¸ƒçš„åŠ¨æ€æ›´æ–°
4. ç»¼åˆçš„æ¶æ„å»ºè®®ç³»ç»Ÿ
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import numpy as np
import sys
import os
import time
from typing import Dict, List

# Add the current directory to the path to import neuroexapt
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from neuroexapt.core.bayesian_architecture_advisor import (
    BayesianArchitectureAdvisor,
    ArchitectureAction,
    ArchitectureState,
    ArchitectureEvidence
)
from neuroexapt.core.bayesian_depth_operators import (
    BayesianDepthOperator,
    get_bayesian_depth_operators
)
from neuroexapt.core.radical_evolution import RadicalEvolutionEngine


class BayesianEvolvableCNN(nn.Module):
    """
    ä¸ºè´å¶æ–¯æ¶æ„æ¼”åŒ–ä¸“é—¨è®¾è®¡çš„CNNã€‚
    
    è¿™ä¸ªç½‘ç»œä¼šè·Ÿè¸ªè‡ªå·±çš„æ¶æ„æ¼”åŒ–å†å²ï¼Œå¹¶ä¸è´å¶æ–¯é¡¾é—®é›†æˆã€‚
    """
    
    def __init__(self, num_classes=10, initial_depth=8, initial_channels=32):
        super(BayesianEvolvableCNN, self).__init__()
        
        # è®°å½•æ¶æ„æ¼”åŒ–å†å²
        self.architecture_history = []
        self.performance_history = []
        self.bayesian_insights = []
        
        # æ„å»ºåˆå§‹æ¶æ„
        self.features = nn.Sequential(
            # ç¬¬ä¸€å±‚ï¼šè¾“å…¥å¤„ç†
            nn.Conv2d(3, initial_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(initial_channels),
            nn.ReLU(inplace=True),
            
            # ç¬¬äºŒå±‚ï¼šç‰¹å¾æå–
            nn.Conv2d(initial_channels, initial_channels * 2, kernel_size=3, padding=1),
            nn.BatchNorm2d(initial_channels * 2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # ç¬¬ä¸‰å±‚ï¼šæ·±åº¦ç‰¹å¾
            nn.Conv2d(initial_channels * 2, initial_channels * 4, kernel_size=3, padding=1),
            nn.BatchNorm2d(initial_channels * 4),
            nn.ReLU(inplace=True),
            
            # ç¬¬å››å±‚ï¼šé«˜çº§ç‰¹å¾
            nn.Conv2d(initial_channels * 4, initial_channels * 8, kernel_size=3, padding=1),
            nn.BatchNorm2d(initial_channels * 8),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # å…¨å±€å¹³å‡æ± åŒ–
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(initial_channels * 8, initial_channels * 4),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(initial_channels * 4, num_classes)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
        
        # è®°å½•åˆå§‹çŠ¶æ€
        self._record_architecture_state("initial")
    
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)
    
    def _record_architecture_state(self, event_type: str):
        """è®°å½•æ¶æ„çŠ¶æ€"""
        
        # è®¡ç®—æ¶æ„ç»Ÿè®¡ä¿¡æ¯
        total_params = sum(p.numel() for p in self.parameters())
        conv_layers = sum(1 for m in self.modules() if isinstance(m, nn.Conv2d))
        linear_layers = sum(1 for m in self.modules() if isinstance(m, nn.Linear))
        
        # è®¡ç®—å¹³å‡é€šé“æ•°
        conv_channels = [m.out_channels for m in self.modules() if isinstance(m, nn.Conv2d)]
        avg_channels = np.mean(conv_channels) if conv_channels else 0
        
        state = {
            'event_type': event_type,
            'timestamp': time.time(),
            'total_params': total_params,
            'conv_layers': conv_layers,
            'linear_layers': linear_layers,
            'avg_channels': avg_channels,
            'max_channels': max(conv_channels) if conv_channels else 0,
            'architecture_depth': conv_layers + linear_layers
        }
        
        self.architecture_history.append(state)
        
        print(f"ğŸ“Š Architecture state recorded ({event_type}):")
        print(f"   Total parameters: {total_params:,}")
        print(f"   Architecture depth: {state['architecture_depth']}")
        print(f"   Average channels: {avg_channels:.1f}")
    
    def update_performance_record(self, performance: Dict[str, float]):
        """æ›´æ–°æ€§èƒ½è®°å½•"""
        self.performance_history.append({
            'timestamp': time.time(),
            'performance': performance
        })
    
    def get_architecture_summary(self) -> Dict[str, any]:
        """è·å–æ¶æ„æ‘˜è¦ä¿¡æ¯"""
        
        if not self.architecture_history:
            return {}
        
        current_state = self.architecture_history[-1]
        initial_state = self.architecture_history[0]
        
        return {
            'evolution_steps': len(self.architecture_history) - 1,
            'parameter_growth': current_state['total_params'] - initial_state['total_params'],
            'depth_growth': current_state['architecture_depth'] - initial_state['architecture_depth'],
            'channel_growth': current_state['avg_channels'] - initial_state['avg_channels'],
            'current_state': current_state,
            'bayesian_insights': self.bayesian_insights[-5:] if self.bayesian_insights else []
        }


def load_demo_data(batch_size=32, num_samples=2000):
    """åŠ è½½æ¼”ç¤ºæ•°æ®"""
    
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
    
    # åˆ›å»ºå°æ•°æ®é›†ç”¨äºå¿«é€Ÿæ¼”ç¤º
    train_indices = torch.randperm(len(trainset))[:num_samples].tolist()
    test_indices = torch.randperm(len(testset))[:num_samples//4].tolist()
    
    train_subset = torch.utils.data.Subset(trainset, train_indices)
    test_subset = torch.utils.data.Subset(testset, test_indices)
    
    trainloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)
    testloader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)
    
    return trainloader, testloader


def evaluate_model(model, testloader, device):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    model.eval()
    correct = 0
    total = 0
    total_loss = 0
    criterion = nn.CrossEntropyLoss()
    
    with torch.no_grad():
        for data, targets in testloader:
            data, targets = data.to(device), targets.to(device)
            outputs = model(data)
            loss = criterion(outputs, targets)
            total_loss += loss.item()
            
            _, predicted = torch.max(outputs.data, 1)
            total += targets.size(0)
            correct += (predicted == targets).sum().item()
    
    accuracy = 100 * correct / total if total > 0 else 0.0
    avg_loss = total_loss / len(testloader) if len(testloader) > 0 else 0.0
    
    model.train()
    return accuracy, avg_loss


def demonstrate_bayesian_evolution():
    """
    æ¼”ç¤ºè´å¶æ–¯æ¶æ„æ¼”åŒ–è¿‡ç¨‹
    """
    
    print("ğŸ§  è´å¶æ–¯æ¶æ„æ¼”åŒ–æ¼”ç¤º")
    print("=" * 60)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“Š åŠ è½½æ¼”ç¤ºæ•°æ®...")
    trainloader, testloader = load_demo_data(batch_size=32, num_samples=2000)
    print(f"è®­ç»ƒæ‰¹æ¬¡: {len(trainloader)}")
    print(f"æµ‹è¯•æ‰¹æ¬¡: {len(testloader)}")
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ—ï¸ åˆ›å»ºè´å¶æ–¯è¿›åŒ–CNN...")
    model = BayesianEvolvableCNN(num_classes=10, initial_depth=8, initial_channels=32).to(device)
    
    # åˆå§‹è¯„ä¼°
    initial_accuracy, initial_loss = evaluate_model(model, testloader, device)
    print(f"åˆå§‹æ€§èƒ½: {initial_accuracy:.2f}% accuracy, {initial_loss:.3f} loss")
    
    # åˆ›å»ºè´å¶æ–¯æ·±åº¦æ“ä½œå™¨
    print("\nğŸ§  åˆå§‹åŒ–è´å¶æ–¯æ·±åº¦æ“ä½œå™¨...")
    bayesian_operators = get_bayesian_depth_operators(
        initial_depth_prior=(10.0, 5.0),  # å…ˆéªŒï¼šæœ€ä¼˜æ·±åº¦çº¦10å±‚ï¼Œä¸ç¡®å®šæ€§5å±‚
        initial_channel_prior=(64.0, 32.0),  # å…ˆéªŒï¼šæœ€ä¼˜é€šé“æ•°çº¦64ï¼Œä¸ç¡®å®šæ€§32
        information_threshold=0.8,
        entropy_threshold=0.6
    )
    
    # åˆ›å»ºæ¼”åŒ–å¼•æ“
    evolution_engine = RadicalEvolutionEngine(
        model=model,
        operators=bayesian_operators,
        input_shape=(3, 32, 32),
        evolution_probability=1.0,  # æ¯æ¬¡éƒ½å°è¯•æ¼”åŒ–
        max_mutations_per_epoch=1,
        enable_validation=True
    )
    
    # è®­ç»ƒå’Œæ¼”åŒ–å¾ªç¯
    print("\nğŸ”„ å¼€å§‹è´å¶æ–¯æ¶æ„æ¼”åŒ–...")
    print("=" * 60)
    
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    num_evolution_cycles = 5
    epochs_per_cycle = 3
    
    for cycle in range(num_evolution_cycles):
        print(f"\nğŸ”„ æ¼”åŒ–å‘¨æœŸ {cycle + 1}/{num_evolution_cycles}")
        print("-" * 40)
        
        # è®­ç»ƒå‡ ä¸ªepoch
        for epoch in range(epochs_per_cycle):
            print(f"\nğŸ“ˆ è®­ç»ƒ Epoch {epoch + 1}/{epochs_per_cycle}")
            
            # è®­ç»ƒå¾ªç¯
            model.train()
            epoch_loss = 0.0
            correct = 0
            total = 0
            
            for batch_idx, (data, targets) in enumerate(trainloader):
                data, targets = data.to(device), targets.to(device)
                
                optimizer.zero_grad()
                outputs = model(data)
                loss = criterion(outputs, targets)
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += targets.size(0)
                correct += (predicted == targets).sum().item()
                
                if batch_idx % 20 == 0:
                    print(f"   Batch {batch_idx:3d}: Loss {loss.item():.3f}")
            
            train_accuracy = 100 * correct / total
            avg_loss = epoch_loss / len(trainloader)
            
            # è¯„ä¼°
            test_accuracy, test_loss = evaluate_model(model, testloader, device)
            
            print(f"   è®­ç»ƒ: {train_accuracy:.2f}% | æµ‹è¯•: {test_accuracy:.2f}% | æŸå¤±: {avg_loss:.3f}")
            
            # æ›´æ–°æ¨¡å‹çš„æ€§èƒ½è®°å½•
            performance = {
                'train_accuracy': train_accuracy,
                'test_accuracy': test_accuracy,
                'train_loss': avg_loss,
                'test_loss': test_loss,
                'epoch': epoch + 1,
                'cycle': cycle + 1
            }
            model.update_performance_record(performance)
        
        # è´å¶æ–¯æ¶æ„æ¼”åŒ–
        print(f"\nğŸ§  è´å¶æ–¯æ¶æ„åˆ†æå’Œæ¼”åŒ–...")
        
        # è·å–å½“å‰æ€§èƒ½æŒ‡æ ‡
        current_performance = {
            'accuracy': test_accuracy,
            'loss': test_loss,
            'convergence_speed': 0.5,  # ç®€åŒ–
            'overfitting': max(0, train_accuracy - test_accuracy) / 100.0,
            'gradient_flow': 0.7,  # ç®€åŒ–
            'efficiency': 0.6,  # ç®€åŒ–
            'epoch': cycle * epochs_per_cycle + epochs_per_cycle
        }
        
        # å°è¯•æ¼”åŒ–
        evolved_model, evolution_action = evolution_engine.evolve(
            epoch=cycle + 1,
            dataloader=trainloader,
            criterion=criterion,
            performance_metrics=current_performance
        )
        
        if evolution_action and evolved_model:
            print(f"   âœ… æ¼”åŒ–æˆåŠŸ: {evolution_action}")
            
            # æ›´æ–°æ¨¡å‹
            model = evolved_model
            
            # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨
            optimizer = optim.Adam(model.parameters(), lr=0.001)
            
            # è¯„ä¼°æ¼”åŒ–åçš„æ€§èƒ½
            post_evolution_accuracy, post_evolution_loss = evaluate_model(model, testloader, device)
            
            print(f"   ğŸ“Š æ¼”åŒ–åæ€§èƒ½: {post_evolution_accuracy:.2f}% accuracy")
            print(f"   ğŸ“ˆ æ€§èƒ½å˜åŒ–: {post_evolution_accuracy - test_accuracy:+.2f}%")
            
            # è®°å½•æ¶æ„çŠ¶æ€
            model._record_architecture_state(f"evolution_cycle_{cycle + 1}")
            
            # è·å–è´å¶æ–¯é¡¾é—®çš„æ´å¯Ÿ
            if hasattr(evolution_engine.operators[0], 'get_advisor_insights'):
                insights = evolution_engine.operators[0].get_advisor_insights()
                model.bayesian_insights.append(insights)
                
                print(f"   ğŸ§  è´å¶æ–¯æ´å¯Ÿ:")
                print(f"      æœ€ä¼˜æ·±åº¦ä¼°è®¡: {insights['optimal_depth_estimate']:.1f} Â± {insights['depth_uncertainty']:.1f}")
                print(f"      æœ€ä¼˜é€šé“ä¼°è®¡: {insights['optimal_channel_estimate']:.1f} Â± {insights['channel_uncertainty']:.1f}")
                print(f"      å†³ç­–æ¬¡æ•°: {insights['decisions_made']}")
                print(f"      è¿‘æœŸè¡ŒåŠ¨: {insights['recent_actions']}")
            
            # æ›´æ–°æ“ä½œå™¨çš„ç»“æœ
            if hasattr(evolution_engine.operators[0], 'update_with_outcome'):
                outcome_performance = {
                    'accuracy': post_evolution_accuracy,
                    'loss': post_evolution_loss,
                    'convergence_speed': 0.5,
                    'overfitting': 0.0,
                    'gradient_flow': 0.7,
                    'efficiency': 0.6,
                    'epoch': cycle + 1
                }
                evolution_engine.operators[0].update_with_outcome(model, outcome_performance)
        
        else:
            print(f"   â„¹ï¸ æ­¤å‘¨æœŸæœªæ‰§è¡Œæ¶æ„æ¼”åŒ–")
    
    # æœ€ç»ˆç»“æœ
    print("\nğŸ“‹ æœ€ç»ˆç»“æœ")
    print("=" * 60)
    
    final_accuracy, final_loss = evaluate_model(model, testloader, device)
    architecture_summary = model.get_architecture_summary()
    
    print(f"æœ€ç»ˆæ€§èƒ½:")
    print(f"  å‡†ç¡®ç‡: {final_accuracy:.2f}% (åˆå§‹: {initial_accuracy:.2f}%)")
    print(f"  æ”¹è¿›: {final_accuracy - initial_accuracy:+.2f}%")
    print(f"  æŸå¤±: {final_loss:.3f}")
    
    print(f"\næ¶æ„æ¼”åŒ–æ‘˜è¦:")
    print(f"  æ¼”åŒ–æ­¥æ•°: {architecture_summary['evolution_steps']}")
    print(f"  å‚æ•°å¢é•¿: {architecture_summary['parameter_growth']:+,}")
    print(f"  æ·±åº¦å¢é•¿: {architecture_summary['depth_growth']:+.0f}")
    print(f"  é€šé“å¢é•¿: {architecture_summary['channel_growth']:+.1f}")
    
    # æ¼”åŒ–å¼•æ“ç»Ÿè®¡
    engine_stats = evolution_engine.get_evolution_stats()
    print(f"\næ¼”åŒ–å¼•æ“ç»Ÿè®¡:")
    print(f"  æ€»çªå˜æ•°: {engine_stats['total_mutations']}")
    print(f"  æˆåŠŸçªå˜æ•°: {engine_stats['successful_mutations']}")
    print(f"  æˆåŠŸç‡: {engine_stats['overall_success_rate']:.2%}")
    
    # è´å¶æ–¯æ´å¯Ÿå†å²
    if model.bayesian_insights:
        print(f"\nè´å¶æ–¯æ´å¯Ÿæ¼”åŒ–:")
        for i, insight in enumerate(model.bayesian_insights):
            print(f"  å‘¨æœŸ {i+1}: æ·±åº¦={insight['optimal_depth_estimate']:.1f}Â±{insight['depth_uncertainty']:.1f}, "
                  f"é€šé“={insight['optimal_channel_estimate']:.1f}Â±{insight['channel_uncertainty']:.1f}")
    
    print(f"\nâœ… è´å¶æ–¯æ¶æ„æ¼”åŒ–æ¼”ç¤ºå®Œæˆ!")
    
    return model, final_accuracy, architecture_summary


if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(42)
    
    try:
        model, accuracy, summary = demonstrate_bayesian_evolution()
        print(f"\nğŸ‰ æ¼”ç¤ºæˆåŠŸå®Œæˆ!")
        print(f"æœ€ç»ˆå‡†ç¡®ç‡: {accuracy:.2f}%")
        print(f"æ¶æ„æ¼”åŒ–æ­¥æ•°: {summary['evolution_steps']}")
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc() 