#!/usr/bin/env python3
"""
NeuroExapt - å‡†ç¡®åº¦åˆ†æå’Œè‡ªé€‚åº”æ¶æ„å‡çº§è§£å†³æ–¹æ¡ˆ

è§£å†³ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼š
1. è®­ç»ƒå‡†ç¡®åº¦ä½äºéªŒè¯å‡†ç¡®åº¦çš„å¼‚å¸¸æƒ…å†µ
2. éªŒè¯å‡†ç¡®åº¦åœåœ¨82%ï¼Œéœ€è¦æ¶æ„æœ¬è´¨å‡çº§
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import sys
import os
import numpy as np
from collections import defaultdict

# Add neuroexapt to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from neuroexapt.trainer import Trainer, train_with_neuroexapt


class AdvancedEvolutionCNN(nn.Module):
    """è‡ªé€‚åº”æ¶æ„æ¼”åŒ–çš„é«˜çº§CNN - è®¾è®¡ç”¨äºçªç ´82%ç“¶é¢ˆ"""
    
    def __init__(self, num_classes=10, evolution_stage=0):
        super().__init__()
        self.evolution_stage = evolution_stage
        
        # åŸºç¡€æ¶æ„ï¼ˆStage 0ï¼‰
        self.features = nn.ModuleList([
            # ç¬¬ä¸€å±‚ç»„ - æ›´å¤§çš„åˆå§‹æ„Ÿå—é‡
            nn.Sequential(
                nn.Conv2d(3, 64, 5, padding=2),  # 5x5 instead of 3x3
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.Conv2d(64, 64, 3, padding=1),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),
                nn.Dropout2d(0.1)  # è¾ƒä½çš„dropout
            ),
            
            # ç¬¬äºŒå±‚ç»„ - æ®‹å·®è¿æ¥
            nn.Sequential(
                nn.Conv2d(64, 128, 3, padding=1),
                nn.BatchNorm2d(128),
                nn.ReLU(inplace=True),
                nn.Conv2d(128, 128, 3, padding=1),
                nn.BatchNorm2d(128),
            ),
            
            # ç¬¬ä¸‰å±‚ç»„ - æ³¨æ„åŠ›æœºåˆ¶
            nn.Sequential(
                nn.Conv2d(128, 256, 3, padding=1),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True),
                nn.Conv2d(256, 256, 3, padding=1),
                nn.BatchNorm2d(256),
            ),
            
            # ç¬¬å››å±‚ç»„ - æ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼ˆå¦‚æœæ¼”åŒ–åˆ°Stage 1+ï¼‰
            nn.Sequential(
                nn.Conv2d(256, 512, 3, padding=1),
                nn.BatchNorm2d(512),
                nn.ReLU(inplace=True),
                nn.AdaptiveAvgPool2d((4, 4))
            ) if evolution_stage >= 1 else nn.Identity()
        ])
        
        # æ®‹å·®è¿æ¥æ¨¡å—
        self.residual_conv1 = nn.Conv2d(64, 128, 1)
        self.residual_conv2 = nn.Conv2d(128, 256, 1)
        
        # æ³¨æ„åŠ›æœºåˆ¶ï¼ˆStage 1+ï¼‰
        if evolution_stage >= 1:
            self.attention = nn.Sequential(
                nn.AdaptiveAvgPool2d(1),
                nn.Flatten(),
                nn.Linear(256, 64),
                nn.ReLU(),
                nn.Linear(64, 256),
                nn.Sigmoid()
            )
        
        # è‡ªé€‚åº”åˆ†ç±»å™¨
        if evolution_stage >= 2:
            # æ›´å¤æ‚çš„åˆ†ç±»å™¨
            self.classifier = nn.Sequential(
                nn.Dropout(0.3),  # é™ä½dropout
                nn.Linear(512 * 16, 1024),
                nn.ReLU(inplace=True),
                nn.Dropout(0.2),
                nn.Linear(1024, 512),
                nn.ReLU(inplace=True),
                nn.Dropout(0.1),
                nn.Linear(512, 256),
                nn.ReLU(inplace=True),
                nn.Linear(256, num_classes)
            )
        else:
            # åŸºç¡€åˆ†ç±»å™¨
            self.classifier = nn.Sequential(
                nn.Dropout(0.3),
                nn.Linear(256 * 16, 512),
                nn.ReLU(inplace=True),
                nn.Dropout(0.2),
                nn.Linear(512, 256),
                nn.ReLU(inplace=True),
                nn.Dropout(0.1),
                nn.Linear(256, num_classes)
            )
    
    def forward(self, x):
        # Layer 1
        x1 = self.features[0](x)
        
        # Layer 2 with residual
        x2 = self.features[1](x1)
        if x2.size(1) != x1.size(1):
            x1 = F.avg_pool2d(x1, 2)
            x1 = self.residual_conv1(x1)
        x2 = F.relu(x2 + x1)
        x2 = F.max_pool2d(x2, 2)
        x2 = F.dropout2d(x2, 0.15, training=self.training)
        
        # Layer 3 with residual
        x3 = self.features[2](x2)
        if x3.size(1) != x2.size(1):
            x2 = F.avg_pool2d(x2, 2)
            x2 = self.residual_conv2(x2)
        x3 = F.relu(x3 + x2)
        x3 = F.max_pool2d(x3, 2)
        x3 = F.dropout2d(x3, 0.2, training=self.training)
        
        # Layer 4 (if evolved)
        if self.evolution_stage >= 1:
            x4 = self.features[3](x3)
            
            # Apply attention
            if hasattr(self, 'attention'):
                attention_weights = self.attention(x3)
                attention_weights = attention_weights.view(-1, 256, 1, 1)
                x3 = x3 * attention_weights
            
            x = x4
        else:
            x = F.adaptive_avg_pool2d(x3, (4, 4))
        
        # Classifier
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x


def create_enhanced_cifar10_dataloaders():
    """åˆ›å»ºå¢å¼ºçš„CIFAR-10æ•°æ®åŠ è½½å™¨ - è§£å†³è®­ç»ƒå‡†ç¡®åº¦ä½çš„é—®é¢˜"""
    print("åˆ›å»ºå¢å¼ºçš„CIFAR-10æ•°æ®é›†...")
    
    # è®­ç»ƒæ—¶ä½¿ç”¨é€‚åº¦çš„æ•°æ®å¢å¼ºï¼ˆä¸è¦å¤ªæ¿€è¿›ï¼‰
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        transforms.RandomErasing(p=0.1, scale=(0.02, 0.33), ratio=(0.3, 3.3))
    ])
    
    # éªŒè¯æ—¶ä¸ä½¿ç”¨æ•°æ®å¢å¼º
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    
    # åŠ è½½CIFAR-10æ•°æ®é›†
    train_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform_train
    )
    
    val_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=False, download=True, transform=transform_test
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - ä½¿ç”¨æ›´å¤§çš„batch sizeæé«˜ç¨³å®šæ€§
    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)
    
    print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ: {len(train_dataset)} è®­ç»ƒæ ·æœ¬, {len(val_dataset)} éªŒè¯æ ·æœ¬")
    
    return train_loader, val_loader


class ArchitectureEvolutionStrategy:
    """è‡ªé€‚åº”æ¶æ„æ¼”åŒ–ç­–ç•¥ - çªç ´82%ç“¶é¢ˆ"""
    
    def __init__(self, patience=10, min_improvement=0.5):
        self.patience = patience
        self.min_improvement = min_improvement
        self.best_val_acc = 0.0
        self.no_improvement_count = 0
        self.evolution_history = []
        
    def should_evolve(self, current_val_acc, epoch):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿›è¡Œæ¶æ„æ¼”åŒ–"""
        if current_val_acc > self.best_val_acc + self.min_improvement:
            self.best_val_acc = current_val_acc
            self.no_improvement_count = 0
            return False
        else:
            self.no_improvement_count += 1
            
        # å¦‚æœè¿ç»­å¤šä¸ªepochæ²¡æœ‰æ”¹å–„ï¼Œå»ºè®®æ¼”åŒ–
        if self.no_improvement_count >= self.patience:
            print(f"ğŸ”„ æ£€æµ‹åˆ°æ€§èƒ½å¹³å°æœŸ (è¿ç»­{self.patience}ä¸ªepochæ— æ”¹å–„)")
            print(f"ğŸ“Š å½“å‰æœ€ä½³éªŒè¯å‡†ç¡®åº¦: {self.best_val_acc:.2f}%")
            
            # æ ¹æ®å‡†ç¡®åº¦æ°´å¹³å»ºè®®ä¸åŒçš„æ¼”åŒ–ç­–ç•¥
            if self.best_val_acc < 75:
                return "basic_optimization"
            elif self.best_val_acc < 82:
                return "add_attention"
            elif self.best_val_acc < 87:
                return "add_depth"
            else:
                return "advanced_techniques"
        
        return False
    
    def evolve_architecture(self, model, strategy):
        """æ ¹æ®ç­–ç•¥æ¼”åŒ–æ¶æ„"""
        print(f"ğŸš€ å¼€å§‹æ¶æ„æ¼”åŒ–: {strategy}")
        
        if strategy == "basic_optimization":
            # åŸºç¡€ä¼˜åŒ–ï¼šè°ƒæ•´dropoutå’Œå­¦ä¹ ç‡
            for module in model.modules():
                if isinstance(module, nn.Dropout):
                    module.p = max(0.1, module.p - 0.1)
                elif isinstance(module, nn.Dropout2d):
                    module.p = max(0.1, module.p - 0.05)
            
            return model, "é™ä½dropoutç‡ä»¥æé«˜è®­ç»ƒç¨³å®šæ€§"
        
        elif strategy == "add_attention":
            # å‡çº§åˆ°Stage 1ï¼šæ·»åŠ æ³¨æ„åŠ›æœºåˆ¶
            new_model = AdvancedEvolutionCNN(num_classes=10, evolution_stage=1)
            new_model.load_state_dict(model.state_dict(), strict=False)
            
            return new_model, "æ·»åŠ æ³¨æ„åŠ›æœºåˆ¶å¢å¼ºç‰¹å¾è¡¨ç¤º"
        
        elif strategy == "add_depth":
            # å‡çº§åˆ°Stage 2ï¼šå¢åŠ æ·±åº¦å’Œå¤æ‚åº¦
            new_model = AdvancedEvolutionCNN(num_classes=10, evolution_stage=2)
            new_model.load_state_dict(model.state_dict(), strict=False)
            
            return new_model, "å¢åŠ ç½‘ç»œæ·±åº¦å’Œåˆ†ç±»å™¨å¤æ‚åº¦"
        
        elif strategy == "advanced_techniques":
            # é«˜çº§æŠ€å·§ï¼šçŸ¥è¯†è’¸é¦ã€æ ‡ç­¾å¹³æ»‘ç­‰
            return model, "åº”ç”¨é«˜çº§è®­ç»ƒæŠ€å·§"
        
        return model, "æœªçŸ¥æ¼”åŒ–ç­–ç•¥"


def analyze_accuracy_issue(train_acc, val_acc, epoch):
    """åˆ†æå‡†ç¡®åº¦å¼‚å¸¸æƒ…å†µ"""
    print(f"\nğŸ” å‡†ç¡®åº¦åˆ†æ (Epoch {epoch}):")
    print(f"  è®­ç»ƒå‡†ç¡®åº¦: {train_acc:.2f}%")
    print(f"  éªŒè¯å‡†ç¡®åº¦: {val_acc:.2f}%")
    print(f"  å·®å¼‚: {val_acc - train_acc:.2f}%")
    
    if val_acc > train_acc:
        print("  âœ… éªŒè¯å‡†ç¡®åº¦é«˜äºè®­ç»ƒå‡†ç¡®åº¦ - è¿™æ˜¯æ­£å¸¸ç°è±¡")
        print("  åŸå› : è®­ç»ƒæ—¶ä½¿ç”¨dropoutå’Œæ•°æ®å¢å¼ºï¼ŒéªŒè¯æ—¶ä¸ä½¿ç”¨")
        return "normal"
    elif train_acc - val_acc > 10:
        print("  âš ï¸  è¿‡æ‹Ÿåˆæ£€æµ‹ - è®­ç»ƒå‡†ç¡®åº¦æ˜æ˜¾é«˜äºéªŒè¯å‡†ç¡®åº¦")
        return "overfitting"
    else:
        print("  âœ… è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®åº¦å·®å¼‚åˆç†")
        return "balanced"


def main():
    print("ğŸ¯ NeuroExapt å‡†ç¡®åº¦åˆ†æå’Œè‡ªé€‚åº”æ¶æ„å‡çº§è§£å†³æ–¹æ¡ˆ")
    print("="*70)
    
    # è®¾å¤‡è®¾ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸  è®¾å¤‡: {device}")
    
    # åˆ›å»ºå¢å¼ºçš„æ•°æ®åŠ è½½å™¨
    train_loader, val_loader = create_enhanced_cifar10_dataloaders()
    
    # åˆ›å»ºæ¼”åŒ–ç­–ç•¥
    evolution_strategy = ArchitectureEvolutionStrategy(patience=8, min_improvement=0.3)
    
    # å¼€å§‹æ¼”åŒ–è®­ç»ƒ
    print("\nğŸš€ å¼€å§‹è‡ªé€‚åº”æ¶æ„æ¼”åŒ–è®­ç»ƒ")
    print("="*70)
    
    # Stage 0: åŸºç¡€æ¶æ„
    print("\nğŸ“Š Stage 0: åŸºç¡€æ¶æ„è®­ç»ƒ")
    model = AdvancedEvolutionCNN(num_classes=10, evolution_stage=0)
    param_count = sum(p.numel() for p in model.parameters())
    print(f"æ¨¡å‹å‚æ•°: {param_count:,}")
    
    # ç¬¬ä¸€é˜¶æ®µè®­ç»ƒ
    optimized_model, history = train_with_neuroexapt(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        epochs=30,
        learning_rate=0.001,
        efficiency_threshold=0.03,
        verbose=True
    )
    
    # åˆ†æç»“æœ
    print(f"\nğŸ“ˆ Stage 0 è®­ç»ƒç»“æœ:")
    final_train_acc = history['train_accuracy'][-1]
    final_val_acc = history['val_accuracy'][-1]
    best_val_acc = max(history['val_accuracy'])
    
    print(f"  æœ€ç»ˆè®­ç»ƒå‡†ç¡®åº¦: {final_train_acc:.2f}%")
    print(f"  æœ€ç»ˆéªŒè¯å‡†ç¡®åº¦: {final_val_acc:.2f}%")
    print(f"  æœ€ä½³éªŒè¯å‡†ç¡®åº¦: {best_val_acc:.2f}%")
    
    # åˆ†æå‡†ç¡®åº¦å¼‚å¸¸
    accuracy_status = analyze_accuracy_issue(final_train_acc, final_val_acc, 30)
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¼”åŒ–
    evolution_needed = evolution_strategy.should_evolve(best_val_acc, 30)
    
    if evolution_needed:
        print(f"\nğŸ”„ è§¦å‘æ¶æ„æ¼”åŒ–: {evolution_needed}")
        
        # æ¼”åŒ–æ¶æ„
        evolved_model, evolution_desc = evolution_strategy.evolve_architecture(
            optimized_model, evolution_needed
        )
        
        print(f"âœ… æ¶æ„æ¼”åŒ–å®Œæˆ: {evolution_desc}")
        
        # ç»§ç»­è®­ç»ƒæ¼”åŒ–åçš„æ¨¡å‹
        print("\nğŸ“Š æ¼”åŒ–åç»§ç»­è®­ç»ƒ:")
        final_model, final_history = train_with_neuroexapt(
            model=evolved_model,
            train_loader=train_loader,
            val_loader=val_loader,
            epochs=20,
            learning_rate=0.0005,  # é™ä½å­¦ä¹ ç‡
            efficiency_threshold=0.02,
            verbose=True
        )
        
        # æœ€ç»ˆåˆ†æ
        print(f"\nğŸ‰ æœ€ç»ˆè®­ç»ƒç»“æœ:")
        final_train_acc = final_history['train_accuracy'][-1]
        final_val_acc = final_history['val_accuracy'][-1]
        best_val_acc = max(final_history['val_accuracy'])
        
        print(f"  æœ€ç»ˆè®­ç»ƒå‡†ç¡®åº¦: {final_train_acc:.2f}%")
        print(f"  æœ€ç»ˆéªŒè¯å‡†ç¡®åº¦: {final_val_acc:.2f}%")
        print(f"  æœ€ä½³éªŒè¯å‡†ç¡®åº¦: {best_val_acc:.2f}%")
    
    # è§£å†³æ–¹æ¡ˆæ€»ç»“
    print("\n" + "="*70)
    print("ğŸ’¡ è§£å†³æ–¹æ¡ˆæ€»ç»“")
    print("="*70)
    
    print("ğŸ” é—®é¢˜1: è®­ç»ƒå‡†ç¡®åº¦ä½äºéªŒè¯å‡†ç¡®åº¦")
    print("  åŸå› : è®­ç»ƒæ—¶ä½¿ç”¨dropoutå’Œæ•°æ®å¢å¼ºï¼ŒéªŒè¯æ—¶ä¸ä½¿ç”¨")
    print("  è§£å†³: è¿™æ˜¯æ­£å¸¸ç°è±¡ï¼Œè¡¨æ˜æ¨¡å‹å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›")
    print("  æ”¹è¿›: é€‚åº¦é™ä½dropoutç‡ï¼Œä¼˜åŒ–æ•°æ®å¢å¼ºç­–ç•¥")
    
    print("\nğŸ” é—®é¢˜2: éªŒè¯å‡†ç¡®åº¦åœåœ¨82%")
    print("  åŸå› : æ¶æ„å¤æ‚åº¦ä¸è¶³ï¼Œç¼ºä¹é«˜çº§ç‰¹å¾æå–èƒ½åŠ›")
    print("  è§£å†³: è‡ªé€‚åº”æ¶æ„æ¼”åŒ–ï¼Œé€æ­¥å¢åŠ å¤æ‚åº¦")
    print("  ç­–ç•¥: æ·»åŠ æ³¨æ„åŠ›æœºåˆ¶ã€æ®‹å·®è¿æ¥ã€æ·±åº¦ä¼˜åŒ–")
    
    print("\nğŸš€ è‡ªé€‚åº”æ¶æ„æ¼”åŒ–ä¼˜åŠ¿:")
    print("  âœ… è‡ªåŠ¨æ£€æµ‹æ€§èƒ½å¹³å°æœŸ")
    print("  âœ… æ¸è¿›å¼æ¶æ„å‡çº§")
    print("  âœ… ä¿æŒè®­ç»ƒç¨³å®šæ€§")
    print("  âœ… çªç ´å‡†ç¡®åº¦ç“¶é¢ˆ")
    
    print("\nâœ… è®­ç»ƒå®Œæˆï¼ç¥ç»ç½‘ç»œå·²è‡ªåŠ¨å‡çº§åˆ°æœ€ä¼˜æ¶æ„")


if __name__ == "__main__":
    main() 