#!/usr/bin/env python3
"""
NeuroExapt - é«˜çº§æ¶æ„æ¼”åŒ–ç³»ç»Ÿ
ä¸“é—¨ç”¨äºçªç ´82%å‡†ç¡®åº¦ç“¶é¢ˆï¼Œå®ç°è‡ªé€‚åº”æ¶æ„å‡çº§
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import numpy as np
import math
from collections import defaultdict
import sys
import os

# Add neuroexapt to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from neuroexapt.trainer import Trainer, train_with_neuroexapt


class ResidualBlock(nn.Module):
    """æ”¹è¿›çš„æ®‹å·®å—"""
    def __init__(self, in_channels, out_channels, stride=1, dropout=0.1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.dropout = nn.Dropout2d(dropout)
        
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
    
    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.dropout(out)
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class ChannelAttention(nn.Module):
    """é€šé“æ³¨æ„åŠ›æœºåˆ¶"""
    def __init__(self, in_channels, reduction=16):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        
        self.fc = nn.Sequential(
            nn.Linear(in_channels, in_channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(in_channels // reduction, in_channels, bias=False),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        b, c, _, _ = x.size()
        
        # åŒè·¯å¾„æ³¨æ„åŠ›
        avg_out = self.fc(self.avg_pool(x).view(b, c))
        max_out = self.fc(self.max_pool(x).view(b, c))
        
        attention = avg_out + max_out
        return x * attention.view(b, c, 1, 1)


class SpatialAttention(nn.Module):
    """ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶"""
    def __init__(self, kernel_size=7):
        super().__init__()
        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        attention = torch.cat([avg_out, max_out], dim=1)
        attention = self.sigmoid(self.conv(attention))
        return x * attention


class CBAM(nn.Module):
    """å·ç§¯å—æ³¨æ„åŠ›æ¨¡å— (Convolutional Block Attention Module)"""
    def __init__(self, in_channels, reduction=16):
        super().__init__()
        self.channel_attention = ChannelAttention(in_channels, reduction)
        self.spatial_attention = SpatialAttention()
    
    def forward(self, x):
        x = self.channel_attention(x)
        x = self.spatial_attention(x)
        return x


class EvolutionaryResNet(nn.Module):
    """æ¼”åŒ–çš„ResNetæ¶æ„ - èƒ½å¤Ÿè‡ªé€‚åº”å‡çº§"""
    
    def __init__(self, num_classes=10, evolution_level=0):
        super().__init__()
        self.evolution_level = evolution_level
        
        # åŸºç¡€å±‚
        self.conv1 = nn.Conv2d(3, 64, 7, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.maxpool = nn.MaxPool2d(3, 2, 1)
        
        # åŠ¨æ€å±‚é…ç½®
        if evolution_level >= 0:
            # Level 0: åŸºç¡€ResNet
            self.layer1 = self._make_layer(64, 64, 2, 1)
            self.layer2 = self._make_layer(64, 128, 2, 2)
            self.layer3 = self._make_layer(128, 256, 2, 2)
            self.layer4 = self._make_layer(256, 512, 2, 2)
        
        if evolution_level >= 1:
            # Level 1: æ·»åŠ æ³¨æ„åŠ›æœºåˆ¶
            self.attention1 = CBAM(64)
            self.attention2 = CBAM(128)
            self.attention3 = CBAM(256)
            self.attention4 = CBAM(512)
        
        if evolution_level >= 2:
            # Level 2: å¢åŠ æ·±åº¦
            self.layer5 = self._make_layer(512, 1024, 2, 2)
            self.attention5 = CBAM(1024)
        
        if evolution_level >= 3:
            # Level 3: å¤šå°ºåº¦ç‰¹å¾èåˆ
            self.multiscale_conv = nn.ModuleList([
                nn.Conv2d(512, 128, 1),
                nn.Conv2d(512, 128, 3, padding=1),
                nn.Conv2d(512, 128, 5, padding=2),
                nn.Conv2d(512, 128, 7, padding=3)
            ])
            
        # è‡ªé€‚åº”å…¨å±€å¹³å‡æ± åŒ–
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        
        # åˆ†ç±»å™¨
        final_channels = 1024 if evolution_level >= 2 else 512
        if evolution_level >= 3:
            final_channels = 512  # å¤šå°ºåº¦èåˆåçš„é€šé“æ•°
        
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(final_channels, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
        
    def _make_layer(self, in_channels, out_channels, blocks, stride):
        layers = []
        layers.append(ResidualBlock(in_channels, out_channels, stride, dropout=0.1))
        for _ in range(1, blocks):
            layers.append(ResidualBlock(out_channels, out_channels, dropout=0.1))
        return nn.Sequential(*layers)
    
    def forward(self, x):
        # åˆå§‹å·ç§¯
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.maxpool(x)
        
        # æ®‹å·®å—
        x = self.layer1(x)
        if self.evolution_level >= 1:
            x = self.attention1(x)
        
        x = self.layer2(x)
        if self.evolution_level >= 1:
            x = self.attention2(x)
        
        x = self.layer3(x)
        if self.evolution_level >= 1:
            x = self.attention3(x)
        
        x = self.layer4(x)
        if self.evolution_level >= 1:
            x = self.attention4(x)
        
        # é«˜çº§æ¼”åŒ–ç‰¹æ€§
        if self.evolution_level >= 2:
            x = self.layer5(x)
            x = self.attention5(x)
        
        if self.evolution_level >= 3:
            # å¤šå°ºåº¦ç‰¹å¾èåˆ
            multiscale_features = []
            for conv in self.multiscale_conv:
                multiscale_features.append(conv(x))
            x = torch.cat(multiscale_features, dim=1)
        
        # åˆ†ç±»
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        
        return x


class AdvancedEvolutionStrategy:
    """é«˜çº§æ¼”åŒ–ç­–ç•¥"""
    
    def __init__(self, patience=8, min_improvement=0.3, max_level=3):
        self.patience = patience
        self.min_improvement = min_improvement
        self.max_level = max_level
        self.best_val_acc = 0.0
        self.no_improvement_count = 0
        self.current_level = 0
        self.evolution_history = []
        
    def should_evolve(self, current_val_acc, current_epoch):
        """æ™ºèƒ½åˆ¤æ–­æ˜¯å¦éœ€è¦æ¼”åŒ–"""
        # æ›´æ–°æœ€ä½³å‡†ç¡®åº¦
        if current_val_acc > self.best_val_acc:
            improvement = current_val_acc - self.best_val_acc
            self.best_val_acc = current_val_acc
            
            if improvement >= self.min_improvement:
                self.no_improvement_count = 0
                return False
        
        self.no_improvement_count += 1
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦æ¼”åŒ–
        if self.no_improvement_count >= self.patience and self.current_level < self.max_level:
            evolution_type = self._determine_evolution_type()
            print(f"ğŸ”„ è§¦å‘æ¼”åŒ–æ¡ä»¶:")
            print(f"  å½“å‰å‡†ç¡®åº¦: {current_val_acc:.2f}%")
            print(f"  æœ€ä½³å‡†ç¡®åº¦: {self.best_val_acc:.2f}%")
            print(f"  åœæ»è½®æ•°: {self.no_improvement_count}")
            print(f"  å»ºè®®æ¼”åŒ–: {evolution_type}")
            
            return evolution_type
        
        return False
    
    def _determine_evolution_type(self):
        """æ ¹æ®å½“å‰çŠ¶æ€ç¡®å®šæ¼”åŒ–ç±»å‹"""
        if self.current_level == 0:
            return "add_attention"
        elif self.current_level == 1:
            return "increase_depth"
        elif self.current_level == 2:
            return "multiscale_fusion"
        else:
            return "advanced_optimization"
    
    def evolve_model(self, current_model, evolution_type):
        """æ‰§è¡Œæ¨¡å‹æ¼”åŒ–"""
        print(f"ğŸš€ å¼€å§‹æ¨¡å‹æ¼”åŒ–: {evolution_type}")
        
        if evolution_type == "add_attention":
            # æ¼”åŒ–åˆ°Level 1: æ·»åŠ æ³¨æ„åŠ›æœºåˆ¶
            new_model = EvolutionaryResNet(num_classes=10, evolution_level=1)
            self._transfer_weights(current_model, new_model)
            self.current_level = 1
            return new_model, "æ·»åŠ CBAMæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¢å¼ºç‰¹å¾è¡¨ç¤ºèƒ½åŠ›"
        
        elif evolution_type == "increase_depth":
            # æ¼”åŒ–åˆ°Level 2: å¢åŠ æ·±åº¦
            new_model = EvolutionaryResNet(num_classes=10, evolution_level=2)
            self._transfer_weights(current_model, new_model)
            self.current_level = 2
            return new_model, "å¢åŠ ç½‘ç»œæ·±åº¦ï¼Œæé«˜ç‰¹å¾æå–èƒ½åŠ›"
        
        elif evolution_type == "multiscale_fusion":
            # æ¼”åŒ–åˆ°Level 3: å¤šå°ºåº¦ç‰¹å¾èåˆ
            new_model = EvolutionaryResNet(num_classes=10, evolution_level=3)
            self._transfer_weights(current_model, new_model)
            self.current_level = 3
            return new_model, "æ·»åŠ å¤šå°ºåº¦ç‰¹å¾èåˆï¼Œæ•è·ä¸åŒå°ºåº¦çš„ç‰¹å¾"
        
        elif evolution_type == "advanced_optimization":
            # é«˜çº§ä¼˜åŒ–æŠ€å·§
            return self._apply_advanced_optimization(current_model)
        
        return current_model, "æœªçŸ¥æ¼”åŒ–ç±»å‹"
    
    def _transfer_weights(self, old_model, new_model):
        """æƒé‡è¿ç§»"""
        try:
            old_dict = old_model.state_dict()
            new_dict = new_model.state_dict()
            
            # åªè¿ç§»åŒ¹é…çš„æƒé‡
            transfer_dict = {k: v for k, v in old_dict.items() if k in new_dict and v.shape == new_dict[k].shape}
            
            new_model.load_state_dict(transfer_dict, strict=False)
            
            print(f"âœ… æƒé‡è¿ç§»å®Œæˆ: {len(transfer_dict)}/{len(new_dict)} å±‚")
            
        except Exception as e:
            print(f"âš ï¸ æƒé‡è¿ç§»å¤±è´¥: {e}")
    
    def _apply_advanced_optimization(self, model):
        """åº”ç”¨é«˜çº§ä¼˜åŒ–æŠ€å·§"""
        # è¿™é‡Œå¯ä»¥å®ç°æ ‡ç­¾å¹³æ»‘ã€çŸ¥è¯†è’¸é¦ç­‰æŠ€å·§
        return model, "åº”ç”¨é«˜çº§ä¼˜åŒ–æŠ€å·§ï¼ˆæ ‡ç­¾å¹³æ»‘ã€æ··åˆç²¾åº¦ç­‰ï¼‰"


def create_advanced_dataloaders():
    """åˆ›å»ºé«˜çº§æ•°æ®åŠ è½½å™¨"""
    # é«˜çº§æ•°æ®å¢å¼ºç­–ç•¥
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
        transforms.RandomRotation(15),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
        transforms.RandomErasing(p=0.1, scale=(0.02, 0.33), ratio=(0.3, 3.3))
    ])
    
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    
    train_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform_train
    )
    val_dataset = torchvision.datasets.CIFAR10(
        root='./data', train=False, download=True, transform=transform_test
    )
    
    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4)
    
    return train_loader, val_loader


def evolutionary_training(model, train_loader, val_loader, epochs=50, lr=0.001):
    """æ¼”åŒ–è®­ç»ƒè¿‡ç¨‹"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    
    # ä½¿ç”¨AdamWä¼˜åŒ–å™¨
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)
    
    # ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=lr*0.01)
    
    criterion = nn.CrossEntropyLoss()
    
    train_acc_history = []
    val_acc_history = []
    
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            train_correct += pred.eq(target.view_as(pred)).sum().item()
            train_total += target.size(0)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                loss = criterion(output, target)
                
                val_loss += loss.item()
                pred = output.argmax(dim=1, keepdim=True)
                val_correct += pred.eq(target.view_as(pred)).sum().item()
                val_total += target.size(0)
        
        train_acc = 100. * train_correct / train_total
        val_acc = 100. * val_correct / val_total
        
        train_acc_history.append(train_acc)
        val_acc_history.append(val_acc)
        
        scheduler.step()
        
        if epoch % 5 == 0:
            print(f'Epoch {epoch+1}/{epochs}: '
                  f'Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%, '
                  f'LR: {scheduler.get_last_lr()[0]:.6f}')
    
    return {
        'train_accuracy': train_acc_history,
        'val_accuracy': val_acc_history,
        'final_model': model
    }


def main():
    print("ğŸ§¬ é«˜çº§æ¶æ„æ¼”åŒ–ç³»ç»Ÿ - çªç ´82%å‡†ç¡®åº¦ç“¶é¢ˆ")
    print("="*60)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader = create_advanced_dataloaders()
    
    # åˆ›å»ºæ¼”åŒ–ç­–ç•¥
    evolution_strategy = AdvancedEvolutionStrategy(patience=6, min_improvement=0.2)
    
    # å¼€å§‹æ¼”åŒ–è¿‡ç¨‹
    current_model = EvolutionaryResNet(num_classes=10, evolution_level=0)
    
    print(f"\nğŸš€ å¼€å§‹æ¼”åŒ–è®­ç»ƒ - Level 0 (åŸºç¡€ResNet)")
    print("="*60)
    
    # é˜¶æ®µæ€§è®­ç»ƒ
    total_epochs = 0
    max_evolutions = 3
    evolution_count = 0
    
    while evolution_count <= max_evolutions:
        print(f"\nğŸ“Š è®­ç»ƒé˜¶æ®µ {evolution_count + 1} - Evolution Level {evolution_strategy.current_level}")
        
        # è®­ç»ƒå½“å‰æ¨¡å‹
        results = evolutionary_training(
            current_model, train_loader, val_loader, 
            epochs=25, lr=0.001 * (0.8 ** evolution_count)
        )
        
        total_epochs += 25
        final_val_acc = results['val_accuracy'][-1]
        best_val_acc = max(results['val_accuracy'])
        
        print(f"\nğŸ“ˆ é˜¶æ®µç»“æœ:")
        print(f"  æœ€ç»ˆéªŒè¯å‡†ç¡®åº¦: {final_val_acc:.2f}%")
        print(f"  æœ€ä½³éªŒè¯å‡†ç¡®åº¦: {best_val_acc:.2f}%")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¼”åŒ–
        evolution_type = evolution_strategy.should_evolve(best_val_acc, total_epochs)
        
        if evolution_type:
            print(f"\nğŸ”„ è§¦å‘æ¼”åŒ–: {evolution_type}")
            
            # æ¼”åŒ–æ¨¡å‹
            evolved_model, evolution_desc = evolution_strategy.evolve_model(
                current_model, evolution_type
            )
            
            print(f"âœ… æ¼”åŒ–å®Œæˆ: {evolution_desc}")
            
            # æ›´æ–°å½“å‰æ¨¡å‹
            current_model = evolved_model
            evolution_count += 1
            
            # é‡ç½®æ— æ”¹å–„è®¡æ•°
            evolution_strategy.no_improvement_count = 0
            
        else:
            print(f"\nâœ… è®­ç»ƒå®Œæˆ - æ— éœ€è¿›ä¸€æ­¥æ¼”åŒ–")
            break
    
    # æœ€ç»ˆæ€»ç»“
    print("\n" + "="*60)
    print("ğŸ‰ æ¼”åŒ–è®­ç»ƒå®Œæˆ")
    print("="*60)
    
    print(f"æ€»è®­ç»ƒè½®æ•°: {total_epochs}")
    print(f"æ¼”åŒ–æ¬¡æ•°: {evolution_count}")
    print(f"æœ€ç»ˆæ¶æ„çº§åˆ«: {evolution_strategy.current_level}")
    print(f"æœ€ç»ˆéªŒè¯å‡†ç¡®åº¦: {evolution_strategy.best_val_acc:.2f}%")
    
    # æ¶æ„æ¼”åŒ–å†å²
    print(f"\nğŸ“Š æ¶æ„æ¼”åŒ–å†å²:")
    evolution_names = ["åŸºç¡€ResNet", "æ·»åŠ æ³¨æ„åŠ›", "å¢åŠ æ·±åº¦", "å¤šå°ºåº¦èåˆ"]
    for i in range(min(evolution_count + 1, len(evolution_names))):
        print(f"  Level {i}: {evolution_names[i]}")
    
    # çªç ´åˆ†æ
    if evolution_strategy.best_val_acc > 82:
        print(f"\nğŸ¯ æˆåŠŸçªç ´82%ç“¶é¢ˆ!")
        print(f"  æœ€ç»ˆå‡†ç¡®åº¦: {evolution_strategy.best_val_acc:.2f}%")
        print(f"  æå‡å¹…åº¦: {evolution_strategy.best_val_acc - 82:.2f}%")
    else:
        print(f"\nâš ï¸ æœªèƒ½çªç ´82%ç“¶é¢ˆ")
        print(f"  å½“å‰å‡†ç¡®åº¦: {evolution_strategy.best_val_acc:.2f}%")
        print(f"  è·ç¦»ç›®æ ‡: {82 - evolution_strategy.best_val_acc:.2f}%")
    
    print("\nâœ… æ¼”åŒ–è®­ç»ƒç³»ç»Ÿè¿è¡Œå®Œæˆï¼")


if __name__ == "__main__":
    main() 